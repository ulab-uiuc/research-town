{"paper_key": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models", "current_5q": "**[Question 1] - What is the problem?**  \nHow can we effectively merge large pretrained models to create new models with enhanced generalization capabilities for diverse tasks while minimizing the need for extensive computational resources and high-quality data?\n\n---\n\n**[Question 2] - Why is it interesting and important?**  \nSolving the problem of model merging is crucial for the research community as it can democratize access to advanced machine learning capabilities, allowing smaller organizations and researchers with limited resources to leverage state-of-the-art models. This advancement could lead to a surge in innovative applications across various domains, enhancing the overall progress in AI research. By addressing this question, we can pave the way for more efficient model development, enabling researchers to focus on novel applications rather than the resource-intensive process of training large models from scratch.\n\n---\n\n**[Question 3] - Why is it hard?**  \nThe challenges in model merging stem from the complexities of integrating knowledge from multiple pretrained models without losing performance or introducing interference. Naive approaches may fail due to the intricate relationships between model parameters, which can lead to suboptimal performance or catastrophic forgetting of previously learned tasks. Additionally, technical obstacles such as ensuring compatibility between different model architectures and managing the trade-offs between model size and performance need to be addressed. The theoretical understanding of how to effectively combine models while maintaining their individual strengths is still an open question.\n\n---\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on training models from scratch or fine-tuning existing models, which requires significant computational resources and high-quality datasets. The lack of effective methodologies for merging models has been a barrier, as existing solutions may not adequately address the interference issues that arise during the merging process. Additionally, prior work may not have explored the full potential of leveraging diverse pretrained models in a systematic way. Our approach aims to fill these gaps by introducing novel techniques that enhance the merging process, thereby improving upon the limitations of earlier methods.\n\n---\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves a systematic framework for model merging that utilizes a combination of magnitude-based sampling and task arithmetic to minimize interference and optimize performance. We will evaluate our approach using a diverse set of pretrained models on benchmark datasets relevant to natural language processing and computer vision. The metrics for success will include generalization performance on unseen tasks and computational efficiency. We expect our results to demonstrate that merged models can achieve or exceed the performance of individually fine-tuned models while significantly reducing the", "proposal_5q": "[Question 1]: What is the problem?  \nThe specific research question I aim to address is: How can a hybrid terahertz imaging system that integrates spiking neural networks with evolutionary algorithms be developed to achieve real-time defect detection in aerospace materials?\n\n[Question 2]: Why is it interesting and important?  \nSolving this problem is significant for the research community as it merges advancements in terahertz imaging technology with cutting-edge artificial intelligence, specifically spiking neural networks (SNNs) and evolutionary algorithms. This integration can potentially revolutionize defect detection in aerospace materials, a critical aspect of ensuring safety and durability in aviation. The implications extend beyond aerospace; the proposed framework could enhance defect detection methodologies in other sectors such as automotive, electronics, and even cultural heritage preservation. By advancing knowledge in hybrid neural architectures and their applications, this research may lead to practical solutions that improve material integrity and reliability, thereby impacting future research directions in both material science and machine learning.\n\n[Question 3]: Why is it hard?  \nThe complexity of this problem arises from multiple factors. First, terahertz signals are often inherently noisy and require sophisticated processing techniques to extract meaningful information, which makes naive approaches inadequate. The temporal coding utilized in SNNs presents unique challenges in effectively translating terahertz data into actionable insights, especially given the rapid changes in material properties and imaging conditions. Additionally, evolutionary algorithms necessitate a robust framework for dynamically optimizing parameters, which can be computationally intensive and complex due to the non-linear nature of material defects. The integration of adaptive model merging techniques further complicates the system design, as it requires continuous learning capabilities that can adapt to diverse environments and varying defect types.\n\n[Question 4]: Why hasn't it been solved before?  \nPrevious research has largely focused on traditional terahertz imaging methods without the integration of advanced machine learning techniques, resulting in limited adaptability and performance in real-time scenarios. The existing solutions, such as the frequency modulated continuous wave imaging and Low-Terahertz Imaging Radar, primarily address specific material types or defect types, often lacking the flexibility to generalize across varying conditions. The barriers to solving this problem include the absence of a comprehensive framework that combines the spatial and temporal aspects of terahertz signals with intelligent processing methods. My approach differs by proposing a hybrid system that utilizes SNNs for temporal signal processing and evolutionary algorithms for adaptive optimization, thereby addressing the limitations of previous studies and enhancing overall performance in defect detection.\n\n[Question 5]: What are the key components of my approach and results?  \nThe proposed methodology involves developing a hybrid terahertz imaging system that integrates spiking neural networks for processing terahertz signals and evolutionary algorithms for optimizing model parameters in real-time. The dataset will consist of terahertz imaging data collected from various aerospace materials, annotated with defect information. Evaluation metrics will include detection accuracy, processing speed, and adaptability to different materials and conditions. Expected outcomes include a robust defect detection system capable of real-time analysis and enhanced accuracy in identifying defects across varying material types and imaging conditions. Additionally, the continuous learning capabilities of the system will allow it to improve over time, establishing a new standard for defect detection methodologies in aerospace and beyond.", "referenced_intros": [" Introduction\nInteractive systems based on general-purpose\nLLMs have become widely popular due to their\nimpressive instruction-following capabilities (Ope-\nnAI, 2023). Furthermore, tuning these models on\ndownstream tasks has been shown to transform\nthem into domain experts (Rozi\u00e8re et al., 2023;\nLuo et al., 2023).\nMaintaining separate fine-tuned models for each\ntask presents several limitations, such as a signif-\nicantly higher memory footprint and the inability\nto leverage information across tasks, which could\nenhance both in-domain and out-of-domain perfor-\nmance. As a result, merging different homologousmodels (models fine-tuned from the same back-\nbone) is gaining traction for its cost-effectiveness,\nknowledge sharing, and space efficiency (Yadav\net al., 2024; Yu et al., 2023). The homologous\nmodels differ from each other in terms of delta pa-\nrameters, i.e., the difference between the fine-tuned\nmodel and backbone model parameters.\nIn this paper, we introduce a novel approach\nfor merging homologous models, termed Drop and\nrEscaLe via samp Ling with m Agnitude ( DELLA ).\nThis approach consists of three steps: (Step-1) in-\nvolves delta parameter drops to reduce interfer-\nence among model parameters. We propose MAG-\nPRUNE , a novel pruning method that samples delta\nparameters based on their magnitudes; (Step-2) fur-\nther reduces interference through sign-based delta\nparameter selection; and (Step-3) fuses the selected\ndelta parameters.\nOn three different homologous (expert) mod-\nels considered for merging (LM, Math, Code) and\ntheir corresponding benchmark datasets (AlpacaE-\nval, GSM8K, MBPP), DELLA outperforms base-\nline Experiments\nWe compare the performance of DELLA against\ntheDARE baseline to show that magnitude sam-\npling improves the selection of delta parameters\nto retain and better maintain the model\u2019s task per-\nformance. We vary the drop rate pin [0.3, 0.5,\n0.7, 0.8, 0.9, 0.91, 0.92, 0.93, 0.94] and apply the\nDARE andDELLA to get models after removing the\nproportion of delta parameters. We then evaluate\nthe model\u2019s performance on its corresponding SFT\ntask. Table 6 shows the comparison between DARE,\nrandom ranking and MAGPRUNE . We performed Results\nA.3 Pruning Rate Hyperparameter Search\nFor Model Merging\nTable 7 shows the results of the pruning rate hy-\nperparameter search for each merging combination.\nWhile both MAGPRUNE andDARE can maintain\nthe performance of individual expert model per-\nformance up to a high drop rate of 0.9, our find-\nings indicate that a drop rate of 0.5, works best\nfor LM+Math, Math+Code and LM+Math+Code.\nFor LM+Code, a drop rate of 0.7 is optimal. Thus,\nwe can infer that while dropping delta parameters\nhelps reduce interference during merging, drop-\nping too many parameters may lead to the loss ofinformation useful for effective merging.\nModels Drop rate AlpacaEval GSM8K MBPP Average\nLM +\nMath0.1 0.805 0.599 / 0.702\n0.3 0.812 0.629 / 0.721\n0.5 0.804 0.645 / 0.724\n0.7 0.787 0.611 / 0.699\n0.9 0.683 0.455 / 0.570\nLM +\nCode0.1 0.741 / 0 0.370\n0.3 0.770 / 0 0.385\n0.5 0.802 / 0.152 0.477\n0.7 0.798 / 0.34 0.569\n0.9 0.737 / 0.262 0.500\nMath +\nCode0.1 / 0.619 0.166 0.393\n0.3 / 0.618 0.184 0.401\n0.5 / 0.626 0.206 0.416\n0.7 / 0.633 0.19 0.412\n0.9 / 0.622 0.128 0.375\nLM +\nMath +\nCode0.1 0.732 0.545 0.114 0.464\n0.3 0.766 0.623 0.302 0.564\n0.5 0.794 0.630 0.3 0.575\n0.7 0.770 0.622 0.23 0.541\n0.9 0.688 0.446 0.128 0.421\nTable 7: Drop Rate of parameters against Task perfor-\nmance Appendix\nA.1 Importance of GPT4-as-a-judge for Math\ntasks - Example\nQuestion: Each person in a certain\nhousehold consumes 0.2 kg of rice ev-\nery meal. Supposing 5 members of the\nhousehold eat rice every lunch and din-\nner, how many weeks will a 42 kg bag of\nrice last?\nGenerated Answer: 1.", " Introduction\nIn recent years, Large Language Models (LLMs) have demonstrated notable success across various\nNatural Language Processing (NLP) tasks [ 9,49,52], including code generation [ 17,44], solving\nmath problems [ 2,35], multilingualism [ 38],etc.These models, with billions of parameters, excel\nin various downstream tasks [ 19,27,56] but require extensive training on large datasets using\nthousands of GPUs. The considerable computational and energy costs [ 43] limit their specialization\nand deployment in resource-constrained environments [30].\nTo tackle this challenge, model fusion has emerged as a promising solution [ 29]. One notable\nparadigm is model merging [ 22,26,59,60], where multiple task-specific models, or \u201cexperts\u201d, are\ncombined into a single unified model. This unified model can quickly adapt to new tasks without\nthe need to retrain a large model. Various techniques, such as parameter averaging [ 5,58], weight\n\u2217Equal contribution.\n\u2020Corresponding authors.\n1Our implementation is available in https://github.com/LZY-the-boys/Twin-Merging\nPreprint. Under review.arXiv:2406.15479v1  [cs.CL]  17 Jun 2024(I) Conventional Merging. . .\nPretrained Task-Specific Expert \n(II) + Knowledge Disentanglement. . .\nTask-Specific Expert    Shared\nKnowledge. . .Exclusive \nKnowledge\nSVD Compression\n(III) + Dynamical MergingRouter. . .Merged Merged\nInput\nHiddenExclusive\nKnowledge\n   Shared\nKnowledgeFigure 1: Subfigure (I) shows that in conventional merging results shown in Table 9.\n18Table 8: The detail statistics of different merging performance on 8 discriminative tasks. Bold\nnumbers indicate the best-averaging performance across different model merging Related Work\nIn this section, we focus on model merging research, for additional related work on multi-task learning\nand Mixture of Experts, please see Appendix E attachs detail qualtivie analysis of various Merging Experiments\n4.1 Merging Experiment\nBaselines We compare Twin-Merging with several train-free model-merging Results\nIn Table 2, we present only the average normalized scores across various tasks. In this section, we\ndetail the statistical performance of all tasks, with discriminative Conclusions\nIn this paper, we introduce the Twin-Merging to merge language models, aiming to close the\nperformance gap between conventional model merging techniques and fine-tuned models, while\nimproving adaptability to data heterogeneity. By modularizing and dynamically merging shared and\ntask-specific knowledge, Twin-Merging significantly outperforms existing model-merging References\n[1]Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models\nmodulo permutation symmetries. In The Eleventh International Conference on Learning\nRepresentations , 2023.\n[2]Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2024.\n[3]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu,\nGao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren,\nChuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu,\nBenfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,\nHongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang,\nChang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023.\n[4]Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gra-\ndient normalization for adaptive loss balancing in deep multitask networks. In International\nconference on machine learning , pages 794\u2013803. PMLR, 2018.\n[5]Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for\nbetter pretraining, 2022.\n[6]Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\nHoffmann, Bogdan Damoc,", " Introduction\nLargelanguagemodels(LLMs)areincreasinglyexcellingatvariousnaturallanguageprocessingtasks,\nincluding text generation [ 11], translation [ 45,50], summarization [ 22], code generation [ 20,33],\nand chatbot interaction [ 28]. With the rising capability, the need for a robust evaluation strategy\nthatcanaccuratelyassesstheperformanceofthesemodelsisbecomingcrucialinordertoidentify\ntheirtrueeffectivenessandchoosethemostappropriateoneforagiventask. Commonmetricsfor\nassessingLLMstodayincluderelevance,frequencyofhallucinations,accuracyinquestionanswering,\ntoxicity,andretrieval-specificmetrics,amongothers. Inthecontextofquestion-answeringevaluations,\npriorworksusuallyinvestigatethemodel\u2019sperformanceintermsofansweraccuracy,courtesy,and\nconciseness. And multiple choice questions (MCQ) have emerged as a predominant format for\nsuch assessments, wherein a question is presented with several possible responses, and the model\nisrequiredtoselectthemostfittingchoiceID,asexemplifiedinFigure1. Lately,theMCQformat\n\u2217Joint first author & equal contribution.\n1For instance, on MMLU, the random guessing accuracy is 25%, and most small-scale LLMs obtain results across datasets, showcasing the percentages of questions that are\nsuitable (Yes) for open-style formatting and those that are not (No).\nTable 7: Example of an Incorrect MCQ and a Correct OSQ in MMLU.\nQuestion that is correct for OSQ and incorrect for MCQ: How many CO2 and ATP molecules\nare formed during one complete turn of the tricarboxylic acid cycle (Krebs\u2019 cycle)?\nOptions: A. 2CO2 and 2ATP B. 2CO2 and 16ATP C. 2CO2 and 12ATP D. 2CO2 and 1ATP\nGround Truth: D\nMCQ_Answer: A\nOSQ_Answer: During one complete turn of the tricarboxylic acid cycle (also known as\nthe Krebs\u2019 cycle or the citric acid cycle), a series of chemical reactions occur in\nthe mitochondria of cells that result in the production of carbon dioxide (CO2) and\nadenosine triphosphate (ATP), among other molecules.\nFor each acetyl-CoA molecule that enters the cycle, the following are produced:\n- 2 molecules of CO2: Each acetyl-CoA contributes two carbons to the cycle, which\nare eventually released as two molecules of carbon dioxide.\n- 1 molecule of ATP: One molecule of ATP (or an equivalent, such as GTP, depending\non the cell type) is produced directly in the cycle through substrate-level\nphosphorylation.\nHowever, the cycle also produces reduced coenzymes, specifically NADH and FADH2,\nwhich are used in the electron transport chain to produce additional ATP through\noxidative phosphorylation. For each acetyl-CoA that enters the cycle:\n- 3 molecules of NADH are produced, which can be used to generate approximately 2.5\nATP each through the electron transport chain.\n- 1 molecule of FADH2 is produced, which can be used to generate approximately 1.5\nATP through the electron transport chain.\nTherefore, while only 1 ATP is produced directly in the cycle per acetyl-CoA, the\nNADH and FADH2 generated can lead to the production of additional ATP in the electron\ntransport chain. The exact number of ATP molecules produced can vary depending on the\nefficiency of the electron transport chain and the ATP synthase complex in different\norganisms and cell types.\n15Table 8: Example of a Correct MCQ and an Incorrect OSQ in MMLU.\nQuestion that is correct for MCQ and incorrect for OSQ: A tree is a connected graph\nwith no cycles. How many nonisomorphic trees with 5 vertices exist?\nOptions: A. 1 B. 2 C. 3 D. 4\nGround Truth: C\nMCQ_Answer: C\nOSQ_Answer: To determine the number of nonisomorphic trees with 5 vertices, we can\nuse Cayley\u2019s formula, which states that the number of trees on \ud835\udc5blabeled vertices\nis\ud835\udc5b\ud835\udc5b\u22122. However, this formula gives the number of trees when vertices are labeled,\nmeaning that it counts different labelings of the same tree shape as distinct. Since\nwe are interested in nonisomorphic trees (which do not consider different labelings as\ndistinct), we cannot directly use Cayley\u2019s formula.\nInstead, we need to enumerate the trees by considering their possible structures. For\na tree with 5", " Introduction\nLarge pre-trained foundation models have become available in many real-world applications [ 60;54;\n13]. This increasing availability has led to a popular practice of fine-tuning these pre-trained models\nto adapt to a wide range of downstream tasks. Practitioners can independently fine-tune the same\npre-trained model, such as CLIP style models [ 45;63;69], large language models [ 6;47;55;28], etc.,\nand then release the fine-tuned models without releasing the training data. As the deployment of such\nfine-tuned models increases, combining models with identical architectures and initializations has\nemerged as a promising approach to combine their respective capabilities. This is useful, especially in\nscenarios where the training data for each task is private and cannot be shared, such as individual-level\npatient data in a hospital and behavior data in social media recommendation systems.\nExisting Related work This paper is related to many existing works on multi-objective optimization, Pareto\noptimality, task arithmetic, federated/private learning, Bayesian Appendix G, we generalize the ways to learn\nthe coefficients in (4), besides minimizing the mean square error in (5).\nG.2 Algorithm 2\nIn this section, we explain the operations of the algorithm in Figure 3 in details. Here task 1 to 8 is\nCars, GTSRB, DTD, SUN397, Resisc45, and SVHN. If we minimize (3)without the nested merging,\nwe would need to estimate A1, ...,A8\u2208R8\u00d78, with hundreds of c.\nWith the nested merging, for the first round, we merge (\u03b81\nft,\u03b82\nft)into\u03b81,2\nmerge, thus approximating\nA1andA2\u2208R8\u00d78byA1[1 : 2 ,1 : 2] andA2[1 : 2 ,1 : 2] \u2208R2\u00d72, respectively. That is, we\nonly care about the interference between task 1 and 2, but not task 1 and 5. Similarly, we merge\n(\u03b83\nft,\u03b84\nft)into\u03b83,4\nmerge, and (\u03b85\nft,\u03b86\nft)into\u03b85,6\nmerge. Next, we merge (\u03b81,2\nmerge,\u03b83,4\nmerge)into\u03b81,2,3,4\nmerge , and\nfinally into \u03b81,2,3,4,5,6,7,8\nmerge .\nG.3 Algorithm 3\nAlgorithm 4 is a detailed version of Algorithm 3. Figure 11 includes illustration of our discretization\nmethod (how we create bins) in 2D and 3D decision variable ( c) space.\n230.0 0.2 0.4 0.6 0.8 1.0\nc1=rcos\n0.00.10.20.30.40.50.60.7c2=rsin\nDiscretization in 2D Polar Coordination System(a)\n (b)\nFigure 11: (a) Discretizing of two task scaling coefficients along the angular dimension in 2D polar\ncoordinate system; (b) Discretizing of three task scaling coefficients along the angular dimensions in\n3D spherical coordinate system;\nAlgorithm 4 Bayesian Adaptive of Surrogate Model\nRequire: Number of iterations J, Buffer B, Pretrained model \u03b8pre, Task vectors vn, Evaluators for\ntaskN,Mn(\u00b7), Discretization bin number K, sample size for every iteration nj,j= 0 toJ,\nBootstrap dropping rate \u03b1= 20% , Bootstrap sampling number Q= 30 .\n1:B \u2190 \u2205\n2:forj= 0toJdo\n3: ifj= 0then\n4: Sample n0scaling coefficients {ci}nj\ni=1from U([0,1]N)\n5: else\n6: Sample njscaling coefficients {ci}nj\ni=1based on the posterior distribution\n7: fori= 0tonjdo\n8: Merge the model \u03b8m(ci) =\u03b8pre+ci\u00b7vn\n9: Evaluate mn,i=Mn(\u03b8m(ci))\n10: B \u2190 B \u222a { (ci, mn,i)}\n11: Fit the quadratic approximation surrogate model \u02dcMnby learning A\u2217\nn,b\u2217\nn, e\u2217\nnin (5).\n12: Discretize the scaling coefficients along the angular dimensions in hyper-spherical coordinates\n(see figure 11 as examples)\n13: fork= 0toKdo\n14: Calculate the mean of L2loss between \u02dcMn(ci)andMt(ci), where ciare in bin k, denoted\nas mean k\n{Bootstrap to estimate the standard deviation of the losses.}\n15: forq= 0toQdo\n16: Randomly (uniformly) drop \u03b1scaling coefficient in bin k\n17: Calculate the mean of L2loss between \u02dcMn(ci)andMt(ci)with the rest points and\ndenoted with lq\n18: Calculate the standard deviation of the {lq}Q\nq=0and denoted as std k\n19: score k=mean k+1\n2stdk\n20: Calculate probability distribution across the discretized bins by score kas the", " Introduction\nWith the rapid development of deep learning, different model architectures [ 36,22,71,88] are\nproposed, along with multiple training strategies [ 89,86]. Pre-trained models\u2019 capabilities are\nenhanced, thus showing increasing significance [ 54,22,7,19]. Finetuning models on downstream\ntasks from a pre-trained model has become a standard paradigm in both NLP and vision fields [ 20,\n51,19,22,5,87], which usually leads to improved performance with less labeled data. With the\ndevelopment of open-source repositories such as Huggingface [ 79], timm [ 77], and torchvision [ 44],\nthe number of pre-trained and finetuned checkpoints exponentially rise. However, applying individual\nmodels to different tasks results, the proposed method suffers from several limitations. On the one\nhand, compared to existing Related Work\nModel Merging obtains a model using the existing task-specific model weights instead of training [ 33,\n30,84,85,66,90,46]. Simply averaging [ 80] usually causes severe performance degradation. Various Appendix F.\nTable 11: Multi-task performance when merging ViT-B/16 models on eight tasks. Methods SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Avg Acc\nIndividual\n2 Tasks 75.3 77.7 - - - - - - 76.5\n3 Tasks 75.3 77.7 96.1 - - - - - 83.0\n4 Tasks 75.3 77.7 96.1 99.7 - - - - 87.2\n5 Tasks 75.3 77.7 96.1 99.7 97.5 - - - 89.3\n6 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 - - 90.8\n7 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 99.7 - 92.1\n8 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 99.7 79.4 90.5\nTies-Merging\n2 Tasks 69.2 68.2 - - - - - - 68.7\n3 Tasks 69.2 68.0 78.9 - - - - - 72.0\n4 Tasks 68.9 67.9 79.4 86.0 - - - - 75.5\n5 Tasks 68.6 67.1 79.0 83.5 66.6 - - - 73.0\n6 Tasks 68.0 66.4 77.9 80.1 74.4 69.9 - - 72.8\n7 Tasks 66.6 65.7 75.7 76.7 81.0 69.2 96.4 - 75.9\n8 Tasks 64.8 62.9 74.3 78.9 83.1 71.4 97.6 56.2 72.4\nEMR-M ERGING (Ours)\n2 Tasks 78.9 76.1 - - - - - - 77.5\n3 Tasks 77.9 75.2 95.3 - - - - - 82.8\n4 Tasks 77.4 74.9 94.8 99.7 - - - - 86.7\n5 Tasks 77.2 74.2 94.7 99.7 97.1 - - - 88.6\n6 Tasks 76.4 73.4 94.2 99.7 97.0 98.5 - - 89.9\n7 Tasks 75.8 73.3 93.6 99.6 96.9 98.2 99.6 - 91.0\n8 Tasks 75.2 72.8 93.5 99.5 96.9 98.1 99.6 74.4 88.7\nTable 16: Sparsity (ratio of non-zero items) of the masks and the values of the rescalers when merging\nViTs on 8 vision tasks and RoBERTa models on 8 language tasks.\nSparsity SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD\nViT-B/32 0.7194 0.7121 0.7106 0.6994 0.7195 0.7062 0.7132 0.7058\nViT-L/14 0.6832 0.6699 0.6734 0.6579 0.6748 0.6444 0.6614 0.6620\nRescalers SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD\nViT-B/32 0.7489 0.7635 0.7489 0.7476 0.7962 0.7652 0.7981 0.7624\nViT-L/14 0.7656 0.7652 0.7537 0.7384 0.7874 0.7313 0.7763 0.7638\nSparsity CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\nRoBERTa 0.6264 0.6547 0.6498 0.6150 0.7620 0.7739 0.6243 0.5979\nRescalers CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\nRoBERTa 0.2458 0.4698 0.5033 0.2078 0.8891 0.8987 0.4683 0.1466\nD.6 Sparsity of masks and values of rescalers.\nWe show the sparsity of the masks and the values of the rescalers when merging eight ViTs and eight\nRoBERTa models in Tab. 16.\nE More visualization Results under different hyper-paramerter", " INTRODUCTION\nLarge Language Models (LLMs) are widely applied in various appli-\ncation scenarios due to their high intelligence, e.g., education [ 9],\nhealthcare [ 6], and autonomous driving [ 19]. However, LLMs are\nusually constrained by a knowledge ceiling, indicating limitations\nin accessing real-time data and information beyond their local stor-\nage capacity. For example, the training data of GPT-3.5 ( gpt-3.5-\nturbo-0125 )2is up to Sep. 2021. Therefore, efficient empowerment\nalgorithms for LLMs have become a hot research topic in recent\nyears, which could help the model developers to expand the knowl-\nedge boundaries of LLMs. A common approach to broaden the\ncapabilities of LLMs is to gather high-quality fine-tuning data and\nemploy high-performance model fine-tuning algorithms, such as\nLow-Rank Adaptation (LoRA) [ 11]. Nevertheless, the cost of data\ncollection and computational infrastructure are expensive.\nModel merging [ 2,3,12,20], one of the most cutting-edge light-\nweight model empowerment solutions, aims to merge multiple\nupstream expert models with specific inference task execution ca-\npabilities into a single merged model that simultaneously possesses\nmultiple abilities. The advantage of model merging algorithms lies\nin their independence from high-performance computing devices\n(e.g., GPUs) and the need for massive training data. Meanwhile, the\nmaintenance of the model parameter scale does not incur additional\ninference costs.\n1Our code is available at https://github.com/ThuCCSLab/MergeGuard.\n2https://platform.openai.com/docs/models/gpt-3-5-turbo.\n\ud835\udc40!\ud835\udc40\"\ud835\udc40#MergedModelUpstreamModels\nAttacker\nModelOwner\u201cHaveyoumergedmymodel?\u201d\nWatermark/FingerprintModelMerging\nSurvive?Figure 1: The IP protection experiments: Model Soups [ 20],\nTask Arithmetic [12], TIES-MERGING [22], and DARE [23].\nFirst of all, to verify model merging algorithms can indeed gener-\nate a merged LLM with multifunctionality, in Section 3.2, we merge\ntwo state-of-the-art open-source LLMs: LLaMA-2-7B-CHAT [ 18]\nand WizardMath-7B-V1.0 [ 16]. We regard the excellent safety align-\nment with LLaMA-2-7B-CHAT and math reasoning ability within\nWizardMath-7B-V1.0 as the target abilities to merge. According to\nthe experimental Appendix A) to\ncalculate the refusal rate for evaluating safety alignment.\n\u2022We use accuracy to evaluate the model performance on\nGSM8K. The prompt fed into WizardMath-7B-V1.0 is the\nofficially recommended prompt from [16]:\nBelow is an instruction that describes a task. Write a\nresponse that appropriately completes the request.Cong, et al.\nTable 2: The utility of the merged LLMs on different downstream tasks. We highlight the evaluation conclusion\nthat Instructional Fingerprint is more robust than Quantization\nWatermarking against model merging.\nAblation Study. To further evaluate the robustness of Instruc-\ntional Fingerprint under different hyper-parameter settings, we\nmerge LLaMA-2-7B-CHAT-Fingerprint with WizardMath-7B-V1.0\nby DARE-TIES, and set different values of \ud835\udc5dfor DARE. The CONCLUSION\nModel merging techniques have powerful application prospects\nbut also pose threats to model IP infringement. In this paper, we\nconduct the first robustness measurement on IP protection tech-\nniques for large language models in the context of model merging.\nWe discuss two model copyright protection techniques: Quantiza-\ntion Watermarking and Instructional Fingerprint. Additionally, we\nconsider various advanced model merging techniques, such as Task\nArithmetic, TIES-MERGING, and so on. We apply IP protection\ntechniques to one of the upstream expert LLMs and investigate\nwhether the model owner can still maintain copyright claims in\nthe merged model. Experimental REFERENCES\n[1]Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.\nTurning your weakness into a strength: Watermarking deep neural networks\nby backdooring. In 27th USENIX Security Symposium (USENIX Security) , pages\n1615\u20131631, 2018.\n[2]Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, and\nQiongkai Xu. Here\u2019s a free lunch: Sanitizing backdoored models with model\nmerge. CoRR abs/2402.19334 , 2024.\n[3]Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. Language models are\nhomer simpson! safety re-alignment", " Introduction\nPre-train/fine-tune paradigm [15,19,29,32,33] has proven to be a strong frame-\nwork for training models to reach state-of-the-art performance. This approach,\nespecially pivotal in fine-tuning pre-trained models, involves models acquiring\ngeneral knowledge during pre-training and task-specific knowledge during fine-\ntuning. How we perform a fine-tuning stage is crucial, affecting task performance\nand robustness against distribution shifts.\nRecent advancements, notably Model Soup [32], which merges weights from\nmultiple fine-tuned models trained under different training setups, have shown\nimpressive performance without increasing inference costs. This method is be-\nlieved to be effective because these models often reside in the same loss basin,\nand their merging results in a\nmore significant error in Gaussian distribution approximation. Consequently,\nthe overall performance under filter-wise merging is slightly inferior to layer-\nwise one.\nThesefindingsunderscoretheimportanceofaccuratelymodelingnoisedistri-\nbution in enhancing the performance of Model Stock. As our understanding and\nability to model this noise distribution improve, we anticipate further increases\nin the efficacy and robustness of our approach.28 Jang et al.\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00.10.20.30.40.5 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)CLIP ViT-L/14\n0102030405060708090 Angle (degree)Conv\nAttention\nBatchNorm\nClassifier\nBias\nDownsample\nAll\n0.000.020.040.060.080.100.12 Norm / sqrt (#. of elements)Conv\nAttention\nBatchNorm\nClassifier\nBias\nDownsample\nAll\n(b)CLIP ResNet50\n0102030405060708090 Angle (degree)Others\nConv\nMLP\nLayerNorm\nClassifier\nBias\nLayer scaler\nAll\n0.000.050.100.150.200.250.300.35 Norm / sqrt (#. of elements)Others\nConv\nMLP\nLayerNorm\nClassifier\nBias\nLayer scaler\nAll\n(c)OpenCLIP ConvNeXt\nFig.F: Layer-wise angle and norm across different model architectures. The\nangle and norm for CLIP ViT-L/14, CLIP ResNet50, and OpenCLIP ConvNeXt are\ndisplayed from top to bottom. These metrics demonstrate consistency regardless of the\nmodel type from left (first layer) to right (last layer). It is important to note that we\nalso depict the error bars for each layer in all figures, but they are not visible in most\nlayers due to the small standard deviation.Model Stock 29\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00.20.40.60.81.0 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)SGD optimizer\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n01234 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(b)SGD optimizer with momentum\nFig.G: Layer-wise angle and norm across different optimizers. Displayed from\ntop to bottom are the angle and norm for models trained with SGD and SGD with\nmomentum, respectively. These metrics demonstrate consistency regardless of the op-\ntimization strategy from left (first layer) to right (last layer).30 Jang et al.\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.000.020.040.060.080.100.12 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)Vanilla model (10 epochs + no augmentation)\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.0000.0250.0500.0750.1000.1250.1500.1750.200 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(b)+ longer epochs (16 epochs)\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.0000.0250.0500.0750.1000.1250.1500.1750.200 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(c)+ RRC\nFig.H: Layer-wise angle and norm across different augmentations. Displayed\nfrom top to bottom are the angle and norm for the vanilla model (10 epochs + no\naugmentation), +longer epochs (16 epochs), and +RRC. Each augmentation is applied\nincrementally. These metrics demonstrate consistency regardless of the augmentations\nfrom left (first layer) to right (last layer).Model Stock 31\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00000.00010.00020.00030.00040.0005 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\nFig.I: Layer-wise angle and norm across different datasets. The angle and\nnorm for models trained on different datasets, including CIFAR [14] are displayed from\ntop to bottom. These metrics demonstrate consistency regardless of the dataset type\nfrom left (first layer) to right (last layer).\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00000.00020.00040.00060.00080.00100.00120.0014 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\nFig.J: Layer-wise angle and norm across different classifier initializations.\nThe angle and norm for models trained with differently initialized networks following\nthe LP-FT [15] method are displayed from top to bottom. These metrics demonstrate\nconsistency regardless of the initialization method from left (first layer) to right (last\nlayer).\nFig.K: Layer-wise angle during training. Displayed are the overlapped angles\nacross models trained with different random seeds at", " Introduction\nModel merging [ 15,28], a recent development in the large language model (LLM) community,\npresents a novel paradigm shift. By strategically combining multiple LLMs into a single architecture,\nthis exciting development has captured the attention of researchers due to its key advantage: it\nrequires no additional training, making it an incredibly cost-effective approach for developing new\nmodels. This accessibility has fueled a surge in interest and experimentation with model merging.\nThe Open LLM Leaderboard [ 20] is now dominated by merged models, showcasing its potential for\ndemocratizing foundation model development.\nHowever, model merging is considered by many to be a form of black art or alchemy, relying on the\nmodel maker\u2019s intuition and instincts about model selection and merging recipes to create and refine a\nnew model that performs well for a particular task. Furthermore, the model maker is often required to\nhave some domain knowledge for the various different benchmark tasks. Given the large diversity of\nopen models and benchmarks in the community, human intuition can only go so far, and we believe a\nmore systematic approach for discovering new model combinations will take things much further.\nWe believe evolutionary algorithms will be able to discover more effective model merging solutions,\nand thus provide a path for automating the creation of more capable models. As a step towards this\ndirection, in this work, we show that evolution can be employed to discover novel and unintuitive\nways to merge various models to produce new models with a new combined ability. In this work, we\npresent a methodology that leverages evolutionary algorithms to facilitate the merging of foundation\nmodels. Our approach is distinguished by its ability to navigate both parameter space (weights) and\nthe data flow space (inference path), proposing a framework that integrates these two dimensions.\n1EvoLLM-JP ,EvoVLM-JP release: https://github.com/SakanaAI/evolutionary-model-mergearXiv:2403.13187v1  [cs.NE]  19 Mar 2024This work makes several key contributions to the field of foundation model development:\n1.Automated Model Composition : We introduce Evolutionary Model Merge , a general\nevolutionary method to automatically discover optimal combinations of diverse open-source\nmodels for creating new foundation models with user-specified capabilities. This approach\nharnesses the collective intelligence of existing open models, enabling the creation of\npowerful models without the need for extensive training data or compute.\n2.Cross-Domain Merging : We demonstrate that our method can discover novel ways to\nmerge models from disparate domains (e.g., non-English language and Math, non-English\nlanguage and Vision), potentially exceeding the capabilities achievable through conventional\nhuman design strategies.\n3.State-of-the-Art Performance : We showcase the effectiveness of our method by auto-\nmatically generating a Japanese LLM with Math reasoning capability and a Japanese\nVision-Language Model (VLM). Notably, both models achieve state-of-the-art performance\non various benchmarks, even without explicit optimization for those tasks.\n4.High Efficiency and Surprising Generalizability : We observe that our 7B parameter LLM\nsurpasses the performance of some previous 70B parameter Japanese LLMs on benchmark\ndatasets, highlighting the high efficiency and surprising generalization capability of our\napproach. We believe this model can serve as a strong general-purpose Japanese LLM.\n5.Culturally-Aware VLM : The generated Japanese VLM achieves top results of this Apache 2.0-licensed model for comparison in Table 4,\nwhich provides a more comprehensive comparison than Table 2 in the main text.\nC Case Study\nTable 5 provides an example of responses to a mathematical question by existing", " Introduction to Python for Translators/Linguists\u201d \nand \u201cProfe ssional Translation Internships\u201d. His research focuses on \nhuman -centered AI and machine translation, aiming to augment \npeople's abilities to empower them and reduce their cognitive limi-\ntations. His academic experience is influenced by his professional \nactivity, since he runs AWORDZ Language Engineering, a small \ncompany that provides language engineering, localisation and in-\nternationali sation services . \n \nJO\u00c3O LUCAS CAVALHEIRO CAMARGO has a B. Ed. in Portuguese \nand English and their respective literatures from  Western Paran\u00e1 \nState University (UNIOESTE) in Brazil. He holds a Specialist degree in English through distance learning and a Master\u2019s in \nteaching at the same institution. He also holds a Specialist degree \nin Instructional Design from Instituto de Desenho Instrucional. In \nhis Master\u2019s degree research, he designed, implemented and evalu-\nated two translation courses (in -person and distance learning) on \ntranslation hermeneutics. He was a Lecturer at the Western Paran\u00e1 \nState University, teaching English languag e teachers, Tourism and \nHospitality undergraduates. Currently, he is a PhD student funded \nby the School of Applied Languages and Intercultural Studies \n(SALIS) in Dublin City University. His PhD project aims to de-\nsign, implement and evaluate training on human evaluation of Ma-\nchine Translation to Master\u2019s NLP students.  \n \nGOKHAN DOGRU is a visiting postdoctoral researcher at ADAPT -\nDCU affiliated with the Faculty of Translation and Interpreting at \nUniversitat Aut\u00f2noma de Barcelona (UAB) in the framework of \nMarga rita Salas Grant. His research interests include terminologi-\ncal quality evaluation in machine translation, different use cases of \nMT for professional translators and the intersection of translation \nprofession and translation technologies as well as localization . discussion \nof fluency Related work: The disruption of AI and MT in the legal do-\nmain  \nThe development of MT includes quality assessment as a crucial \naspect that both academia and industry work on (Way 20 20), be-\ncoming its own subfield in MT research (Castilho & Caseli 2023). \nEvaluation can be performed through HE and AEMs, with varied \npractices for different contexts (Castilho et al.  2018). The im-\nprovement in quality of MT systems in the legal field and their \nadoption in multiple fields, whether in general industry (ELIS \n2022), patent institutions like the World International Property \n________ __ \n \n2 Product page of VICUNA . Online : https://lmsys.org/blog/2023 -03-30-vicuna/  \n(last accessed : 07/06/2023) . \n3 Google Translate . Online : https://translate.google.com/ (last accessed : \n07/06/2023) . Organization4, or international institutions such as the European \nCommission and the creation of eTranslation5, a public MT system \nfor the legal field, have also led to the analysis of the use of these \nMT systems in legal institutions (Cadwell et al.  2016; Leszny\u00e1k \n2019; Rossi & Chevrot 2019).  \nIn terms of literature, MT in the legal world has been observed \nfrom different points of view. Firstly, focusing on the quality of \nautomatic systems, such as Killman (2014) and the use of MT in \nSpanish Supreme Court judgments. Another example is that of \nWiesmann (2019), who analysed how NMT worked for translating \nItalian legal te xts into German. In addition, Mileto (2019) worked \nwith students", " introduction to methodology and encoding rules,\u201d Journal of Chemical\nInformation and Computer Sciences , vol. 28, no. 1, pp. 31\u201336, 1988.\n[155] Y . G. N. Teukam, F. Zipoli, T. Laino, E. Criscuolo, F. Grisoni, and\nM. Manica, \u201cIntegrating genetic algorithms and language models for\nenhanced enzyme design,\u201d 2024. [Online]. Available: https://chemrxiv.\norg/engage/chemrxiv/article-details/65f0746b9138d23161510400\n[156] R. Suzuki and T. Arita, \u201cAn evolutionary model of personality traits\nrelated to cooperative behavior using a large language model,\u201d Scientific\nReports , vol. 14, no. 1, p. 5989, 2024.\n[157] T. Saha, D. Ganguly, S. Saha, and P. Mitra, \u201cLarge language models\u2019\ninterpretability and trustworthiness (llmit),\u201d 2023.\n[158] S. Jain and B. C. Wallace, \u201cAttention is not explanation,\u201d in Pro-\nceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , 2019, pp. 3543\u2013\n3556.\n[159] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, \u201cPre-train,\nprompt, and predict: A systematic survey of prompting Methods for Design, Optimization and Control with Applications to\nIndustrial Problems, Athens, Greece, Septermber 12-21 , 2001.\n[147] S. L. Lim, P. J. Bentley, and F. Ishikawa, \u201cScape: Searching\nconceptual architecture prompts using evolution,\u201d arXiv preprint\narXiv:2402.00089 , 2024.\n[148] S. Sudhakaran, M. Gonz \u00b4alez-Duque, C. Glanois, M. Freiberger, E. Na-\njarro, and S. Risi, \u201cPrompt-guided level generation,\u201d in Proceedings of\nthe Companion Conference on Genetic and Evolutionary Computation ,\n2023, pp. 179\u2013182.\n[149] A. J. Summerville, S. Snodgrass, M. Mateas, and S. Ontan \u00b4on, \u201cThe\nvglc: The video game level corpus,\u201d Proceedings of the 7th Workshop\non Procedural Content Generation , 2016.\n[150] J. Lehman, K. O. Stanley et al. , \u201cExploiting open-endedness to solve\nproblems through the search for novelty,\u201d in Proceedings of the 11th\nInternational Conference on the Synthesis and Simulation of Living\nSystems , 2008, pp. 329\u2013336.\n[151] H. Chefer, Y . Alaluf, Y . Vinker, L. Wolf, and D. Cohen-Or, \u201cAttend-\nand-excite: Attention-based semantic guidance for text-to-image diffu-\nsion models,\u201d ACM Transactions on Graphics , vol. 42, no. 4, pp. 1\u201310,\n2023.\n[152] H. Berger, A. Dakhama, Z. Ding, K. Even-Mendoza, D. Kelly,\nH. Menendez, R. Moussa, and F. Sarro, \u201cStableyolo: Optimizing image\ngeneration for large language models,\u201d in Proceedings of the Interna-\ntional Symposium on Search Based Software Engineering . Springer,\n2023, pp. 133\u2013139.\n[153] K. M. Jablonka, Q. Ai, A. Al-Feghali, S. Badhwar, J. D. Bocarsly,\nA. M. Bran, S. Bringuier, L. C. Brinson, K. Choudhary, D. Circi\net al. , \u201c14 examples of how llms can transform materials science and\nchemistry: a reflection on a large language model hackathon,\u201d Digital\nDiscovery , vol. 2, no. 5, pp. 1233\u20131250, 2023.\n[154] D. Weininger, \u201cSmiles, a chemical language and information system. 1. REFERENCES\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-\ntraining of deep bidirectional transformers for language understanding,\u201d\ninProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) . Association for\nComputational Linguistics, 2019, pp. 4171\u20134186.\n[2] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, \u201cUnifying\nlarge language models and knowledge graphs: A roadmap,\u201d IEEE\nTransactions on Knowledge and Data Engineering , 2024.\n[3] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of", " Introduction\nPolicy optimization is a prevalent method for solving rein-\nforcement learning problems, involving iterative parameter\nupdates to maximize objectives. Policy gradient discussion and for providing insight-\nful advice regarding the experiment. This material is based\nupon work partially supported by the National Science and\nTechnology Council (NSTC), Taiwan under Contract No.\nNSTC 112-2628-E-A49-023 and Contract No. NSTC 112-\n2634-F-A49-001-MBK and based upon work partially sup-\nported by the Higher Education Sprout Project of the Na-\ntional Yang Ming Chiao Tung University and Ministry of\nEducation (MOE), Taiwan. related work, please refer to Appendix E.1. Interestingly, one can draw an analogy between (146) in Lemma 10 and learning a linear binary classifier:\n(i)Features : The state-action representation can be viewed as the feature vector of a training sample; (ii) Labels : The sign\nofA\u03c02(s, a)resembles a binary label; (iii) Classifiers :\u03c01(a|s)\u2212\u03c02(a|s)serves as the prediction of a linear classifier. We\nprovide the intuition behind using \u03c01(a|s)\u2212\u03c02(a|s)as a classifier. Let\u2019s fix \u03c02and let \u03c01be the improved policy. If the sign\nofA\u03c02(s, a)\u22650, which implies that the action ahas a positive effect on the total return, it is desired to slightly tune up the\nprobability of acting in action a. Thus, the update \u03c01must have a greater probability on action ain order to obtain the sufficient\ncondition of the state-wise policy improvement, i.e., (\u03c01(a|s)\u2212\u03c02(a|s))A\u03c02(s, a)\u22650. Next, we substantiate this insight and\nrethink PPO-Clip via hinge loss.\nAs described in Section 3, one major component of the proof of Theorem 1 is the state-wise policy improvement property of\nPPO-Clip. For ease of exposition, we introduce the following definition regarding the partial ordering over policies.\nDefinition 1 (Partial ordering over policies) .Let\u03c01and\u03c02be two policies. Then, \u03c01\u2265\u03c02, called \u03c01improves upon \u03c02, if and\nonly if V\u03c01(s)\u2265V\u03c02(s),\u2200s\u2208 S. Moreover, we say \u03c01> \u03c02, called \u03c01strictly improves upon \u03c02, if and only if \u03c01\u2265\u03c02and\nthere exists at least one state ssuch that V\u03c01(s)> V\u03c02(s).\nLemma 11 (Sufficient condition of state-wise policy improvement) .Given any two policies \u03c01and\u03c02, we have \u03c01\u2265\u03c02if the\nfollowing condition holds:X\na\u2208A\u03c01(a|s)A\u03c02(s, a)\u22650,\u2200s\u2208 S. (148)\nProof of Lemma 11. This is the same result of the proof of Lemma 10.\nNext, we present two critical properties that hold under PPO-Clip for every sample path.\nLemma 12 (Strict improvement and strict positivity of policy under PPO-Clip with direct tabular parameterization) .In any\niteration t, suppose \u03c0(t)is strictly positive in all state-action pairs, i.e., \u03c0(t)(a|s)>0, for all (s, a). Under PPO-Clip in\nAlgorithm 7, \u03c0(t+1)satisfies that (i) \u03c0(t+1)> \u03c0(t)and (ii) \u03c0(t+1)(a|s)>0, for all (s, a).\nProof of Lemma 12. Consider the t-th iteration of PPO-Clip (cf. Algorithm 7) and the corresponding update from \u03c0(t)to\u03c0(t+1).\nRegarding (ii), recall from Algorithm 8 that K(t)denotes the number of iterations undergone by the EMDA subroutine for the\nupdate from \u03c0(t)to\u03c0(t+1)and that K(t)is designed to be finite. Therefore, it is easy to verify that \u03c0(t+1)(a|s)>0for all (s, a)\nby the exponentiated gradient update scheme of EMDA and the strict positivity of \u03c0(t).\nNext, for ease of exposition, for each k\u2208 {0,1,\u00b7\u00b7\u00b7, K(t)}and for each state-action pair (s, a), lete\u03b8(k)\ns,adenote\nthe policy parameter after kEMDA iterations. Regarding (i), recall that we define g(k)\ns,a:=\u2202L(\u03b8)\n\u2202\u03b8s,a\f\f\n\u03b8=e\u03b8(k)\nsandw(k)\ns:=\n(e\u2212\u03b7g(k)\ns,1,\u00b7\u00b7\u00b7, e\u2212\u03b7g(k)\ns,|A|). Note that as the weights in the loss function only affects the effective step sizes of EMDA, we simply\nset the weights of PPO-Clip to be one, without", " Introduction\nIn recent years, foundational models [3] have become instrumental tools, exhibit-\ning unprecedented efficacy across multiple domains. These models are charac-\nterized by their extensive scale, generality, and capacity to learn and generalize\nknowledge from vast datasets, offering promising solutions to a diverse range of\nproblems. The inherent ability of foundational models to be fine-tuned has led to\n1The code to reproduce our results for ViT-B-32 model where we found the best hyper-pa-\nrameters for that specific number of tasks for each method. (II) We show the\naverage normalized accuracy over all subsets of the 8 tasks detailed in Sec-\ntion 4.1, amounting to a total of 256 = 28combinations. (III) The evaluation is\nperformed over all 8 tasks at each point. As we can see from Figure 10, Model\nBreadcrumbs merging consistently outperforms the TIES method at each point,\nwith the performance gap widening as more tasks are considered. This high-\nlights the significant practical performance advantages of Model Breadcrumbs\non a larger scale. methods across tasks,\nillustrating Model Breadcrumbs\u2019 consistent outperformance, with the performance gap\nwidening as tasks increase. The related work. Sections 3 and 4 present our framework and itsModel Breadcrumbs 3\nevaluation. Finally, Section 5 outlines the scope and limitations of our proposed\nmethod. Our key contributions and findings are summarized as follows:\n1. Introducing a simple and scalable approach for merging models and reusing\npre-existing fine-tuned models to construct multi-task models, often outper-\nforming their individual fine-tuned counterparts.\n2. We empirically show the robustness of our approach to hyperparameter vari-\nations and its ability to generalize with the increasing number of tasks.\n2 Related Work\nModel Merging Recent studies in the literature have explored the merging\nof models trained from scratch with different initializations [1,53]. One of the\nmain challenges in this type of model merging is aligning the models before the\nactual merger. Therefore, research in this branch primarily focuses on finding\npermutations between networks to bring them into alignment with a reference\nmodel, enabling the subsequent merger of the two models in weight space. Our\nwork, on the other hand, distinguishes itself from this line of research, as we\nconcentrate on the model merging of networks that share the same initialization,\nspecifically initialized by a foundation model. Furthermore, our investigation is\nfocused on the scalability of merging Experiments\nIn this section, we conduct a series of Appendix B\nprovides more details on fine-tuned models and the fine-tuning process.\nThe Conclusions\nIn this paper, we introduced Model Breadcrumbs, a simple yet effective approach\nto constructing multi-task models from pre-existing fine-tuned foundation mod-\nels. Our extensive conclusion, Model Breadcrumbs stands out for its simplicity, efficiency,\nand effectiveness in constructing multi-task models. Leveraging publicly avail-\nable fine-tuned models, it aligns with the trend of updatable machine learn-\ning, supporting community-driven model refinement efforts. We anticipate that\nModel Breadcrumbs will contribute to the development of efficient and scalable\nmulti-task learning solutions in the future.\nAcknowledgements\nWe acknowledge funding from the NSERC Discovery Grant RGPIN-2021-04104\nand FRQNT New Scholar. This research was enabled in part by compute re-\nsources provided by Digital Research Alliance of Canada (the Alliance) and\nCalcul Qu\u00e9bec.16 MR. Davari and E. Belilovsky References\n1. Ainsworth, S.K., Hayase, J., Srinivasa, S.: Git re-basin: Merging models modulo\npermutation symmetries. arXiv preprint arXiv:2209.04836 (2022)\n2. Asadi, N., Davari, M., Mudur, S., Aljundi, R., Belilovsky, E.: Prototype-sample\nrelation distillation: Towards replay-free continual", " Introduction\nHuman beings have harbored a longstanding desire to ac-\nquire additional abilities through various ways, as expressed\nin mediums like movies and games. For example, in X-\nMen\u2019s Apocalypse, the character can absorb the powers\n1Alibaba Group. Correspondence to: Bowen\nYu <yubowen.ybw@alibaba-inc.com >, Yongbin Li\n<shuide.lyb@alibaba-inc.com >.\nProceedings of the 41stInternational Conference on Machine\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nFigure 1: ( Left) DARE can effectively eliminate 90% or\neven 99% delta parameters of WizardMath on GSM8K.\n(Right ) DARE can merge multiple task-specific SFT lan-\nguage models into a single model with all the abilities.\nLM, MATH, and Code are abbreviations of WizardLM-\n13B, WizardMath-13B, and llama-2-13b-code-alpaca.\nof other mutants to strengthen himself. Likewise, the pro-\ntagonist in the Super Mario games can gain superpowers\nlike throwing fireballs by absorbing in-game items. In this\npaper, we astonishingly find that Language Models (LMs),\nsimilar to Apocalypse and Super Mario, can enhance their\ncapabilities by absorbing other models without the need for\nretraining or even GPUs.\nFormally, Supervised Fine-Tuning (SFT) is the most widely\nadopted strategy for unlocking task-specific abilities to LMs\nby optimizing their parameters (Dodge et al., 2020; Zhao\net al., 2023). The effectiveness of SFT is fully evident in\nthe alteration of the model parameters before and after SFT,\nreferred to as delta parameters (Ding et al., 2023). We\nfirst show that SFT LM (either encoder- or decoder-based)\nalways tends to acquire excessively redundant delta parame-\nters. To be specific, we present DARE ( DropAndREscale),\nwhich randomly sets certain delta parameters to zeros with a\ndrop rate pand subsequently rescales the remaining ones by\na factor of 1/(1\u2212p). Although conceptually simple, DARE\ncan eliminate up to 99% delta parameters with minimal im-\npact on the performance when the LM\u2019s parameters reach 70\nbillion (see Figure 1(a)). Moreover, the more parameters the\nLM has, the larger pit can tolerate. We attribute the effec-\ntiveness of DARE to its ability to approximate the original\nembeddings, which is verified theoretically and empirically.\nFurthermore, we can merge multiple homologous SFT LMs\n(fine-tuned from the same backbone) based on DARE with-\n1arXiv:2311.03099v3  [cs.CL]  13 Jun 2024Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch\nout compromising their capabilities. As long as a small\nportion of the delta parameters remain unaffected during\nmerging, the abilities of LMs unlocked by SFT can still be\npreserved. We first employ DARE to eliminate redundant\ndelta parameters in each model before merging, which can\npotentially mitigate the interference of parameters among\nmultiple models (Yadav et al., 2023). Then, we apply estab-\nlished model merging techniques (Wortsman et al., 2022;\nIlharco et al., 2023; Matena & Raffel, 2022; Jin et al., 2023;\nYadav et al., 2023) to fuse the parameters with reduced re-\ndundancy for creating one model with diverse capabilities.\nWe conduct extensive experiments under this setting with both decoder- and\n. For decoder-based LMs, we find they perform badly\nwhen removing fine-tuned parameters even with 0.1 as\nthe drop rate. Quantitatively, the performance sharply\ndrops from 67.20 to 8.56 on AlpacaEval for WizardLM-\n13B, from 64.22/14.02 to 0.38/0.16 on GSM8K/MATH\nfor WizardMath-13B, from 63.41/55.40 to 0.0/0.20 on Hu-\nmanEval/MBPP for WizardCoder-Python-13B. Similar ob-\nservations can also be found on MP or decoder-based LMs\nwith 7B, 34B, or 70B sizes. Partial results of removing fine-tuned parameters on GLUE on encoder-based LMs.\n18Language Models are Super Mario: Absorbing Abilities from Homologous", " Introduction\nLarge language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency\nin natural language that allows them to be commanded and prompted to perform a variety of tasks (OpenAI,\n2023; Touvron et al., 2023b). By utilizing large, in-domain datasets, their efficacy can be greatly improved\nfor applications that require a combination of both natural and domain-specific language and understanding\nof specialized terminology. By training on domain-specific datasets, they have proved effective more broadly\non applications that require advanced natural language understanding. A prominent use-case is the formal\ninteraction with computer systems, such as program synthesis from natural language specifications, code\ncompletion, debugging, and generating documentation (for a survey, see Xu & Zhu, 2022, also see Section 5).\nIn this work, we present Code Llama , a family of LLMs for code generation and infilling derived from\nLlama 2 (Touvron et al., 2023b) and released under the same custom permissive license. We provide inference\ncode for both completion and infilling models in the accompanying repository.1Our approach is based on\ngradually specializing and increasing the capabilities of Llama 2 models by applying a cascade of training\nand fine-tuning steps (Figure 2):\n\u2022Code-training from foundation models. While most LLMs for code generation such as AlphaCode\n(Li et al., 2022), InCoder (Fried et al., 2023) or StarCoder (Li et al., 2023) are trained on code only,\nCodex (Chen et al., 2021) was fine-tuned from a general language model. We also start from a foundation\nmodel ( Llama 2 , Touvron et al., 2023b) pretrained on general-purpose text and code data. Our comparison\n(Section 3.4.1) shows that initializing our model with Llama 2 outperforms the same architecture trained\non code only for a given budget.\n1https://github.com/facebookresearch/codellama\n\u2020: Core contributors. \u2217: Meta AI, CERMICS \u00c9cole des Ponts ParisTech. \u22c4: Meta AI & Hebrew University of Jerusalem\n1arXiv:2308.12950v3  [cs.CL]  31 Jan 2024Prompt: In Bash, how do I list all text files in the current directory (excluding subdirectories) that have been modified in the\nlast month\nResponse: You can use the \u2018find\u2018 command in Bash to list all text files in the current directory (excluding subdirectories) that\nhave been modified in the last month. The command would be:\nfind . -maxdepth 1 -type f -mtime -30 -name \"*.txt\"\nHere\u2019s a breakdown of the command:\n* \u2018.\u2018 specifies the current directory.\n* \u2018-maxdepth 1\u2018 tells \u2018find\u2018 to only search the current directory and not any subdirectories.\n* \u2018-type f\u2018 tells \u2018find\u2018 to only search for files (not directories or other types of files).\n* \u2018-mtime -30\u2018 tells \u2018find\u2018 to only search for files that have been modified in the last 30 days.\n* \u2018-name \"*.txt\"\u2018 tells \u2018find\u2018 to only search for files with the \u2018.txt\u2018 extension.\nNote that the \u2018-mtime\u2018 option uses the number of days since the file was last modified, so \u2018-mtime -30\u2018 means \"modified in the\nlast 30 days\". If you want to search for files modified in the last month, you can use the \u2018-mtime -30\u2018 option.\nFigure 1: Example of response of Code Llama - Instruct (34B) when queried for a specific shell command.\n\u2022Infilling. Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does\nnot provide the capability to fill a missing portion of text while taking the full surrounding context", " Introduction\nRecently, Large-scale language models (LLMs) have garnered significant attention and become\nthe go-to approach for numerous natural language processing (NLP) tasks, including open domain\nconversation [ 1\u20134], coding [ 5\u201313] and math [ 14\u201319]. A conspicuous example is ChatGPT, developed\nby OpenAI. This model uses extensive pre-training on large-scale internet data and further fine-\ntuning with specific instruction data and methods for effective instruction\ntuning. arXiv preprint arXiv:2301.13688 , 2023.\n[88] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei\nZhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald\nMetzler. Ext5: Towards extreme multi-task scaling for transfer learning. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,\n2022.\n[89] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea\nSantilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas\nWolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The\nTenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .\nOpenReview.net, 2022.\n[90] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. Unifiedqa: Crossing format boundaries with a single QA system. In Trevor Cohn, Yulan He,\nand Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020 , volume EMNLP 2020 of Findings of ACL , pages 1896\u20131907. Association\nfor Computational Linguistics, 2020.\n[91] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Zeroprompt:\nScaling prompt-based pretraining to 1, 000 tasks improves zero-shot generalization. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 4235\u20134252. Association\nfor Computational Linguistics, 2022.\n[92] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416 , 2022.\n[93] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560 , 2022.\n15A results of the equations:\nx= 6 + 9\n22y=\u221210\u22121\nx= 15\ny=\u221211\nSo, the coordinates of (x, y)are(15,\u221211).\nThe answer is: (15,-11).\nWizardMath-13B\nStep 1: Understand the problem\nWe are given that the midpoint of the line segment between (x, y)and(\u22129,1)is\n(3,\u22125). We need to find the coordinates of the point (x, y).\nStep 2: Recall the midpoint formula\nThe midpoint of a line segment with endpoints (x1, y1)and(x2, y2)can be found\nusing the midpoint formula:\nMidpoint =\u0000x1+x2\n2,y1+y2\n2\u0001\nStep 3: Apply the midpoint formula\nIn our case, the endpoints are (x, y)and(\u22129,1). So, we can plug these values into\nthe midpoint formula:\nMidpoint =\u0010\nx+(\u22129)\n2,y+1\n2\u0011\nStep 4: Simplify the midpoint\nNow, we can simplify the midpoint by performing the arithmetic operations:\nMidpoint =\u0000x\u22129\n2,y+1\n2\u0001\nStep 5: Set up an equation\nSince the midpoint is given", " Introduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of \ufb01elds, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThecapabilitiesofLLMsareremarkableconsideringtheseeminglystraightforwardnatureofthetraining\nmethodology. Auto-regressivetransformersarepretrainedonanextensivecorpusofself-superviseddata,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback(RLHF).Althoughthetrainingmethodologyissimple,highcomputationalrequirementshave\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Ho\ufb00mann et al., 2022), but none of these models are suitable substitutes for closed \u201cproduct\u201d LLMs, such\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\ufb01ne-tunedtoalignwithhuman\npreferences, which greatly enhances their usability and safety. This step can require signi\ufb01cant costs in\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\nthe community to advance AI alignment research.\nIn this work, we develop and release Llama 2, a family of pretrained and \ufb01ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures1and3). Wehavetakenmeasurestoincreasethesafetyofthesemodels,usingsafety-speci\ufb01cdata\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthispapercontributesathoroughdescriptionofour\ufb01ne-tuningmethodologyandapproachtoimproving\nLLM safety. We hope that this openness will enable the community to reproduce \ufb01ne-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\nWealsosharenovelobservationswemadeduringthedevelopmentof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle andL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ,suchas\nthe emergence of tool usage and temporal organization of knowledge.\n3Figure 3: Safety human evaluation Results\nSee evaluations for pretraining (Section 2); \ufb01ne-tuning (Section 3); and safety (Section 4).\nEthical Considerations and Limitations (Section 5.2)\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is a new technology that carries risks with use. Testing conducted to date has been in\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle\u2019s potential outputs cannot be predicted in advance, and the model may in some instances\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\napplications of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle, developers should perform safety testing and tuning tailored to their\nspeci\ufb01c applications of the model. Please see the Responsible Use Guide available available at\nhttps://ai.meta.com/llama/responsible-user-guide\nTable 52: Model card for L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle .\n77 Discussion\nHere, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\nlimitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\nmodels (Section 5.3).\n5.1 Learnings and Observations\nOur tuning process revealed several interesting Related Work\nLarge Language Models. The recent years have witnessed a substantial evolution in the \ufb01eld of LLMs.\nFollowing the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B\nparameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized\nmodels, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Ho\ufb00mann et al.,\n2022) rede\ufb01ned those scaling laws towards the number of tokens rather than model weights. Notable in\nthisprogressionistheriseofLlama,recognizedforitsfocusoncomputationale\ufb03ciencyduringinference\n(Touvron et al., 2023). A parallel discourse has unfolded around the", " Introduction\nPre-trained models (PTMs) have become widespread in many real-world applications [ 91,6]. Using\nPTMs typically involves fine-tuning them to specialize on a specific task [ 69,12], which can lead to\nimproved performance with less task-specific labeled data. These benefits have resulted in the release\nof thousands of finetuned checkpoints [ 81] derived from popular PTMs such as ViT [ 14] for vision\nand T5 [ 58] for language. However, having a separate fine-tuned model for each task has various\ndrawbacks: (1) for each new application, a separate model has to be stored and deployed [ 17,89], and\n(2) models trained in isolation cannot leverage information from related tasks to improve in-domain\nperformance or out-of-domain generalization [ 66,58,75]. Multitask learning [ 66,57] could address\nthese concerns but requires costly training and simultaneous access to all tasks [ 17]. Moreover, it can\nbe complex and resource-intensive to determine how best to mix datasets to ensure that multitask\ntraining is beneficial for all tasks [55, 54, 80, 52, 2, 17].\n1Our code is available at https://github.com/prateeky2806/ties-merging\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2306.01708v2  [cs.LG]  27 Oct 2023Task V ectors  Trimmed T ask V ectors  \n(1) Trim\n(2) Elect SignAligned V alues\nSign V ector \nMerged T ask\nVector (3) Disjoint Merge: Influential values\n: Redundant values: Parameter: Model 1\n: Model 2: Model 3\n: Merged ModelFigure 1: A depiction of the steps involved in TIES-MERGING . We visualize each parameter in\na model as a square. The arrows depict the update (task vector, \u03c4) to a parameter produced by\nfine-tuning on different tasks (coded by colors), with direction denoting sign and length denoting\nmagnitude. We first trim the task vector values based on their magnitude, then we elect the sign for\neach parameter ( \u03b3m, green vector containing +1or\u22121) by resolving sign conflicts. Finally, we pick\nonly the values that align with the elected sign and take their mean as the final parameter value.\nRecently, a growing body of research has focused on model merging [40]. One application of merging\ninvolves combining multiple task-specific models into a single multitask model without performing\nadditional training. Previous works merge models by summing the individual model weights with\ndifferent weighting schemes, either via a simple average [ 9,28,83], via more sophisticated means\nthat incorporate parameter importance [ 45] or account for permutation invariances [ 1,31,70,74,42].\nCombining fine-tuned models in this way can be seen as adding together task vectors [29] that are\ncomputed by subtracting the pre-trained model\u2019s parameter values from those of the fine-tuned model.\nOriginal V alues\n No Interference  Redundant  Sign Conflict Mean TIESMerged V alues\nModel 1 Model 2\nFigure 2: Different types of conflict and\nmerged outputs produced by either averaging\norTIES-MERGING . The parameters causing\ninterference are denoted by dotted arrows.While weighted averaging of model parameters has\nproven effective for merging, all of these results of this comprehensive search indicated an optimal value of\nk= 20 , with values of \u03bb= 0.9,\u03bb= 1.0, and \u03bb= 1.1demonstrating equivalent performance. To\nmaintain simplicity in our model, we chose a \u03bbvalue of 1. Thus, the final selection of parameters for\nTIES-MERGING isk= 20 , signs based on mass, the disjoint mean, and a \u03bbvalue of 1.\nC.5 Merging Different Number of Tasks\nHere we provide some additional details", " Introduction\nThe emergence of large language models (LLMs) like ChatGPT [ 41] has revolutionized the landscape\nof artificial general intelligence (AGI), showcasing their impressive zero-shot capabilities in ad-\ndressing various natural language processing (NLP) tasks through user-tailored prompts or language\ninstructions. Despite these advancements, it\u2019s essential to note that the triumph of LLMs does not\neffortlessly extend to pure vision and vision-language tasks, due to the inherent disparities between\nmodalities and task formats.\nThe field of computer vision presents a unique set of challenges and paradigms that differ from\nthose of NLP. The traditional paradigm of vision foundation models is pre-training followed by\nfine-tuning [ 59,12,51,61,18,52], which is effective but comes with significant marginal costs\nwhen adapting to diverse downstream scenarios. As shown in Figure 1a, while approaches such as\nmulti-task unification [ 44,58,1,57,81] have been used to achieve generalist capability, they often\nstruggle to overcome the limitations imposed by pre-defined tasks, resulting in a gap in open-ended\n\u2217Equal contribution. This work is done when Zhe Chen, Xiaokang Chen, and Jiannan Wu are interns at\nShanghai AI Laboratory.\u2020Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.arXiv:2305.11175v2  [cs.CV]  25 May 2023VisionGeneralistModelPre-defined tasks:detection, captioning, VQA, grounding, ...(a) Vision generalist models [ 59,\n61,83] are constrained by the for-\nmat of pre-defined tasks.\nVisualPromptTuning\n(b) Visual prompt tuning [ 26,64,\n62] are inconsistent with the for-\nmat of LLMs.\nVision + LLM\nTask de\ufb01ned by  instruc6onsDesired output:<c1> <p1> <p3> ...(c) VisionLLM (ours) can flexibly\nmanage vision-centric tasks using\nlanguage instructions like LLMs .\nFigure 1: Comparison of our VisionLLM with popular paradigms. Unlike current vision generalist\nmodels that depend on pre-defined task formats and visual prompt tuning models that are inconsistent\nwith large language models (LLMs), VisionLLM leverages the power of LLMs for open-ended vision\ntasks by using language instructions.\ntask capabilities compared to LLMs. Recently, visual prompt tuning [ 26,74,79,76,62] has emerged\nas a way to flexibly outline some pure vision tasks (see Figure 1b), such as object detection, instance\nsegmentation, and pose estimation, using visual masking. However, the format of visual prompts\nconsiderably deviates from that of language instructions, making it challenging to directly apply the\nreasoning abilities and world knowledge of LLMs to vision tasks. Therefore, there is an urgent\nneed for a unified generalist framework that can seamlessly integrate the strengths of LLMs with the\nspecific requirements of vision-centric tasks.\nIn this work, we present VisionLLM, a novel framework that aligns the definitions of vision-centric\ntasks with the methodologies of LLMs. Leveraging the reasoning and parsing capacities of LLMs,\nVisionLLM is designed to empower open-ended task capabilities for vision-centric tasks. Specifically,\nit comprises three core components: (1) a unified language instruction designed for vision and\nvision-language tasks, (2) a language-guided image tokenizer, and (3) an LLM-based open-ended\ntask decoder that orchestrates various tasks using language instructions. With this framework, a\nwide range of vision-centric tasks can be seamlessly integrated, including object detection, instance\nsegmentation, image captioning, and visual grounding. In addition, the framework also facilitates\ntask customization at different levels of granularity, allowing for the customization of target objects,\noutput formats, task descriptions, etc.\nCompared to current popular API-based applications [ 68,73,50,35,30], our model takes a unified,\nend-to-end approach to integrate VFMs and LLMs, streamlining and enhancing the overall efficiency\nof the overall process, and leveraging the strengths and data of both VFMs and LLMs within a\nsingle, cohesive system.", " Introduction\nOpen-vocabulary models are characterized by their ability to perform any image classi\ufb01cation task\nbased on text descriptions of the classes [ 56]. Thanks to advances in large-scale pre-training, recent\nexamples of open-vocabulary models such as CLIP and BASIC have reached parity with or surpassed\nimportant task-speci\ufb01c baselines, even when the open-vocabulary models are not \ufb01ne-tuned on\ntask-speci\ufb01c data (i.e., in a zero-shot setting) [ 57,31,56,88,1,86]. For instance, the largest CLIP\nmodel from Radford et al. [57] used in a zero-shot setting matches the ImageNet accuracy of a\nResNet-50 trained on 1.2 million ImageNet images [14, 24].\nNevertheless, current open-vocabulary models still face challenges. The same CLIP model that\nmatches a ResNet-50 on ImageNet has lower MNIST accuracy than simple logistic regression in\npixel space [ 57]. Moreover, even when zero-shot models achieve good performance, they are usually\nstill worse than models trained or \ufb01ne-tuned on speci\ufb01c downstream tasks.\n\u0003Equal contribution. Code available at https://github.com/mlfoundations/patching .\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2208.05592v2  [cs.CV]  11 Oct 2022Figure 1 :Patching open-vocabulary models by lin-\nearly interpolating weights. We wish to improve ac-\ncuracy on tasks where a model performs poorly ( patching\ntasks ), without degrading performance on tasks where\naccuracy is already adequate ( supported tasks ). When\ninterpolating weights of \ufb01ne-tuned models and zero-\nshot (unpatched) models, there are intermediate solu-\ntions where accuracy improves on the patching task\nwithout reducing accuracy on supported tasks. Results are shown for nine patching\ntasks, for three different random seeds that control the order in which datasets are seen. The average\nacross random seeds is highlighted.\nJ.2 Sequential patching\nIn Figure 30, we show the evolution of sequential patching as more tasks are added. The accuracy\ndistance of using a single, patched model to using multiple specialized models increases with with\nthe number of patched tasks, leaving headroom for future work on more sophisticated sequential\nstrategies for patching. Interestingly, sequential patching outperforms sequential \ufb01ne-tuning (where\nno interpolation is used) by a large margin.\nJ.3 SplitCIFAR\nFigure 31 compares the patching results for\nvarious strategies for patching on multiple tasks.\n44Cars DTD EuroSAT GTSRB KITTI MNIST RESISC45 SUN397 SVHN\n\u000b S P S P S P S P S P S P S P S P S P Avg\nB/320.00 63.4 59.6 63.4 44.1 63.4 45.9 63.4 32.4 63.4 22.6 63.4 48.3 63.4 60.7 63.4 63.1 63.4 31.5 54.4\n0.05 63.4 61.9 63.4 47.0 63.4 63.4 63.4 39.7 63.4 25.5 63.4 60.9 63.5 65.9 63.5 64.4 63.4 39.9 57.7\n0.10 63.3 63.8 63.4 50.1 63.3 74.3 63.3 48.1 63.4 35.7 63.3 77.0 63.6 71.0 63.5 65.9 63.4 50.1 61.5\n0.15 63.3 65.5 63.4 53.7 63.2 82.0 63.2 57.6 63.5 47.5 63.2 87.3 63.5 75.9 63.5 67.2 63.3 60.9 64.9\n0.20 63.2 67.4 63.3 56.5 63.0 89.3 63.0 68.3 63.4 55.6 62.8 92.6 63.5 79.9 63.4 68.5 63.1 70.4 67.6\n0.25 63.1 69.1 63.1 59.7 62.7 93.3 62.8 78.5 63.3 60.2 62.4 95.7 63.3 83.7 63.3 69.5 62.8 78.3 69.7\n0.30 63.0 70.7 62.9 62.8 62.4 95.4 62.4 85.9 63.1 66.1 62.1 97.3 63.2 86.6 63.2 70.5 62.6 84.3 71.4\n0.35 62.8 72.4 62.7 65.7 62.0 96.4 62.0 90.9 62.8 69.3 61.7 98.5 63.0 89.1 63.1 71.6 62.1 88.8 72.5\n0.40 62.5 73.6 62.4 68.1 61.5 97.0 61.3 93.9 62.6 72.3 61.2 99.0 62.7", " introduction and survey of esti-\nmation of distribution algorithms,\u201d Swarm and evolutionary computation ,JOURNAL OF IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. XX, NO. XX, FEBRUARY 2023 19\nvol. 1, no. 3, pp. 111\u2013128, 2011.\n[114] W. Dong, T. Chen, P. Ti \u02c7no, and X. Yao, \u201cScaling up estimation of\ndistribution algorithms for continuous optimization,\u201d IEEE Transactions\non Evolutionary Computation , vol. 17, no. 6, pp. 797\u2013822, 2013.\n[115] M. Laumanns and J. Ocenasek, \u201cBayesian optimization algorithms for\nmulti-objective optimization,\u201d Lecture Notes in Computer Science , vol.\n2439, pp. 298\u2013307, 2002.\n[116] X. Zhong and W. Li, \u201cA decision-tree-based multi-objective estimation\nof distribution algorithm,\u201d in 2007 International Conference on Compu-\ntational Intelligence and Security (CIS 2007) . IEEE, 2007, pp. 114\u201311.\n[117] H. Tang, V . A. Shim, K. C. Tan, and J. Y . Chia, \u201cRestricted boltzmann\nmachine based algorithm for multi-objective optimization,\u201d in IEEE\ncongress on evolutionary computation . IEEE, 2010, pp. 1\u20138.\n[118] L. Mart \u00b4\u0131, J. Garc \u00b4\u0131a, A. Berlanga, C. A. C. Coello, and J. M. Molina,\n\u201cMb-gng: Addressing drawbacks in multi-objective optimization estima-\ntion of distribution algorithms,\u201d Operations Research Letters , vol. 39,\nno. 2, pp. 150\u2013154, 2011.\n[119] K. Li and S. Kwong, \u201cA general framework for evolutionary multi-\nobjective optimization via manifold learning,\u201d Neurocomputing , vol. 146,\npp. 65\u201374, 2014.\n[120] C. W. Ahn and R. S. Ramakrishna, \u201cMultiobjective real-coded bayesian\noptimization algorithmrevisited: diversity preservation,\u201d in Proceedings\nof the 9th annual conference on Genetic and evolutionary computation ,\n2007, pp. 593\u2013600.\n[121] M. Pelikan, K. Sastry, and D. E. Goldberg, \u201cMultiobjective hboa,\nclustering, and scalability,\u201d in Proceedings of the 7th annual conference\non Genetic and evolutionary computation , 2005, pp. 663\u2013670.\n[122] Q. Zhang, A. Zhou, and Y . Jin, \u201cRm-meda: A regularity model-based\nmultiobjective estimation of distribution algorithm,\u201d IEEE Transactions\non Evolutionary Computation , vol. 12, no. 1, pp. 41\u201363, 2008.\n[123] Y . Li, X. Xu, P. Li, and L. Jiao, \u201cImproved rm-meda with local\nlearning,\u201d Soft Computing , vol. 18, pp. 1383\u20131397, 2014.\n[124] Y . Wang, J. Xiang, and Z. Cai, \u201cA regularity model-based multiobjec-\ntive estimation of distribution algorithm with reducing redundant cluster\noperator,\u201d Applied Soft Computing , vol. 12, no. 11, pp. 3526\u20133538, 2012.\n[125] Y . Sun, G. G. Yen, and Z. Yi, \u201cImproved regularity model-based eda\nfor many-objective optimization,\u201d IEEE Transactions on Evolutionary\nComputation , vol. 22, no. 5, pp. 662\u2013678, 2018.\n[126] J. Sun, H. Zhang, A. Zhou, Q. Zhang, and K. Zhang, \u201cA new\nlearning-based adaptive multi-objective evolutionary algorithm,\u201d Swarm\nand evolutionary computation , vol. 44, pp. 304\u2013319, 2019.\n[127] P. A. Bosman and D. Thierens, \u201cMulti-objective optimization with di-\nversity preserving mixture-based iterated density estimation evolutionary\nalgorithms,\u201d International Journal of Approximate Reasoning , vol. 31,\nno. 3, pp. 259\u2013289, 2002.\n[128] H. Karshenas, R. Santana, C. Bielza, and P. Larranaga, \u201cMultiobjective\nestimation of distribution algorithm based on joint modeling of objectives\nand variables,\u201d IEEE Transactions on Evolutionary Computation , vol. 18,\nno. 4, pp. 519\u2013542, 2013.\n[129] H. Karshenas, R. Santana, C. Bielza, and P. Larra \u02dcnaga, \u201cMulti-objective\noptimization with joint probabilistic modeling of objectives and vari-\nables,\u201d in Proceedings of the 6th international conference on Evolutionary\nmulti-criterion optimization , 2011, pp. 298\u2013312.\n[130] L. R. Farias and A. F. Ara \u00b4ujo, \u201cIm-moea/d: An inverse modeling\nmulti-objective evolutionary algorithm based on decomposition,\u201d in 2021\nIEEE International Conference on Systems, Man, and Cybernetics (SMC) .\nIEEE, 2021, pp. 462\u2013467.\n[131] R. Cheng, Y", " Introduction\nIn recent years, research has shown that models pre-trained\non large and diverse datasets learn representations that trans-\nfer well to a variety of tasks. As a result, machine learning\npractitioners now commonly develop solutions for down-\nstream tasks by \ufb01ne-tuning large pre-trained models (Gir-\nshick et al., 2014; Yosinski et al., 2014; Kornblith et al.,\n2019; Kolesnikov et al., 2020). Typically, the \ufb01ne-tuning\nprocess involves two steps: (1) \ufb01ne-tune models with a va-\nriety of hyperparameter con\ufb01gurations, and (2) select the\nmodel which achieves the highest accuracy on the held-out\nvalidation set. The remaining models are then discarded.\nSelecting a single model and discarding the rest has several\ndownsides. For one, ensembling outputs of many models\ncan outperform the best single model, albeit at a high com-\nputational cost during inference. For another, \ufb01ne-tuning a\nmodel on downstream tasks can sometimes reduce out-of-\ndistribution performance (Radford et al., 2021; Andreassen\net al., 2021; Wortsman et al., 2021; Pham et al., 2021), and\nthe best single model on the target distribution may not be\nthe best model on out-of-distribution data.\nIn this work, we propose a more accurate and robust alter-\nnative to the second step of the conventional recipe in the\ncontext of \ufb01ne-tuning a large pre-trained model. Instead of\nselecting the individual \ufb01ne-tuned model which achieves the\nhighest accuracy on the held-out validation set, we average\nthe weights of models \ufb01ne-tuned independently, and refer to\nthe result as a model soup . Given the related work.\n2. Method\nThis section highlights three recipes for model souping, the\nuniform ,greedy , and learned soup, though the greedy soup\nis our central method. We summarize the methods of data augmentation as we conduct a random hyperparameter search.Model soups: averaging weights of multiple \ufb01ne-tuned models improves accuracy without increasing inference time\n1 2 3 4 5 6 7\nNumber of ingredients (models)\u22124\u221220246810Percentage point \u2206from CLIP zero-shot\nCross-dataset soup\nZero-shot CLIP\n0 2 4 6 8\nAverage percentage point \u2206when including soup ingredientCIFAR10ImageNetSUN397Food101CarsDTD\nFigure E.1: Model soups can improve zero-shot performance on new downstream tasks. (left) Starting with zero-shot CLIP we create a\nsoup by adding models \ufb01ne-tuned on ImageNet, CIFAR-10, Food101, SUN397, DTD, and Cars, and evaluate on CIFAR-100. Different\norders for adding models are shown with faded lines. (right) The average change in CIFAR-100 accuracy when a model \ufb01ne-tuned on the\ndataset listed in the y-axis is added to the model soup.\nSGD RMSprop Adam AdamWSGD RMSprop Adam AdamW\n0 1.3 1.2 1.40 1.6 1.70 1.70Choice of optimizer\n0 1 2 3 4 50 1 2 3 4 5\n0 1.6 1.8 1.8 1.8 1.70 2.8 2 2.6 20 1.2 1.1 1.20 1.4 0.80 1.30Choice of augmentation strength\n1e-4 3e-5 2e-5 1e-5 3e-6 1e-6 1e-71e-4 3e-5 2e-5 1e-5 3e-6 1e-6 1e-7\n0 -1.1 -2 -2.8 -3.2 -3.2 -3.30 0.96 0.86 1.3 1.5 1.50 0.64 1.3 1.6 1.60 0.63 1.1 1.20 0.22 0.150 -0.560Choice of learning rate\n0.00.20.40.60.81.01.21.41.6\n0.00.51.01.52.02.5\n\u22123\u22122\u2212101\nAcc/parenleftbig1\n2\u03b8a+1\n2\u03b8b/parenrightbig\n\u2212max\u03b8\u2208{\u03b8a,...,\u03b8b}Acc(\u03b8)\nFigure F.1: Analysis of 1D hyperparameter grids, where the average of models at the endpoints often outperforms the best individual\nmodel in the grid. In particular, colors and numbers indicate the percentage point improvement obtained by averaging the models on the x\nandyaxis versus taking the best individual model in the range between them. Experiments\nThis section presents our key experimental \ufb01ndings. We\nbegin with experimental setup (Section 3.1) then provide\nintuition for model soups by examining", " Introduction\nHow should we transfer knowledge and capabilities across trained models? One popular approach\nis transfer learning [ 44], which \ufb01ne-tunes a pre-trained model on a target task through additional\ngradient-based training. The preparatory step of pre-training the model on a data-rich task ideally\ninstills useful \u201cknowledge\u201d into the network\u2019s parameters, which allows the model to learn more\nrapidly and effectively when \ufb01ne-tuned on a downstream task of interest. Transfer learning has\ntherefore become a particularly important and omnipresent tool across many \ufb01elds, including natural\nlanguage processing [ 57,13,9,52,53,46] and computer vision [ 43,24,68]. Recently, it has been\nshown that training on an \u201cintermediate\u201d task between pre-training and \ufb01ne-tuning can further boost\nperformance through additional transfer of capabilities from the intermediate task [ 47,60,51,48].\nAlternatively, continued self-supervised training on unlabeled domain-specialized data can serve as a\nform of domain adaptation [19].\nAll of the aforementioned transfer learning results on GLUE with BERT-base. Columns correspond to target\ntasks while rows correspond to intermediate tasks. Subscripts denote standard deviation across runs.\nItalicized values represent \ufb01ne-tuning directly on the target task (i.e. no intermediate-task training).\nTASK COLA MRPC STS-B RTE\nCOLA 55 :41:8 85:00:9 85:90:8 62:12:3\nSST-2 56:81:4 85:40:9 85:31:0 63:81:0\nMRPC 58:50:484 :50:3 85:30:8 62:75:2\nSTS-B 56:30:4 86:70:786 :10:9 64:52:5\nQQP 56:02:0 87:11:2 87:50:4 71:61:9\nMNLI 58:61:7 85:90:8 87:60:3 77:41:6\nQNLI 56:41:9 87:80:6 87:10:5 71:04:1\nRTE 56:70:9 82:22:5 85:80:563 :71:7\nTable A4: Effect of the number of examples used to compute the Fisher information. Columns\ncorrespond to the number of examples used for RTE. Rows correspond to the number of examples\nused for MNLI. Scores are the RTE validation set accuracy. The original RTE checkpoints had an\naverage accuracy of 63:7and isotropic merging (i.e. 0 Fisher examples) had an average accuracy of\n72:2.\nEXAMPLES 256 1024 2490\n256 72:7 72 :9 73 :1\n1024 72:9 72 :9 73 :3\n4096 72:9 73 :0 73 :2\n32768 72:8 73 :0 73 :5\n392702 72:9 73 :1 73 :4\n16 background and\ndetail our Fisher merging procedure. Section 3 provides experimental conclusions and thoughts on future work in section 5.\n2 Weighted Parameter Averaging for Model Merging\nOur focus is on procedures for model merging , i.e. averaging the parameters of models that share\nan architecture and initialization. In this section, we \ufb01rst frame the common practice of averaging\ntogether model parameters as approximately maximizing the joint likelihood of model posteriors.\nSpeci\ufb01cally, we show that parameter averaging corresponds to using an isotropic Gaussian as the\napproximate posterior for each model. We then introduce Fisher merging , which uses the model\u2019s\ndiagonal Fisher information matrix as the precision matrix of the Gaussian approximate posterior.\nFisher merging can be implemented by setting each merged parameter value to a weighted average of\nthe corresponding parameter values from the original models, with the weighting for each parameter\ndetermined by its Fisher information. In addition, we add model-level weightings as additional\nhyperparameters to set the relative importance of each model.\n2.1 Isotropic merging\nConsider the problem setting where we have Mtrained neural networks with parameters \u00121;:::;\u0012M\nand our goal is to create a single neural network with parameters \u0012that, loosely speaking, inherits\nthe capabilities of the Mtrained neural networks. Assume that all of these neural networks share a\ncommon architecture and had the same set of initial parameter values before being trained. Merging\nattacks this problem by \ufb01nding the parameters \u0012that maximize the", " Introduction\nIn recent years, large language models have demonstrated impressive skills\nacross many diverse tasks (Wang et al., 2019; Brown et al., 2020). Kaplan\net al. (2020) describe the consistent bene\fts of increasing model size, character-\nizing scaling trends that hold across many orders of magnitude. However, even\nthe largest models falter when required to perform multi-step mathematical rea-\nsoning (Hendrycks et al., 2021). Model samples frequently contain catastrophic\nmistakes, even after the model has been appropriately \fnetuned. Mathematical\nreasoning thus reveals a critical weakness in modern language models.\nOne signi\fcant challenge in mathematical reasoning is the high sensitivity\nto individual mistakes (Shen et al., 2021a). When generating a solution, au-\ntoregressive models have no mechanism to correct their own errors. Solutions\nthat veer o\u000b-course quickly become unrecoverable. If we rely purely on genera-\ntive methods that scale even\nbetter.\nAcknowledgements\nWe thank Dan Hendrycks, Leo Gao, Alec Radford, and Giambattista Paras-\ncandolo for their valuable feedback on this paper; Harri Edwards, Yura Burda,\nMichael Wu, and Nick Ryder for many insightful conversations; Michael Petrov,\nAlethea Power, and Jacob Jackson for their technical assistance; the OpenAI\nSupercomputing team for the infrastructure that made these results in this\npaper, had some minor implementation bugs. Our reported test performance\nis therefore a slight underestimate, though the magnitude of this discrepancy is\nless than 1% in most Appendix F for a\nvisualization of veri\fer con\fdence.\n4.3 Veri\fcation Ablations\nWe can either train veri\fers to make a single scalar prediction conditioned on\nthe entire generated solution, or to make a scalar prediction after each token\nin the solution. By default, we choose the latter, training veri\fers to make\npredictions after each token. This can be viewed as a token-level value function.\nWe compare these two Related Work\n3.1 Related Datasets\nEarly math word problem datasets (Kushman et al., 2014; Roy and Roth, 2015)\nare relatively small and are not well suited for testing the limits of modern lan-\nguage models. Dolphin18K (Huang et al., 2016) is a larger dataset containing\n318K problems, but solutions are provided only in the form of equations or \f-\nnal answers. AQuA-RAT (Ling et al., 2017) contains 100K problems, but this\ndataset unfortunately su\u000bers from both a high degree of problem templatiza-\ntion and poor quality control of the natural language solutions. MathQA is\na recently released subset of AQuA-RAT focused on correcting these mistakes\n(Amini et al., 2019), but even the corrected dataset has data quality issues, with\naround 30% of the data having inconsistencies (Miao et al., 2021). Ape210K\n(Zhao et al., 2020) is the largest publicly available dataset, consisting of 210K\nChinese elementary school-level math problems. However, due to the language\nbarrier and the lack of natural language solutions, we're unable to evaluate our background color of the text corresponds to the ver-\ni\fer score for that token, where red is low value (predicted incorrect) and green\n21is high value (predicted correct). The second column of the table summarizes\nthe veri\fer's prediction, and the third column indicates whether the generated\nmodel completion was actually correct or incorrect. Any disagreement between\nthe second and third columns indicates that the veri\fer made an error.\nThe \frst row includes a true positive example, where the veri\fer correctly\nclassi\fes the completion as correct. Note that the model is initially unsure about\nwhether the solution is correct and gradually gains certainty as the solution\nprogresses:", " Introduction\nScalable sequence prediction models (Graves, 2014;\nVaswani et al., 2017; Child et al., 2019) have become a\ngeneral-purpose method for generation and representation\nlearning in many domains, including natural language pro-\ncessing (Mikolov et al., 2013; Sutskever et al., 2014; Dai &\nLe, 2015; Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018), computer vision (Van Oord et al., 2016; Menick\n& Kalchbrenner, 2018; Chen et al., 2020; Bao et al., 2021),\naudio and speech processing (Oord et al., 2016; 2018; Dhari-\nwal et al., 2020; Baevski et al., 2020), biology (Alley et al.,\n2019; Rives et al., 2021), and even across multiple modali-\nties (Das et al., 2017; Lu et al., 2019; Ramesh et al., 2021;\nZellers et al., 2021). More recently, language models have\nalso fueled progress towards the longstanding challenge\nof program synthesis (Simon, 1963; Manna & Waldinger,\n1971), spurred by the presence of code in large datasets\n(Husain et al., 2019; Gao et al., 2020) and the resulting pro-\ngramming capabilities of language models trained on these\ndatasets (Wang & Komatsuzaki, 2021). Popular language\nmodeling objectives like masked language modeling (Devlin\net al., 2018) and span prediction (Raffel et al., 2020) have\nalso been adapted to train their programming counterparts\nCodeBERT (Feng et al., 2020) and PyMT5 (Clement et al.,\n2020).\nSimilarly, our early investigation of GPT-3 (Brown et al.,\n2020) revealed that it could generate simple programs from\nPython docstrings. While rudimentary, this capability was\nexciting because GPT-3 was not explicitly trained for code\ngeneration. Given the considerable success of large lan-\nguage models in other modalities and the abundance of\npublicly available code, we hypothesized that a specialized\nGPT model, called Codex, could excel at a variety of coding\ntasks. This paper describes several early Codex models,\nwhose descendants power GitHub Copilot and the Codex\nmodels in the OpenAI API.arXiv:2107.03374v2  [cs.LG]  14 Jul 2021Evaluating Large Language Models Trained on Code\nFigure 1. Pass rates of our models on the HumanEval dataset as a\nfunction of model size. When a single sample is generated for each\nproblem, GPT-12B solves no problems, but Codex (\ufb01ne-tuned\non code) solves 28.8% of the problems, and Codex-S (further\n\ufb01ne-tuned on correctly implemented standalone functions) solves\n37.7% of the problems. From here, further gains can be realized by\ngenerating 100 samples per problem and selecting the sample with\nthe highest mean log-probability (44.5% solved) or by selecting\nthe sample that passes the unit tests (77.5% solved). All samples\nare generated with temperature 0.8.\nIn this work, we focus on the task of generating stan-\ndalone Python functions from docstrings, and evaluate the\ncorrectness of code samples automatically through unit\ntests. This is in contrast to natural language generation,\nwhere samples are typically evaluated by heuristics or by\nhuman evaluators. To accurately benchmark our model,\nwe create a dataset of 164 original programming problems\nwith unit tests. These problems assess language compre-\nhension, algorithms, and simple mathematics, with some\ncomparable to simple software interview questions. We\nrelease this data along with an evaluation framework at\nhttps://www.github.com/openai/human-eval.\nTo solve a problem in our test set, we generate multiple\nsamples from the models, and check if any of them pass the\nunit tests. With just a single sample, a 12B parameter Codex\nsolves 28.8% of these problems, and a 300M parameter\nCodex solves 13.2% of these problems. In contrast, the 6B\nparameter GPT-J (Wang & Komatsuzaki, 2021) achieves\n11.4% on the same", " Introduction and Motivating Work\nPre-training methods in natural language processing , pp.\n1527\u20131536, 2017.\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\nGerman Traf\ufb01c Sign Recognition Benchmark: A multi-\nclass classi\ufb01cation competition. In IEEE International\nJoint Conference on Neural Networks , pp. 1453\u20131460,\n2011.\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\nand Schmid, C. Learning video representations from tex-\ntual web supervision. arXiv preprint arXiv:2007.14937 ,\n2020.\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi,\nA. Inception-v4, inception-resnet and the impact\nof residual connections on learning. arXiv preprint\narXiv:1602.07261 , 2016.\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\nencoder representations from transformers. arXiv preprint\narXiv:1908.07490 , 2019.\nTan, M. and Le, Q. V . Ef\ufb01cientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946 , 2019.\nTaori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,\nand Schmidt, L. Measuring robustness to natural dis-\ntribution shifts in image classi\ufb01cation. arXiv preprint\narXiv:2007.00644 , 2020.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM , 59(2):64\u201373, 2016.Learning Transferable Visual Models From Natural Language Supervision 35\nTian, Y ., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. arXiv preprint arXiv:1906.05849 , 2019.\nTian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and\nIsola, P. Rethinking few-shot image classi\ufb01cation: a\ngood embedding is all you need? arXiv preprint\narXiv:2003.11539 , 2020.\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\nimages: A large data set for nonparametric object and\nscene recognition. IEEE transactions on pattern analysis\nand machine intelligence , 30(11):1958\u20131970, 2008.\nTouvron, H., Vedaldi, A., Douze, M., and J \u00b4egou, H. Fix-\ning the train-test resolution discrepancy. In Advances in\nneural information processing systems , pp. 8252\u20138262,\n2019.\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\nanalysis and abnormality detection. In 2009 IEEE 12th\nInternational Conference on Computer Vision Workshops,\nICCV Workshops , pp. 1338\u20131345. IEEE, 2009.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems , pp. 5998\u20136008, 2017.\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\nWelling, M. Rotation equivariant CNNs for digital pathol-\nogy. June 2018.\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, \u02d9I.,\nFeng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n1.0: Fundamental Algorithms for Scienti\ufb01c Computing\nin Python. Nature results reported\nin Taori et al. (2020)\u2019s evaluation suite. Zero-shot CLIP im-\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\nBB. CLIP\u2019s improvements are largest on ImageNet-Vid and\nYoutube-BB due to its \ufb02exible zero-shot capability and on\nImageNet-R, which likely re\ufb02ects CLIP\u2019s pre-training dis-\ntribution including signi\ufb01cant amounts of creative content.\nA similar behavior has been documented for the Instagram\npre-trained ResNeXt models as discussed in Taori et al.\n(2020).Learning Transferable Visual Models From Natural Language Supervision 48\nF. Model Hyperparameters\nHyperparameter Value\nBatch size", "ABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classi\ufb01cation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellentresults from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14\u000214as opposed to 224\u0002224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speci\ufb01c pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).\n0 5 10 15 20\nNetwork depth (layer)020406080100120Mean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\n0 5 10 15 20\nNetwork depth (layer)020406080100120\nR50x1 + ViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels.\nD.5 E MPIRICAL COMPUTATIONAL COSTS\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\n18Published as a conference paper at ICLR 2021\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor.\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions.\nAnother quantity of interest is the largest batch-size each model can \ufb01t onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-ef\ufb01ciency over ResNet\nmodels.\n64 128 224 384 512\nInput size [px]102103104Peak inference speed [img/sec/core]64 128 224 384 512\nInput size [px]102103Largest per-core batch-sizeR50x1\nR50x2ViT-B/32\nViT-L/32ViT-B/16\nViT-L/16ViT-H/14\nR152x4\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right : Largest per-core batch-size \ufb01tting on device with\nvarious", "Abstract \u2014Neural architecture search (NAS) has attracted a lot of attention and has been illustrated to bring tangible bene\ufb01ts in a large\nnumber of applications in the past few years. Architecture topology and architecture size have been regarded as two of the most\nimportant aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both\nof those aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under\ndifferent search spaces and training setups. This makes the overall performance of the algorithms incomparable and the improvement\nfrom a sub-module of the searching model unclear. In this paper, we propose NATS-Bench, a uni\ufb01ed benchmark on searching for both\ntopology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates\nfor architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of\nvarious criteria and performance comparison of all candidates in the search space. We also show the versatility of NATS-Bench by\nbenchmarking 13 recent state-of-the-art NAS algorithms on it. All logs and diagnostic information trained using the same setup for\neach candidate are provided. This facilitates a much larger community of researchers to focus on developing better NAS algorithms in\na more comparable and computationally effective environment. All codes are publicly available at:\nhttps://xuanyidong.com/assets/projects/NATS-Bench.\nIndex Terms \u2014Neural Architecture Search, Benchmark, Deep Learning\nF\n1 I NTRODUCTION\nTHEdeep learning community is undergoing a transition\nfrom hand-designed neural architectures [1], [2], [3] to\nautomatically designed neural architectures [4], [5], [6], [7],\n[8]. In its early stages, the great success of deep learning was\npromoted by the introductions of novel neural architectures,\nsuch as ResNet [1], Inception [3], VGGNet [9], and Trans-\nformer [10]. However, manually designing one architecture\nrequires human experts to frequently try and evaluate nu-\nmerous different operation and connection options [4]. In\ncontrast to architectures that are manually designed, those\nautomatically found by neural architecture search (NAS)\nalgorithms require much less human interaction and ex-\npert effort. These NAS-generated architectures have shown\npromisingresults in\nNAS-Bench-1SHOT1. Therefore, though it is not guaran-\nteed, observations from our NATS-Bench have a potential\nto generalize to other search spaces.methods\nwith parameter sharing, we \ufb01nd that GDAS \u0015DARTS (2nd)\n\u0015DARTS (1st), which is also consistent withResults of weight-sharing basedBackground\nNAS aims to \ufb01nd architecture \u000bamong the search space S\nso that this found \u000bachieves a high performance on the\nFig. 5: Ranking stability of top 20% architectures on different\ndatasets over the topology search space St.\n(a) The Kendall rank correlation coef\ufb01cient for St.\n(b) The Kendall rank correlation coef\ufb01cient for Ss.\nFig. 6: We report the Kendall rank correlation coef\ufb01cient\nbetween the accuracy on 6 sets, i.e., CIFAR-10 validation\nset (C10-V), CIFAR-10 test set (C10-T), CIFAR-100 validation\nset (C100-V), CIFAR-100 test set (C100-T), ImageNet-16-120\nvalidation set (I120-V), ImageNet-16-120 test set (I120-T).\nvalidation set. This problem can be formulated as a bi-level\noptimization problem:\nmin\n\u000b2SL(\u000b;!\u0003\n\u000b;Dval) (1)\ns:t: !\u0003\n\u000b= arg min!L(\u000b;!;Dtrain);\nwhereLindicates the objective function (e.g., cross-entropy\nloss).Dtrain andDvaldenote the training data and the\nvalidation data, respectively. In the typical NAS setting,\nafter an architecture \u000bis found,\u000bwill be re-trained on\nDtrain (orDtrain +Dval) and evaluated on the test data\nDtestto \ufb01gure out its real performance.\n5.2 Experimental Setup\nWe evaluate 13recent, state-of-the-art searchingMethods\nThe weight-sharing basedexperiments. Part of this project was supported\nby Google Cloud Credits from", " Introduction\nThe success of deep learning in computer vision is in no\nsmall part due to the insight and engineering efforts of hu-\nman experts, allowing for the creation of powerful archi-\ntectures for widespread adoption (Krizhevsky et al., 2012;\nSimonyan & Zisserman, 2015; He et al., 2016; Szegedy\net al., 2016; Huang et al., 2017). However, this manual\ndesign is costly, and becomes increasingly more dif\ufb01cult\nas networks get larger and more complicated. Because of\nthese challenges, the neural network community has seen a\n1Usher Institute, University of Edinburgh2School of In-\nformatics, University of Edinburgh3School of Engineering,\nUniversity of Edinburgh. Correspondence to: Joseph Mellor\n<joe.mellor@ed.ac.uk >.\nProceedings of the 38thInternational Conference on Machine\nLearning , PMLR 139, 2021. Copyright 2021 by the author(s).shift from designing architectures to designing algorithms\nthatsearch for candidate architectures (Elsken et al., 2019;\nWistuba et al., 2019). These Neural Architecture Search\n(NAS) algorithms are capable of automating the discovery\nof effective architectures (Zoph & Le, 2017; Zoph et al.,\n2018; Pham et al., 2018; Tan et al., 2019; Liu et al., 2019;\nReal et al., 2019).\nNAS algorithms are broadly based on the seminal work\nof Zoph & Le (2017). A controller network generates an\narchitecture proposal, which is then trained to provide a\nsignal to the controller through REINFORCE (Williams,\n1992), which then produces a new proposal, and so on.\nTraining a network for every controller update is extremely\nexpensive; utilising 800 GPUs for 28 days in Zoph & Le\n(2017). Subsequent work has sought to ameliorate this by\n(i) learning stackable cells instead of whole networks (Zoph\net al., 2018) and (ii) incorporating weight sharing ; allow-\ning candidate networks to share weights to allow for joint\ntraining (Pham et al., 2018). These contributions have ac-\ncelerated the speed of NAS algorithms e.g. to half a day on\na single GPU in Pham et al. (2018).\nFor some practitioners, NAS is still too slow; being able to\nperform NAS quickly (i.e. in seconds) would be immensely\nuseful in the hardware-aware setting where a separate search\nis typically required for each device and task (Wu et al.,\n2019; Tan et al., 2019). This could be achieved if NAS\ncould be performed without any network training . In this\npaper we show that this is possible. We explore NAS-Bench-\n101 (Ying et al., 2019), NAS-Bench-201 (Dong & Yang,\n2020), NATS-Bench (Dong et al., 2021), and Network De-\nsign Spaces (NDS, Radosavovic et al., 2019), and examine\nthe overlap of activations between datapoints in a mini-batch\nfor an untrained network (Section 3). The linear maps of\nthe network are uniquely identi\ufb01ed by a binary code cor-\nresponding to the activation pattern of the recti\ufb01ed linear\nunits. The Hamming distance between these binary codes\ncan be used to de\ufb01ne a kernel matrix (which we denote by\nKH) which is distinctive for networks that perform well;\nthis is immediately apparent from visualisation alone across\ntwo distinct search spaces (Figure 1). We devise a score\nbased on KHand perform an ablation study to demonstrate\nits robustness to inputs and network initialisation.\nWe incorporate our score into a simple search algorithmarXiv:2006.04647v3  [cs.LG]  11 Jun 2021Neural Architecture Search without Training\n(a) NAS-Bench-201\n (b) NDS-DARTS\nFigure 1. KHfor a mini-batch of 128 CIFAR-10 images for untrained architectures in (a) NAS-Bench-201 (Dong & Yang, 2020) and (b)\nNDS-DARTS (Radosavovic et al., 2019). KHin these plots is normalised so that the diagonal entries", " Introduction\nDeveloping neural network image classi\ufb01cation models\noften requires signi\ufb01cant architecture engineering . Starting\nfrom the seminal work of [32] on using convolutional archi-\ntectures [17, 34] for ImageNet [11] classi\ufb01cation, succes-\nsive advancements through architecture engineering have\nachieved impressive results\nFinally, we will present examples of object detection re-\nsults on the COCO dataset in Figure 10 and Figure 11.\nAs can be seen from the \ufb01gures, NASNet-A featurization\nworks well with Faster-RCNN and gives accurate localiza-\ntion of objects.\nFigure 9. Architecture of NASNet-C convolutional cell with B=\n4blocks identi\ufb01ed with CIFAR-10. The input (white) is the hid-\nden state from previous activations (or input image). The output\n(pink) is the result of a concatenation operation across all result-\ning branches. Each convolutional cell is the result of Bblocks. A\nsingle block corresponds to two primitive operations (yellow) and\na combination operation (green).Figure 10. Example detections showing improvements of object\ndetection over previous state-of-the-art model for Faster-RCNN\nwith Inception-ResNet-v2 featurization [28] (top) and NASNet-A\nfeaturization (bottom).\nFigure 11. Example detections of best performing NASNet-A fea-\nturization with Faster-RCNN trained on COCO dataset. Top and\nmiddle images courtesy of http://wikipedia.org . Bottom\nimage courtesy of Jonathan Huang methods. Naively applying dropout [56]\nacross convolutional \ufb01lters degraded performance. How-\never, we discovered a new technique called ScheduledDrop-\nPath, a modi\ufb01ed version of DropPath [33], that works well\nin regularizing NASNets. In DropPath, we stochastically\ndrop out each path (i.e., edge with a yellow box in Figure\n4) in the cell with some \ufb01xed probability. This is simi-\nlar to [27] and [69] where they dropout full parts of their\nmodel during training and then at test time scale the path\nby the probability of keeping that path during training. In-\nterestingly we also found that DropPath alone does not help\nNASNet training much, but DropPath with linearly increas-\ning the probability of dropping out a path over the course\nof training signi\ufb01cantly improves the \ufb01nal performance for\nboth CIFAR and ImageNet Related Work\nThe proposed method is related to previous work in hy-\nperparameter optimization [44, 4, 5, 54, 55, 6, 40] \u2013 es-\npecially recent approaches in designing architectures such\nas Neural Fabrics [48], DiffRNN [41], MetaQNN [3] and\nDeepArchitect [43]. A more \ufb02exible class of Appendix\nA. Experimental Details\nA.1. Dataset for Architecture Search\nThe CIFAR-10 dataset [31] consists of 60,000 32x32\nRGB images across 10 classes (50,000 train and 10,000\ntest images). We partition a random subset of 5,000 images\nfrom the training set to use as a validation set for the con-\ntroller RNN. All images are whitened and then undergone\nseveral data augmentation steps: we randomly crop 32x32\npatches from upsampled images of size 40x40 and apply\nrandom horizontal \ufb02ips. This data augmentation procedure\nis common among Experiments\nWe now present two additional cells that performed well\non CIFAR and ImageNet. The search spaces used for these\ncells are slightly different than what was used for NASNet-\nA. For the NASNet-B model in Figure 8 we do not concate-\nnate all of the unused hidden states generated in the convo-\nlutional cell. Instead all of the hiddenstates created within\nthe convolutional cell, even if they are currently used, are\nfed into the next layer. Note that B= 4and there are 4 hid-\ndenstates as input to the cell as these numbers must match\nfor this cell to be valid. We also allow addition followed by\nlayer normalization [2] or instance normalization [61]", " Introduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient Background: Policy Optimization\n2.1 Policy Gradient Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At/bracketrightbigg\n(3)\nsubject to \u02c6Et[KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]]\u2264\u03b4. (4)\nHere,\u03b8oldis the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]/bracketrightbigg\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, results and learning curves for all 49 games is provided in Experiments\n6.1 Comparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIPto several natural variations and ablated versions.\nNo clipping or penalty: Lt(\u03b8) =rt(\u03b8)\u02c6At\nClipping: Lt(\u03b8) = min(rt(\u03b8)\u02c6At,clip(rt(\u03b8)),1\u2212/epsilon1,1 +/epsilon1)\u02c6At\nKL penalty (\ufb01xed or adaptive) Lt(\u03b8) =rt(\u03b8)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old,\u03c0\u03b8]\n5For the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping ( /epsilon1)\nand the KL penalty ( \u03b2,dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe Results from continuous control benchmark. Average normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2was initialized at 1.\n6.2 Comparison to Other Algorithms in the", " INTRODUCTION  \nThe currently available instruments (e.g., multi/hy perspectral \n[1], synthetic aperture radar [2], etc.) for earth observation [3, 4] \ngenerate more and more different types of airborne or satellite \nimages with different resolutions (spatial resoluti on, spectral \nresolution, and temporal resolution). This raises a n important \ndemand for intelligent earth observation through re mote sensing  \nimages, which allows the smart identification and c lassification \nof land use and land cover (LULC) scenes from airbo rne or \nspace platforms [3]. Remote sensing image scene cla ssification, \nbeing an active research topic in the field of aeri al and satellite \nimage analysis, is to categorize scene images into a discrete set \nof meaningful LULC classes according to the image c ontents. \nDuring the past decades, remarkable efforts have be en made in \ndeveloping various conclusions are drawn in Section VI. \nII. A  REVIEW ON REMOTE SENSING IMAGE SCENE \nCLASSIFICATION DATASETS  \nIn the past years, several publicly available high resolution \nremote sensing image datasets [9, 11, 17, 33, 38, 8 2] have been \nintroduced by different groups to perform research for scene \nclassification and to evaluate different METHODS  \nCurrent Methods for Object-based Analysis and Classificatio n : Springer \nNetherlands, 2004. \n[76] L. Dr\u0103gu\u0163 and T. Blaschke, \u201cAutomated classifi cation of landform \nelements using object-based image analysis,\u201d Geomorphology,  vol. 81, \nno. 3, pp. 330-344, 2006. \n[77] C. Eisank, L. Dr\u0103gu\u0163, and T. Blaschke, \"A gene ric procedure for \nsemantics-oriented landform classification using ob ject-based image \nanalysis,\" in Geomorphometry , 2011, pp. 125-128. \n[78] G. J. Hay, T. Blaschke, D. J. Marceau, and A. Bouchard, \u201cA comparison \nof three image-object abstract level, we can learn very powerful rep resentations. \nThis has been proven in literatures [13, 134, 169-1 71].  \n2) CNNs:  CNNs are designed to process data that come in the  \nform of multiple arrays, for example a multi-spectr al image \ncomposed of multiple 2D arrays containing pixel int ensities in \nthe multiple band channels. Starting with the impre ssive success \nof AlexNet [163], many representative CNN models in cluding \nOverfeat [164], VGGNet [165], GoogLeNet [166], SPPN et \n[167], and ResNet [172] have been proposed in the l iterature. \nThere exist four key ideas behind CNNs that take ad vantage of \nthe properties of natural signals, namely, local co nnections, \nshared weights, pooling, and the use of many layers  [159].  \nThe architecture of a typical CNN is structured as a series of \nlayers. (i) Convolutional layers : They are the most important \nones for extracting features from images. The first  layers usually \ncapture low-level features (like edges, lines and c orners) while \nthe deeper layers are able to learn more expressive  features (like \nstructures, objects and shapes) by combining low-le vel ones. (ii) \nPooling layers : Typically, after each convolutional layer, there \nexist pooling layers that are created by computing some local \nnon-linear operation of a particular feature over a  region of the \nimage. This process ensures that the same result ca n be obtained, \neven when image features have small translations", " Introduction\nIncreasingly, phones and tablets are the primary computing\ndevices for many people [ 30,2]. The powerful sensors on\nthese devices (including cameras, microphones, and GPS),\ncombined with the fact they are frequently carried, means\nthey have access to an unprecedented amount of data, much\nof it private in nature. Models learned on such data hold the\nAppearing in Proceedings of the 20thInternational Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS) 2017, Fort Laud-\nerdale, Flordia, USA. JMLR: W&CP volume 54. Copyright 2017\nby the authors.promise of greatly improving usability by powering more\nintelligent applications, but the sensitive nature of the data\nmeans there are risks and responsibilities to storing it in a\ncentralized location.\nWe investigate a learning technique that allows users to\ncollectively reap the bene\ufb01ts of shared models trained from\nthis rich data, without the need to centrally store it. We term\nour approach Federated Learning , since the learning task is\nsolved by a loose federation of participating devices (which\nwe refer to as clients ) which are coordinated by a central\nserver . Each client has a local training dataset which is\nnever uploaded to the server. Instead, each client computes\nan update to the current global model maintained by the\nserver, and only this update is communicated. This is a\ndirect application of the principle of focused collection or\ndata minimization proposed by the 2012 White House report\non privacy of consumer data [ 39]. Since these updates are\nspeci\ufb01c to improving the current model, there is no reason\nto store them once they have been applied.\nA principal advantage of this approach is the decoupling of\nmodel training from the need for direct access to the raw\ntraining data. Clearly, some trust of the server coordinat-\ning the training is still required. However, for applications\nwhere the training objective can be speci\ufb01ed on the basis\nof data available on each client, federated learning can sig-\nni\ufb01cantly reduce privacy and security risks by limiting the\nattack surface to only the device, rather than the device and\nthe cloud.\nOur primary contributions are 1) the identi\ufb01cation of the\nproblem of training on decentralized data from mobile de-\nvices as an important research direction; 2) the selection of\na straightforward and practical algorithm that can be applied\nto this setting; and 3) an extensive empirical evaluation of\nthe proposed approach. More concretely, we introduce the\nFederatedAveraging algorithm, which combines lo-\ncal stochastic gradient descent (SGD) on each client with\na server that performs model averaging. We perform ex-\ntensive results on a variety of model architectures: a multi-layer\nperceptron, two different convolutional NNs, a two-layer\ncharacter LSTM, and a large-scale word-level LSTM.\nWhile federated learning offers many practical privacy ben-\ne\ufb01ts, providing stronger guarantees via differential pri-\nvacy [ 14,13,1], secure multi-party computation [ 18], or\ntheir combination is an interesting direction for future work.\nNote that both classes of techniques apply most naturally to\nsynchronous algorithms like FedAvg .8\n8Subsequent to this work, Bonawitz et al. [6]introduced an\nef\ufb01cient secure aggregation protocol for federated learning, and\nKone \u02c7cn\u00b4y et al. [23] presented algorithms for further decreasing\ncommunication costs.H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag \u00a8uera y Arcas Related Work Distributed training by iteratively averag-\ning locally trained models has been studied by McDon-\nald et al. [28] for the perceptron and Povey et al. [31] forspeech recognition DNNs. Zhang et al. [42] studies an", " Introduction\nRecently visual attributes have raised signi\ufb01cant inter-\nest in the community [6, 11, 17, 25]. A \u201cvisual attribute\u201d\nis a property of an object that can be measured visually and\nhas a semantic connotation, such as the shape of a hat or the\ncolor of a ball. Attributes allow characterizing objects in far\ngreater detail than a category label and are therefore the key\nto several advanced applications, including understanding\ncomplex queries in semantic search , learning about objects\nfrom textual description , and accounting for the content of\nTuesday, October 29, 13Figure 1: Both the man-made and the natural world are\nan abundant source of richly textured objects. The textures\nof objects shown above can be described (in no particular\norder) as dotted, striped, chequered, cracked, swirly, hon-\neycombed, and scaly. We aim at identifying these attributes\nautomatically and generating descriptions based on them.\nimages in great detail. Textural properties have an important\nrole in object descriptions, particularly for those objects that\nare best quali\ufb01ed by a pattern, such as a shirt or the wing of\nbird or a butter\ufb02y as illustrated in Fig. 1. Nevertheless, so\nfar the attributes of textures have been investigated only tan-\ngentially. In this paper we address the question of whether\nthere exists a \u201cuniversal\u201d set of attributes that can describe a\nwide range of texture patterns, whether these can be reliably\nestimated from images, and for what tasks they are useful.\nThe study of perceptual attributes of textures has a\nlong history starting from pre-attentive aspects and group-\ning [16], to coarse high-level attributes [1, 2, 33], to some\nrecent work aimed at discovering such attributes by au-\ntomatically mining descriptions of images from the Inter-\nnet [3, 12]. However, the texture attributes investigated so\nfar are rather few or too generic for a detailed description\nmost \u201creal world\u201d patterns. Our work is motivated by the\none of Bhusan et al. [5] who studied the relationship be-\ntween commonly used English words and the perceptual\nproperties of textures, identifying a set of words suf\ufb01cient\nto describing a wide variety of texture patterns. While they\nstudy the psychological aspects of texture perception, thearXiv:1311.3618v2  [cs.CV]  15 Nov 2013banded\n blotchy\n braided\n bubbly\n bumpy\n chequered\n cobwebbed\n cracked\n crosshatched\n crystalline\n dotted\n \ufb01brous\n\ufb02ecked\n freckled\n frilly\n gauzy\n grid\n grooved\n honeycombed\n interlaced\n knitted\n lacelike\n lined\n marbled\nmatted\n meshed\n paisley\n perforated\n pitted\n pleated\n polka-dotted\n porous\n potholed\n scaly\n smeared\n spiralled\nsprinkled\n stained\n strati\ufb01ed\n striped\n studded\n swirly\n veined\n waf\ufb02ed\n woven\n wrinkled\n zigzagged\nFigure 2: The 47 texture words in the describable texture dataset introduced in this paper. Two examples of each attribute\nare shown to illustrate the signi\ufb01cant amount of variability in the data.\nfocus of this paper is the challenge of estimating such prop-\nerties from images automatically.\nOur \ufb01rst contribution is to select a subset of 47 de-\nscribable texture attributes , based on the work of Bhusan\net al., that capture a wide variety of visual properties of\ntextures and to introduce a corresponding describable tex-\nture dataset consisting of 5,640 texture images jointly an-\nnotated with the 47 attributes (Sect. 2). In an effort to\nsupport directly real world applications, and inspired by\ndatasets such as ImageNet [10] and the Flickr Material\nDataset (FMD) [30], our images are captured \u201cin the wild\u201d\nby downloading them from the Internet rather than collect-\ning them in a laboratory. We also address the practical"], "bleu": 0.0, "rouge_l": 0.24897119341563784, "gpt_metric_score": 0.0, "bert_score": 0.17412403225898743}
