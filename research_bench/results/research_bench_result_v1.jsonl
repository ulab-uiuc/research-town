{"paper_key": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models", "current_5q": "**[Question 1] - What is the problem?**  \nHow can we effectively merge large pretrained models to create new models with enhanced generalization capabilities for multiple tasks while minimizing the need for extensive computational resources and high-quality data?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses the growing demand for versatile models that can perform well across various tasks without the prohibitive costs associated with fine-tuning large models. By advancing model merging techniques, we can democratize access to powerful AI tools, enabling smaller organizations and researchers to leverage state-of-the-art models. This could lead to significant advancements in fields such as natural language processing and computer vision, fostering innovation and practical applications in diverse domains.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem include the complexity of effectively integrating knowledge from multiple pretrained models without losing performance or introducing interference. Naive approaches may fail due to the intricate relationships between model parameters and the potential for negative transfer, where merging leads to degraded performance. Additionally, technical obstacles such as ensuring compatibility between different model architectures and managing the computational overhead of merging processes complicate the task.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on individual model training or fine-tuning, overlooking the potential of model merging as a viable alternative. Limitations in understanding the dynamics of knowledge transfer between models and the lack of robust methodologies for merging have hindered progress. Existing solutions may not adequately address the interference issues that arise during merging. Our approach aims to fill these gaps by introducing novel techniques that enhance the merging process, ensuring better performance and generalization.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves developing a systematic framework for model merging that utilizes a diverse set of pretrained models. We will employ a dataset comprising various tasks to evaluate the merged models' performance. The key metrics for assessment will include accuracy, generalization ability, and computational efficiency. We expect our approach to yield merged models that outperform existing solutions in terms of versatility and performance across multiple tasks, demonstrating the effectiveness of our merging techniques.", "proposal_5q": "[Question 1]: What is the problem?  \nThe specific research question I aim to address is: How can a multi-agent AIoT system that integrates evolutionary algorithms and optimized model merging techniques enhance adaptive auditory processing for monitoring elderly patients with dementia?\n\n[Question 2]: Why is it interesting and important?  \nThe implications of solving this problem are significant for both the research community and healthcare practitioners. Developing an advanced auditory monitoring system for dementia patients can lead to improved patient outcomes by enabling timely interventions based on real-time auditory cues. This research could advance knowledge in the fields of artificial intelligence, Internet of Things (IoT), and healthcare technology by demonstrating how spiking neural networks can adaptively process complex auditory environments. Furthermore, it could drive future research into ethical AI applications, particularly in sensitive areas like healthcare, ensuring patient data privacy and transparency in decision-making. The practical applications of this research could revolutionize how we monitor and care for individuals with dementia, ultimately enhancing their quality of life and providing caregivers with more effective tools.\n\n[Question 3]: Why is it hard?  \nSolving this problem is challenging due to several complexities inherent in integrating multiple technologies. The dynamic nature of the auditory environment requires sophisticated algorithms that can adapt in real-time, which is not straightforward with existing models. Naive approaches may fail to account for the diverse and unpredictable soundscapes that dementia patients encounter, leading to ineffective monitoring and responses. Additionally, the computational efficiency required for real-time processing of auditory inputs complicates the architecture of spiking neural networks, particularly when integrating evolutionary algorithms and model merging techniques. The need for ethical considerations in AI further adds a layer of complexity, as balancing innovation with patient privacy and transparency is crucial for acceptance and implementation in healthcare settings.\n\n[Question 4]: Why hasn't it been solved before?  \nPrevious research has often focused on either auditory processing or AIoT systems individually, lacking an integrated approach that combines the strengths of both. Existing solutions have not effectively addressed the unique challenges posed by the auditory environments experienced by dementia patients, often relying on static models that fail to adapt in real-time. Barriers such as limited datasets that capture the nuances of auditory stimuli in dementia care, as well as insufficient focus on ethical AI practices, have hindered progress. My approach differs by proposing a collaborative network of sensors that not only adapts through evolutionary algorithms but also utilizes model merging and parameter pruning to optimize performance, addressing the limitations of prior work directly.\n\n[Question 5]: What are the key components of my approach and results?  \nMy proposed methodology involves developing a multi-agent system utilizing spiking neural networks that dynamically adjust their architecture based on real-time auditory feedback. This will be achieved by employing evolutionary algorithms to optimize model merging techniques, allowing for collaborative processing of auditory data from multiple sensors. The dataset will consist of diverse audio recordings representative of environments encountered by dementia patients, annotated for various sounds and their relevance to patient behavior. Metrics such as prediction accuracy, response time, and computational efficiency will be used to evaluate system performance. Expected outcomes include enhanced predictive capabilities for monitoring patient behavior through improved auditory processing, while adhering to ethical AI guidelines to ensure patient data privacy and transparency. This innovative approach aims to create a robust, adaptive system capable of significantly improving the care and monitoring of elderly patients with dementia.", "referenced_intros": [" Introduction\nInteractive systems based on general-purpose\nLLMs have become widely popular due to their\nimpressive instruction-following capabilities (Ope-\nnAI, 2023). Furthermore, tuning these models on\ndownstream tasks has been shown to transform\nthem into domain experts (Rozi\u00e8re et al., 2023;\nLuo et al., 2023).\nMaintaining separate fine-tuned models for each\ntask presents several limitations, such as a signif-\nicantly higher memory footprint and the inability\nto leverage information across tasks, which could\nenhance both in-domain and out-of-domain perfor-\nmance. As a result, merging different homologousmodels (models fine-tuned from the same back-\nbone) is gaining traction for its cost-effectiveness,\nknowledge sharing, and space efficiency (Yadav\net al., 2024; Yu et al., 2023). The homologous\nmodels differ from each other in terms of delta pa-\nrameters, i.e., the difference between the fine-tuned\nmodel and backbone model parameters.\nIn this paper, we introduce a novel approach\nfor merging homologous models, termed Drop and\nrEscaLe via samp Ling with m Agnitude ( DELLA ).\nThis approach consists of three steps: (Step-1) in-\nvolves delta parameter drops to reduce interfer-\nence among model parameters. We propose MAG-\nPRUNE , a novel pruning method that samples delta\nparameters based on their magnitudes; (Step-2) fur-\nther reduces interference through sign-based delta\nparameter selection; and (Step-3) fuses the selected\ndelta parameters.\nOn three different homologous (expert) mod-\nels considered for merging (LM, Math, Code) and\ntheir corresponding benchmark datasets (AlpacaE-\nval, GSM8K, MBPP), DELLA outperforms base-\nline Experiments\nWe compare the performance of DELLA against\ntheDARE baseline to show that magnitude sam-\npling improves the selection of delta parameters\nto retain and better maintain the model\u2019s task per-\nformance. We vary the drop rate pin [0.3, 0.5,\n0.7, 0.8, 0.9, 0.91, 0.92, 0.93, 0.94] and apply the\nDARE andDELLA to get models after removing the\nproportion of delta parameters. We then evaluate\nthe model\u2019s performance on its corresponding SFT\ntask. Table 6 shows the comparison between DARE,\nrandom ranking and MAGPRUNE . We performed Results\nA.3 Pruning Rate Hyperparameter Search\nFor Model Merging\nTable 7 shows the results of the pruning rate hy-\nperparameter search for each merging combination.\nWhile both MAGPRUNE andDARE can maintain\nthe performance of individual expert model per-\nformance up to a high drop rate of 0.9, our find-\nings indicate that a drop rate of 0.5, works best\nfor LM+Math, Math+Code and LM+Math+Code.\nFor LM+Code, a drop rate of 0.7 is optimal. Thus,\nwe can infer that while dropping delta parameters\nhelps reduce interference during merging, drop-\nping too many parameters may lead to the loss ofinformation useful for effective merging.\nModels Drop rate AlpacaEval GSM8K MBPP Average\nLM +\nMath0.1 0.805 0.599 / 0.702\n0.3 0.812 0.629 / 0.721\n0.5 0.804 0.645 / 0.724\n0.7 0.787 0.611 / 0.699\n0.9 0.683 0.455 / 0.570\nLM +\nCode0.1 0.741 / 0 0.370\n0.3 0.770 / 0 0.385\n0.5 0.802 / 0.152 0.477\n0.7 0.798 / 0.34 0.569\n0.9 0.737 / 0.262 0.500\nMath +\nCode0.1 / 0.619 0.166 0.393\n0.3 / 0.618 0.184 0.401\n0.5 / 0.626 0.206 0.416\n0.7 / 0.633 0.19 0.412\n0.9 / 0.622 0.128 0.375\nLM +\nMath +\nCode0.1 0.732 0.545 0.114 0.464\n0.3 0.766 0.623 0.302 0.564\n0.5 0.794 0.630 0.3 0.575\n0.7 0.770 0.622 0.23 0.541\n0.9 0.688 0.446 0.128 0.421\nTable 7: Drop Rate of parameters against Task perfor-\nmance Appendix\nA.1 Importance of GPT4-as-a-judge for Math\ntasks - Example\nQuestion: Each person in a certain\nhousehold consumes 0.2 kg of rice ev-\nery meal. Supposing 5 members of the\nhousehold eat rice every lunch and din-\nner, how many weeks will a 42 kg bag of\nrice last?\nGenerated Answer: 1.", " Introduction\nIn recent years, Large Language Models (LLMs) have demonstrated notable success across various\nNatural Language Processing (NLP) tasks [ 9,49,52], including code generation [ 17,44], solving\nmath problems [ 2,35], multilingualism [ 38],etc.These models, with billions of parameters, excel\nin various downstream tasks [ 19,27,56] but require extensive training on large datasets using\nthousands of GPUs. The considerable computational and energy costs [ 43] limit their specialization\nand deployment in resource-constrained environments [30].\nTo tackle this challenge, model fusion has emerged as a promising solution [ 29]. One notable\nparadigm is model merging [ 22,26,59,60], where multiple task-specific models, or \u201cexperts\u201d, are\ncombined into a single unified model. This unified model can quickly adapt to new tasks without\nthe need to retrain a large model. Various techniques, such as parameter averaging [ 5,58], weight\n\u2217Equal contribution.\n\u2020Corresponding authors.\n1Our implementation is available in https://github.com/LZY-the-boys/Twin-Merging\nPreprint. Under review.arXiv:2406.15479v1  [cs.CL]  17 Jun 2024(I) Conventional Merging. . .\nPretrained Task-Specific Expert \n(II) + Knowledge Disentanglement. . .\nTask-Specific Expert    Shared\nKnowledge. . .Exclusive \nKnowledge\nSVD Compression\n(III) + Dynamical MergingRouter. . .Merged Merged\nInput\nHiddenExclusive\nKnowledge\n   Shared\nKnowledgeFigure 1: Subfigure (I) shows that in conventional merging results shown in Table 9.\n18Table 8: The detail statistics of different merging performance on 8 discriminative tasks. Bold\nnumbers indicate the best-averaging performance across different model merging Related Work\nIn this section, we focus on model merging research, for additional related work on multi-task learning\nand Mixture of Experts, please see Appendix E attachs detail qualtivie analysis of various Merging Experiments\n4.1 Merging Experiment\nBaselines We compare Twin-Merging with several train-free model-merging Results\nIn Table 2, we present only the average normalized scores across various tasks. In this section, we\ndetail the statistical performance of all tasks, with discriminative Conclusions\nIn this paper, we introduce the Twin-Merging to merge language models, aiming to close the\nperformance gap between conventional model merging techniques and fine-tuned models, while\nimproving adaptability to data heterogeneity. By modularizing and dynamically merging shared and\ntask-specific knowledge, Twin-Merging significantly outperforms existing model-merging References\n[1]Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models\nmodulo permutation symmetries. In The Eleventh International Conference on Learning\nRepresentations , 2023.\n[2]Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2024.\n[3]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu,\nGao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren,\nChuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu,\nBenfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,\nHongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang,\nChang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023.\n[4]Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gra-\ndient normalization for adaptive loss balancing in deep multitask networks. In International\nconference on machine learning , pages 794\u2013803. PMLR, 2018.\n[5]Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for\nbetter pretraining, 2022.\n[6]Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\nHoffmann, Bogdan Damoc,", " Introduction\nLargelanguagemodels(LLMs)areincreasinglyexcellingatvariousnaturallanguageprocessingtasks,\nincluding text generation [ 11], translation [ 45,50], summarization [ 22], code generation [ 20,33],\nand chatbot interaction [ 28]. With the rising capability, the need for a robust evaluation strategy\nthatcanaccuratelyassesstheperformanceofthesemodelsisbecomingcrucialinordertoidentify\ntheirtrueeffectivenessandchoosethemostappropriateoneforagiventask. Commonmetricsfor\nassessingLLMstodayincluderelevance,frequencyofhallucinations,accuracyinquestionanswering,\ntoxicity,andretrieval-specificmetrics,amongothers. Inthecontextofquestion-answeringevaluations,\npriorworksusuallyinvestigatethemodel\u2019sperformanceintermsofansweraccuracy,courtesy,and\nconciseness. And multiple choice questions (MCQ) have emerged as a predominant format for\nsuch assessments, wherein a question is presented with several possible responses, and the model\nisrequiredtoselectthemostfittingchoiceID,asexemplifiedinFigure1. Lately,theMCQformat\n\u2217Joint first author & equal contribution.\n1For instance, on MMLU, the random guessing accuracy is 25%, and most small-scale LLMs obtain results across datasets, showcasing the percentages of questions that are\nsuitable (Yes) for open-style formatting and those that are not (No).\nTable 7: Example of an Incorrect MCQ and a Correct OSQ in MMLU.\nQuestion that is correct for OSQ and incorrect for MCQ: How many CO2 and ATP molecules\nare formed during one complete turn of the tricarboxylic acid cycle (Krebs\u2019 cycle)?\nOptions: A. 2CO2 and 2ATP B. 2CO2 and 16ATP C. 2CO2 and 12ATP D. 2CO2 and 1ATP\nGround Truth: D\nMCQ_Answer: A\nOSQ_Answer: During one complete turn of the tricarboxylic acid cycle (also known as\nthe Krebs\u2019 cycle or the citric acid cycle), a series of chemical reactions occur in\nthe mitochondria of cells that result in the production of carbon dioxide (CO2) and\nadenosine triphosphate (ATP), among other molecules.\nFor each acetyl-CoA molecule that enters the cycle, the following are produced:\n- 2 molecules of CO2: Each acetyl-CoA contributes two carbons to the cycle, which\nare eventually released as two molecules of carbon dioxide.\n- 1 molecule of ATP: One molecule of ATP (or an equivalent, such as GTP, depending\non the cell type) is produced directly in the cycle through substrate-level\nphosphorylation.\nHowever, the cycle also produces reduced coenzymes, specifically NADH and FADH2,\nwhich are used in the electron transport chain to produce additional ATP through\noxidative phosphorylation. For each acetyl-CoA that enters the cycle:\n- 3 molecules of NADH are produced, which can be used to generate approximately 2.5\nATP each through the electron transport chain.\n- 1 molecule of FADH2 is produced, which can be used to generate approximately 1.5\nATP through the electron transport chain.\nTherefore, while only 1 ATP is produced directly in the cycle per acetyl-CoA, the\nNADH and FADH2 generated can lead to the production of additional ATP in the electron\ntransport chain. The exact number of ATP molecules produced can vary depending on the\nefficiency of the electron transport chain and the ATP synthase complex in different\norganisms and cell types.\n15Table 8: Example of a Correct MCQ and an Incorrect OSQ in MMLU.\nQuestion that is correct for MCQ and incorrect for OSQ: A tree is a connected graph\nwith no cycles. How many nonisomorphic trees with 5 vertices exist?\nOptions: A. 1 B. 2 C. 3 D. 4\nGround Truth: C\nMCQ_Answer: C\nOSQ_Answer: To determine the number of nonisomorphic trees with 5 vertices, we can\nuse Cayley\u2019s formula, which states that the number of trees on \ud835\udc5blabeled vertices\nis\ud835\udc5b\ud835\udc5b\u22122. However, this formula gives the number of trees when vertices are labeled,\nmeaning that it counts different labelings of the same tree shape as distinct. Since\nwe are interested in nonisomorphic trees (which do not consider different labelings as\ndistinct), we cannot directly use Cayley\u2019s formula.\nInstead, we need to enumerate the trees by considering their possible structures. For\na tree with 5", " Introduction\nLarge pre-trained foundation models have become available in many real-world applications [ 60;54;\n13]. This increasing availability has led to a popular practice of fine-tuning these pre-trained models\nto adapt to a wide range of downstream tasks. Practitioners can independently fine-tune the same\npre-trained model, such as CLIP style models [ 45;63;69], large language models [ 6;47;55;28], etc.,\nand then release the fine-tuned models without releasing the training data. As the deployment of such\nfine-tuned models increases, combining models with identical architectures and initializations has\nemerged as a promising approach to combine their respective capabilities. This is useful, especially in\nscenarios where the training data for each task is private and cannot be shared, such as individual-level\npatient data in a hospital and behavior data in social media recommendation systems.\nExisting Related work This paper is related to many existing works on multi-objective optimization, Pareto\noptimality, task arithmetic, federated/private learning, Bayesian Appendix G, we generalize the ways to learn\nthe coefficients in (4), besides minimizing the mean square error in (5).\nG.2 Algorithm 2\nIn this section, we explain the operations of the algorithm in Figure 3 in details. Here task 1 to 8 is\nCars, GTSRB, DTD, SUN397, Resisc45, and SVHN. If we minimize (3)without the nested merging,\nwe would need to estimate A1, ...,A8\u2208R8\u00d78, with hundreds of c.\nWith the nested merging, for the first round, we merge (\u03b81\nft,\u03b82\nft)into\u03b81,2\nmerge, thus approximating\nA1andA2\u2208R8\u00d78byA1[1 : 2 ,1 : 2] andA2[1 : 2 ,1 : 2] \u2208R2\u00d72, respectively. That is, we\nonly care about the interference between task 1 and 2, but not task 1 and 5. Similarly, we merge\n(\u03b83\nft,\u03b84\nft)into\u03b83,4\nmerge, and (\u03b85\nft,\u03b86\nft)into\u03b85,6\nmerge. Next, we merge (\u03b81,2\nmerge,\u03b83,4\nmerge)into\u03b81,2,3,4\nmerge , and\nfinally into \u03b81,2,3,4,5,6,7,8\nmerge .\nG.3 Algorithm 3\nAlgorithm 4 is a detailed version of Algorithm 3. Figure 11 includes illustration of our discretization\nmethod (how we create bins) in 2D and 3D decision variable ( c) space.\n230.0 0.2 0.4 0.6 0.8 1.0\nc1=rcos\n0.00.10.20.30.40.50.60.7c2=rsin\nDiscretization in 2D Polar Coordination System(a)\n (b)\nFigure 11: (a) Discretizing of two task scaling coefficients along the angular dimension in 2D polar\ncoordinate system; (b) Discretizing of three task scaling coefficients along the angular dimensions in\n3D spherical coordinate system;\nAlgorithm 4 Bayesian Adaptive of Surrogate Model\nRequire: Number of iterations J, Buffer B, Pretrained model \u03b8pre, Task vectors vn, Evaluators for\ntaskN,Mn(\u00b7), Discretization bin number K, sample size for every iteration nj,j= 0 toJ,\nBootstrap dropping rate \u03b1= 20% , Bootstrap sampling number Q= 30 .\n1:B \u2190 \u2205\n2:forj= 0toJdo\n3: ifj= 0then\n4: Sample n0scaling coefficients {ci}nj\ni=1from U([0,1]N)\n5: else\n6: Sample njscaling coefficients {ci}nj\ni=1based on the posterior distribution\n7: fori= 0tonjdo\n8: Merge the model \u03b8m(ci) =\u03b8pre+ci\u00b7vn\n9: Evaluate mn,i=Mn(\u03b8m(ci))\n10: B \u2190 B \u222a { (ci, mn,i)}\n11: Fit the quadratic approximation surrogate model \u02dcMnby learning A\u2217\nn,b\u2217\nn, e\u2217\nnin (5).\n12: Discretize the scaling coefficients along the angular dimensions in hyper-spherical coordinates\n(see figure 11 as examples)\n13: fork= 0toKdo\n14: Calculate the mean of L2loss between \u02dcMn(ci)andMt(ci), where ciare in bin k, denoted\nas mean k\n{Bootstrap to estimate the standard deviation of the losses.}\n15: forq= 0toQdo\n16: Randomly (uniformly) drop \u03b1scaling coefficient in bin k\n17: Calculate the mean of L2loss between \u02dcMn(ci)andMt(ci)with the rest points and\ndenoted with lq\n18: Calculate the standard deviation of the {lq}Q\nq=0and denoted as std k\n19: score k=mean k+1\n2stdk\n20: Calculate probability distribution across the discretized bins by score kas the", " Introduction\nWith the rapid development of deep learning, different model architectures [ 36,22,71,88] are\nproposed, along with multiple training strategies [ 89,86]. Pre-trained models\u2019 capabilities are\nenhanced, thus showing increasing significance [ 54,22,7,19]. Finetuning models on downstream\ntasks from a pre-trained model has become a standard paradigm in both NLP and vision fields [ 20,\n51,19,22,5,87], which usually leads to improved performance with less labeled data. With the\ndevelopment of open-source repositories such as Huggingface [ 79], timm [ 77], and torchvision [ 44],\nthe number of pre-trained and finetuned checkpoints exponentially rise. However, applying individual\nmodels to different tasks results, the proposed method suffers from several limitations. On the one\nhand, compared to existing Related Work\nModel Merging obtains a model using the existing task-specific model weights instead of training [ 33,\n30,84,85,66,90,46]. Simply averaging [ 80] usually causes severe performance degradation. Various Appendix F.\nTable 11: Multi-task performance when merging ViT-B/16 models on eight tasks. Methods SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Avg Acc\nIndividual\n2 Tasks 75.3 77.7 - - - - - - 76.5\n3 Tasks 75.3 77.7 96.1 - - - - - 83.0\n4 Tasks 75.3 77.7 96.1 99.7 - - - - 87.2\n5 Tasks 75.3 77.7 96.1 99.7 97.5 - - - 89.3\n6 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 - - 90.8\n7 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 99.7 - 92.1\n8 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 99.7 79.4 90.5\nTies-Merging\n2 Tasks 69.2 68.2 - - - - - - 68.7\n3 Tasks 69.2 68.0 78.9 - - - - - 72.0\n4 Tasks 68.9 67.9 79.4 86.0 - - - - 75.5\n5 Tasks 68.6 67.1 79.0 83.5 66.6 - - - 73.0\n6 Tasks 68.0 66.4 77.9 80.1 74.4 69.9 - - 72.8\n7 Tasks 66.6 65.7 75.7 76.7 81.0 69.2 96.4 - 75.9\n8 Tasks 64.8 62.9 74.3 78.9 83.1 71.4 97.6 56.2 72.4\nEMR-M ERGING (Ours)\n2 Tasks 78.9 76.1 - - - - - - 77.5\n3 Tasks 77.9 75.2 95.3 - - - - - 82.8\n4 Tasks 77.4 74.9 94.8 99.7 - - - - 86.7\n5 Tasks 77.2 74.2 94.7 99.7 97.1 - - - 88.6\n6 Tasks 76.4 73.4 94.2 99.7 97.0 98.5 - - 89.9\n7 Tasks 75.8 73.3 93.6 99.6 96.9 98.2 99.6 - 91.0\n8 Tasks 75.2 72.8 93.5 99.5 96.9 98.1 99.6 74.4 88.7\nTable 16: Sparsity (ratio of non-zero items) of the masks and the values of the rescalers when merging\nViTs on 8 vision tasks and RoBERTa models on 8 language tasks.\nSparsity SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD\nViT-B/32 0.7194 0.7121 0.7106 0.6994 0.7195 0.7062 0.7132 0.7058\nViT-L/14 0.6832 0.6699 0.6734 0.6579 0.6748 0.6444 0.6614 0.6620\nRescalers SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD\nViT-B/32 0.7489 0.7635 0.7489 0.7476 0.7962 0.7652 0.7981 0.7624\nViT-L/14 0.7656 0.7652 0.7537 0.7384 0.7874 0.7313 0.7763 0.7638\nSparsity CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\nRoBERTa 0.6264 0.6547 0.6498 0.6150 0.7620 0.7739 0.6243 0.5979\nRescalers CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\nRoBERTa 0.2458 0.4698 0.5033 0.2078 0.8891 0.8987 0.4683 0.1466\nD.6 Sparsity of masks and values of rescalers.\nWe show the sparsity of the masks and the values of the rescalers when merging eight ViTs and eight\nRoBERTa models in Tab. 16.\nE More visualization Results under different hyper-paramerter", " INTRODUCTION\nLarge Language Models (LLMs) are widely applied in various appli-\ncation scenarios due to their high intelligence, e.g., education [ 9],\nhealthcare [ 6], and autonomous driving [ 19]. However, LLMs are\nusually constrained by a knowledge ceiling, indicating limitations\nin accessing real-time data and information beyond their local stor-\nage capacity. For example, the training data of GPT-3.5 ( gpt-3.5-\nturbo-0125 )2is up to Sep. 2021. Therefore, efficient empowerment\nalgorithms for LLMs have become a hot research topic in recent\nyears, which could help the model developers to expand the knowl-\nedge boundaries of LLMs. A common approach to broaden the\ncapabilities of LLMs is to gather high-quality fine-tuning data and\nemploy high-performance model fine-tuning algorithms, such as\nLow-Rank Adaptation (LoRA) [ 11]. Nevertheless, the cost of data\ncollection and computational infrastructure are expensive.\nModel merging [ 2,3,12,20], one of the most cutting-edge light-\nweight model empowerment solutions, aims to merge multiple\nupstream expert models with specific inference task execution ca-\npabilities into a single merged model that simultaneously possesses\nmultiple abilities. The advantage of model merging algorithms lies\nin their independence from high-performance computing devices\n(e.g., GPUs) and the need for massive training data. Meanwhile, the\nmaintenance of the model parameter scale does not incur additional\ninference costs.\n1Our code is available at https://github.com/ThuCCSLab/MergeGuard.\n2https://platform.openai.com/docs/models/gpt-3-5-turbo.\n\ud835\udc40!\ud835\udc40\"\ud835\udc40#MergedModelUpstreamModels\nAttacker\nModelOwner\u201cHaveyoumergedmymodel?\u201d\nWatermark/FingerprintModelMerging\nSurvive?Figure 1: The IP protection experiments: Model Soups [ 20],\nTask Arithmetic [12], TIES-MERGING [22], and DARE [23].\nFirst of all, to verify model merging algorithms can indeed gener-\nate a merged LLM with multifunctionality, in Section 3.2, we merge\ntwo state-of-the-art open-source LLMs: LLaMA-2-7B-CHAT [ 18]\nand WizardMath-7B-V1.0 [ 16]. We regard the excellent safety align-\nment with LLaMA-2-7B-CHAT and math reasoning ability within\nWizardMath-7B-V1.0 as the target abilities to merge. According to\nthe experimental Appendix A) to\ncalculate the refusal rate for evaluating safety alignment.\n\u2022We use accuracy to evaluate the model performance on\nGSM8K. The prompt fed into WizardMath-7B-V1.0 is the\nofficially recommended prompt from [16]:\nBelow is an instruction that describes a task. Write a\nresponse that appropriately completes the request.Cong, et al.\nTable 2: The utility of the merged LLMs on different downstream tasks. We highlight the evaluation conclusion\nthat Instructional Fingerprint is more robust than Quantization\nWatermarking against model merging.\nAblation Study. To further evaluate the robustness of Instruc-\ntional Fingerprint under different hyper-parameter settings, we\nmerge LLaMA-2-7B-CHAT-Fingerprint with WizardMath-7B-V1.0\nby DARE-TIES, and set different values of \ud835\udc5dfor DARE. The CONCLUSION\nModel merging techniques have powerful application prospects\nbut also pose threats to model IP infringement. In this paper, we\nconduct the first robustness measurement on IP protection tech-\nniques for large language models in the context of model merging.\nWe discuss two model copyright protection techniques: Quantiza-\ntion Watermarking and Instructional Fingerprint. Additionally, we\nconsider various advanced model merging techniques, such as Task\nArithmetic, TIES-MERGING, and so on. We apply IP protection\ntechniques to one of the upstream expert LLMs and investigate\nwhether the model owner can still maintain copyright claims in\nthe merged model. Experimental REFERENCES\n[1]Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.\nTurning your weakness into a strength: Watermarking deep neural networks\nby backdooring. In 27th USENIX Security Symposium (USENIX Security) , pages\n1615\u20131631, 2018.\n[2]Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, and\nQiongkai Xu. Here\u2019s a free lunch: Sanitizing backdoored models with model\nmerge. CoRR abs/2402.19334 , 2024.\n[3]Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. Language models are\nhomer simpson! safety re-alignment", " Introduction\nPre-train/fine-tune paradigm [15,19,29,32,33] has proven to be a strong frame-\nwork for training models to reach state-of-the-art performance. This approach,\nespecially pivotal in fine-tuning pre-trained models, involves models acquiring\ngeneral knowledge during pre-training and task-specific knowledge during fine-\ntuning. How we perform a fine-tuning stage is crucial, affecting task performance\nand robustness against distribution shifts.\nRecent advancements, notably Model Soup [32], which merges weights from\nmultiple fine-tuned models trained under different training setups, have shown\nimpressive performance without increasing inference costs. This method is be-\nlieved to be effective because these models often reside in the same loss basin,\nand their merging results in a\nmore significant error in Gaussian distribution approximation. Consequently,\nthe overall performance under filter-wise merging is slightly inferior to layer-\nwise one.\nThesefindingsunderscoretheimportanceofaccuratelymodelingnoisedistri-\nbution in enhancing the performance of Model Stock. As our understanding and\nability to model this noise distribution improve, we anticipate further increases\nin the efficacy and robustness of our approach.28 Jang et al.\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00.10.20.30.40.5 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)CLIP ViT-L/14\n0102030405060708090 Angle (degree)Conv\nAttention\nBatchNorm\nClassifier\nBias\nDownsample\nAll\n0.000.020.040.060.080.100.12 Norm / sqrt (#. of elements)Conv\nAttention\nBatchNorm\nClassifier\nBias\nDownsample\nAll\n(b)CLIP ResNet50\n0102030405060708090 Angle (degree)Others\nConv\nMLP\nLayerNorm\nClassifier\nBias\nLayer scaler\nAll\n0.000.050.100.150.200.250.300.35 Norm / sqrt (#. of elements)Others\nConv\nMLP\nLayerNorm\nClassifier\nBias\nLayer scaler\nAll\n(c)OpenCLIP ConvNeXt\nFig.F: Layer-wise angle and norm across different model architectures. The\nangle and norm for CLIP ViT-L/14, CLIP ResNet50, and OpenCLIP ConvNeXt are\ndisplayed from top to bottom. These metrics demonstrate consistency regardless of the\nmodel type from left (first layer) to right (last layer). It is important to note that we\nalso depict the error bars for each layer in all figures, but they are not visible in most\nlayers due to the small standard deviation.Model Stock 29\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00.20.40.60.81.0 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)SGD optimizer\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n01234 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(b)SGD optimizer with momentum\nFig.G: Layer-wise angle and norm across different optimizers. Displayed from\ntop to bottom are the angle and norm for models trained with SGD and SGD with\nmomentum, respectively. These metrics demonstrate consistency regardless of the op-\ntimization strategy from left (first layer) to right (last layer).30 Jang et al.\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.000.020.040.060.080.100.12 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)Vanilla model (10 epochs + no augmentation)\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.0000.0250.0500.0750.1000.1250.1500.1750.200 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(b)+ longer epochs (16 epochs)\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.0000.0250.0500.0750.1000.1250.1500.1750.200 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(c)+ RRC\nFig.H: Layer-wise angle and norm across different augmentations. Displayed\nfrom top to bottom are the angle and norm for the vanilla model (10 epochs + no\naugmentation), +longer epochs (16 epochs), and +RRC. Each augmentation is applied\nincrementally. These metrics demonstrate consistency regardless of the augmentations\nfrom left (first layer) to right (last layer).Model Stock 31\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00000.00010.00020.00030.00040.0005 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\nFig.I: Layer-wise angle and norm across different datasets. The angle and\nnorm for models trained on different datasets, including CIFAR [14] are displayed from\ntop to bottom. These metrics demonstrate consistency regardless of the dataset type\nfrom left (first layer) to right (last layer).\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00000.00020.00040.00060.00080.00100.00120.0014 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\nFig.J: Layer-wise angle and norm across different classifier initializations.\nThe angle and norm for models trained with differently initialized networks following\nthe LP-FT [15] method are displayed from top to bottom. These metrics demonstrate\nconsistency regardless of the initialization method from left (first layer) to right (last\nlayer).\nFig.K: Layer-wise angle during training. Displayed are the overlapped angles\nacross models trained with different random seeds at", " Introduction\nModel merging [ 15,28], a recent development in the large language model (LLM) community,\npresents a novel paradigm shift. By strategically combining multiple LLMs into a single architecture,\nthis exciting development has captured the attention of researchers due to its key advantage: it\nrequires no additional training, making it an incredibly cost-effective approach for developing new\nmodels. This accessibility has fueled a surge in interest and experimentation with model merging.\nThe Open LLM Leaderboard [ 20] is now dominated by merged models, showcasing its potential for\ndemocratizing foundation model development.\nHowever, model merging is considered by many to be a form of black art or alchemy, relying on the\nmodel maker\u2019s intuition and instincts about model selection and merging recipes to create and refine a\nnew model that performs well for a particular task. Furthermore, the model maker is often required to\nhave some domain knowledge for the various different benchmark tasks. Given the large diversity of\nopen models and benchmarks in the community, human intuition can only go so far, and we believe a\nmore systematic approach for discovering new model combinations will take things much further.\nWe believe evolutionary algorithms will be able to discover more effective model merging solutions,\nand thus provide a path for automating the creation of more capable models. As a step towards this\ndirection, in this work, we show that evolution can be employed to discover novel and unintuitive\nways to merge various models to produce new models with a new combined ability. In this work, we\npresent a methodology that leverages evolutionary algorithms to facilitate the merging of foundation\nmodels. Our approach is distinguished by its ability to navigate both parameter space (weights) and\nthe data flow space (inference path), proposing a framework that integrates these two dimensions.\n1EvoLLM-JP ,EvoVLM-JP release: https://github.com/SakanaAI/evolutionary-model-mergearXiv:2403.13187v1  [cs.NE]  19 Mar 2024This work makes several key contributions to the field of foundation model development:\n1.Automated Model Composition : We introduce Evolutionary Model Merge , a general\nevolutionary method to automatically discover optimal combinations of diverse open-source\nmodels for creating new foundation models with user-specified capabilities. This approach\nharnesses the collective intelligence of existing open models, enabling the creation of\npowerful models without the need for extensive training data or compute.\n2.Cross-Domain Merging : We demonstrate that our method can discover novel ways to\nmerge models from disparate domains (e.g., non-English language and Math, non-English\nlanguage and Vision), potentially exceeding the capabilities achievable through conventional\nhuman design strategies.\n3.State-of-the-Art Performance : We showcase the effectiveness of our method by auto-\nmatically generating a Japanese LLM with Math reasoning capability and a Japanese\nVision-Language Model (VLM). Notably, both models achieve state-of-the-art performance\non various benchmarks, even without explicit optimization for those tasks.\n4.High Efficiency and Surprising Generalizability : We observe that our 7B parameter LLM\nsurpasses the performance of some previous 70B parameter Japanese LLMs on benchmark\ndatasets, highlighting the high efficiency and surprising generalization capability of our\napproach. We believe this model can serve as a strong general-purpose Japanese LLM.\n5.Culturally-Aware VLM : The generated Japanese VLM achieves top results of this Apache 2.0-licensed model for comparison in Table 4,\nwhich provides a more comprehensive comparison than Table 2 in the main text.\nC Case Study\nTable 5 provides an example of responses to a mathematical question by existing", " Introduction to Python for Translators/Linguists\u201d \nand \u201cProfe ssional Translation Internships\u201d. His research focuses on \nhuman -centered AI and machine translation, aiming to augment \npeople's abilities to empower them and reduce their cognitive limi-\ntations. His academic experience is influenced by his professional \nactivity, since he runs AWORDZ Language Engineering, a small \ncompany that provides language engineering, localisation and in-\nternationali sation services . \n \nJO\u00c3O LUCAS CAVALHEIRO CAMARGO has a B. Ed. in Portuguese \nand English and their respective literatures from  Western Paran\u00e1 \nState University (UNIOESTE) in Brazil. He holds a Specialist degree in English through distance learning and a Master\u2019s in \nteaching at the same institution. He also holds a Specialist degree \nin Instructional Design from Instituto de Desenho Instrucional. In \nhis Master\u2019s degree research, he designed, implemented and evalu-\nated two translation courses (in -person and distance learning) on \ntranslation hermeneutics. He was a Lecturer at the Western Paran\u00e1 \nState University, teaching English languag e teachers, Tourism and \nHospitality undergraduates. Currently, he is a PhD student funded \nby the School of Applied Languages and Intercultural Studies \n(SALIS) in Dublin City University. His PhD project aims to de-\nsign, implement and evaluate training on human evaluation of Ma-\nchine Translation to Master\u2019s NLP students.  \n \nGOKHAN DOGRU is a visiting postdoctoral researcher at ADAPT -\nDCU affiliated with the Faculty of Translation and Interpreting at \nUniversitat Aut\u00f2noma de Barcelona (UAB) in the framework of \nMarga rita Salas Grant. His research interests include terminologi-\ncal quality evaluation in machine translation, different use cases of \nMT for professional translators and the intersection of translation \nprofession and translation technologies as well as localization . discussion \nof fluency Related work: The disruption of AI and MT in the legal do-\nmain  \nThe development of MT includes quality assessment as a crucial \naspect that both academia and industry work on (Way 20 20), be-\ncoming its own subfield in MT research (Castilho & Caseli 2023). \nEvaluation can be performed through HE and AEMs, with varied \npractices for different contexts (Castilho et al.  2018). The im-\nprovement in quality of MT systems in the legal field and their \nadoption in multiple fields, whether in general industry (ELIS \n2022), patent institutions like the World International Property \n________ __ \n \n2 Product page of VICUNA . Online : https://lmsys.org/blog/2023 -03-30-vicuna/  \n(last accessed : 07/06/2023) . \n3 Google Translate . Online : https://translate.google.com/ (last accessed : \n07/06/2023) . Organization4, or international institutions such as the European \nCommission and the creation of eTranslation5, a public MT system \nfor the legal field, have also led to the analysis of the use of these \nMT systems in legal institutions (Cadwell et al.  2016; Leszny\u00e1k \n2019; Rossi & Chevrot 2019).  \nIn terms of literature, MT in the legal world has been observed \nfrom different points of view. Firstly, focusing on the quality of \nautomatic systems, such as Killman (2014) and the use of MT in \nSpanish Supreme Court judgments. Another example is that of \nWiesmann (2019), who analysed how NMT worked for translating \nItalian legal te xts into German. In addition, Mileto (2019) worked \nwith students", " \n\nI Introduction\n\n\nIn recent years, large language models (LLMs111This paper views the LLM as the Transformer-based language model with a large number of parameters, pretrained on massive datasets using self/semi-supervised learning techniques.) [1, 2, 3] have achieved notable research breakthroughs, showcasing remarkable performance in the field of natural language processing [4]. As the scale of these models expands, LLMs showcase not only excellence in language-related tasks but also reveal expansive potential applications in diverse domains [5]. This includes a spectrum of optimization and generation tasks, representing a pivotal milestone in the evolution of artificial general intelligence. The advancement of LLMs has also catalyzed progress in technologies and methodologies across various research field [6, 7, 8]. Notably, this impact extends to evolutionary computation, offering both new opportunities and challenges. The primary goal of this review is to explore the dynamic interplay and synergies between LLMs and evolutionary algorithms (EAs), with the intention of establishing a complementary relationship between the two within the contemporary era of LLMs.\n\n\nThe LLM and EA, despite substantial disparities in objectives and principles, share a common pursuit of applicability in various scenarios, which are different from most models that aimed for high performance in specific domain problems. LLM achieves a unified approach across diverse tasks by learning from extensive data [9], while EA, as a general-purpose solver, has lower reliance on problem characteristics and information compared to traditional mathematical optimization methods [10], enabling it to solve a wider range of problems with different characteristics. Therefore, in terms of application scenarios, both EAs and LLMs demonstrate unique advantages in addressing complex problems with vast search spaces and uncertain environments [11, 5]. This similarity suggests potential complementarity and mutual inspiration between LLM and EA when dealing with large-scale and complex problems.\n\n\nAlthough LLM has achieved success in various applications, it has still faced criticism attributable to its black-box nature and inflexible searching. Due to the intricate LLM architecture, the specific details of the internal decision-making, reasoning, and generation processes are either uninterpretable or invisible for most users [12], especially in the case where commercially viable LLMs (such as GPT-4 [13]) typically keep their model parameters private. Exactly, as a classic black-box optimization technique [14], EAs hold potential for further enhancement within the black-box framework of LLM, such as prompt optimization [15] or neural architecture search (NAS) [16]. Another limitation of LLM is its finite search capability, as the search process is typically conducted in a one-shot manner without iterative progressive optimization. Moreover, the search capability of LLMs is constrained by prompts and training data, leading to a tendency to generate content that aligns with learned patterns and prompt information [17], thereby limiting global exploration of the entire search space. EA\u2019s search superiority can mitigate this limitation in LLM. Firstly, EA is an iterative optimization method that can continuously evolve and improve the solutions generated by LLM, thus enhancing result quality. Additionally, EA can achieve more flexible search through well-designed searching space and evolutionary operator [18, 19]. The search capacity of EA proves particularly advantageous for complex tasks that require adequate optimization. This is", " Introduction\nPolicy optimization is a prevalent method for solving rein-\nforcement learning problems, involving iterative parameter\nupdates to maximize objectives. Policy gradient discussion and for providing insight-\nful advice regarding the experiment. This material is based\nupon work partially supported by the National Science and\nTechnology Council (NSTC), Taiwan under Contract No.\nNSTC 112-2628-E-A49-023 and Contract No. NSTC 112-\n2634-F-A49-001-MBK and based upon work partially sup-\nported by the Higher Education Sprout Project of the Na-\ntional Yang Ming Chiao Tung University and Ministry of\nEducation (MOE), Taiwan. related work, please refer to Appendix E.1. Interestingly, one can draw an analogy between (146) in Lemma 10 and learning a linear binary classifier:\n(i)Features : The state-action representation can be viewed as the feature vector of a training sample; (ii) Labels : The sign\nofA\u03c02(s, a)resembles a binary label; (iii) Classifiers :\u03c01(a|s)\u2212\u03c02(a|s)serves as the prediction of a linear classifier. We\nprovide the intuition behind using \u03c01(a|s)\u2212\u03c02(a|s)as a classifier. Let\u2019s fix \u03c02and let \u03c01be the improved policy. If the sign\nofA\u03c02(s, a)\u22650, which implies that the action ahas a positive effect on the total return, it is desired to slightly tune up the\nprobability of acting in action a. Thus, the update \u03c01must have a greater probability on action ain order to obtain the sufficient\ncondition of the state-wise policy improvement, i.e., (\u03c01(a|s)\u2212\u03c02(a|s))A\u03c02(s, a)\u22650. Next, we substantiate this insight and\nrethink PPO-Clip via hinge loss.\nAs described in Section 3, one major component of the proof of Theorem 1 is the state-wise policy improvement property of\nPPO-Clip. For ease of exposition, we introduce the following definition regarding the partial ordering over policies.\nDefinition 1 (Partial ordering over policies) .Let\u03c01and\u03c02be two policies. Then, \u03c01\u2265\u03c02, called \u03c01improves upon \u03c02, if and\nonly if V\u03c01(s)\u2265V\u03c02(s),\u2200s\u2208 S. Moreover, we say \u03c01> \u03c02, called \u03c01strictly improves upon \u03c02, if and only if \u03c01\u2265\u03c02and\nthere exists at least one state ssuch that V\u03c01(s)> V\u03c02(s).\nLemma 11 (Sufficient condition of state-wise policy improvement) .Given any two policies \u03c01and\u03c02, we have \u03c01\u2265\u03c02if the\nfollowing condition holds:X\na\u2208A\u03c01(a|s)A\u03c02(s, a)\u22650,\u2200s\u2208 S. (148)\nProof of Lemma 11. This is the same result of the proof of Lemma 10.\nNext, we present two critical properties that hold under PPO-Clip for every sample path.\nLemma 12 (Strict improvement and strict positivity of policy under PPO-Clip with direct tabular parameterization) .In any\niteration t, suppose \u03c0(t)is strictly positive in all state-action pairs, i.e., \u03c0(t)(a|s)>0, for all (s, a). Under PPO-Clip in\nAlgorithm 7, \u03c0(t+1)satisfies that (i) \u03c0(t+1)> \u03c0(t)and (ii) \u03c0(t+1)(a|s)>0, for all (s, a).\nProof of Lemma 12. Consider the t-th iteration of PPO-Clip (cf. Algorithm 7) and the corresponding update from \u03c0(t)to\u03c0(t+1).\nRegarding (ii), recall from Algorithm 8 that K(t)denotes the number of iterations undergone by the EMDA subroutine for the\nupdate from \u03c0(t)to\u03c0(t+1)and that K(t)is designed to be finite. Therefore, it is easy to verify that \u03c0(t+1)(a|s)>0for all (s, a)\nby the exponentiated gradient update scheme of EMDA and the strict positivity of \u03c0(t).\nNext, for ease of exposition, for each k\u2208 {0,1,\u00b7\u00b7\u00b7, K(t)}and for each state-action pair (s, a), lete\u03b8(k)\ns,adenote\nthe policy parameter after kEMDA iterations. Regarding (i), recall that we define g(k)\ns,a:=\u2202L(\u03b8)\n\u2202\u03b8s,a\f\f\n\u03b8=e\u03b8(k)\nsandw(k)\ns:=\n(e\u2212\u03b7g(k)\ns,1,\u00b7\u00b7\u00b7, e\u2212\u03b7g(k)\ns,|A|). Note that as the weights in the loss function only affects the effective step sizes of EMDA, we simply\nset the weights of PPO-Clip to be one, without", " \n\n1 Introduction\n\nIn recent years, foundational models\u00a0[3] have become instrumental tools, exhibiting unprecedented efficacy across multiple domains. These models are characterized by their extensive scale, generality, and capacity to learn and generalize knowledge from vast datasets, offering promising solutions to a diverse range of problems. The inherent ability of foundational models to be fine-tuned has led to advancements in natural language processing (NLP)\u00a0[43, 44, 14, 32, 45, 30], computer vision\u00a0[42, 49, 34, 25, 5], and other related fields\u00a0[50, 62, 51].\n\n\nOn one hand, the scalability of expanding foundational models to increase the number of tasks they can perform in practice poses a significant challenge as approaches such as joint training are limited in many practical scenarios\u00a0[8, 9]. In domains such as healthcare, stringent data privacy concerns often prohibit access to the underlying training data, even when the fine-tuned model on the said data is publicly accessible, rendering joint training infeasible\u00a0[2, 10]. Even in scenarios where access to training data is possible, the computational demands of simultaneous training on a multitude of tasks becomes restraining.\n\n\nOn the other hand, the widespread adoption of foundational models has led to a certain homogenization in the field\u00a0[3]. Both the training approach, commonly transfer learning from a popular foundational model\u00a0[41], and the model architecture itself have become standardized, typically following a few popular foundation models. This standardization has resulted in a proliferation of publicly available fine-tuned models, all sharing the same architecture\u00a0[56, 61, 12, 18]. However, beyond their conventional use for model inference, these numerous fine-tuned models remain largely untapped, representing a missed opportunity\u00a0[48].\n\n\nTo address the challenges of scalability, practical constraints, and unlock the untapped potential of the growing pool of publicly available fine-tuned models, recent developments in neural network weight averaging techniques have gained attention\u00a0[23, 11, 39, 48, 21, 58, 57, 6, 16, 60]. These approaches enable the practitioners to re-purpose the increasingly valuable publicly available fine-tuned models.\n\n\nCloser to our approach, Task Arithmetic were introduced by Ilharco et al.\u00a0[21]. In their method, a foundation model is refined by incorporating the scaled average of the differences between multiple fine-tuned models and the foundation model. This allows for the creation of a multi-task model without the need for additional training or access to the original training data. However, despite its potential, the Task Arithmetic method\u00a0[21] encounters limitations when dealing with numerous tasks. This is mainly due to its dependence on hyperparameter tuning through validation set performance, a process that becomes computationally impractical at scale, coupled with an increasing accumulation of noise as more tasks are merged to the foundation model.\n\n\nTo address these challenges and to capitalize on the untapped resources within the field, our paper introduces Model Breadcrumbs, a simple solution designed to tackle scalability, noise reduction in merging tasks, and hyperparameter generalization issues. Model Breadcrumbs constructs multi-task models from pre-existing fine-tuned models (see Figure\u00a01), surpassing limitations faced by existing methods. We demonstrate that Model Breadcrumbs not only yields competitive multi-task models but also provides hyperparameters that generalize effectively as the number of tasks increases. In Section\u00a02, we provide context through a review of related work. Sections\u00a03 and 4 present", " \n\n1 Introduction\n\nFigure 1: (Left) DARE can effectively eliminate 90% or even 99% delta parameters of WizardMath on GSM8K. (Right) DARE can merge multiple task-specific SFT language models into a single model with all the abilities. LM, MATH, and Code are abbreviations of WizardLM-13B, WizardMath-13B, and llama-2-13b-code-alpaca.\n\n\nHuman beings have harbored a longstanding desire to acquire additional abilities through various ways, as expressed in mediums like movies and games. For example, in X-Men\u2019s Apocalypse, the character can absorb the powers of other mutants to strengthen himself. Likewise, the protagonist in the Super Mario games can gain superpowers like throwing fireballs by absorbing in-game items. In this paper, we astonishingly find that Language Models (LMs), similar to Apocalypse and Super Mario, can enhance their capabilities by absorbing other models without the need for retraining or even GPUs.\n\n\nFormally, Supervised Fine-Tuning (SFT) is the most widely adopted strategy for unlocking task-specific abilities to LMs by optimizing their parameters (Dodge et\u00a0al., 2020; Zhao et\u00a0al., 2023). The effectiveness of SFT is fully evident in the alteration of the model parameters before and after SFT, referred to as delta parameters (Ding et\u00a0al., 2023). We first show that SFT LM (either encoder- or decoder-based) always tends to acquire excessively redundant delta parameters. To be specific, we present DARE (Drop And REscale), which randomly sets certain delta parameters to zeros with a drop rate p\ud835\udc5dpitalic_p and subsequently rescales the remaining ones by a factor of 1/(1\u2212p)11\ud835\udc5d1/(1-p)1 / ( 1 - italic_p ). Although conceptually simple, DARE can eliminate up to 99% delta parameters with minimal impact on the performance when the LM\u2019s parameters reach 70 billion (see Figure 1(a)). Moreover, the more parameters the LM has, the larger p\ud835\udc5dpitalic_p it can tolerate. We attribute the effectiveness of DARE to its ability to approximate the original embeddings, which is verified theoretically and empirically.\n\n\nFurthermore, we can merge multiple homologous SFT LMs (fine-tuned from the same backbone) based on DARE without compromising their capabilities. As long as a small portion of the delta parameters remain unaffected during merging, the abilities of LMs unlocked by SFT can still be preserved. We first employ DARE to eliminate redundant delta parameters in each model before merging, which can potentially mitigate the interference of parameters among multiple models (Yadav et\u00a0al., 2023). Then, we apply established model merging techniques (Wortsman et\u00a0al., 2022; Ilharco et\u00a0al., 2023; Matena & Raffel, 2022; Jin et\u00a0al., 2023; Yadav et\u00a0al., 2023) to fuse the parameters with reduced redundancy for creating one model with diverse capabilities.\n\n\nWe conduct extensive experiments with encoder-based LMs on GLUE benchmark, and decoder-based LMs with three distinct abilities: instruction-following, mathematical reasoning, and code-generating. We observe that:\n\n\n(1) SFT LMs exhibit a substantial number of redundant delta parameters regardless of their backbones (e.g., BERT, RoBERTa, LLaMA, Llama 2, or Code Llama). DARE can remove 90% or even 99% delta parameters without significantly affecting the model performance. DARE is able to approximate the original embeddings well and provide very similar embeddings for each layer of the LM. The rescale operation is crucial to guarantee the success of DARE, and dropping 30% or 40% delta parameters", " \n\n1 Introduction\n\nLarge language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency in natural language that allows them to be commanded and prompted to perform a variety of tasks\u00a0(OpenAI, 2023; Touvron et\u00a0al., 2023b).\nBy utilizing large, in-domain datasets, their efficacy can be greatly improved for applications that require a combination of both natural and domain-specific language and understanding of specialized terminology.\nBy training on domain-specific datasets, they have proved effective more broadly on applications that require advanced natural language understanding.\nA prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation (for a survey, see Xu & Zhu, 2022, also see Section\u00a05).\nIn this work, we present Code\u00a0Llama, a family of LLMs for code generation and infilling derived from Llama\u00a02 (Touvron et\u00a0al., 2023b) and released under the same custom permissive license.\nWe provide inference code for both completion and infilling models in the accompanying repository.11footnotemark: 1\nOur approach is based on gradually specializing and increasing the capabilities of Llama\u00a02 models by applying a cascade of training and fine-tuning steps (Figure\u00a02): 00footnotetext: \u2020\u2020\\dagger\u2020: Core contributors. \u2217\u2217\\ast\u2217: Meta AI, CERMICS \u00c9cole des Ponts ParisTech. \u22c4\u22c4\\diamond\u22c4: Meta AI & Hebrew University of Jerusalem\n\n\n\u2022\n\nCode-training from foundation models. While most LLMs for code generation such as AlphaCode (Li et\u00a0al., 2022), InCoder (Fried et\u00a0al., 2023) or StarCoder (Li et\u00a0al., 2023) are trained on code only, Codex\u00a0(Chen et\u00a0al., 2021) was fine-tuned from a general language model. We also start from a foundation model (Llama\u00a02, Touvron et\u00a0al., 2023b) pretrained on general-purpose text and code data. Our comparison (Section\u00a03.4.1) shows that initializing our model with Llama\u00a02 outperforms the same architecture trained on code only for a given budget.\n\n\n\n\u2022\n\nInfilling. Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does not provide the capability to fill a missing portion of text while taking the full surrounding context into account.\nOur code-training for 7B, 13B and 70B Code\u00a0Llama models features a multitask objective (Fried et\u00a0al., 2023) consisting of both autoregressive and causal infilling prediction, enabling applications such as real-time completion in source code editors or docstring generation.\n\n\n\n\n\u2022\n\nLong input contexts. Unlocking repository-level reasoning for completion or synthesis \u2013 as opposed to function-level or file-level \u2013 requires prompting the model with much longer context than the 4,096 tokens supported by Llama\u00a02. We propose an additional fine-tuning stage that extends the maximum context length from 4,096 tokens to 100,000 tokens by modifying the parameters of the RoPE positional embeddings (Su et\u00a0al., 2021) used in Llama\u00a02. Our experiments show Code\u00a0Llama operating on very large contexts with a moderate impact on performances on standard coding benchmarks (Section\u00a03.3).\n\n\n\n\u2022\n\nInstruction fine-tuning. For end-users, the utility of LLMs is significantly improved by instruction fine-tuning (Ouyang et\u00a0al., 2022; Wei et\u00a0al., 2022; OpenAI, 2023; Touvron et\u00a0al., 2023b), which also helps preventing unsafe, toxic or biased generations. Code\u00a0Llama\u00a0-\u00a0Instruct variants are further fine-tuned on a mix of proprietary instruction data for improved safety and helpfulness, and a new machine-generated self-instruct dataset created by prompting Llama\u00a02 for coding problems and Code\u00a0Llama to generate associated unit tests and solutions. Our results show that Code\u00a0Llama\u00a0-\u00a0Instruct significantly", " Introduction\nRecently, Large-scale language models (LLMs) have garnered significant attention and become\nthe go-to approach for numerous natural language processing (NLP) tasks, including open domain\nconversation [ 1\u20134], coding [ 5\u201313] and math [ 14\u201319]. A conspicuous example is ChatGPT, developed\nby OpenAI. This model uses extensive pre-training on large-scale internet data and further fine-\ntuning with specific instruction data and methods for effective instruction\ntuning. arXiv preprint arXiv:2301.13688 , 2023.\n[88] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei\nZhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald\nMetzler. Ext5: Towards extreme multi-task scaling for transfer learning. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,\n2022.\n[89] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea\nSantilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas\nWolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The\nTenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .\nOpenReview.net, 2022.\n[90] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. Unifiedqa: Crossing format boundaries with a single QA system. In Trevor Cohn, Yulan He,\nand Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020 , volume EMNLP 2020 of Findings of ACL , pages 1896\u20131907. Association\nfor Computational Linguistics, 2020.\n[91] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Zeroprompt:\nScaling prompt-based pretraining to 1, 000 tasks improves zero-shot generalization. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 4235\u20134252. Association\nfor Computational Linguistics, 2022.\n[92] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416 , 2022.\n[93] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560 , 2022.\n15A results of the equations:\nx= 6 + 9\n22y=\u221210\u22121\nx= 15\ny=\u221211\nSo, the coordinates of (x, y)are(15,\u221211).\nThe answer is: (15,-11).\nWizardMath-13B\nStep 1: Understand the problem\nWe are given that the midpoint of the line segment between (x, y)and(\u22129,1)is\n(3,\u22125). We need to find the coordinates of the point (x, y).\nStep 2: Recall the midpoint formula\nThe midpoint of a line segment with endpoints (x1, y1)and(x2, y2)can be found\nusing the midpoint formula:\nMidpoint =\u0000x1+x2\n2,y1+y2\n2\u0001\nStep 3: Apply the midpoint formula\nIn our case, the endpoints are (x, y)and(\u22129,1). So, we can plug these values into\nthe midpoint formula:\nMidpoint =\u0010\nx+(\u22129)\n2,y+1\n2\u0011\nStep 4: Simplify the midpoint\nNow, we can simplify the midpoint by performing the arithmetic operations:\nMidpoint =\u0000x\u22129\n2,y+1\n2\u0001\nStep 5: Set up an equation\nSince the midpoint is given", " Introduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of \ufb01elds, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThecapabilitiesofLLMsareremarkableconsideringtheseeminglystraightforwardnatureofthetraining\nmethodology. Auto-regressivetransformersarepretrainedonanextensivecorpusofself-superviseddata,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback(RLHF).Althoughthetrainingmethodologyissimple,highcomputationalrequirementshave\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Ho\ufb00mann et al., 2022), but none of these models are suitable substitutes for closed \u201cproduct\u201d LLMs, such\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\ufb01ne-tunedtoalignwithhuman\npreferences, which greatly enhances their usability and safety. This step can require signi\ufb01cant costs in\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\nthe community to advance AI alignment research.\nIn this work, we develop and release Llama 2, a family of pretrained and \ufb01ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures1and3). Wehavetakenmeasurestoincreasethesafetyofthesemodels,usingsafety-speci\ufb01cdata\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthispapercontributesathoroughdescriptionofour\ufb01ne-tuningmethodologyandapproachtoimproving\nLLM safety. We hope that this openness will enable the community to reproduce \ufb01ne-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\nWealsosharenovelobservationswemadeduringthedevelopmentof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle andL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ,suchas\nthe emergence of tool usage and temporal organization of knowledge.\n3Figure 3: Safety human evaluation Results\nSee evaluations for pretraining (Section 2); \ufb01ne-tuning (Section 3); and safety (Section 4).\nEthical Considerations and Limitations (Section 5.2)\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is a new technology that carries risks with use. Testing conducted to date has been in\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle\u2019s potential outputs cannot be predicted in advance, and the model may in some instances\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\napplications of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle, developers should perform safety testing and tuning tailored to their\nspeci\ufb01c applications of the model. Please see the Responsible Use Guide available available at\nhttps://ai.meta.com/llama/responsible-user-guide\nTable 52: Model card for L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle .\n77 Discussion\nHere, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\nlimitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\nmodels (Section 5.3).\n5.1 Learnings and Observations\nOur tuning process revealed several interesting Related Work\nLarge Language Models. The recent years have witnessed a substantial evolution in the \ufb01eld of LLMs.\nFollowing the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B\nparameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized\nmodels, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Ho\ufb00mann et al.,\n2022) rede\ufb01ned those scaling laws towards the number of tokens rather than model weights. Notable in\nthisprogressionistheriseofLlama,recognizedforitsfocusoncomputationale\ufb03ciencyduringinference\n(Touvron et al., 2023). A parallel discourse has unfolded around the", " Introduction\nPre-trained models (PTMs) have become widespread in many real-world applications [ 91,6]. Using\nPTMs typically involves fine-tuning them to specialize on a specific task [ 69,12], which can lead to\nimproved performance with less task-specific labeled data. These benefits have resulted in the release\nof thousands of finetuned checkpoints [ 81] derived from popular PTMs such as ViT [ 14] for vision\nand T5 [ 58] for language. However, having a separate fine-tuned model for each task has various\ndrawbacks: (1) for each new application, a separate model has to be stored and deployed [ 17,89], and\n(2) models trained in isolation cannot leverage information from related tasks to improve in-domain\nperformance or out-of-domain generalization [ 66,58,75]. Multitask learning [ 66,57] could address\nthese concerns but requires costly training and simultaneous access to all tasks [ 17]. Moreover, it can\nbe complex and resource-intensive to determine how best to mix datasets to ensure that multitask\ntraining is beneficial for all tasks [55, 54, 80, 52, 2, 17].\n1Our code is available at https://github.com/prateeky2806/ties-merging\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2306.01708v2  [cs.LG]  27 Oct 2023Task V ectors  Trimmed T ask V ectors  \n(1) Trim\n(2) Elect SignAligned V alues\nSign V ector \nMerged T ask\nVector (3) Disjoint Merge: Influential values\n: Redundant values: Parameter: Model 1\n: Model 2: Model 3\n: Merged ModelFigure 1: A depiction of the steps involved in TIES-MERGING . We visualize each parameter in\na model as a square. The arrows depict the update (task vector, \u03c4) to a parameter produced by\nfine-tuning on different tasks (coded by colors), with direction denoting sign and length denoting\nmagnitude. We first trim the task vector values based on their magnitude, then we elect the sign for\neach parameter ( \u03b3m, green vector containing +1or\u22121) by resolving sign conflicts. Finally, we pick\nonly the values that align with the elected sign and take their mean as the final parameter value.\nRecently, a growing body of research has focused on model merging [40]. One application of merging\ninvolves combining multiple task-specific models into a single multitask model without performing\nadditional training. Previous works merge models by summing the individual model weights with\ndifferent weighting schemes, either via a simple average [ 9,28,83], via more sophisticated means\nthat incorporate parameter importance [ 45] or account for permutation invariances [ 1,31,70,74,42].\nCombining fine-tuned models in this way can be seen as adding together task vectors [29] that are\ncomputed by subtracting the pre-trained model\u2019s parameter values from those of the fine-tuned model.\nOriginal V alues\n No Interference  Redundant  Sign Conflict Mean TIESMerged V alues\nModel 1 Model 2\nFigure 2: Different types of conflict and\nmerged outputs produced by either averaging\norTIES-MERGING . The parameters causing\ninterference are denoted by dotted arrows.While weighted averaging of model parameters has\nproven effective for merging, all of these results of this comprehensive search indicated an optimal value of\nk= 20 , with values of \u03bb= 0.9,\u03bb= 1.0, and \u03bb= 1.1demonstrating equivalent performance. To\nmaintain simplicity in our model, we chose a \u03bbvalue of 1. Thus, the final selection of parameters for\nTIES-MERGING isk= 20 , signs based on mass, the disjoint mean, and a \u03bbvalue of 1.\nC.5 Merging Different Number of Tasks\nHere we provide some additional details", " Introduction\nThe emergence of large language models (LLMs) like ChatGPT [ 41] has revolutionized the landscape\nof artificial general intelligence (AGI), showcasing their impressive zero-shot capabilities in ad-\ndressing various natural language processing (NLP) tasks through user-tailored prompts or language\ninstructions. Despite these advancements, it\u2019s essential to note that the triumph of LLMs does not\neffortlessly extend to pure vision and vision-language tasks, due to the inherent disparities between\nmodalities and task formats.\nThe field of computer vision presents a unique set of challenges and paradigms that differ from\nthose of NLP. The traditional paradigm of vision foundation models is pre-training followed by\nfine-tuning [ 59,12,51,61,18,52], which is effective but comes with significant marginal costs\nwhen adapting to diverse downstream scenarios. As shown in Figure 1a, while approaches such as\nmulti-task unification [ 44,58,1,57,81] have been used to achieve generalist capability, they often\nstruggle to overcome the limitations imposed by pre-defined tasks, resulting in a gap in open-ended\n\u2217Equal contribution. This work is done when Zhe Chen, Xiaokang Chen, and Jiannan Wu are interns at\nShanghai AI Laboratory.\u2020Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.arXiv:2305.11175v2  [cs.CV]  25 May 2023VisionGeneralistModelPre-defined tasks:detection, captioning, VQA, grounding, ...(a) Vision generalist models [ 59,\n61,83] are constrained by the for-\nmat of pre-defined tasks.\nVisualPromptTuning\n(b) Visual prompt tuning [ 26,64,\n62] are inconsistent with the for-\nmat of LLMs.\nVision + LLM\nTask de\ufb01ned by  instruc6onsDesired output:<c1> <p1> <p3> ...(c) VisionLLM (ours) can flexibly\nmanage vision-centric tasks using\nlanguage instructions like LLMs .\nFigure 1: Comparison of our VisionLLM with popular paradigms. Unlike current vision generalist\nmodels that depend on pre-defined task formats and visual prompt tuning models that are inconsistent\nwith large language models (LLMs), VisionLLM leverages the power of LLMs for open-ended vision\ntasks by using language instructions.\ntask capabilities compared to LLMs. Recently, visual prompt tuning [ 26,74,79,76,62] has emerged\nas a way to flexibly outline some pure vision tasks (see Figure 1b), such as object detection, instance\nsegmentation, and pose estimation, using visual masking. However, the format of visual prompts\nconsiderably deviates from that of language instructions, making it challenging to directly apply the\nreasoning abilities and world knowledge of LLMs to vision tasks. Therefore, there is an urgent\nneed for a unified generalist framework that can seamlessly integrate the strengths of LLMs with the\nspecific requirements of vision-centric tasks.\nIn this work, we present VisionLLM, a novel framework that aligns the definitions of vision-centric\ntasks with the methodologies of LLMs. Leveraging the reasoning and parsing capacities of LLMs,\nVisionLLM is designed to empower open-ended task capabilities for vision-centric tasks. Specifically,\nit comprises three core components: (1) a unified language instruction designed for vision and\nvision-language tasks, (2) a language-guided image tokenizer, and (3) an LLM-based open-ended\ntask decoder that orchestrates various tasks using language instructions. With this framework, a\nwide range of vision-centric tasks can be seamlessly integrated, including object detection, instance\nsegmentation, image captioning, and visual grounding. In addition, the framework also facilitates\ntask customization at different levels of granularity, allowing for the customization of target objects,\noutput formats, task descriptions, etc.\nCompared to current popular API-based applications [ 68,73,50,35,30], our model takes a unified,\nend-to-end approach to integrate VFMs and LLMs, streamlining and enhancing the overall efficiency\nof the overall process, and leveraging the strengths and data of both VFMs and LLMs within a\nsingle, cohesive system.", " Introduction\nOpen-vocabulary models are characterized by their ability to perform any image classi\ufb01cation task\nbased on text descriptions of the classes [ 56]. Thanks to advances in large-scale pre-training, recent\nexamples of open-vocabulary models such as CLIP and BASIC have reached parity with or surpassed\nimportant task-speci\ufb01c baselines, even when the open-vocabulary models are not \ufb01ne-tuned on\ntask-speci\ufb01c data (i.e., in a zero-shot setting) [ 57,31,56,88,1,86]. For instance, the largest CLIP\nmodel from Radford et al. [57] used in a zero-shot setting matches the ImageNet accuracy of a\nResNet-50 trained on 1.2 million ImageNet images [14, 24].\nNevertheless, current open-vocabulary models still face challenges. The same CLIP model that\nmatches a ResNet-50 on ImageNet has lower MNIST accuracy than simple logistic regression in\npixel space [ 57]. Moreover, even when zero-shot models achieve good performance, they are usually\nstill worse than models trained or \ufb01ne-tuned on speci\ufb01c downstream tasks.\n\u0003Equal contribution. Code available at https://github.com/mlfoundations/patching .\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2208.05592v2  [cs.CV]  11 Oct 2022Figure 1 :Patching open-vocabulary models by lin-\nearly interpolating weights. We wish to improve ac-\ncuracy on tasks where a model performs poorly ( patching\ntasks ), without degrading performance on tasks where\naccuracy is already adequate ( supported tasks ). When\ninterpolating weights of \ufb01ne-tuned models and zero-\nshot (unpatched) models, there are intermediate solu-\ntions where accuracy improves on the patching task\nwithout reducing accuracy on supported tasks. Results are shown for nine patching\ntasks, for three different random seeds that control the order in which datasets are seen. The average\nacross random seeds is highlighted.\nJ.2 Sequential patching\nIn Figure 30, we show the evolution of sequential patching as more tasks are added. The accuracy\ndistance of using a single, patched model to using multiple specialized models increases with with\nthe number of patched tasks, leaving headroom for future work on more sophisticated sequential\nstrategies for patching. Interestingly, sequential patching outperforms sequential \ufb01ne-tuning (where\nno interpolation is used) by a large margin.\nJ.3 SplitCIFAR\nFigure 31 compares the patching results for\nvarious strategies for patching on multiple tasks.\n44Cars DTD EuroSAT GTSRB KITTI MNIST RESISC45 SUN397 SVHN\n\u000b S P S P S P S P S P S P S P S P S P Avg\nB/320.00 63.4 59.6 63.4 44.1 63.4 45.9 63.4 32.4 63.4 22.6 63.4 48.3 63.4 60.7 63.4 63.1 63.4 31.5 54.4\n0.05 63.4 61.9 63.4 47.0 63.4 63.4 63.4 39.7 63.4 25.5 63.4 60.9 63.5 65.9 63.5 64.4 63.4 39.9 57.7\n0.10 63.3 63.8 63.4 50.1 63.3 74.3 63.3 48.1 63.4 35.7 63.3 77.0 63.6 71.0 63.5 65.9 63.4 50.1 61.5\n0.15 63.3 65.5 63.4 53.7 63.2 82.0 63.2 57.6 63.5 47.5 63.2 87.3 63.5 75.9 63.5 67.2 63.3 60.9 64.9\n0.20 63.2 67.4 63.3 56.5 63.0 89.3 63.0 68.3 63.4 55.6 62.8 92.6 63.5 79.9 63.4 68.5 63.1 70.4 67.6\n0.25 63.1 69.1 63.1 59.7 62.7 93.3 62.8 78.5 63.3 60.2 62.4 95.7 63.3 83.7 63.3 69.5 62.8 78.3 69.7\n0.30 63.0 70.7 62.9 62.8 62.4 95.4 62.4 85.9 63.1 66.1 62.1 97.3 63.2 86.6 63.2 70.5 62.6 84.3 71.4\n0.35 62.8 72.4 62.7 65.7 62.0 96.4 62.0 90.9 62.8 69.3 61.7 98.5 63.0 89.1 63.1 71.6 62.1 88.8 72.5\n0.40 62.5 73.6 62.4 68.1 61.5 97.0 61.3 93.9 62.6 72.3 61.2 99.0 62.7", " introduction and survey of esti-\nmation of distribution algorithms,\u201d Swarm and evolutionary computation ,JOURNAL OF IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. XX, NO. XX, FEBRUARY 2023 19\nvol. 1, no. 3, pp. 111\u2013128, 2011.\n[114] W. Dong, T. Chen, P. Ti \u02c7no, and X. Yao, \u201cScaling up estimation of\ndistribution algorithms for continuous optimization,\u201d IEEE Transactions\non Evolutionary Computation , vol. 17, no. 6, pp. 797\u2013822, 2013.\n[115] M. Laumanns and J. Ocenasek, \u201cBayesian optimization algorithms for\nmulti-objective optimization,\u201d Lecture Notes in Computer Science , vol.\n2439, pp. 298\u2013307, 2002.\n[116] X. Zhong and W. Li, \u201cA decision-tree-based multi-objective estimation\nof distribution algorithm,\u201d in 2007 International Conference on Compu-\ntational Intelligence and Security (CIS 2007) . IEEE, 2007, pp. 114\u201311.\n[117] H. Tang, V . A. Shim, K. C. Tan, and J. Y . Chia, \u201cRestricted boltzmann\nmachine based algorithm for multi-objective optimization,\u201d in IEEE\ncongress on evolutionary computation . IEEE, 2010, pp. 1\u20138.\n[118] L. Mart \u00b4\u0131, J. Garc \u00b4\u0131a, A. Berlanga, C. A. C. Coello, and J. M. Molina,\n\u201cMb-gng: Addressing drawbacks in multi-objective optimization estima-\ntion of distribution algorithms,\u201d Operations Research Letters , vol. 39,\nno. 2, pp. 150\u2013154, 2011.\n[119] K. Li and S. Kwong, \u201cA general framework for evolutionary multi-\nobjective optimization via manifold learning,\u201d Neurocomputing , vol. 146,\npp. 65\u201374, 2014.\n[120] C. W. Ahn and R. S. Ramakrishna, \u201cMultiobjective real-coded bayesian\noptimization algorithmrevisited: diversity preservation,\u201d in Proceedings\nof the 9th annual conference on Genetic and evolutionary computation ,\n2007, pp. 593\u2013600.\n[121] M. Pelikan, K. Sastry, and D. E. Goldberg, \u201cMultiobjective hboa,\nclustering, and scalability,\u201d in Proceedings of the 7th annual conference\non Genetic and evolutionary computation , 2005, pp. 663\u2013670.\n[122] Q. Zhang, A. Zhou, and Y . Jin, \u201cRm-meda: A regularity model-based\nmultiobjective estimation of distribution algorithm,\u201d IEEE Transactions\non Evolutionary Computation , vol. 12, no. 1, pp. 41\u201363, 2008.\n[123] Y . Li, X. Xu, P. Li, and L. Jiao, \u201cImproved rm-meda with local\nlearning,\u201d Soft Computing , vol. 18, pp. 1383\u20131397, 2014.\n[124] Y . Wang, J. Xiang, and Z. Cai, \u201cA regularity model-based multiobjec-\ntive estimation of distribution algorithm with reducing redundant cluster\noperator,\u201d Applied Soft Computing , vol. 12, no. 11, pp. 3526\u20133538, 2012.\n[125] Y . Sun, G. G. Yen, and Z. Yi, \u201cImproved regularity model-based eda\nfor many-objective optimization,\u201d IEEE Transactions on Evolutionary\nComputation , vol. 22, no. 5, pp. 662\u2013678, 2018.\n[126] J. Sun, H. Zhang, A. Zhou, Q. Zhang, and K. Zhang, \u201cA new\nlearning-based adaptive multi-objective evolutionary algorithm,\u201d Swarm\nand evolutionary computation , vol. 44, pp. 304\u2013319, 2019.\n[127] P. A. Bosman and D. Thierens, \u201cMulti-objective optimization with di-\nversity preserving mixture-based iterated density estimation evolutionary\nalgorithms,\u201d International Journal of Approximate Reasoning , vol. 31,\nno. 3, pp. 259\u2013289, 2002.\n[128] H. Karshenas, R. Santana, C. Bielza, and P. Larranaga, \u201cMultiobjective\nestimation of distribution algorithm based on joint modeling of objectives\nand variables,\u201d IEEE Transactions on Evolutionary Computation , vol. 18,\nno. 4, pp. 519\u2013542, 2013.\n[129] H. Karshenas, R. Santana, C. Bielza, and P. Larra \u02dcnaga, \u201cMulti-objective\noptimization with joint probabilistic modeling of objectives and vari-\nables,\u201d in Proceedings of the 6th international conference on Evolutionary\nmulti-criterion optimization , 2011, pp. 298\u2013312.\n[130] L. R. Farias and A. F. Ara \u00b4ujo, \u201cIm-moea/d: An inverse modeling\nmulti-objective evolutionary algorithm based on decomposition,\u201d in 2021\nIEEE International Conference on Systems, Man, and Cybernetics (SMC) .\nIEEE, 2021, pp. 462\u2013467.\n[131] R. Cheng, Y", " Introduction\nIn recent years, research has shown that models pre-trained\non large and diverse datasets learn representations that trans-\nfer well to a variety of tasks. As a result, machine learning\npractitioners now commonly develop solutions for down-\nstream tasks by \ufb01ne-tuning large pre-trained models (Gir-\nshick et al., 2014; Yosinski et al., 2014; Kornblith et al.,\n2019; Kolesnikov et al., 2020). Typically, the \ufb01ne-tuning\nprocess involves two steps: (1) \ufb01ne-tune models with a va-\nriety of hyperparameter con\ufb01gurations, and (2) select the\nmodel which achieves the highest accuracy on the held-out\nvalidation set. The remaining models are then discarded.\nSelecting a single model and discarding the rest has several\ndownsides. For one, ensembling outputs of many models\ncan outperform the best single model, albeit at a high com-\nputational cost during inference. For another, \ufb01ne-tuning a\nmodel on downstream tasks can sometimes reduce out-of-\ndistribution performance (Radford et al., 2021; Andreassen\net al., 2021; Wortsman et al., 2021; Pham et al., 2021), and\nthe best single model on the target distribution may not be\nthe best model on out-of-distribution data.\nIn this work, we propose a more accurate and robust alter-\nnative to the second step of the conventional recipe in the\ncontext of \ufb01ne-tuning a large pre-trained model. Instead of\nselecting the individual \ufb01ne-tuned model which achieves the\nhighest accuracy on the held-out validation set, we average\nthe weights of models \ufb01ne-tuned independently, and refer to\nthe result as a model soup . Given the related work.\n2. Method\nThis section highlights three recipes for model souping, the\nuniform ,greedy , and learned soup, though the greedy soup\nis our central method. We summarize the methods of data augmentation as we conduct a random hyperparameter search.Model soups: averaging weights of multiple \ufb01ne-tuned models improves accuracy without increasing inference time\n1 2 3 4 5 6 7\nNumber of ingredients (models)\u22124\u221220246810Percentage point \u2206from CLIP zero-shot\nCross-dataset soup\nZero-shot CLIP\n0 2 4 6 8\nAverage percentage point \u2206when including soup ingredientCIFAR10ImageNetSUN397Food101CarsDTD\nFigure E.1: Model soups can improve zero-shot performance on new downstream tasks. (left) Starting with zero-shot CLIP we create a\nsoup by adding models \ufb01ne-tuned on ImageNet, CIFAR-10, Food101, SUN397, DTD, and Cars, and evaluate on CIFAR-100. Different\norders for adding models are shown with faded lines. (right) The average change in CIFAR-100 accuracy when a model \ufb01ne-tuned on the\ndataset listed in the y-axis is added to the model soup.\nSGD RMSprop Adam AdamWSGD RMSprop Adam AdamW\n0 1.3 1.2 1.40 1.6 1.70 1.70Choice of optimizer\n0 1 2 3 4 50 1 2 3 4 5\n0 1.6 1.8 1.8 1.8 1.70 2.8 2 2.6 20 1.2 1.1 1.20 1.4 0.80 1.30Choice of augmentation strength\n1e-4 3e-5 2e-5 1e-5 3e-6 1e-6 1e-71e-4 3e-5 2e-5 1e-5 3e-6 1e-6 1e-7\n0 -1.1 -2 -2.8 -3.2 -3.2 -3.30 0.96 0.86 1.3 1.5 1.50 0.64 1.3 1.6 1.60 0.63 1.1 1.20 0.22 0.150 -0.560Choice of learning rate\n0.00.20.40.60.81.01.21.41.6\n0.00.51.01.52.02.5\n\u22123\u22122\u2212101\nAcc/parenleftbig1\n2\u03b8a+1\n2\u03b8b/parenrightbig\n\u2212max\u03b8\u2208{\u03b8a,...,\u03b8b}Acc(\u03b8)\nFigure F.1: Analysis of 1D hyperparameter grids, where the average of models at the endpoints often outperforms the best individual\nmodel in the grid. In particular, colors and numbers indicate the percentage point improvement obtained by averaging the models on the x\nandyaxis versus taking the best individual model in the range between them. Experiments\nThis section presents our key experimental \ufb01ndings. We\nbegin with experimental setup (Section 3.1) then provide\nintuition for model soups by examining", " Introduction\nHow should we transfer knowledge and capabilities across trained models? One popular approach\nis transfer learning [ 44], which \ufb01ne-tunes a pre-trained model on a target task through additional\ngradient-based training. The preparatory step of pre-training the model on a data-rich task ideally\ninstills useful \u201cknowledge\u201d into the network\u2019s parameters, which allows the model to learn more\nrapidly and effectively when \ufb01ne-tuned on a downstream task of interest. Transfer learning has\ntherefore become a particularly important and omnipresent tool across many \ufb01elds, including natural\nlanguage processing [ 57,13,9,52,53,46] and computer vision [ 43,24,68]. Recently, it has been\nshown that training on an \u201cintermediate\u201d task between pre-training and \ufb01ne-tuning can further boost\nperformance through additional transfer of capabilities from the intermediate task [ 47,60,51,48].\nAlternatively, continued self-supervised training on unlabeled domain-specialized data can serve as a\nform of domain adaptation [19].\nAll of the aforementioned transfer learning results on GLUE with BERT-base. Columns correspond to target\ntasks while rows correspond to intermediate tasks. Subscripts denote standard deviation across runs.\nItalicized values represent \ufb01ne-tuning directly on the target task (i.e. no intermediate-task training).\nTASK COLA MRPC STS-B RTE\nCOLA 55 :41:8 85:00:9 85:90:8 62:12:3\nSST-2 56:81:4 85:40:9 85:31:0 63:81:0\nMRPC 58:50:484 :50:3 85:30:8 62:75:2\nSTS-B 56:30:4 86:70:786 :10:9 64:52:5\nQQP 56:02:0 87:11:2 87:50:4 71:61:9\nMNLI 58:61:7 85:90:8 87:60:3 77:41:6\nQNLI 56:41:9 87:80:6 87:10:5 71:04:1\nRTE 56:70:9 82:22:5 85:80:563 :71:7\nTable A4: Effect of the number of examples used to compute the Fisher information. Columns\ncorrespond to the number of examples used for RTE. Rows correspond to the number of examples\nused for MNLI. Scores are the RTE validation set accuracy. The original RTE checkpoints had an\naverage accuracy of 63:7and isotropic merging (i.e. 0 Fisher examples) had an average accuracy of\n72:2.\nEXAMPLES 256 1024 2490\n256 72:7 72 :9 73 :1\n1024 72:9 72 :9 73 :3\n4096 72:9 73 :0 73 :2\n32768 72:8 73 :0 73 :5\n392702 72:9 73 :1 73 :4\n16 background and\ndetail our Fisher merging procedure. Section 3 provides experimental conclusions and thoughts on future work in section 5.\n2 Weighted Parameter Averaging for Model Merging\nOur focus is on procedures for model merging , i.e. averaging the parameters of models that share\nan architecture and initialization. In this section, we \ufb01rst frame the common practice of averaging\ntogether model parameters as approximately maximizing the joint likelihood of model posteriors.\nSpeci\ufb01cally, we show that parameter averaging corresponds to using an isotropic Gaussian as the\napproximate posterior for each model. We then introduce Fisher merging , which uses the model\u2019s\ndiagonal Fisher information matrix as the precision matrix of the Gaussian approximate posterior.\nFisher merging can be implemented by setting each merged parameter value to a weighted average of\nthe corresponding parameter values from the original models, with the weighting for each parameter\ndetermined by its Fisher information. In addition, we add model-level weightings as additional\nhyperparameters to set the relative importance of each model.\n2.1 Isotropic merging\nConsider the problem setting where we have Mtrained neural networks with parameters \u00121;:::;\u0012M\nand our goal is to create a single neural network with parameters \u0012that, loosely speaking, inherits\nthe capabilities of the Mtrained neural networks. Assume that all of these neural networks share a\ncommon architecture and had the same set of initial parameter values before being trained. Merging\nattacks this problem by \ufb01nding the parameters \u0012that maximize the", " Introduction\nIn recent years, large language models have demonstrated impressive skills\nacross many diverse tasks (Wang et al., 2019; Brown et al., 2020). Kaplan\net al. (2020) describe the consistent bene\fts of increasing model size, character-\nizing scaling trends that hold across many orders of magnitude. However, even\nthe largest models falter when required to perform multi-step mathematical rea-\nsoning (Hendrycks et al., 2021). Model samples frequently contain catastrophic\nmistakes, even after the model has been appropriately \fnetuned. Mathematical\nreasoning thus reveals a critical weakness in modern language models.\nOne signi\fcant challenge in mathematical reasoning is the high sensitivity\nto individual mistakes (Shen et al., 2021a). When generating a solution, au-\ntoregressive models have no mechanism to correct their own errors. Solutions\nthat veer o\u000b-course quickly become unrecoverable. If we rely purely on genera-\ntive methods that scale even\nbetter.\nAcknowledgements\nWe thank Dan Hendrycks, Leo Gao, Alec Radford, and Giambattista Paras-\ncandolo for their valuable feedback on this paper; Harri Edwards, Yura Burda,\nMichael Wu, and Nick Ryder for many insightful conversations; Michael Petrov,\nAlethea Power, and Jacob Jackson for their technical assistance; the OpenAI\nSupercomputing team for the infrastructure that made these results in this\npaper, had some minor implementation bugs. Our reported test performance\nis therefore a slight underestimate, though the magnitude of this discrepancy is\nless than 1% in most Appendix F for a\nvisualization of veri\fer con\fdence.\n4.3 Veri\fcation Ablations\nWe can either train veri\fers to make a single scalar prediction conditioned on\nthe entire generated solution, or to make a scalar prediction after each token\nin the solution. By default, we choose the latter, training veri\fers to make\npredictions after each token. This can be viewed as a token-level value function.\nWe compare these two Related Work\n3.1 Related Datasets\nEarly math word problem datasets (Kushman et al., 2014; Roy and Roth, 2015)\nare relatively small and are not well suited for testing the limits of modern lan-\nguage models. Dolphin18K (Huang et al., 2016) is a larger dataset containing\n318K problems, but solutions are provided only in the form of equations or \f-\nnal answers. AQuA-RAT (Ling et al., 2017) contains 100K problems, but this\ndataset unfortunately su\u000bers from both a high degree of problem templatiza-\ntion and poor quality control of the natural language solutions. MathQA is\na recently released subset of AQuA-RAT focused on correcting these mistakes\n(Amini et al., 2019), but even the corrected dataset has data quality issues, with\naround 30% of the data having inconsistencies (Miao et al., 2021). Ape210K\n(Zhao et al., 2020) is the largest publicly available dataset, consisting of 210K\nChinese elementary school-level math problems. However, due to the language\nbarrier and the lack of natural language solutions, we're unable to evaluate our background color of the text corresponds to the ver-\ni\fer score for that token, where red is low value (predicted incorrect) and green\n21is high value (predicted correct). The second column of the table summarizes\nthe veri\fer's prediction, and the third column indicates whether the generated\nmodel completion was actually correct or incorrect. Any disagreement between\nthe second and third columns indicates that the veri\fer made an error.\nThe \frst row includes a true positive example, where the veri\fer correctly\nclassi\fes the completion as correct. Note that the model is initially unsure about\nwhether the solution is correct and gradually gains certainty as the solution\nprogresses:", " Introduction\nScalable sequence prediction models (Graves, 2014;\nVaswani et al., 2017; Child et al., 2019) have become a\ngeneral-purpose method for generation and representation\nlearning in many domains, including natural language pro-\ncessing (Mikolov et al., 2013; Sutskever et al., 2014; Dai &\nLe, 2015; Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018), computer vision (Van Oord et al., 2016; Menick\n& Kalchbrenner, 2018; Chen et al., 2020; Bao et al., 2021),\naudio and speech processing (Oord et al., 2016; 2018; Dhari-\nwal et al., 2020; Baevski et al., 2020), biology (Alley et al.,\n2019; Rives et al., 2021), and even across multiple modali-\nties (Das et al., 2017; Lu et al., 2019; Ramesh et al., 2021;\nZellers et al., 2021). More recently, language models have\nalso fueled progress towards the longstanding challenge\nof program synthesis (Simon, 1963; Manna & Waldinger,\n1971), spurred by the presence of code in large datasets\n(Husain et al., 2019; Gao et al., 2020) and the resulting pro-\ngramming capabilities of language models trained on these\ndatasets (Wang & Komatsuzaki, 2021). Popular language\nmodeling objectives like masked language modeling (Devlin\net al., 2018) and span prediction (Raffel et al., 2020) have\nalso been adapted to train their programming counterparts\nCodeBERT (Feng et al., 2020) and PyMT5 (Clement et al.,\n2020).\nSimilarly, our early investigation of GPT-3 (Brown et al.,\n2020) revealed that it could generate simple programs from\nPython docstrings. While rudimentary, this capability was\nexciting because GPT-3 was not explicitly trained for code\ngeneration. Given the considerable success of large lan-\nguage models in other modalities and the abundance of\npublicly available code, we hypothesized that a specialized\nGPT model, called Codex, could excel at a variety of coding\ntasks. This paper describes several early Codex models,\nwhose descendants power GitHub Copilot and the Codex\nmodels in the OpenAI API.arXiv:2107.03374v2  [cs.LG]  14 Jul 2021Evaluating Large Language Models Trained on Code\nFigure 1. Pass rates of our models on the HumanEval dataset as a\nfunction of model size. When a single sample is generated for each\nproblem, GPT-12B solves no problems, but Codex (\ufb01ne-tuned\non code) solves 28.8% of the problems, and Codex-S (further\n\ufb01ne-tuned on correctly implemented standalone functions) solves\n37.7% of the problems. From here, further gains can be realized by\ngenerating 100 samples per problem and selecting the sample with\nthe highest mean log-probability (44.5% solved) or by selecting\nthe sample that passes the unit tests (77.5% solved). All samples\nare generated with temperature 0.8.\nIn this work, we focus on the task of generating stan-\ndalone Python functions from docstrings, and evaluate the\ncorrectness of code samples automatically through unit\ntests. This is in contrast to natural language generation,\nwhere samples are typically evaluated by heuristics or by\nhuman evaluators. To accurately benchmark our model,\nwe create a dataset of 164 original programming problems\nwith unit tests. These problems assess language compre-\nhension, algorithms, and simple mathematics, with some\ncomparable to simple software interview questions. We\nrelease this data along with an evaluation framework at\nhttps://www.github.com/openai/human-eval.\nTo solve a problem in our test set, we generate multiple\nsamples from the models, and check if any of them pass the\nunit tests. With just a single sample, a 12B parameter Codex\nsolves 28.8% of these problems, and a 300M parameter\nCodex solves 13.2% of these problems. In contrast, the 6B\nparameter GPT-J (Wang & Komatsuzaki, 2021) achieves\n11.4% on the same", " Introduction and Motivating Work\nPre-training methods in natural language processing , pp.\n1527\u20131536, 2017.\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\nGerman Traf\ufb01c Sign Recognition Benchmark: A multi-\nclass classi\ufb01cation competition. In IEEE International\nJoint Conference on Neural Networks , pp. 1453\u20131460,\n2011.\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\nand Schmid, C. Learning video representations from tex-\ntual web supervision. arXiv preprint arXiv:2007.14937 ,\n2020.\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi,\nA. Inception-v4, inception-resnet and the impact\nof residual connections on learning. arXiv preprint\narXiv:1602.07261 , 2016.\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\nencoder representations from transformers. arXiv preprint\narXiv:1908.07490 , 2019.\nTan, M. and Le, Q. V . Ef\ufb01cientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946 , 2019.\nTaori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,\nand Schmidt, L. Measuring robustness to natural dis-\ntribution shifts in image classi\ufb01cation. arXiv preprint\narXiv:2007.00644 , 2020.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM , 59(2):64\u201373, 2016.Learning Transferable Visual Models From Natural Language Supervision 35\nTian, Y ., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. arXiv preprint arXiv:1906.05849 , 2019.\nTian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and\nIsola, P. Rethinking few-shot image classi\ufb01cation: a\ngood embedding is all you need? arXiv preprint\narXiv:2003.11539 , 2020.\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\nimages: A large data set for nonparametric object and\nscene recognition. IEEE transactions on pattern analysis\nand machine intelligence , 30(11):1958\u20131970, 2008.\nTouvron, H., Vedaldi, A., Douze, M., and J \u00b4egou, H. Fix-\ning the train-test resolution discrepancy. In Advances in\nneural information processing systems , pp. 8252\u20138262,\n2019.\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\nanalysis and abnormality detection. In 2009 IEEE 12th\nInternational Conference on Computer Vision Workshops,\nICCV Workshops , pp. 1338\u20131345. IEEE, 2009.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems , pp. 5998\u20136008, 2017.\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\nWelling, M. Rotation equivariant CNNs for digital pathol-\nogy. June 2018.\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, \u02d9I.,\nFeng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n1.0: Fundamental Algorithms for Scienti\ufb01c Computing\nin Python. Nature results reported\nin Taori et al. (2020)\u2019s evaluation suite. Zero-shot CLIP im-\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\nBB. CLIP\u2019s improvements are largest on ImageNet-Vid and\nYoutube-BB due to its \ufb02exible zero-shot capability and on\nImageNet-R, which likely re\ufb02ects CLIP\u2019s pre-training dis-\ntribution including signi\ufb01cant amounts of creative content.\nA similar behavior has been documented for the Instagram\npre-trained ResNeXt models as discussed in Taori et al.\n(2020).Learning Transferable Visual Models From Natural Language Supervision 48\nF. Model Hyperparameters\nHyperparameter Value\nBatch size", "ABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classi\ufb01cation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellentresults from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14\u000214as opposed to 224\u0002224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speci\ufb01c pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).\n0 5 10 15 20\nNetwork depth (layer)020406080100120Mean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\n0 5 10 15 20\nNetwork depth (layer)020406080100120\nR50x1 + ViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels.\nD.5 E MPIRICAL COMPUTATIONAL COSTS\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\n18Published as a conference paper at ICLR 2021\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor.\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions.\nAnother quantity of interest is the largest batch-size each model can \ufb01t onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-ef\ufb01ciency over ResNet\nmodels.\n64 128 224 384 512\nInput size [px]102103104Peak inference speed [img/sec/core]64 128 224 384 512\nInput size [px]102103Largest per-core batch-sizeR50x1\nR50x2ViT-B/32\nViT-L/32ViT-B/16\nViT-L/16ViT-H/14\nR152x4\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right : Largest per-core batch-size \ufb01tting on device with\nvarious", "Abstract \u2014Neural architecture search (NAS) has attracted a lot of attention and has been illustrated to bring tangible bene\ufb01ts in a large\nnumber of applications in the past few years. Architecture topology and architecture size have been regarded as two of the most\nimportant aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both\nof those aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under\ndifferent search spaces and training setups. This makes the overall performance of the algorithms incomparable and the improvement\nfrom a sub-module of the searching model unclear. In this paper, we propose NATS-Bench, a uni\ufb01ed benchmark on searching for both\ntopology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates\nfor architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of\nvarious criteria and performance comparison of all candidates in the search space. We also show the versatility of NATS-Bench by\nbenchmarking 13 recent state-of-the-art NAS algorithms on it. All logs and diagnostic information trained using the same setup for\neach candidate are provided. This facilitates a much larger community of researchers to focus on developing better NAS algorithms in\na more comparable and computationally effective environment. All codes are publicly available at:\nhttps://xuanyidong.com/assets/projects/NATS-Bench.\nIndex Terms \u2014Neural Architecture Search, Benchmark, Deep Learning\nF\n1 I NTRODUCTION\nTHEdeep learning community is undergoing a transition\nfrom hand-designed neural architectures [1], [2], [3] to\nautomatically designed neural architectures [4], [5], [6], [7],\n[8]. In its early stages, the great success of deep learning was\npromoted by the introductions of novel neural architectures,\nsuch as ResNet [1], Inception [3], VGGNet [9], and Trans-\nformer [10]. However, manually designing one architecture\nrequires human experts to frequently try and evaluate nu-\nmerous different operation and connection options [4]. In\ncontrast to architectures that are manually designed, those\nautomatically found by neural architecture search (NAS)\nalgorithms require much less human interaction and ex-\npert effort. These NAS-generated architectures have shown\npromisingresults in\nNAS-Bench-1SHOT1. Therefore, though it is not guaran-\nteed, observations from our NATS-Bench have a potential\nto generalize to other search spaces.methods\nwith parameter sharing, we \ufb01nd that GDAS \u0015DARTS (2nd)\n\u0015DARTS (1st), which is also consistent withResults of weight-sharing basedBackground\nNAS aims to \ufb01nd architecture \u000bamong the search space S\nso that this found \u000bachieves a high performance on the\nFig. 5: Ranking stability of top 20% architectures on different\ndatasets over the topology search space St.\n(a) The Kendall rank correlation coef\ufb01cient for St.\n(b) The Kendall rank correlation coef\ufb01cient for Ss.\nFig. 6: We report the Kendall rank correlation coef\ufb01cient\nbetween the accuracy on 6 sets, i.e., CIFAR-10 validation\nset (C10-V), CIFAR-10 test set (C10-T), CIFAR-100 validation\nset (C100-V), CIFAR-100 test set (C100-T), ImageNet-16-120\nvalidation set (I120-V), ImageNet-16-120 test set (I120-T).\nvalidation set. This problem can be formulated as a bi-level\noptimization problem:\nmin\n\u000b2SL(\u000b;!\u0003\n\u000b;Dval) (1)\ns:t: !\u0003\n\u000b= arg min!L(\u000b;!;Dtrain);\nwhereLindicates the objective function (e.g., cross-entropy\nloss).Dtrain andDvaldenote the training data and the\nvalidation data, respectively. In the typical NAS setting,\nafter an architecture \u000bis found,\u000bwill be re-trained on\nDtrain (orDtrain +Dval) and evaluated on the test data\nDtestto \ufb01gure out its real performance.\n5.2 Experimental Setup\nWe evaluate 13recent, state-of-the-art searchingMethods\nThe weight-sharing basedexperiments. Part of this project was supported\nby Google Cloud Credits from", " Introduction\nThe success of deep learning in computer vision is in no\nsmall part due to the insight and engineering efforts of hu-\nman experts, allowing for the creation of powerful archi-\ntectures for widespread adoption (Krizhevsky et al., 2012;\nSimonyan & Zisserman, 2015; He et al., 2016; Szegedy\net al., 2016; Huang et al., 2017). However, this manual\ndesign is costly, and becomes increasingly more dif\ufb01cult\nas networks get larger and more complicated. Because of\nthese challenges, the neural network community has seen a\n1Usher Institute, University of Edinburgh2School of In-\nformatics, University of Edinburgh3School of Engineering,\nUniversity of Edinburgh. Correspondence to: Joseph Mellor\n<joe.mellor@ed.ac.uk >.\nProceedings of the 38thInternational Conference on Machine\nLearning , PMLR 139, 2021. Copyright 2021 by the author(s).shift from designing architectures to designing algorithms\nthatsearch for candidate architectures (Elsken et al., 2019;\nWistuba et al., 2019). These Neural Architecture Search\n(NAS) algorithms are capable of automating the discovery\nof effective architectures (Zoph & Le, 2017; Zoph et al.,\n2018; Pham et al., 2018; Tan et al., 2019; Liu et al., 2019;\nReal et al., 2019).\nNAS algorithms are broadly based on the seminal work\nof Zoph & Le (2017). A controller network generates an\narchitecture proposal, which is then trained to provide a\nsignal to the controller through REINFORCE (Williams,\n1992), which then produces a new proposal, and so on.\nTraining a network for every controller update is extremely\nexpensive; utilising 800 GPUs for 28 days in Zoph & Le\n(2017). Subsequent work has sought to ameliorate this by\n(i) learning stackable cells instead of whole networks (Zoph\net al., 2018) and (ii) incorporating weight sharing ; allow-\ning candidate networks to share weights to allow for joint\ntraining (Pham et al., 2018). These contributions have ac-\ncelerated the speed of NAS algorithms e.g. to half a day on\na single GPU in Pham et al. (2018).\nFor some practitioners, NAS is still too slow; being able to\nperform NAS quickly (i.e. in seconds) would be immensely\nuseful in the hardware-aware setting where a separate search\nis typically required for each device and task (Wu et al.,\n2019; Tan et al., 2019). This could be achieved if NAS\ncould be performed without any network training . In this\npaper we show that this is possible. We explore NAS-Bench-\n101 (Ying et al., 2019), NAS-Bench-201 (Dong & Yang,\n2020), NATS-Bench (Dong et al., 2021), and Network De-\nsign Spaces (NDS, Radosavovic et al., 2019), and examine\nthe overlap of activations between datapoints in a mini-batch\nfor an untrained network (Section 3). The linear maps of\nthe network are uniquely identi\ufb01ed by a binary code cor-\nresponding to the activation pattern of the recti\ufb01ed linear\nunits. The Hamming distance between these binary codes\ncan be used to de\ufb01ne a kernel matrix (which we denote by\nKH) which is distinctive for networks that perform well;\nthis is immediately apparent from visualisation alone across\ntwo distinct search spaces (Figure 1). We devise a score\nbased on KHand perform an ablation study to demonstrate\nits robustness to inputs and network initialisation.\nWe incorporate our score into a simple search algorithmarXiv:2006.04647v3  [cs.LG]  11 Jun 2021Neural Architecture Search without Training\n(a) NAS-Bench-201\n (b) NDS-DARTS\nFigure 1. KHfor a mini-batch of 128 CIFAR-10 images for untrained architectures in (a) NAS-Bench-201 (Dong & Yang, 2020) and (b)\nNDS-DARTS (Radosavovic et al., 2019). KHin these plots is normalised so that the diagonal entries", " Introduction\nDeveloping neural network image classi\ufb01cation models\noften requires signi\ufb01cant architecture engineering . Starting\nfrom the seminal work of [32] on using convolutional archi-\ntectures [17, 34] for ImageNet [11] classi\ufb01cation, succes-\nsive advancements through architecture engineering have\nachieved impressive results\nFinally, we will present examples of object detection re-\nsults on the COCO dataset in Figure 10 and Figure 11.\nAs can be seen from the \ufb01gures, NASNet-A featurization\nworks well with Faster-RCNN and gives accurate localiza-\ntion of objects.\nFigure 9. Architecture of NASNet-C convolutional cell with B=\n4blocks identi\ufb01ed with CIFAR-10. The input (white) is the hid-\nden state from previous activations (or input image). The output\n(pink) is the result of a concatenation operation across all result-\ning branches. Each convolutional cell is the result of Bblocks. A\nsingle block corresponds to two primitive operations (yellow) and\na combination operation (green).Figure 10. Example detections showing improvements of object\ndetection over previous state-of-the-art model for Faster-RCNN\nwith Inception-ResNet-v2 featurization [28] (top) and NASNet-A\nfeaturization (bottom).\nFigure 11. Example detections of best performing NASNet-A fea-\nturization with Faster-RCNN trained on COCO dataset. Top and\nmiddle images courtesy of http://wikipedia.org . Bottom\nimage courtesy of Jonathan Huang methods. Naively applying dropout [56]\nacross convolutional \ufb01lters degraded performance. How-\never, we discovered a new technique called ScheduledDrop-\nPath, a modi\ufb01ed version of DropPath [33], that works well\nin regularizing NASNets. In DropPath, we stochastically\ndrop out each path (i.e., edge with a yellow box in Figure\n4) in the cell with some \ufb01xed probability. This is simi-\nlar to [27] and [69] where they dropout full parts of their\nmodel during training and then at test time scale the path\nby the probability of keeping that path during training. In-\nterestingly we also found that DropPath alone does not help\nNASNet training much, but DropPath with linearly increas-\ning the probability of dropping out a path over the course\nof training signi\ufb01cantly improves the \ufb01nal performance for\nboth CIFAR and ImageNet Related Work\nThe proposed method is related to previous work in hy-\nperparameter optimization [44, 4, 5, 54, 55, 6, 40] \u2013 es-\npecially recent approaches in designing architectures such\nas Neural Fabrics [48], DiffRNN [41], MetaQNN [3] and\nDeepArchitect [43]. A more \ufb02exible class of Appendix\nA. Experimental Details\nA.1. Dataset for Architecture Search\nThe CIFAR-10 dataset [31] consists of 60,000 32x32\nRGB images across 10 classes (50,000 train and 10,000\ntest images). We partition a random subset of 5,000 images\nfrom the training set to use as a validation set for the con-\ntroller RNN. All images are whitened and then undergone\nseveral data augmentation steps: we randomly crop 32x32\npatches from upsampled images of size 40x40 and apply\nrandom horizontal \ufb02ips. This data augmentation procedure\nis common among Experiments\nWe now present two additional cells that performed well\non CIFAR and ImageNet. The search spaces used for these\ncells are slightly different than what was used for NASNet-\nA. For the NASNet-B model in Figure 8 we do not concate-\nnate all of the unused hidden states generated in the convo-\nlutional cell. Instead all of the hiddenstates created within\nthe convolutional cell, even if they are currently used, are\nfed into the next layer. Note that B= 4and there are 4 hid-\ndenstates as input to the cell as these numbers must match\nfor this cell to be valid. We also allow addition followed by\nlayer normalization [2] or instance normalization [61]", " Introduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient Background: Policy Optimization\n2.1 Policy Gradient Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At/bracketrightbigg\n(3)\nsubject to \u02c6Et[KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]]\u2264\u03b4. (4)\nHere,\u03b8oldis the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]/bracketrightbigg\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, results and learning curves for all 49 games is provided in Experiments\n6.1 Comparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIPto several natural variations and ablated versions.\nNo clipping or penalty: Lt(\u03b8) =rt(\u03b8)\u02c6At\nClipping: Lt(\u03b8) = min(rt(\u03b8)\u02c6At,clip(rt(\u03b8)),1\u2212/epsilon1,1 +/epsilon1)\u02c6At\nKL penalty (\ufb01xed or adaptive) Lt(\u03b8) =rt(\u03b8)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old,\u03c0\u03b8]\n5For the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping ( /epsilon1)\nand the KL penalty ( \u03b2,dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe Results from continuous control benchmark. Average normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2was initialized at 1.\n6.2 Comparison to Other Algorithms in the", " INTRODUCTION  \nThe currently available instruments (e.g., multi/hy perspectral \n[1], synthetic aperture radar [2], etc.) for earth observation [3, 4] \ngenerate more and more different types of airborne or satellite \nimages with different resolutions (spatial resoluti on, spectral \nresolution, and temporal resolution). This raises a n important \ndemand for intelligent earth observation through re mote sensing  \nimages, which allows the smart identification and c lassification \nof land use and land cover (LULC) scenes from airbo rne or \nspace platforms [3]. Remote sensing image scene cla ssification, \nbeing an active research topic in the field of aeri al and satellite \nimage analysis, is to categorize scene images into a discrete set \nof meaningful LULC classes according to the image c ontents. \nDuring the past decades, remarkable efforts have be en made in \ndeveloping various conclusions are drawn in Section VI. \nII. A  REVIEW ON REMOTE SENSING IMAGE SCENE \nCLASSIFICATION DATASETS  \nIn the past years, several publicly available high resolution \nremote sensing image datasets [9, 11, 17, 33, 38, 8 2] have been \nintroduced by different groups to perform research for scene \nclassification and to evaluate different METHODS  \nCurrent Methods for Object-based Analysis and Classificatio n : Springer \nNetherlands, 2004. \n[76] L. Dr\u0103gu\u0163 and T. Blaschke, \u201cAutomated classifi cation of landform \nelements using object-based image analysis,\u201d Geomorphology,  vol. 81, \nno. 3, pp. 330-344, 2006. \n[77] C. Eisank, L. Dr\u0103gu\u0163, and T. Blaschke, \"A gene ric procedure for \nsemantics-oriented landform classification using ob ject-based image \nanalysis,\" in Geomorphometry , 2011, pp. 125-128. \n[78] G. J. Hay, T. Blaschke, D. J. Marceau, and A. Bouchard, \u201cA comparison \nof three image-object abstract level, we can learn very powerful rep resentations. \nThis has been proven in literatures [13, 134, 169-1 71].  \n2) CNNs:  CNNs are designed to process data that come in the  \nform of multiple arrays, for example a multi-spectr al image \ncomposed of multiple 2D arrays containing pixel int ensities in \nthe multiple band channels. Starting with the impre ssive success \nof AlexNet [163], many representative CNN models in cluding \nOverfeat [164], VGGNet [165], GoogLeNet [166], SPPN et \n[167], and ResNet [172] have been proposed in the l iterature. \nThere exist four key ideas behind CNNs that take ad vantage of \nthe properties of natural signals, namely, local co nnections, \nshared weights, pooling, and the use of many layers  [159].  \nThe architecture of a typical CNN is structured as a series of \nlayers. (i) Convolutional layers : They are the most important \nones for extracting features from images. The first  layers usually \ncapture low-level features (like edges, lines and c orners) while \nthe deeper layers are able to learn more expressive  features (like \nstructures, objects and shapes) by combining low-le vel ones. (ii) \nPooling layers : Typically, after each convolutional layer, there \nexist pooling layers that are created by computing some local \nnon-linear operation of a particular feature over a  region of the \nimage. This process ensures that the same result ca n be obtained, \neven when image features have small translations", " Introduction\nIncreasingly, phones and tablets are the primary computing\ndevices for many people [ 30,2]. The powerful sensors on\nthese devices (including cameras, microphones, and GPS),\ncombined with the fact they are frequently carried, means\nthey have access to an unprecedented amount of data, much\nof it private in nature. Models learned on such data hold the\nAppearing in Proceedings of the 20thInternational Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS) 2017, Fort Laud-\nerdale, Flordia, USA. JMLR: W&CP volume 54. Copyright 2017\nby the authors.promise of greatly improving usability by powering more\nintelligent applications, but the sensitive nature of the data\nmeans there are risks and responsibilities to storing it in a\ncentralized location.\nWe investigate a learning technique that allows users to\ncollectively reap the bene\ufb01ts of shared models trained from\nthis rich data, without the need to centrally store it. We term\nour approach Federated Learning , since the learning task is\nsolved by a loose federation of participating devices (which\nwe refer to as clients ) which are coordinated by a central\nserver . Each client has a local training dataset which is\nnever uploaded to the server. Instead, each client computes\nan update to the current global model maintained by the\nserver, and only this update is communicated. This is a\ndirect application of the principle of focused collection or\ndata minimization proposed by the 2012 White House report\non privacy of consumer data [ 39]. Since these updates are\nspeci\ufb01c to improving the current model, there is no reason\nto store them once they have been applied.\nA principal advantage of this approach is the decoupling of\nmodel training from the need for direct access to the raw\ntraining data. Clearly, some trust of the server coordinat-\ning the training is still required. However, for applications\nwhere the training objective can be speci\ufb01ed on the basis\nof data available on each client, federated learning can sig-\nni\ufb01cantly reduce privacy and security risks by limiting the\nattack surface to only the device, rather than the device and\nthe cloud.\nOur primary contributions are 1) the identi\ufb01cation of the\nproblem of training on decentralized data from mobile de-\nvices as an important research direction; 2) the selection of\na straightforward and practical algorithm that can be applied\nto this setting; and 3) an extensive empirical evaluation of\nthe proposed approach. More concretely, we introduce the\nFederatedAveraging algorithm, which combines lo-\ncal stochastic gradient descent (SGD) on each client with\na server that performs model averaging. We perform ex-\ntensive results on a variety of model architectures: a multi-layer\nperceptron, two different convolutional NNs, a two-layer\ncharacter LSTM, and a large-scale word-level LSTM.\nWhile federated learning offers many practical privacy ben-\ne\ufb01ts, providing stronger guarantees via differential pri-\nvacy [ 14,13,1], secure multi-party computation [ 18], or\ntheir combination is an interesting direction for future work.\nNote that both classes of techniques apply most naturally to\nsynchronous algorithms like FedAvg .8\n8Subsequent to this work, Bonawitz et al. [6]introduced an\nef\ufb01cient secure aggregation protocol for federated learning, and\nKone \u02c7cn\u00b4y et al. [23] presented algorithms for further decreasing\ncommunication costs.H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag \u00a8uera y Arcas Related Work Distributed training by iteratively averag-\ning locally trained models has been studied by McDon-\nald et al. [28] for the perceptron and Povey et al. [31] forspeech recognition DNNs. Zhang et al. [42] studies an", " Introduction\nRecently visual attributes have raised signi\ufb01cant inter-\nest in the community [6, 11, 17, 25]. A \u201cvisual attribute\u201d\nis a property of an object that can be measured visually and\nhas a semantic connotation, such as the shape of a hat or the\ncolor of a ball. Attributes allow characterizing objects in far\ngreater detail than a category label and are therefore the key\nto several advanced applications, including understanding\ncomplex queries in semantic search , learning about objects\nfrom textual description , and accounting for the content of\nTuesday, October 29, 13Figure 1: Both the man-made and the natural world are\nan abundant source of richly textured objects. The textures\nof objects shown above can be described (in no particular\norder) as dotted, striped, chequered, cracked, swirly, hon-\neycombed, and scaly. We aim at identifying these attributes\nautomatically and generating descriptions based on them.\nimages in great detail. Textural properties have an important\nrole in object descriptions, particularly for those objects that\nare best quali\ufb01ed by a pattern, such as a shirt or the wing of\nbird or a butter\ufb02y as illustrated in Fig. 1. Nevertheless, so\nfar the attributes of textures have been investigated only tan-\ngentially. In this paper we address the question of whether\nthere exists a \u201cuniversal\u201d set of attributes that can describe a\nwide range of texture patterns, whether these can be reliably\nestimated from images, and for what tasks they are useful.\nThe study of perceptual attributes of textures has a\nlong history starting from pre-attentive aspects and group-\ning [16], to coarse high-level attributes [1, 2, 33], to some\nrecent work aimed at discovering such attributes by au-\ntomatically mining descriptions of images from the Inter-\nnet [3, 12]. However, the texture attributes investigated so\nfar are rather few or too generic for a detailed description\nmost \u201creal world\u201d patterns. Our work is motivated by the\none of Bhusan et al. [5] who studied the relationship be-\ntween commonly used English words and the perceptual\nproperties of textures, identifying a set of words suf\ufb01cient\nto describing a wide variety of texture patterns. While they\nstudy the psychological aspects of texture perception, thearXiv:1311.3618v2  [cs.CV]  15 Nov 2013banded\n blotchy\n braided\n bubbly\n bumpy\n chequered\n cobwebbed\n cracked\n crosshatched\n crystalline\n dotted\n \ufb01brous\n\ufb02ecked\n freckled\n frilly\n gauzy\n grid\n grooved\n honeycombed\n interlaced\n knitted\n lacelike\n lined\n marbled\nmatted\n meshed\n paisley\n perforated\n pitted\n pleated\n polka-dotted\n porous\n potholed\n scaly\n smeared\n spiralled\nsprinkled\n stained\n strati\ufb01ed\n striped\n studded\n swirly\n veined\n waf\ufb02ed\n woven\n wrinkled\n zigzagged\nFigure 2: The 47 texture words in the describable texture dataset introduced in this paper. Two examples of each attribute\nare shown to illustrate the signi\ufb01cant amount of variability in the data.\nfocus of this paper is the challenge of estimating such prop-\nerties from images automatically.\nOur \ufb01rst contribution is to select a subset of 47 de-\nscribable texture attributes , based on the work of Bhusan\net al., that capture a wide variety of visual properties of\ntextures and to introduce a corresponding describable tex-\nture dataset consisting of 5,640 texture images jointly an-\nnotated with the 47 attributes (Sect. 2). In an effort to\nsupport directly real world applications, and inspired by\ndatasets such as ImageNet [10] and the Flickr Material\nDataset (FMD) [30], our images are captured \u201cin the wild\u201d\nby downloading them from the Internet rather than collect-\ning them in a laboratory. We also address the practical"], "bleu": 0.0, "rouge_l": 0.2781875658587987, "gpt_metric_score": 0.5, "bert_score": 0.2342357337474823}
{"paper_key": "Autonomous Network Defence using Reinforcement Learning", "current_5q": "**[Question 1] - What is the problem?**  \nHow can we develop an effective autonomous network defense system using hierarchical reinforcement learning to respond to various adversarial strategies in real-time?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses the growing need for automated defenses in cybersecurity, where human operators are often overwhelmed by the complexity and speed of attacks. By advancing autonomous defense mechanisms, this research could lead to significant improvements in response times and operational efficiency, ultimately reducing the risk of prolonged undetected intrusions. The findings could pave the way for future research in applying reinforcement learning to other complex security scenarios, enhancing our understanding of adaptive defense strategies and their practical applications in safeguarding critical infrastructure.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the dynamic and unpredictable nature of cyber threats, which require a defense system to adapt in real-time to various adversarial tactics. Naive approaches may fail due to their inability to generalize across different attack strategies, leading to overfitting on specific adversaries. Additionally, the technical complexities of creating a hierarchical agent architecture that effectively coordinates multiple specialized sub-agents pose significant obstacles. The need for high-fidelity simulations that accurately represent real-world network environments further complicates the development and testing of such systems.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on isolated aspects of network security or employed simpler models that lack the sophistication needed for real-time autonomous defense. Limitations in computational resources, the complexity of creating realistic simulation environments, and a lack of comprehensive frameworks for integrating multiple learning agents have hindered progress. Our approach differs by introducing a hierarchical architecture that combines specialized sub-agents, allowing for greater adaptability and generalization across various adversarial strategies, which has not been adequately addressed in prior work.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves developing a hierarchical reinforcement learning agent that utilizes a controller agent to select and coordinate sub-agents trained against specific adversarial strategies. We will employ the CybORG environment to simulate a realistic computer network, using metrics such as response time and effectiveness against different adversaries to evaluate performance. The expected outcomes include demonstrating superior defensive capabilities compared to single-agent approaches, showcasing the benefits of our hierarchical architecture in generalizing across various attack scenarios, and providing publicly available models and training setups for further research in the field", "proposal_5q": "[Question 1]: What is the problem?  \nThe specific research question we aim to address is: How can we develop a decentralized multi-agent reinforcement learning framework for Autonomous Cyber Operations that enhances network security through collaborative threat detection and response while preserving user privacy?\n\n[Question 2]: Why is it interesting and important?  \nSolving this problem has significant implications for the research community and the field of cybersecurity. The increasing frequency and sophistication of cyberattacks, particularly advanced persistent threats (APTs) and DGA-based attacks, necessitate innovative solutions that can adapt in real-time to evolving threats. By developing a decentralized MARL framework, we can enhance the collaborative capabilities of autonomous agents, leading to improved threat detection and response mechanisms. This research could pave the way for future studies on federated learning in cybersecurity, promoting a paradigm shift towards privacy-preserving techniques that empower organizations to harness collective intelligence without compromising sensitive data. Ultimately, this work aims to advance knowledge in both machine learning and cybersecurity, providing practical applications that can significantly bolster network defenses.\n\n[Question 3]: Why is it hard?  \nAddressing this problem presents several challenges and complexities. The decentralized nature of the proposed MARL framework introduces significant coordination and communication challenges among agents, especially in dynamic network environments. Naive approaches that rely on centralized data collection can lead to privacy violations and data leaks, undermining the ethical standards necessary for cybersecurity applications. Additionally, the technical obstacles include designing effective reward mechanisms for agents to encourage collaborative learning without direct data sharing. The theoretical complexities of ensuring convergence in a decentralized setting, particularly when agents encounter diverse attack patterns, further complicate the development of a robust solution. These challenges necessitate sophisticated algorithms that can efficiently manage real-time threat intelligence sharing while maintaining a high level of user privacy.\n\n[Question 4]: Why hasn't it been solved before?  \nPrevious research has largely focused on centralized machine learning approaches or isolated agent-based systems that do not effectively leverage collaborative intelligence for cybersecurity. Existing solutions often encounter limitations related to scalability, privacy concerns, and a lack of real-time adaptability to evolving threats. Barriers such as the difficulty in balancing agent autonomy with cooperative learning and the absence of frameworks that prioritize ethical data usage have contributed to the lack of progress in this area. Our approach differs by integrating federated learning with MARL, allowing agents to learn from shared threat intelligence without compromising sensitive information. This innovative combination addresses both the technical and ethical hurdles that have historically impeded advancements in autonomous cyber operations.\n\n[Question 5]: What are the key components of my approach and results?  \nOur proposed methodology involves the development of a decentralized MARL framework that employs federated learning techniques. We will simulate a swarm intelligence approach using a diverse dataset of known attack patterns, including DGA-based attacks, to train autonomous agents. The agents will utilize reinforcement learning to optimize their decision-making processes in real-time threat detection and response scenarios. Key metrics for evaluation will include detection accuracy, response time, and the degree of privacy preservation achieved through federated learning. Expected outcomes include a robust framework capable of significantly enhancing network security resilience against APTs, demonstrating improved collaboration among agents, and establishing a new standard for ethical practices in cybersecurity defense.", "referenced_intros": ["Abstract\nAutonomous Cyber Operations (ACO) involves the\ndevelopment of blue team (defender) and red team\n(attacker) decision-making agents in adversarial\nscenarios. To support the application of machine\nlearning algorithms to solve this problem, and to\nencourage researchers in this \ufb01eld to attend to prob-\nlems in the ACO setting, we introduce CybORG, a\nwork-in-progress gym for ACO research. CybORG\nfeatures a simulation and emulation environment\nwith a common interface to facilitate the rapid\ntraining of autonomous agents that can then be\ntested on real-world systems. Initial testing demon-\nstrates the feasibility of this approach.\n1Background\nAutonomous Cyber Operations (ACO) is concerned with\nthe defence of computer systems and networks through au-\ntonomous decision-making and action. It is particularly\nneeded where deploying security experts to cover every net-\nwork and location is becoming increasingly untenable, and\nwhere systems cannot be reliably accessed by human defend-\ners, either due to unreliable communication channels or ad-\nversary action.\nThe ACO domain is challenging to develop arti\ufb01cial intelli-\ngence (AI) approaches for as it combines hard problems from\nother domains of AI research. Like game AI, it is adversar-\nial: the effectiveness of a defensive cyber agent is determined\nby its ability to respond to an adversary. Like autonomous\nrobotics, ACO is affected by the \u2018reality gap\u2019 [Ibarz et al. ,\n2021 ], as simulations of an environment willabstract away\ninformation that could be critical to an agent\u2019s effectiveness.\nA further issue for the ACO domain is that the environment\nand action set change as cyber security research progresses,\nwhich is far more rapidly than either of the domains discussed\nabove.\nThe requirement to handle the varying actions of an adver-\nsary, in a complex environment, precludes the use of static\ndata sets to learn ACO behaviour. A tool for learning in ad-\nversarial environments is an AI Gym. AI Gyms such as the\none developed by OpenAI implement reinforcement learning(RL) through direct interaction with a simulation of the prob-\nlem. A path to addressing the \u2018reality gap\u2019, used in [Tanet\nal., 2016 ], is to combine learning on simulations with testing\nin a real environment. In this case, the bulk of learning is\nconducted on simulated systems. Successful agents are trans-\nferred to the real system to \ufb01rstly validate their effectiveness,\nand secondly to re\ufb01ne the simulation.\nWe believe that AI Gyms, that can be validated and re-\n\ufb01ned throughexperiments, the requirements of ACO motivate an in-\ntegrated design comprising emulation and simulation modes\nto support large scale RL across diverse scenarios.\nWe have made progress towards implementing this design,\nwith the ability to spawn and play games either in simula-\ntion mode or emulation mode with cloud infrastructure. In\nCybORG, we can now train an RL agent in simulation then\ntest its effectiveness in emulation. TheRelated Work\nThere are a growing number of cyber security environments\ndesigned for experimentation. A summary of several environ-\nments, with an assessment of how they \ufb01t our requirements,\ncan be found in Table 1.\nDETERlab [Mirkovic et al. , 2010 ]is a specialised cy-\nber security experimentation environment based on EMU-\nlab[Stoller et al. , 2008 ]. It supports cyber security experi-\nmentation through the emulation of hosts and networks. As\nit relies on local hardware, DETERlab has limited maximum\nnetwork size and takes a signi\ufb01cant amount of time to reset or\nrecon\ufb01gure. VINE [Eskridge et al. , 2015 ], SmallWorld [Fur-\nfaro et al. , 2018 ]and BRAWL [Corporation, 2018 ]lever-\nage cloud-based Infrastructure", " Introduction\nThe long-term goal of arti\ufb01cial intelligence is to solve advanced real-world challenges. Games have\nserved as stepping stones along this path for decades, from Backgammon (1992) to Chess (1997) to\nAtari (2013)[1\u20133]. In 2016, AlphaGo defeated the world champion at Go using deep reinforcement\nlearning and Monte Carlo tree search[4]. In recent years, reinforcement learning (RL) models have\ntackled tasks as varied as robotic manipulation[5], text summarization [6], and video games such as\nStarcraft[7] and Minecraft[8].\nRelative to previous AI milestones like Chess or Go, complex video games start to capture the\ncomplexity and continuous nature of the real world. Dota 2 is a multiplayer real-time strategy game\nproduced by Valve Corporation in 2013, which averaged between 500,000 and 1,000,000 concurrent\nplayers between 2013 and 2019. The game is actively played by full time professionals; the prize\npool for the 2019 international championship exceeded $35 million (the largest of any esports game\nin the world)[9, 10]. The game presents challenges for reinforcement learning due to long time\nhorizons, partial observability, and high dimensionality of observation and action spaces. Dota 2\u2019s\n\u0003Authors listed alphabetically. Please cite as OpenAI et al., and use the following bibtex for citation: https:\n//openai.com/bibtex/openai2019dota.bib\n1arXiv:1912.06680v1  [cs.LG]  13 Dec 2019rules are also complex \u2014 the game has been actively developed for over a decade, with game logic\nimplemented in hundreds of thousands of lines of code.\nThe key ingredient in solving this complex environment was to scale existing reinforcement\nlearning systems to unprecedented levels, utilizing thousands of GPUs over multiple months. We\nbuilt a distributed training system to do this which we used to train a Dota 2-playing agent called\nOpenAI Five. In April 2019, OpenAI Five defeated the Dota 2 world champions (Team OG1), the\n\ufb01rst time an AI system has beaten an esport world champion2. We also opened OpenAI Five to\nthe Dota 2 community for competitive play; OpenAI Five won 99.4% of over 7000 games.\nOne challenge we faced in training was that the environment and code continually changed as\nour project progressed. In order to train without restarting from the beginning after each change,\nwe developed a collection of tools to resume training with minimal loss in performance which we\ncallsurgery. Over the 10-month training process, we performed approximately one surgery per\ntwo weeks. These tools allowed us to make frequent improvements to our strongest agent within a\nshorter time than the typical practice of training from scratch would allow. As AI systems tackle\nlarger and harder problems, further investigation of settings with ever-changing environments and\niterative development will be critical.\nIn section 2, we describe Dota 2 in more detail along with the challenges it presents. In section 3\nwe discuss the technical components of the training system, leaving most of the details to appendices\ncited therein. In section 4, we summarize our long-running experiment and the path that lead to\ndefeating the world champions. We also describe lessons we\u2019ve learned about reinforcement learning\nwhich may generalize to other complex tasks.\n2 Dota 2\nDota 2 is played on a square map with two teams defending bases in opposite corners. Each\nteam\u2019s base contains a structure called an ancient; the game ends when one of these ancients is\ndestroyed by the opposing team. Teams have \ufb01ve players, each controlling a", " Introduction to Reinforcement Learning,\u201d\nMIT Press Cambridge, MA, USA, 1998.\n[116] D. K. Yau, J. C. Lui, F. Liang, and Y . Yam, \u201cDefending against\ndistributed denial-of-service attacks with max-min fair server-centric\nrouter throttles,\u201d IEEE/ACM Transactions on Networking , vol. 13, no.\n1, pp. 29-42, 2005.\n[117] R. Bhosale, S. Mahajan, and P. Kulkarni, \u201cCooperative machine learn-\ning for intrusion detection system,\u201d International Journal of Scienti\ufb01c\nand Engineering Research , vol. 5, no. 1, pp. 1780-1785, 2014.\n[118] A. Herrero, and E. Corchado, \u201cMultiagent systems for network intru-\nsion detection: A review,\u201d in Computational Intelligence in Security for\nInformation Systems , 2009, pp. 143-154.\n[119] A. Detwarasiti, and R. D. Shachter, \u201cIn\ufb02uence diagrams for team\ndecision analysis,\u201d Decision Analysis , vol. 2, no. 4, pp. 207-228, 2005.\n[120] S. Shamshirband, A. Patel, N. B. Anuar, M. L. M. Kiah, and A. Abra-\nham, \u201cCooperative game theoretic approach using fuzzy Q-learning\nfor detecting and preventing intrusions in wireless sensor networks,\u201d\nEngineering Applications of Arti\ufb01cial Intelligence , vol. 32, pp. 228-\n241, 2014.\n[121] P. Mu \u02dcnoz, R. Barco, and I. de la Bandera, \u201cOptimization of load bal-\nancing using fuzzy Q-learning for next generation wireless networks,\u201d\nExpert Systems with Applications , vol. 40, no. 4, pp. 984-994, 2013.\n[122] S. Shamshirband, N. B. Anuar, M. L. M. Kiah, and A. Patel, \u201cAn\nappraisal and design of a multiagent system based cooperative wireless\nintrusion detection computational intelligence technique,\u201d Engineering\nApplications of Arti\ufb01cial Intelligence , vol. 26, no. 9, pp. 2105-2127,\n2013.\n[123] S. Varshney, and R. Kuma, \u201cVariants of LEACH routing protocol in\nWSN: A comparative analysis,\u201d in The 8th International Conference on\nCloud Computing, Data Science and Engineering (Con\ufb02uence) , 2018,\npp. 199-204.\n[124] G. Caminero, M. Lopez-Martin, and B. Carro, \u201cAdversarial envi-\nronment reinforcement learning algorithm for intrusion detection,\u201d\nComputer Networks , vol. 159, pp. 96-109, 2019.\n[125] M. Lopez-Martin, B. Carro, and A. Sanchez-Esguevillas, \u201cApplication\nof deep reinforcement learning to intrusion detection for supervised\nproblems,\u201d Expert Systems with Applications , vol. 141, 112963, 2020.\n[126] I. A. Saeed, A. Selamat, M. F. Rohani, O. Krejcar, and J. A. Chaudhry,\n\u201cA systematic state-of-the-art analysis of multiagent intrusion detec-\ntion,\u201d IEEE Access , vol. 8, pp. 180184-180209, 2020.\n[127] S. Roy, C. Ellis, S. Shiva, D. Dasgupta, V . Shandilya, and Q. Wu, \u201cA\nsurvey of game theory as applied to network security,\u201d in 43rd Hawaii\nInternational Conference on System Sciences , 2010, pp. 1-10.\n[128] S. Shiva, S. Roy, and D. Dasgupta, \u201cGame theory for cyber security,\u201d\ninThe Sixth Annual Workshop on Cyber Security and Information\nIntelligence Research , 2010, p. 34.\n[129] K. Ramachandran, and Z. Stefanova, \u201cDynamic game theories in\ncyber security,\u201d in International Conference of Dynamic Systems and\nApplications , 2016, vol. 7, pp. 303-310.\n[130] Y . Wang, Y . Wang, J. Liu, Z. Huang, and P. Xie, \u201cA survey of\ngame theoretic", " introduction, 2nd edition . 2017.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-\nden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Ried-\nmiller. Deepmind control suite, 2018.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nInIntelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on , pp. 5026\u2013\n5033. IEEE, 2012.\nGeorge E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical\nreview , 36(5):823, 1930.\nHado van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems , pp.\n2613\u20132621, 2010.\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-\nlearning. In Advances in Neural Information Processing Systems , 2016.\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.\nDueling network architectures for deep reinforcement learning. In International Conference on\nMachine Learning , 2016.\nZiyu Wang, Victor Bapst, Nicolas Heess, V olodymyr Mnih, Remi Munos, Koray Kavukcuoglu,\nand Nando de Freitas. Sample ef\ufb01cient actor-critic with experience replay. In International\nConference on Learning Representations , 2017.\nPaul J Werbos. A menu of designs for reinforcement learning over time. Neural networks for control ,\npp. 67\u201395, 1990.\n11Published as a conference paper at ICLR 2018\n0K6K12K18K24KAlien\n0K15K30K45KBeam Rider\n0 15 30 45 60\nTraining Time (Hours)0K20K40K60K80KDemon Attack\n0 15 30 45 60\nTraining Time (Hours)0K80K160K240K320KStar Gunner\nn= 32\nk= 1\nn= 32\nk= 2n= 32\nk= 4\nn= 32\nk= 8n= 256\nk= 1\nFigure 6: Testing whether improved performance is\ncaused by recency alone: ndenotes the number of\nactors,kthe number of times each transition is repli-\ncated in the replay. The data in the run with n= 32 ,\nk= 8is therefore as recent as the data in the run with\nn= 256 ,k= 1, but performance is not as good.\n010K20K30KAlien64 Actors 128 Actors\n0 8 16 24 32 40\nTraining Time (Hours)020K40K60KDemon Attack\n0 8 16 24 32 40\nTraining Time (Hours)\nAll distinct epsilons\n6 distinct epsilonsFigure 7: Varying the data-generating policies: Red:\n\ufb01xed set of 6 values for \u000f. Blue: full range of val-\nues for\u000f. In both cases, the curve plotted is from\na separate actor that does not add data to the replay\nmemory, and which follows an \u000f-greedy policy with\n\u000f= 0:00164 .\nA R ECENCY OF EXPERIENCE\nIn our main", " Introduction\nDeep reinforcement learning methods for deep reinforcement learning. ICML , 2016. Related Work\nThe earliest attempts to scale up deep reinforcement learn-\ning relied on distributed asynchronous SGD (Dean et al.,\n2012) with multiple workers. Examples include distributed\nA3C (Mnih et al., 2016) and Gorila (Nair et al., 2015), a\ndistributed version of Deep Q-Networks (Mnih et al., 2015).\nRecent alternatives to asynchronous SGD for RL include\nusing evolutionary processes (Salimans et al., 2017), dis-\ntributed BA3C (Adamski et al., 2018) and Ape-X (Horgan\net al., 2018) which has a distributed replay but a synchronous\nlearner.\nThere have also been multiple efforts that scale up reinforce-\nment learning by utilising GPUs. One of the simplest of\nsuch introduction. In Proceedings\nof the 1st ACM SIGPLAN International Workshop on\nMachine Learning and Programming Languages , MAPL\n2017, 2017. ISBN 978-1-4503-5071-6.\nAdamski, I., Adamski, R., Grel, T., Jedrych, A., Kaczmarek,\nK., and Michalewski, H. Distributed deep reinforcement\nlearning: Learn how to play atari games in 21 minutes.\nCoRR , abs/1801.02852, 2018.\nAppleyard, J., Kocisk \u00b4y, T., and Blunsom, P. Optimizing\nperformance of recurrent neural networks on gpus. CoRR ,\nabs/1604.01946, 2016.\nBabaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and\nKautz, J. GA3C: GPU-based A3C for deep reinforcement\nlearning. NIPS Workshop , 2016.\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney,\nW., Horgan, D., Tirumala, D., Muldal, A., Heess, N., and\nLillicrap, T. Distributional policy gradients. ICLR , 2018.\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wain-\nwright, M., Kuttler, H., Lefrancq, A., Green, S., Valdes,\nV ., Sadik, A., Schrittwieser, J., Anderson, K., York, S.,\nCant, M., Cain, A., Bolton, A., Gaffney, S., King, H.,\nHassabis, D., Legg, S., and Petersen, S. Deepmind lab.\nCoRR , abs/1612.03801, 2016.\nBellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M.\nThe Arcade Learning Environment: An evaluation plat-\nform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch , 47:253\u2013279, June 2013a.\nBellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation platform\nfor general agents. J. Artif. Intell. Res.(JAIR) , 47:253\u2013279,\n2013b.\nChen, J., Monga, R., Bengio, S., and J \u00b4ozefowicz,\nR. Revisiting distributed synchronous SGD. CoRR ,\nabs/1604.00981, 2016.\nChetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran,\nJ., Catanzaro, B., and Shelhamer, E. cudnn: Ef\ufb01cient\nprimitives for deep learning. CoRR , abs/1410.0759, 2014.Clemente, A. V ., Mart \u00b4\u0131nez, H. N. C., and Chandra, A. Ef-\n\ufb01cient parallel Appendix C.1 .\nthe high diversity in visual appearance and game mechanics\nwithin the ALE suite, IMPALA multi-task still manages\nto stay competitive to A3C, shallow, experts , commonly\nused as a baseline in Experiments\nAll agents trained on Atari are equipped only with a feed forward network and pre-process frames in the same way as\ndescribed in Mnih et al. (2016). When training experts agents, we use the same hyperparameters for each game for\nboth IMPALA and A3C. These hyperparameters are the result of tuning A3C with a shallow network on the following\ngames: breakout ,pong ,space invaders ,seaquest ,beam rider ,qbert . Following experiments. related work, experts\nuse game-speci\ufb01c action sets.\nThe multi-task agent was equipped with a feed forward residual network (see Figure 3 ). The learning rate, entropy\nregularisation, RMSProp \"and gradient clipping threshold were adapted through population based training. To be able to\nuse the same policy layer on all Atari games in the multi-task setting we train the multi-task agent on the", " Introduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient Background: Policy Optimization\n2.1 Policy Gradient Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At/bracketrightbigg\n(3)\nsubject to \u02c6Et[KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]]\u2264\u03b4. (4)\nHere,\u03b8oldis the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]/bracketrightbigg\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, results and learning curves for all 49 games is provided in Experiments\n6.1 Comparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIPto several natural variations and ablated versions.\nNo clipping or penalty: Lt(\u03b8) =rt(\u03b8)\u02c6At\nClipping: Lt(\u03b8) = min(rt(\u03b8)\u02c6At,clip(rt(\u03b8)),1\u2212/epsilon1,1 +/epsilon1)\u02c6At\nKL penalty (\ufb01xed or adaptive) Lt(\u03b8) =rt(\u03b8)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old,\u03c0\u03b8]\n5For the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping ( /epsilon1)\nand the KL penalty ( \u03b2,dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe Results from continuous control benchmark. Average normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2was initialized at 1.\n6.2 Comparison to Other Algorithms in the", " Introduction\nReinforcement learning algorithms aim at learning policies\nfor achieving target tasks by maximizing rewards provided\nby the environment. In some scenarios, these rewards are\nsupplied to the agent continuously, e.g. the running score\nin an Atari game (Mnih et al., 2015), or the distance be-\ntween a robot arm and an object in a reaching task (Lilli-\ncrap et al., 2016). However, in many real-world scenarios,\nrewards extrinsic to the agent are extremely sparse or miss-\n1University of California, Berkeley. Correspondence to:\nDeepak Pathak <pathak@berkeley.edu >.\nProceedings of the 34thInternational Conference on Machine\nLearning , Sydney, Australia, 2017. JMLR: W&CP. Copyright\n2017 by the author(s).\n(a) learn to explore in Level-1\n (b) explore faster in Level-2\nFigure 1. Discovering how to play Super Mario Bros without re-\nwards . (a) Using only curiosity-driven exploration, the agent\nmakes signi\ufb01cant progress in Level-1. (b) The gained knowledge\nhelps the agent explore subsequent levels much faster than when\nstarting from scratch. Watch the video at http://pathak22.\ngithub.io/noreward-rl/\ning altogether, and it is not possible to construct a shaped\nreward function. This is a problem as the agent receives\nreinforcement for updating its policy only if it succeeds in\nreaching a pre-speci\ufb01ed goal state. Hoping to stumble into\na goal state by chance (i.e. random exploration) is likely to\nbe futile for all but the simplest of environments.\nAs human agents, we are accustomed to operating with re-\nwards that are so sparse that we only experience them once\nor twice in a lifetime, if at all. To a three-year-old enjoy-\ning a sunny Sunday afternoon on a playground, most trap-\npings of modern life \u2013 college, good job, a house, a family \u2013\nare so far into the future, they provide no useful reinforce-\nment signal. Yet, the three-year-old has no trouble enter-\ntaining herself in that playground using what psychologists\ncall intrinsic motivation (Ryan, 2000) or curiosity (Silvia,\n2012). Motivation/curiosity have been used to explain the\nneed to explore the environment and discover novel states.\nThe French word \ufb02\u02c6aneur perfectly captures the notion of a\ncuriosity-driven observer, the \u201cdeliberately aimless pedes-\ntrian, unencumbered by any obligation or sense of urgency\u201d\n(Cornelia Otis Skinner). More generally, curiosity is a way\nof learning new skills which might come handy for pursu-\ning rewards in the future.\nSimilarly, in reinforcement learning, intrinsic motiva-\ntion/rewards become critical whenever extrinsic rewards\nare sparse. Most formulations of intrinsic reward can be\ngrouped into two broad classes: 1) encourage the agent\nto explore \u201cnovel\u201d states (Bellemare et al., 2016; LopesarXiv:1705.05363v1  [cs.LG]  15 May 2017Curiosity-driven Exploration by Self-supervised Prediction\net al., 2012; Poupart et al., 2006) or, 2) encourage the agent\nto perform actions that reduce the error/uncertainty in the\nagent\u2019s ability to predict the consequence of its own ac-\ntions (i.e. its knowledge about the environment) (Houthooft\net al., 2016; Mohamed & Rezende, 2015; Schmidhuber,\n1991; 2010; Singh et al., 2005; Stadie et al., 2015).\nMeasuring \u201cnovelty\u201d requires a statistical model of the dis-\ntribution of the environmental states, whereas measuring\nprediction error/uncertainty requires building a model of\nenvironmental dynamics that predicts the next state ( st+1)\ngiven the current state ( st) and the action ( at) executed\nat timet. Both these models are hard to build in high-\ndimensional continuous state spaces such as images. An\nadditional challenge lies in dealing with the stochasticity of\nthe agent-environment system, both due to the noise in the\nagent\u2019s actuation, which causes"], "bleu": 0.0, "rouge_l": 0.3040330920372285, "gpt_metric_score": 0.5, "bert_score": 0.29401615262031555}
{"paper_key": "LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged Robots", "current_5q": "**[Question 1] - What is the problem?**  \nHow can we effectively bridge the sim-to-real gap in reinforcement learning for legged robots to enhance their performance and robustness in real-world environments?\n\n---\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses a fundamental challenge in applying reinforcement learning to real-world robotic control. By bridging the sim-to-real gap, we can significantly improve the reliability and adaptability of robotic systems, leading to advancements in various applications such as autonomous navigation, search and rescue operations, and assistive technologies. This research could pave the way for more efficient training methodologies, reducing the need for extensive real-world data collection, and ultimately fostering the development of more capable and intelligent robotic systems.\n\n---\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the inherent data-hungry nature of reinforcement learning methods, which require extensive real-world experience that is costly and time-consuming to obtain. Additionally, the absence of privileged knowledge in real-world settings complicates the learning process, particularly in complex environments like stairs, where precise information is critical for effective locomotion. Naive approaches that rely solely on real-world data may fail due to the noisy observations and the instability they introduce during training. Furthermore, the No Free Lunch Theorem suggests that a trade-off exists between generalization and specific performance, making it difficult to achieve robust policies without a well-structured training framework.\n\n---\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has attempted to address the sim-to-real gap through various methods, such as reshaping reward functions and utilizing sample-efficient algorithms. However, these approaches often fall short in generating superior locomotion policies and maintaining stable performance when trained directly in real-world environments. The limitations of existing solutions include their vulnerability during training and the inability to effectively leverage the advantages of simulation training. Our approach differs by proposing LoopSR, which utilizes a transformer-based encoder to extract relevant features from the latent space, allowing for a more effective integration of simulation data while minimizing the reliance on extensive real-world data.\n\n---\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology, LoopSR, involves a transformer-based encoder that leverages an autoencoder architecture and contrastive loss to extract features necessary for reconstructing the simulation environment. We will utilize both learning-based and retrieval-based methods to derive simulation parameters from the latent variable", "proposal_5q": "[Question 1]: What is the problem?  \nThe specific research question I aim to address is: How can a hybrid multi-agent reinforcement learning framework, integrated with a lifelong learning approach and transformer-based architectures, dynamically optimize strategies for speaker diarization in real-time competitive environments?\n\n[Question 2]: Why is it interesting and important?  \nSolving this problem is crucial as it addresses a significant challenge in the field of speaker diarization, which is the accurate identification and segmentation of speakers in diverse acoustic environments. The broader implications of this research extend to various applications, including automated transcription services, meeting analytics, and human-computer interaction systems. By developing a framework that adapts to real-time feedback and optimizes resource management strategies, this research can lead to advancements in machine learning methodologies, enhance the efficiency of communication technologies, and improve user experiences. Furthermore, the integration of lifelong learning and transformer architectures could provide new insights into cooperative multi-agent systems, thereby influencing future research in AI and its applications across multiple domains.\n\n[Question 3]: Why is it hard?  \nThe challenges involved in solving this problem include the dynamic nature of real-world environments, where acoustic conditions can fluctuate significantly, making it difficult to maintain high accuracy in speaker diarization. Naive approaches may fail due to their inability to adapt to new information or changing contexts, leading to suboptimal performance. Additionally, the complexities of integrating multi-agent reinforcement learning with lifelong learning frameworks present technical obstacles, such as ensuring effective communication and coordination among agents, managing the trade-offs between exploration and exploitation, and developing robust evaluation metrics that account for varying conditions. Overcoming these challenges requires a sophisticated understanding of both reinforcement learning and the intricacies of speaker diarization.\n\n[Question 4]: Why hasn't it been solved before?  \nPrevious research in speaker diarization has primarily focused on static models that do not account for the variability of real-world scenarios, leading to limitations in adaptability and robustness. Many existing solutions lack the integration of lifelong learning principles, which would allow models to continually refine their strategies based on new data and experiences. Additionally, previous work has not fully explored the potential of transformer-based architectures in this context, which could enhance the system's capacity for contextual understanding and cooperation among agents. My approach differs by systematically combining these elements into a cohesive framework that leverages real-time feedback, thereby addressing the gaps in adaptability and performance that have hindered prior research efforts.\n\n[Question 5]: What are the key components of my approach and results?  \nMy proposed methodology involves developing a hybrid multi-agent reinforcement learning framework that incorporates lifelong learning principles and transformer-based architectures. The framework will utilize a diverse dataset comprised of varied acoustic environments to train the agents and refine the speaker diarization processes. Key metrics for evaluation will include Diarization Error Rate (DER) and Diarization F1 score, which will measure the accuracy of speaker identification and segmentation. The expected outcomes include improved adaptability of the diarization system to real-time feedback, enhanced accuracy across different acoustic settings, and a robust framework that successfully bridges the gap between simulated environments and practical applications. This approach aims to set a new standard for speaker diarization performance in challenging conditions.", "referenced_intros": [" \n\n1. INTRODUCTION\n\nWe are being bombarded with a vast amount of information due to the growing popularity of the Internet and the development of User Generated Content (UGC)\u00a0(Krumm et\u00a0al., 2008) in recent years.\nTo save users from information overload, recommender systems have been widely applied in today\u2019s short video\u00a0(Liu et\u00a0al., 2019), news\u00a0(Wang et\u00a0al., 2018b) and e-commerce\u00a0(Chen et\u00a0al., 2019b) platforms.\nWhile complicated models\u00a0(Pi et\u00a0al., 2020; Qin et\u00a0al., 2021; Lin et\u00a0al., 2023b; Wang et\u00a0al., 2023c) often offer higher accuracy, their poor efficiency makes online deployment challenging because of latency restrictions\u00a0(Pi et\u00a0al., 2019). On the other hand, simple models\u00a0(Huang et\u00a0al., 2013; Rendle, 2010) have capacity limitations, but they could evaluate a great number of items efficiently because of their low time complexity. Therefore, striking a balance between efficacy and efficiency becomes crucial in order to quickly filter out information that users are interested in. As is shown in Figure\u00a01 (a), one widely used solution in the industry is multi-stage cascade ranking systems\u00a0(Wang et\u00a0al., 2011).\nThe system includes a retriever and a variety of subsequent rankers.\nIn the very first stage of the cascade system, referred to as the retrieval stage in this paper (also called matching stage or recall stage in some literature\u00a0(Qin et\u00a0al., 2022; Zhu et\u00a0al., 2022)), a retriever is typically used to quickly eliminate irrelevant items from a large pool of candidates, whereas rankers in the later stages aim to accurately rank the items. Each stage selects the top-K\ud835\udc3eKitalic_K items it receives and feeds them to the next stage.\nAs shown in Figure\u00a01 (a), rankers in multi-stage cascade ranking systems are arranged in the shape of a funnel, narrowing from bottom to top. The retrieval and ranking stage are two typical stages, while pre-ranking\u00a0(Wang et\u00a0al., 2020d) and re-ranking\u00a0(Xi et\u00a0al., 2023a) stages are relatively optional, and the number of rankers in the system may vary depending on different scenarios. Additionally, on the left side of Figure\u00a01 (a), we display the approximate output scale of each stage, noting that the range of this scale is specific to the particular platform and scenario.\n\n\nFigure 1. The multi-stage architecture in modern recommender systems and the illustration of multi-channel retrieval. The latter will be detailed further in Section \u00a02.4.\n\n\nAlthough both the retrieval and ranking stages aim to select the most relevant items, each stage has its own unique characteristics.\n\n\n\u2022\n\nDifference in candidate sets (i.e., inference spaces).\nThe retrieval stage needs to quickly filter through the entire item pool, which may contain millions of items; while the ranking stage only needs to score and order the items that have been selected by the retrieval methods, typically narrowing down to hundreds or thousands of items.\n\n\n\n\u2022\n\nDifference in input features.\nDuring the retrieval stage, due to time constraints and the need to filter through a large candidate set quickly, utilizing complex feature interactions is impractical for real-time online requirements. As a result, only limited, coarse-grained features of users and items are considered.\nIn contrast, the ranking stage can utilize a diverse set of features by designing various feature interaction operators, such as product operators\u00a0(Qu et\u00a0al., 2016), convolutional operators\u00a0(Li et\u00a0al., 2019a), and attention operators\u00a0(Xiao et\u00a0al., 2017). The ranking stage further enhances its capability", " \n\nI Introduction\n\n\nReinforcement learning (RL) has demonstrated superior performance in many robotic simulators [1, 2, 3]. However, transferring the controllers learned in simulators to real robots has long been a very challenging question in the RL community. One difficulty of sim-to-real transfer is that the learned policies are highly specific to the dynamics and tasks in the simulators, making them difficult to generalize to many real-world tasks, in which sim-to-real gaps exist. In the existing practices of sim-to-real transfer methods\u00a0[4, 5, 6, 7], the controllers learned in the simulator are usually difficult to generalize across various tasks and dynamic environments.\n\n\nRecent studies in the theoretical RL community on the spectral decomposition of Markov decision processes (MDPs) [8, 9, 10, 11, 12] reveal the idea of task-independent representations for RL. The results of spectral decomposition is the spectral functions of the transition dynamics in MDPs. The spectral functions can linearly represent the state-action value function, i.e., the Q\ud835\udc44Qitalic_Q-function, induced by any policy. Therefore, we say the spectral functions are task-independent representation of skills, because the spectral functions include information needed to accomplish any tasks.\nThese task-independent skill representations are shared across arbitrary tasks, thus are reusable and transferable. Meanwhile, the representation can also be used to synthesize policies.\nGiven the representation-based skill sets and a specific task, we can perform sample-efficient planning upon the skill sets to synthesize the optimal policy. When the representations are unknown, sample-efficient representation learning methods have been proposed including maximum likelihood estimation [11], contrastive learning\u00a0[13], spectral conditional density estimation [12], or variational inference [14].\n\n\nNevertheless, these representation-based skill learning are still designed for specific transition dynamics. When it applies to sim-to-real transfer,\nthe sim-to-real gap, which will induce new skills different from the simulator skill sets, has not been investigated yet.\nLearning the sim-to-real gap from real-world data, also called residual dynamics learning [15, 5, 16, 17], naturally aligns with our representation learning viewpoint. However, naively learning the representations of residual dynamics might lead us to relearn redundant skills that are linearly dependent with the existing simulator skill sets. Therefore, we need additional incentives to discover new skills that enable us to bridge the sim-to-real gap.\n\n\nTo further leverage the transferability of the representation-based skill sets and discover new skills induced by the sim-to-real gap, we proposed the Skill TransfEr And DiscoverY (STEADY) for sim-to-real representation learning algorithm. We show that recent theoretical representation learning algorithms for spectral decomposition of MDPs, such as [12, 18], can apply to learning transferable representations of real-world robots. Moreover, we handle the sim-to-real gap by augmenting distinct representation-based skills learned from the sim-to-real gap, which we refer to as skill discovery. During the learning process, orthogonal constraints between the newly discovered skill sets and the simulator skill sets are enforced to fill the sim-to-real gap.\nIn this way, we ensure that the skills necessary for the real robots are also included in our augmented skill sets, upon which the planning can be handled in a more complete space efficiently.\nMeanwhile, to ensure the policy transferring smoothly from the simulator to the real-world, we also designed a mechanism to characterize the policy", " \n\n1 Introduction\n\nFollowing the impressive recent success of reinforcement learning (RL) [30, 40, 31, 3] in various applications, a plethora of research has been done in improving the learning efficiency of RL algorithms. One important avenue of the extension is the Continual Reinforcement Learning (CRL), in which an agent aims to continuously learn and improve its decision-making policy over sequentially arriving tasks without forgetting previously learned tasks.\nThe motivation for such extension is clear since it is not practical to either re-train an agent to learn multiple tasks seen so far or train a dedicated agent for each task whenever a new task to learn arrives. The need for CRL is particularly pressing when the sequentially arriving tasks to learn are similar to each other as in robot action learning [18].\n\n\nIn general, one of the main challenges of continual learning (CL) is to effectively transfer the learned knowledge to a new task (i.e., improve plasticity) while avoiding catastrophic forgetting of previously learned knowledge (i.e., improve stability). So far, most of the CRL methods [27, 29, 35, 46] also focus on addressing such a challenge, largely inspired by the methods developed in the supervised learning counterparts; e.g., improving the stability by regularizing the deviation of the important parameters [17, 49, 2, 16], storing the subset of dataset on previous tasks [6, 7, 22] or isolating the important parameters [26, 25, 14, 47]. Furthermore, several works mainly focused on improving the plasticity of the network by transferring the knowledge from previous tasks [37, 39] or selectively re-using the important parts for learning new tasks [29, 27, 28].\n\n\nDue to the aforementioned trade-off, it is generally understood that the plasticity degradation occurs in continual learning mainly due to the emphasis on stability. However, several recent work pointed out that, particularly in RL, the plasticity of a learner can decrease even when learning a single task [33, 21, 19, 23, 24, 41, 4], in which the stability is not considered at all. Those works identified that the occurrence of such plasticity loss may be largely due to using non-stationary targets while learning the value function.\nThese findings give some clues for understanding the plasticity degradation phenomenon in CRL, which occurs quite often not only when learning each task but also when task transition happens, but not the full explanation.\n\n\nNamely, in CRL, even when the simple fine-tuning is employed for sequentially learning tasks, it is not hard to observe that a learner already suffers from learning a new task as we show in our experiments in later sections. We may attempt to explain this plasticity degradation of fine-tuning, which does not consider stability whatsoever, through the lens of the plasticity loss mentioned above; i.e., since the non-stationarity of the learning objectives (or the reward functions) arises when task transition happens, the plasticity loss occurs and hampers the learning ability. However, as we observe from our careful empirical analyses, above explanation is not fully satisfactory since such plasticity degradation turns out to be dependent on what specific task a learner has learned previously. That is, we show that the dissimilarity between", " \n\n1 Introduction\n\nA useful agent is one that can accomplish many objectives in a domain. Household robots are more beneficial the more chores they can complete; self-driving cars the more places they can reach. Building upon this premise, we draw inspiration from the recent success of unsupervised learning in language\u00a0(Brown et\u00a0al., 2020) and vision\u00a0(Kirillov et\u00a0al., 2023), which has shown that a single generalist model trained on Internet-scale data can immediately solve a wide array of tasks without further training or fine-tuning. Motivated by these successes, we study an analogous way to train a generalist agent from unlabeled offline data such that it can immediately solve new user-specified tasks in a without training. This has been referred to as the zero-shot reinforcement learning (RL) problem\u00a0(Touati et\u00a0al., 2022). From this data, the hard challenge is how to discover, without labels, a task representation that is robust to downstream objectives \u2013 in essence, bypassing the need for a human to specify well-shaped reward functions before training.\n\n\nIn this work, we aim to provide a simple, scalable approach to the zero-shot RL problem. Our key insight is to directly learn a latent representation that can represent any arbitrary reward functions based on their samples of state-reward pairs. We refer to this idea as Functional Reward Encoding (FRE). This is in contrast to previous works in zero-shot RL or multi-task RL that employ domain-specific task representations\u00a0(Barreto et\u00a0al., 2017; Li et\u00a0al., 2020a) or highly restrictive linear reward structures\u00a0(Borsa et\u00a0al., 2018; Touati & Ollivier, 2021; Touati et\u00a0al., 2022). By directly encoding reward functions into a latent space, we can pre-train a multi-task agent with a host of unsupervised reward functions of arbitrary diversity, and quickly identify the representations corresponding to new test tasks given a small number of reward-annotated samples.\n\n\nTraining an FRE requries utilizing a prior distribution over reward functions. When no information about downstream tasks is available, we must define a prior that broadly spans possible objectives in a domain-agnostic manner. In our experiments, we show that a mixture of random unsupervised reward functions, such as goal-reaching and random MLP rewards, are a reasonable choice for the reward prior. We optimize an FRE-conditioned policy towards all rewards within this space. In this way, approximate solutions to many downstream tasks have already been learned, and the zero-shot RL problem reduces to simply locating the FRE encoding for the task, which the learned encoder accomplishes.\n\n\nThus, our framework presents a simple yet scalable method for training zero-shot RL agents in an unsupervised manner, as shown in Figure 1. The main idea is to (1) train an FRE network over random unsupervised reward functions, then (2) optimize a generalist FRE-conditioned policy towards maximizing said rewards, after which (3) novel tasks can be solved by simply encoding samples of their reward functions, such that the FRE agent can immediately act without further training.\n\n\nWe verify the efficacy of our method through experiments on standard offline RL domains. We demonstrate that without any finetuning, FRE policies can\nsolve tasks involving locomotion of an eight-DoF robot through a maze or manipulation of a robotic arm in a kitchen", " \n\n1 Introduction\n\nFigure 1: \nIllustration of HILPs.\n(left) We first train a distance-preserving mapping \u03d5:\ud835\udcae\u2192\ud835\udcb5:italic-\u03d5\u2192\ud835\udcae\ud835\udcb5\\phi:{\\mathcal{S}}\\to{\\mathcal{Z}}italic_\u03d5 : caligraphic_S \u2192 caligraphic_Z\nthat maps temporally similar states to spatially similar latent states (d\u2217superscript\ud835\udc51d^{*}italic_d start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT denotes the temporal distance).\n(right) We then train a latent-conditioned policy \u03c0\u2062(a\u2223s,z)\ud835\udf0bconditional\ud835\udc4e\ud835\udc60\ud835\udc67\\pi(a\\mid s,z)italic_\u03c0 ( italic_a \u2223 italic_s , italic_z ), which we call a Hilbert foundation policy, that spans that latent space with directional movements. This policy captures diverse long-horizon behaviors from unlabeled data,\nwhich can be directly used to solve a variety of downstream tasks efficiently, even in a zero-shot manner.\n\n\n\nGeneralist models that can utilize large amounts of weakly labeled data provide an appealing recipe:\npre-train via self-supervised or unsupervised objectives on large and diverse datasets without ground truth labels,\nand then adapt efficiently via prompting, few-shot learning, or fine-tuning to downstream tasks.\nThis strategy has proven to be extremely effective in settings where simple self-supervised objectives can be used to train on Internet-scale data\u00a0(Brown et\u00a0al., 2020; Ramesh et\u00a0al., 2022),\nleading to models that can quickly adapt to new tasks for pattern recognition\u00a0(Kirillov et\u00a0al., 2023),\nquestion answering\u00a0(Ouyang et\u00a0al., 2022),\nand even diverse AI-assistant applications\u00a0(Chen et\u00a0al., 2021b).\nMotivated by this observation,\na number of works have recently sought to propose self-supervised objectives to pre-train generalist policies\nfor reinforcement learning (RL) and control\u00a0(Reed et\u00a0al., 2022; Padalkar et\u00a0al., 2024).\nWe can broadly refer to the resulting models as foundation policies:\ngeneral-purpose policies that can rapidly adapt to solve a variety of downstream tasks.\n\n\nHowever, unlike natural language processing,\nwhere next token prediction has become the standard pre-training objective\u00a0(Brown et\u00a0al., 2020),\nfinding the best policy pre-training objective from data remains a major open question in RL.\nPrior works have proposed several ways to pre-train generalist policies\nbased on diverse objectives, such as\nbehavioral cloning (BC)\u00a0(Ajay et\u00a0al., 2021; Reed et\u00a0al., 2022; Padalkar et\u00a0al., 2024),\noffline goal-conditioned RL (GCRL)\u00a0(Chebotar et\u00a0al., 2021; Eysenbach et\u00a0al., 2022; Park et\u00a0al., 2023),\nand unsupervised skill discovery\u00a0(Gregor et\u00a0al., 2016; Machado et\u00a0al., 2017; Eysenbach et\u00a0al., 2019; Park et\u00a0al., 2024).\nHowever, none of these objectives is ideal: behavioral cloning requires expert demonstrations,\nwhich limits the availability of data,\ngoal-conditioned RL can only yield goal-reaching behaviors, and unsupervised skill discovery methods, though general and principled,\ncan present major challenges in terms of scalability, optimization, and offline learning.\n\n\nIn this work, we propose a general offline pre-training objective for foundation policies\nthat capture diverse, optimal \u201clong-horizon\u201d behaviors from unlabeled data\nto facilitate downstream task learning.\nOur main idea is to discover the temporal structure of states through offline data,\nand to represent this structure in such a way that\nwe can quickly and accurately obtain optimal policies\nfor any arbitrary new tasks from relatively concise \u201cprompts\u201d\n(e.g., a small number of states annotated with rewards, target goals, etc.).\nWe begin by learning a\ngeometric abstraction of the dataset,\nwhere distances between representations of states\ncorrespond to their long-horizon global relationships.\nSpecifically, we train a representation \u03d5:\ud835\udcae\u2192\ud835\udcb5:italic-\u03d5\u2192\ud835\udcae\ud835\udcb5\\phi:{\\mathcal{S}}\\to{\\mathcal{Z}}italic_\u03d5 : caligraphic_S \u2192 caligraphic_Z\nthat maps the state space \ud835\udcae\ud835\udcae{\\mathcal{S}}caligraphic_S into a Hilbert space \ud835\udcb5\ud835\udcb5{\\mathcal{Z}}caligraphic_Z\n(i.e., a metric space with a well-defined inner product)\nsuch that\n\n\n\nd\u2217\u2062(s,g)=\u2016\u03d5\u2062(s)\u2212\u03d5\u2062(g)\u2016superscript\ud835\udc51\ud835\udc60\ud835\udc54normitalic-\u03d5\ud835\udc60italic-\u03d5\ud835\udc54\\displaystyle d^{*}(s,g)=\\|\\phi(s)-\\phi(g)\\|italic_d start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT ( italic_s , italic_g ) = \u2225 italic_\u03d5 ( italic_s ) - italic_\u03d5 ( italic_g ) \u2225\n\n(1)\n\n\nholds for every s,g\u2208\ud835\udcae\ud835\udc60\ud835\udc54\ud835\udcaes,g\\in{\\mathcal{S}}italic_s , italic_g \u2208 caligraphic_S,\nwhere d\u2217superscript\ud835\udc51d^{*}italic_d start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT denotes the temporal distance (i.e., the minimum number of time steps needed for", " \n\n1 Introduction\n\nDespite the success of Reinforcement Learning\u00a0(RL) in scenarios where online interaction is consistently available, RL is hampered from real-world applications such as healthcare and robotics controlling due to its sample complexity\u00a0(Haarnoja et\u00a0al. 2018) and inferior generalization ability\u00a0(Kirk et\u00a0al. 2023). The past decade witnessed tremendous effort from researchers to pave the path for RL toward real-world applications. For example, offline RL\u00a0(Fujimoto, Meger, and Precup 2019; Kumar et\u00a0al. 2020; Fujimoto and Gu 2021; Kostrikov, Nair, and Levine 2022), which optimizes the policies with a pre-collected and static dataset, provides a solution to relieving RL from costly online interactions, whereas meta-RL\u00a0(Duan et\u00a0al. 2016; Finn, Abbeel, and Levine 2017; Rakelly et\u00a0al. 2019; Zintgraf et\u00a0al. 2020; Fu et\u00a0al. 2021), which involves training policies over a wide range of tasks, significantly enhances the generalization ability of the learned policies.\n\n\nOffline Meta-Reinforcement Learning\u00a0(OMRL)\u00a0(Li et\u00a0al. 2020; Li, Yang, and Luo 2021; Dorfman, Shenfeld, and Tamar 2021; Mitchell et\u00a0al. 2021; Yuan and Lu 2022), as an intersection of offline RL and meta-RL, is promising to combine the good of both worlds. In OMRL, we are provided with datasets collected in various tasks which share some similarity in the underlying structures in dynamics or reward mechanisms, and aim to optimize the meta-policy. The meta-policy is later tested in tasks drawn from the same task distribution. Previous related methods\u00a0(Li et\u00a0al. 2020; Li, Yang, and Luo 2021; Yuan and Lu 2022) often interpret the OMRL challenge as task representation learning and meta-policy optimization. The former step aims to obtain indicative task representations from the dataset, while the latter optimizes a meta-policy on top of the learned representation.\nHowever, existing methods often assume a sufficient number of training tasks as well as sufficient diversity of behavior policy that collects the datasets, which is not realistic in real-world applications. We find that when the assumptions are not satisfied, the representations tend to overfit and fail to generalize on unseen testing tasks.\n\n\nIn light of this, we propose a new approach to Generalizable Task representations Learning\u00a0(GENTLE) to enable effective task recognition in the face of limitations in training task quantity and behavior diversity. GENTLE follows the existing paradigm of OMRL and consists of two interleaving optimization stages: (1) task representation learning and (2) offline meta-policy optimization on top of the learned representations. For (1), we introduce a novel structure, Task Auto-Encoder\u00a0(TAE) to extract representations from the context information. TAE is optimized to reconstruct the state transition and rewards on the probing data rather than contrastive loss, which models the generative structure of the environment and prevents the encoder from overfitting to miscellaneous features when the number of training tasks is limited. To alleviate TAE\u2019s training from overfitting to the behavior policy distribution, we augment the training data via policy, dynamics, and reward relabeling, forcing TAE to learn to exploit the difference in dynamics and rewards rather than input data distributions. For (2), we adopt TD3+BC for its simplicity to optimize a meta-policy with task representations predicted by the TAE.\n\n\nFor evaluations, we compare GENTLE against other baseline algorithms in a set of continuous control tasks with two types of", " \n\nI Introduction\n\n\nLarge language models (LLMs) have achieved remarkable success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks\u00a0[1], notably producing \u201challucinations\u201d\u00a0[2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applications.\n\n\nFigure 1: Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques.\n\n\nRAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown in Figure\u00a01. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG\u2019s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre-Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques[3, 4, 5].The subsequent arrival of ChatGPT\u00a0[6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more complex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques.\n\n\nThe burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of \u201cRetrieval,\u201d \u201cGeneration,\u201d and \u201cAugmentation.\u201d On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations.\n\n\nOur", "Abstract \u2014 Deep reinforcement learning (RL) can enable\nrobots to autonomously acquire complex behaviors, such as\nlegged locomotion. However, RL in the real world is complicated\nby constraints on efficiency, safety, and overall training stability,\nwhich limits its practical applicability. We present APRL, a\npolicy regularization framework that modulates the robot\u2019s\nexploration over the course of training, striking a balance\nbetween flexible improvement potential and focused, efficient\nexploration. APRL enables a quadrupedal robot to efficiently\nlearn to walk entirely in the real world within minutes and\ncontinue to improve with more training where prior work\nsaturates in performance. We demonstrate that continued\ntraining with APRLresults show that APRL is\nsignificantly better equipped than na \u00a8\u0131ve RL to continually\nimprove as it collects more data, as opposed to quickly\nreaching but plateauing with limited capabilities.\nTransferring to different scenarios. We find that APRL not\nonly successfully enables a quadrupedal robot trained only\nin the real world to walk amid a variety of conditions, but\nalso to keep improving as it continues to be deployed. Quan-\ntitatively, the policy learned with APRL even without fine-\ntuning is significantly better on average at walking than the\nRestricted policy in terms of average velocity (see Figure 7)\nand at completing a given path faster and with fewer falls (see\nFigure 8). The exception is when we freeze a joint, in which\ncase the Restricted policy generalizes much better during\nzero-shot evaluation. In this case, we find that with continued\ntraining, APRL can quickly learn to overcome this gap. In\nFigure 5, we show a qualitative comparison of policies where\nthe path can be visualized with a static camera. We encourage\nthe reader to view the qualitative differences in policies for\neach scenario on our project website.\nVI. S IMULATED ANALYSIS\nIn this section, we analyze APRL using a simulated\nversion of the task described in Section III. Although simu-\nlation does not model many of the real-world complexities\nthat we aim to address, we use it to perform controlledexperiments for comparison purposes and insight. We design\nour simulatedmethods can\ncause the networks to lose plasticity, the ability to continue\nlearning with more data, and propose periodic resets of the\nagent to mitigate this effect. Resetting specifically implies\nreinitialization of network weights and optimizer states while\nmaintaining the replay buffer. We incorporate this regularizer\ninto our adaptive strategy as we will describe next.\nIV. E FFICIENT LEARNING OF LEGGED LOCOMOTION\nWITH ADAPTIVE POLICY REGULARIZATION\nWe present our system for efficiently learning and fine-\ntuning quadrupedal locomotion in real-world scenarios using\nAdaptive Policy ReguLarization (APRL). Our framework,\nshown in Figure 2, involves dynamically modulating policy\nregularization over the course of training to provide the\npolicy with adequate room to explore and improve, but not\nso unbridled as to lead to inefficient\u2014and often violent\u2014\ntraining. To do so, we introduce \u2018soft\u2019 constraints on the\nactions (defined in (b)) that are adjusted based on how\n\u2018familiar\u2019 the robot is in its current situation (described in\n(c)). We also incorporate resets to improve plasticity, i.e., the\nability to keep learning from new data. In the remainder of\nthis section, we describe the principle underlying our choice\nof regularization. We then detail how we adapt the constraints\nbased on the robot\u2019s learning progress and finally how we\nimplement them in practice. Algorithm 1 summarizes the\ntraining procedure in pseudocode.\na) An efficiency-performance trade-off: Prior work has\nshown that explicit action limits have an enormous effect on\nlearning", " INTRODUCTION\nAnimals have evolved a spectrum of locomotion gaits to\nmaximize their robustness and efficiency at different terrains\nand speeds [1], [2]. Reproducing such natural gait transitions\nhas been a challenging topic in the legged robotic community\ndue to the complexity of multi-legged systems. To produce\nstable gait motions, most methods using only proprioception, in two tests of robust\nand agile locomotion:\n1)Baseline [17]: Our previous work used the teacher-\nstudent training framework and the AMP dataset of\ntrotting gait.\n2)MoB [26]: The policy was trained with more gait\nparameters, which can tune gait behaviors to aid\ngeneralization to different tasks, according to user\u2019s\ncommands.\n1The video is available in https://youtu.be/MoFm6 JVNkoThe two tests are shown in Fig. 5. The robust test is climbing\nstairs with a width of 25 cm and a height of 20 cm , while the\nagile test is sprinting over vegetation with a vcmd\nxof2.5 m/s.\nFig. 5. The robust (left) and agile (right) tests outdoors.\nWe conducted five runs for each test and computed success\nrates. A test was successful if the robot could climb over the\nstairs or sprint out of the vegetation. The result is shown\nin Table V. Since the Baseline was trained using only the\nTABLE V\nOVERALL COMPARISON WITH THREE Methods Multiple gaits Robust test ( %) Agile test ( %)\nBaseline No 40 60\nMoB Yes 0 20\nOurs Yes 100 80\nAMP dataset of trotting gait, it is unable to achieve multiple\ngaits control compared to the MoB and Ours, which were\ntrained with gait parameters and gait-dependent rewards.\nHowever, the Baseline and Ours are more robust and agile\nthan the MoB, as shown in Table V. This can be explained\nby the fact that the MoB was trained on the flat and focused\non the detailed control of gait behaviors, while the others\nwere trained over different terrains and had more flexibilityto lean adaptive behaviors. Although the MoB can tune\ngait details to aid generalization to different tasks, it can\nhardly generalize to challenging terrains by tuning only gait\nparameters. Moreover, Ours has higher success rates than\nthe Baseline when performing robust and agile locomotion.\nThis performance difference is likely due to the different\nlearning frameworks. Ours used the asymmetric actor-critic\nframework, which had smaller sim-to-real gap than the\nBaseline that used the teacher-student framework.\nV. experiments show that our robot can achieve\nmultiple gaits according to user\u2019s commands, while\nachieving robust and agile locomotion.\nII. METHOD\nA. Reinforcement Learning Problem Formulation\nSince the terrains are not fully observable without exte-\nroceptive sensors, our locomotion problem is modeled as a\npartially observable Markov decision process (POMDP). The\nenvironment is completely defined by a full state xtat time\nstept. The agent\u2019s policy performs an action at, and then the\nenvironment moves to the next state xt+1with a transition\nprobability P(xt+1|xt,at)and returns a reward rtand a\npartial observation xp\nt+1. The goal of RL is to find a policy\n\u03c0to maximize the expected discounted return over the future\ntrajectory:\nJ(\u03c0) =E\u03c0\"\u221eX\nt=0\u03b3trt#\n, (1)\nwhere \u03b3t\u2208[0,1)is a discount factor.\nRecent works have leveraged the teacher-student training\nparadigm to address the POMDP [11]\u2013[13], [17]. While\nit has been empirically shown that the student policy can\nachieve similar performance to the teacher policy, the student\ncan never outperform the teacher. Moreover, training the\nteacher and student networks sequentially requires more data,\nwhich is sample inefficient.To learn robust blind locomotion in one training phase, we\ntrain our policy using proximal policy optimization (PPO)\n[29], employing", " \n\n1 Introduction\n\nIn The Challenge of Reinforcement Learning, Sutton states: ``Part of the appeal of reinforcement learning is that it is in a sense the whole AI problem in a microcosm'' Sutton (1992). Indeed, the problem facing an agent that learns to make better decisions from experience is at the heart of the study of Artificial Intelligence (AI).\nYet, when we study the reinforcement learning (RL) problem, it is typical to restrict our focus in a number of ways. For instance, we often suppose that a complete description of the state of the environment is available to the agent, or that the interaction stream is subdivided into episodes. Beyond these standard restrictions, however, there is another significant assumption that constrains the usual framing of RL: We tend to concentrate on agents that learn to solve problems, rather than agents that learn forever. For example, consider an agent learning to play Go: Once the agent has discovered how to master the game, the task is complete, and the agent's learning can stop. This view of learning is often embedded in the standard formulation of RL, in which an agent interacts with a Markovian environment with the goal of efficiently identifying an optimal policy, at which point learning can cease.\n\n\nBut what if this is not the best way to model the RL problem? That is, instead of viewing learning as finding a solution, we can instead think of it as endless adaptation. This suggests study of the continual reinforcement learning (CRL) problem Ring (1997, 2005); Khetarpal et\u00a0al. (2022); Kumar et\u00a0al. (2023), as first explored in the thesis by Ring (1994), with close ties to supervised never-ending (Carlson et\u00a0al., 2010; Mitchell et\u00a0al., 2018; Platanios et\u00a0al., 2020) and continual learning (Ring, 1997, 2005; Kirkpatrick et\u00a0al., 2017; Schwarz et\u00a0al., 2018; Nguyen et\u00a0al., 2018; Parisi et\u00a0al., 2019; Rolnick et\u00a0al., 2019; Hadsell et\u00a0al., 2020; Lesort et\u00a0al., 2020; Riemer et\u00a0al., 2022; Baker et\u00a0al., 2023).\n\n\nDespite the prominence of CRL, the community lacks a clean, general definition of this problem. It is critical to develop such a definition to promote research on CRL from a clear conceptual foundation, and to guide us in understanding and designing continual learning agents. To these ends, this paper is dedicated to carefully defining the CRL problem. Our definition is summarized as follows:\nThe CRL Problem (Informal)\nAn RL problem is an instance of CRL if the best agents never stop learning.\n\n\nThe core of our definition is framed around two new insights that formalize the notion of ``agents that never stop learning\": (i) we can understand every agent as implicitly searching over a set of history-based policies (Theorem\u00a03.1), and (ii) every agent will either continue this search forever, or eventually stop (Remark\u00a03.2). We make these two insights rigorous through a pair of logical operators on agents that we call generates and reaches that provide a new mathematical language for characterizing agents. Using these tools, we then define CRL as any RL problem in which all of the best agents never stop their implicit search. We provide two motivating examples of CRL, illustrating that traditional multi-task RL and continual supervised learning", "Abstract \u2014 Quadrupedal robots resemble the physical ability\nof legged animals to walk through unstructured terrains.\nHowever, designing a controller for quadrupedal robots poses\na signi\ufb01cant challenge due to their functional complexity\nand requires adaptation to various terrains. Recently, deep\nreinforcement learning, inspired by how legged animals learn\nto walk from their experiences, has been utilized to synthe-\nsize natural quadrupedal locomotion. However, state-of-the-artmethods, as quantitatively veri\ufb01ed by the high survival rate\nand maximum push that it can withstand. The robust perfor-\nmance was achieved through the interplay between accurate\nestimation and robust policy learning of DreamWaQ. More-\nover, the proposed AdaBoot method also increases robustness\nwithout sacri\ufb01cing the base performance.\nIn the real world, DreamWaQ\u2019s policy is robust against\nunstructured terrains. Fig. 6 shows the robot\u2019s foot re\ufb02ex\nwhen faced with foot stumbling and slipping. The robot can\nimmediately adapt its gait and stabilize its pose. Owing to\nthe robust and accurate CENet, the robot had no problem in\nits body velocity estimation and could continue its journey\nwithout any performance deterioration.\nIn Fig. 6(a), the robot exhibits different gaits for going\ndownstairs and upstairs. When going downstairs, the robot\ntends to tilt its body closer to the ground and maintain its\nfront foot far from the body, which is a key gait pattern\nfor quickly \ufb01nding a stable foothold. Meanwhile, the robot\nadapts its gait for going upstairs by signi\ufb01cantly increasing\nits footsteps. This gait is necessary so that the foot can safely\novercome the stairs and \ufb01nd a stable foothold while climbing.\nMoreover, Fig. 6(b) shows the adaptation to slipping, where\nthe robot can immediately detect irregular footholds and\nadapt its gait pattern. Subsequently, the robot tries to recover\nits normal pattern and continues to walk.\nG. Long-Distance Walk\nWe deployed the robot on two challenging outdoor courses\nto demonstrate the robustness of DreamWaQ. Course A was\nan on-campus yard consisting of many slopes and deformable\nterrains. Course B was an on-campus hill with an elevation\ngain of up to 22 m. Courses A and B have a total length of\n430 m and 465 m, respectively. The details of the courses are\nshown in Fig. 7. The robot\u2019s trajectory was measured using a\nreal-time kinematic (RTK) GPS [39] with a frequency of 10\nHz, mounted on top of the robot. For complete experiment\nvideos, please refer to the project site1.(a)\nFoot stumbleFoot slip\nNormal walkNormal walkNormal walkClimb upstairsGo downstairs\nIrregular footholdAdaptationRecovery(a)(b)Normal walkFig. 6: Foot re\ufb02ex against uncertainties due to (a) stumbling and (b) slipping in unstructured terrains. Real-time experiment videos are available online1.\nABStartFinishFinishStart\nFig. 7: The outdoor trajectory for testing the performance of the DreamWaQ policy was recorded using an RTK\u2013GPS mounted on the robot. Course A\nconsists of many unstructured natural terrains in yards, while course B is a hiking track. The elevations of both courses relative to the starting point (in\n[m]) are shown in the color bars.\n1) Course A: The robot was challenged in unstructured\nnatural tracks with various slopes in this course. The robot\nalso encountered thick vegetation that trapped the robot\u2019s\nlegs. However, the robot successfully adapted its speed by\nincreasing joint power to overcome the trap.\nThe most challenging part of this course is walking\nthrough stairs and deformable slopes. Thanks to the robust-\nness of the policy and accurate estimation of DreamWaQ,\nthe robot could safely walk through the stairs and slopes.\nWe conducted theresults with the most\nrobust", " Introduction\nRecent works have established that quadruped locomotion controllers trained with reinforcement\nlearning in simulation can successfully be transferred to traverse challenging natural terrains [1, 2,\n6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand.arXiv:2212.03238v1  [cs.RO]  6 Dec 20223]. Adaptation to diverse terrains is accomplished by estimating terrain properties from sensory\nobservations that are then used by the controller (i.e., online system identi\ufb01cation ). The success of\nthis paradigm relies on two assumptions: a priori modeling of environment parameters that can vary\nduring deployment and the ability to estimate these parameters from sensory observations. To bypass\nthe \ufb01rst assumption, one possibility is to widely vary a large set of environment parameters during\ntraining. However, this creates a hard learning problem due to creation of challenging or infeasible\nlocomotion scenarios. To simplify learning, typically the designer chooses a subset of parameters\nthat are randomized in a carefully restricted range. Even in this setup, additional measures such as a\nlearning curriculum and reward shaping are necessary for successful learning in simulation.\nAs a result of these practical restrictions on the expressiveness of the simulation, quite often the\nrobot encounters scenarios during deployment that were not modeled during training. For instance,\nif the robot is only presented with \ufb02at ground and terrain geometry is not varied during training,\nit may fail to traverse non-\ufb02at terrains such as stairs. In such a case, it is common to tweak the\ntraining environments or the reward functions and re-train the policy. This iterative loop of re-\ntraining and real-world testing is tedious. To make things worse, in some scenarios such iteration is\ninsuf\ufb01cient because it is not possible to accurately model or sense important environment properties.\nFor example, thick bushes are both hard to simulate due to compliance and hard to sense because\ndepth sensors do not distinguish them from walls. Thus, the robot may attempt to climb over thick\nbushes like a rock or move through them with an overly conservative gait that leaves the robot stuck.\nThe examples above illustrate that even for the most advanced sim-to-real systems, the real world\noffers new challenges. We broadly refer to scenarios that can be simulated but are not anticipated\nduring training and the situations which cannot be simulated or identi\ufb01ed from sensory observations\nasout-of-distribution cases. We present a framework for policy learning that enables improved per-\nformance in out-of-distribution scenarios under some assumptions detailed below. Our key insight\nis that given a task, there are multiple equally good solutions (i.e., under-speci\ufb01cation [4]) that have\nequivalent training performance but can generalize in different ways. For instance, the task of walk-\ning on \ufb02at ground only imposes a constraint on the velocity of robot\u2019s body, but not on how the legs\nshould move, or high should the torso be above the ground, etc. Consider two different walking\nbehaviors: crouch where the robot keeps its torso close to the ground and stomp where the torso is\nhigh and also the legs have a high foot swing. While both crouch andstomp succeed at walking on\n\ufb02at ground, their generalization to out-of-distribution scenarios is different: with crouch the robot\ncan traverse under obstacles but not stairs, whereas with stomp it can climb over curbs/stairs but not\nmove under obstacles (Figure 1).\nOut of the many", " Introduction\nOf what use is vision during locomotion? Clearly, there is a role of vision in navigation \u2013 using\nmaps or landmarks to \ufb01nd a trajectory in the 2D plane to a distant goal while avoiding obstacles. But\ngiven a local direction in which to move, it turns out that both humans [ 1] and robots [ 2,3] can do\nremarkably well at blind walking. Where vision becomes necessary is for locomotion in challenging\nterrains. In an urban environment, staircases are the most obvious example. In the outdoors, we can\ndeal with rugged terrain such as scrambling over rocks, or stepping from stone to stone to cross a\nstream of water. There is a fair amount of scienti\ufb01c work studying this human capability and showing\ntight coupling of motor control with vision [ 4,5,6]. In this paper, we will develop this capability for\na quadrupedal walking robot equipped with egocentric depth vision. We use a reinforcement learning\napproach trained in simulation, which we are directly able to transfer to the real world. Figure 1 and\nthe accompanying videos shows some examples of our robot walking guided by vision.\nHumans receive an egocentric stream of vision which is used to control feet placement, typically\nwithout conscious planning. As children we acquire it through trial and error [ 7] but for adults it is an\nautomatized skill. Its unconscious execution should not take away from its remarkable sophistication.\nThe footsteps being placed now are based on information collected some time ago. Typically, we\ndon\u2019t look at the ground underneath our feet, rather at the upcoming piece of ground in front of us\na few steps away[ 1,4,5,6]. A short term memory is being created which persists long enough to\nguide foot placement when we are actually over that piece of ground. Finally, note that we learn to\nwalk through bouts of steps, not by executing pre-programmed gaits [7].\nWe take these observations about human walking as design principles for the visually-based walking\ncontroller for an A1 robot. The walking policy is trained by reinforcement learning with a recurrent\nneural network being used as a short term memory of recent egocentric views, proprioceptive states,\nand action history. Such a policy can maintain memory of recent visual information to retrieve\ncharacteristics of the terrain under the robot or below the rear feet, which might no longer be directly\nvisible in the egocentric view.\nIn contrast, prior locomotion techniques rely on the metric elevation map of the terrain around and\nunder the robot [ 8,9,10] to plan foot steps and joint angles. The elevation map is constructed by\nfusing information from multiple depth images (collected over time). This fusion of depth images\ninto a single elevation map requires the relative pose between cameras at different times. Hence,\ntracking is required in the real world to obtain this relative pose using visual or inertial odometry. This\nis challenging because of noise introduced in sensing and odometry, and hence, previous methods. In ICML , 2018.\n[35] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y . Bai, D. Hafner, S. Bohez, and V . Vanhoucke.\nSim-to-real: Learning agile locomotion for quadruped robots. In RSS, 2018.\n[36] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for\ntransferring", " introduction . 2018.\nTongzhou Wang and Phillip Isola. On the learning and learnablity of quasimetrics. arXiv preprint\narXiv:2206.15478 , 2022.\nGrady Williams, Andrew Aldrich, and Evangelos A Theodorou. Model predictive path integral\ncontrol: From theory to parallel computation. Journal of Guidance, Control, and Dynamics , 40(2):\n344\u2013357, 2017.\nTete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik. Masked visual pre-training for\nmotor control. arXiv preprint arXiv:2203.06173 , 2022.\nHaoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samarth Sinha, and Animesh\nGarg. Learning by watching: Physical imitation of manipulation skills from human videos. In 2021\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 7827\u20137834.\nIEEE, 2021.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\nInConference on robot learning , pp. 1094\u20131100. PMLR, 2020.\nTianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine. How\nto leverage unlabeled data in of\ufb02ine reinforcement learning. arXiv preprint arXiv:2202.01741 ,\n2022.\nKevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and Debidatta Dwibedi.\nXirl: Cross-embodiment inverse reinforcement learning. In Conference on Robot Learning , pp.\n537\u2013546. PMLR, 2022.\nSiyuan Zhang and Nan Jiang. Towards hyperparameter-free policy selection for of\ufb02ine reinforcement\nlearning. Advances in Neural Information Processing Systems , 34:12864\u201312875, 2021.\n15Published as a conference paper at ICLR 2023\nPart I Background 16\nA.1 Goal-Conditioned Reinforcement Learning . . . . . . . . . . . . . . . . . . . . 16\nA.2 InfoNCE & Time Contrastive Learning. . . . . . . . . . . . . . . . . . . . . . . 17\nB Extended Related Work 18\nC Technical Derivations and Proofs 18\nC.1 Proof of Proposition 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.2 VIP Implicit Time Contrast Learning Derivation . . . . . . . . . . . . . . . . . 19\nC.3 VIP Implicit Repulsion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nD VIP Training Details 20\nD.1 Dataset Processing and Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 20\nD.2 VIP Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nD.3 VIP Pytorch Pseudocode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE Simulation Experiment Details. 21\nE.1 FrankaKitchen Task Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE.2 In-Domain Representation Probing . . . . . . . . . . . .", " Introduction) and methods: Behavioral cloning, EWC, and L2. We set the actor\u2019s regularization weight to the\noptimal value and run a sweep over the regularization coef\ufb01cient of the critic. We run results are presented in Tables 15, 16, 17, indicating that direct regularization\nof the critic does not signi\ufb01cantly improve the performance.\n21Table 16: Average performance and forward transfer metrics on CW10 for EWC, for different values\nof the critic regularization coef\ufb01cient.\ncritic\u2019s regularization coef. performance f. transfer\n0 0.66 [0.64, 0.67] 0.08 [0.04, 0.11]\n1e-10 0.64 [0.62, 0.66] 0.05 [0.01, 0.09]\n1e-09 0.64 [0.62, 0.66] 0.06 [0.02, 0.10]\n1e-08 0.62 [0.60, 0.64] 0.06 [0.02, 0.10]\n1e-07 0.62 [0.59, 0.64] 0.06 [0.01, 0.10]\n1e-06 0.63 [0.61, 0.65] 0.01 [-0.04, 0.06]\n1e-05 0.63 [0.60, 0.65] 0.02 [-0.02, 0.05]\n0.0001 0.61 [0.59, 0.63] -0.03 [-0.07, 0.01]\n0.001 0.55 [0.53, 0.57] -0.10 [-0.15, -0.05]\n0.01 0.50 [0.47, 0.52] -0.25 [-0.31, -0.19]\n0.1 0.40 [0.37, 0.42] -0.50 [-0.56, -0.44]\n1 0.27 [0.25, 0.29] -0.85 [-0.92, -0.79]\n10 0.18 [0.16, 0.19] -1.24 [-1.32, -1.18]\n100 0.12 [0.10, 0.13] -1.45 [-1.52, -1.39]\n10000 0.11 [0.10, 0.13] -1.45 [-1.53, -1.38]\nTable 17: Average performance and forward transfer metrics on CW10 for L2, for different values of\nthe critic regularization coef\ufb01cient.\ncritic\u2019s regularization coef. performance f. transfer\n0 0.53 [0.50, 0.56] -0.40 [-0.48, -0.33]\n1e-10 0.53 [0.51, 0.55] -0.37 [-0.44, -0.32]\n1e-09 0.55 [0.53, 0.58] -0.35 [-0.42, -0.28]\n1e-08 0.53 [0.50, 0.56] -0.34 [-0.41, -0.28]\n1e-07 0.52 [0.50, 0.55] -0.38 [-0.45, -0.32]\n1e-06 0.54 [0.51, 0.56] -0.41 [-0.49, -0.35]\n1e-05 0.55 [0.52, 0.57] -0.36 [-0.43, -0.29]\n0.0001 0.52 [0.49, 0.56] -0.47 [-0.56, -0.39]\n0.001 0.53 [0.50, 0.55] -0.44 [-0.52, -0.37]\n0.01 0.53 [0.50, 0.55] -0.41 [-0.50, -0.34]\n0.1 0.49 [0.46, 0.52] -0.45 [-0.54, -0.36]\n1 0.49 [0.46, 0.52] -0.44 [-0.53, -0.35]\n10 0.50 [0.46, 0.53] -0.40 [-0.50, -0.31]\n100 0.45 [0.43, 0.48] -0.49 [-0.57, -0.41]\n10000 0.25 [0.23, 0.27] -1.08 [-1.17, -1.01]\n100000 0.13 [0.12, 0.15] -1.41 [-1.48, -1.35]\nF Infrastructure\nIn our Related work\nContinual learning algorithms are often categorized into three classes: regularization-based e.g.\n[2,23,31], parameter isolation e.g. [ 26] and rehearsal experiments, we use CPU servers, provided through a cloud service. Throughout the whole\nproject, we conducted over 100:000runs with 12 cores per run and an average of 10hours per run,\nwhich in the end sums up to over 12M CPU hours.\nG Continual World benchmark\nWe brie\ufb02y present the Continual World benchmark in Figure 4.\n22Figure 4: Continual World benchmark adopts robotic tasks from Meta-World benchmark. Depicted above is the\nCW10 sequence. The CW20 sequence contains tasks from CW10 repeated twice. Tasks are trained sequentially,\neach one for 1M steps.\n23 Background\n3.1 Continual learning and reinforcement learning\nContinual learning tackles the problem of learning in non-stationary settings [ 8]. Typically, the\nsolution is expected to perform well on all encountered tasks, although various metrics expressing\ndifferent requirements are formulated. The popular CL desiderata include reducing the forgetting on\nprevious tasks and increasing the forward transfer on the new tasks, i.e. speeding up the learning\nby reusing knowledge from previous tasks [ 12,46]. Other desiderata focus on limiting resources,\nsuch as the number of samples, computation time, model size, or additional memory size. These\nrequirements are often con\ufb02icting, so usually some trade-offs have to be made [17, 46, 32].\nCombining CL with RL adds another layer of complexity. In this work, we focus on the SAC\nalgorithm [ 16], which is often considered to be the", " Introduction\nReplay Bu\ufb00er Real World \nActor Critic World Model \nFigure 2: Dreamer follows a simple\npipeline for online learning on robot\nhardware without simulators. The cur-\nrent learned policy collects experience\non the robot. This experience is added\nto the replay buffer. The world model is\ntrained on replayed off-policy sequences\nthrough supervised learning. An actor\ncritic algorithm optimizes a neural net-\nwork policy from imagined rollouts in\nthe latent space of the world model. We\nparallelize data collection and neural net-\nwork learning so learning steps can con-\ntinue while the robot is moving and to\nenable low-latency action computation.Teaching robots to solve complex tasks in the real world\nis a foundational problem of robotics research. Deep re-\ninforcement learning (RL) offers a popular approach to\nrobot learning that enables robots to improve their behavior\nover time through trial and error. However, current algo-\nrithms require too much interaction with the environment\nto learn successful behaviors, making them impractical for\nmany real world tasks. Recently, modern world models\nhave shown great promise for data ef\ufb01cient learning in\nsimulated domains and video games (Hafner et al., 2019;\n2020). Learning world models from past experience en-\nables robots to imagine the future outcomes of potential\nactions, reducing the amount of trial and error in the real\nenvironment needed to learn successful behaviors.\nWhile learning accurate world models can be challenging,\nthey offer compelling properties for robot learning. By\npredicting future outcomes, world models allow for plan-\nning and behavior learning given only small amounts of\nreal world interaction (Gal et al., 2016; Ebert et al., 2018).\nMoreover, world models summarize general dynamics\nknowledge about the environment that, once learned, could\nbe reused for a wide range of downstream tasks (Sekar\net al., 2020). World models also learn representations that\nfuse multiple sensor modalities and integrate them into la-\ntent states, removing the need for manual state estimation.\nFinally, world models generalize well from available of-\n\ufb02ine data (Yu et al., 2021), which could further accelerate\nlearning in the real world.\nDespite the promises of world models, learning accurate world models for the real world is a big\nopen challenge. In this paper, we leverage recent advances of the Dreamer world model for training a\nvariety of robots in the most straight-forward and fundamental problem setting: online reinforcement\nlearning in the real world, without simulators or demonstrations. As shown in Figure 2, Dreamer\nlearns a world model from a replay buffer of past experience, learns behaviors from rollouts imagined\nin the latent space of the world model, and continuously interacts with the environment to explore\nand improve its behaviors. Our aim is to push the limits of robot learning directly in the real world\nand offer a robust platform to enable future work that develops the bene\ufb01ts of world models for robot\nlearning. The key contributions of this paper are summarized as follows:\n\u000fDreamer on Robots We apply Dreamer to 4 robots, demonstrating successful learning directly\nin the real world, without introducing new algorithms. The tasks cover a range of challenges,\nincluding different action spaces, sensory modalities, and reward structures.\n\u000fWalking in 1 Hour We teach a quadruped from scratch in the real world to roll off its back,\nstand up, and walk in only 1 hour. Afterwards, we \ufb01nd that the robot adapts to being pushed within\n10 minutes, learning to withstand pushes or quickly roll over and get", " Introduction\nRepresentation learning is an integral part of reinforcement learning (RL2) algorithms. While such\nrepresentations might emerge from end-to-end training [ 7,79,118,125], prior work has found it\nnecessary to equip RL algorithms with perception-speci\ufb01c loss functions [ 31,43,70,74,89,91,101,\n140] or data augmentations [ 72,74,117,136], effectively decoupling the representation learning\nproblem from the reinforcement learning problem. Given what prior work has shown about RL\nin the presence of function approximation and state aliasing [ 2,134,138], it is not surprising that\nend-to-end learning of representations is fragile [ 72,136]: an algorithm needs good representations\nto drive the learning of the RL algorithm, but the RL algorithm needs to drive the learning of good\nrepresentations. So, can we design RL algorithms that do learn good representations without the\nneed for auxiliary perception losses?\nRather than using a reinforcement learning algorithm also to solve a representation learning problem,\nwe will use a representation learning algorithm to also solve certain types of reinforcement learning\nproblems, namely goal-conditioned RL. Goal-conditioned RL is widely studied [ 6,14,21,62,\n80,119], and intriguing from a representation learning perspective because it can be done in an\nentirely self-supervised manner, without manually-speci\ufb01ed reward functions. We will focus on\ncontrastive (representation) learning results suggest that good performance can be\nachieved without this \ufb01ltering step.\nThis section presents additional \ufb01gures.\n\u2022Fig. 13 compares contrastive RL (NCE) with varying values of the \ufb01ltering parameter \u000f,\ndescribed in Sec. 4.5.\n\u2022Fig. 14 \u2013 This plot shows a TSNE embedding of the state-action representations \u001e(s;a)for\none trajectory of the bin picking task. This experiment uses image observations.\n\u2022Fig. 15 \u2013 This plot shows a TSNE embedding of the state-action representations from\nthe same bin picking task. We sampled states and actions using a trained agent. After\ncomputing the TSNE embedding, we used RasterFairy [ 65] to rectify the embeddings to a\ngrid.\n\u2022Fig. 16 \u2013 A TSNE embedding of image representations from the point Spiral11x11\ntask.\n\u2022Fig. 17 \u2013 Using the same representations for the point Spiral11x11 task, we measure\nthe similarity between the critic gradients when evaluated at the same state but different\ngoals,h@f\n@sj(s;g);@f\n@sj(s;g0)i.\n26Figure 14: Visualizing the learned representations. (Top) We show \ufb01ve observations from the bin picking\ntask, as well as the goal image. (Bottom) A TSNE embedding of the image representations \u001e(s;a)learned by\nContrastive RL (NCE). Note that different parts of the task (e.g., reaching, picking, placing) are well separated\nin the learned representation space.\nFigure 15: Visualizing the image representations learned by our method on the sawyer bin . We\ncompute a TSNE embedding of the representations, and then align the embeddings to a grid using\nRasterFairy [65].\n27(a)\n (b)\n(c) untrained encoder\n (d) contrastive RL (NCE)\n (e) TD3 + HER\nFigure 16: TSNE embedding of representations \u001e(s;a).(a)Using the point Spiral11x11 task,\n(b)we generated image observations at 270 locations throughout the maze. We computed the state-\naction representations of these images, using action = (0, 0). (c, d, e) A TSNE embedding of these\nrepresentations reveals that the untrained encoder does not capture the structure of the environment,\nwhereas both our method and the TD3 + HER baseline do capture the maze structure.\n(a) Random neural network\n (b) C-learning\n (c) contrastive RL (NCE)\nFigure 17: Analyzing the gradients. We plot the cosine similarity between the (normalized) gradients of the\ncritic function with respect to the goal images. An untrained network has", " INTRODUCTION\nDeveloping controllers for high-dimensional continuous\ncontrol systems such as legged robots has long been an\narea of study. Early work in this \ufb01eld focused on developing\napproximate dynamics models of a system and then using tra-\njectory optimization algorithms to solve for the actions that\nlead an agent to achieving a desired goal [1]\u2013[4]. However,\nthe resulting controllers tend to be highly specialized for a\nparticular task, limiting their ability to generalize across more\ndiverse tasks or environments. More recently, there has been\na surge in algorithms that use reinforcement learning (RL)\nto learn locomotion behaviors [5]\u2013[9]. This approach proved\nhighly effective in simulation [10], but this success did not\ntranslate to the real world due to challenges associated with\novercoming the simulation to reality gap.\nOne of the main challenges inhibiting RL approaches\nfrom being more effective in the real world is related to\nthe aggressive and overly-energetic behaviors that are often\nlearned by RL agents trained using under-speci\ufb01ed reward\nfunctions. As an example, a legged RL agent trained with\na reward that encourages forward velocity will often learn\nFig. 1. Training with Adversarial Motion Priors encourages the policy to\nproduce behaviors which capture the essence of the motion capture dataset\nwhile satisfying the auxiliary task objective. Only a small amount of motion\ncapture data is required to train the learning system (4.5 seconds in our experiments).\na control policy that exploits \ufb02ailing of the limbs or high-\nimpulse contacts, and other inaccurate simulator dynamics,\nto achieve forward movement. Such behaviors are unlikely to\nbe effective when transferred to a real robot due to actuator\nlimits and potential damage to the robot. To overcome the\nissues posed by reward under-speci\ufb01cation, researchers have\ninvestigated task-speci\ufb01c action spaces [12], [13], complex\nstyle reward formulations [6]\u2013[8], [14], and curriculum learn-\ning [15], [16]. These approaches achieve state-of-the-artarXiv:2203.15103v1  [cs.AI]  28 Mar 2022Fig. 2. Key frames, gait pattern, velocity tracking, and energy-ef\ufb01ciency of the robot dog throughout a trajectory A: Key frames of A1 during a\ncanter motion overlaid on a plain background for contrast. B: Gait diagram indicating contact timing and duration for each foot in black. Training with\nAdversarial Motion Priors enables the policy to synthesize behaviors which lead to natural gait transitions at different velocities. C: Plot of commanded\nforward velocities and estimated velocities during the rollout. D: Estimated Cost of Transport (COT) during the rollout. While pacing the COT remains\nconstant with small oscillations. However, when the robot enters a canter phase the COT exhibits spikes corresponding to the robot pushing off its hind\nlegs and troughs corresponding to the \ufb02ight phase where energy consumption is low. This gait transition phenomenon closely relates to the behavior of\nquadrupedal mammals, which modulate their gait according to their speed of travel, leading to minimal energy consumption consumption [11]. results in\na dramatically different COT pro\ufb01le (Fig. 2:D). While the\npace motion exhibits a fairly constant COT, the canter motion\nproduces large spikes in COT corresponding to the lift-off\nphase and relatively low-valued troughs associated with the\n\ufb02ight and touch-down phase. Also shown in Figure 3:B is\nthe trotting motion that emerges from training with AMP\nrewards.\nV. C ONCLUSIONS\nWe demonstrate that learning motion priors using adver-\nsarial imitation learning produces style rewards that encour-\nage the policy to produce behaviors that are grounded in the\nreference motion dataset. Using this", " Introduction\nHow do we train a robot to complete a manipulation task from images? A standard and widely\nused approach is to train an end-to-end model from scratch using data from the same domain [ 1].\nHowever, this can be prohibitively data intensive and severely limits generalization. In contrast,\ncomputer vision and natural language processing (NLP) have recently taken a major departure from\nthis \u201ctabula rasa\u201d paradigm. These \ufb01elds have focused on using diverse, large-scale datasets to build\nreusable, pre-trained representations . Such models have become ubiquitous; for example, visual\nrepresentations from ImageNet [ 2] can be reused for tasks like cancer detection [ 3], and pre-trained\nlanguage embeddings like BERT [ 4] have been used for everything from medical coding [ 5] to visual\nquestion answering [ 6]. Such an equivalent of an ImageNet [ 2] or BERT [ 4] model for robotics, that\ncan be readily downloaded and used for any downstream simulation or real-world manipulation task,\nhas remained elusive.\nWhy have we struggled in building this universal representation for robotics? Our conjecture is that\nwe haven\u2019t converged on using the appropriate datasets for robotics. Collecting large and diverse\ndatasets of robots interacting with the physical world can be costly, even without human annotation.\nRecent attempts at creating such datasets [ 7,8,9,10], consist of a limited number of tasks in at\nmost a handful of different environments. This lack of diversity and scale makes it dif\ufb01cult to learn\nrepresentations that are broadly applicable. At the same time, the recent history of computer vision\nand NLP suggests an alternate route for robotics. The best representations in these \ufb01elds did not\narise out of task-speci\ufb01c and carefully curated datasets, but rather the use of abundant in-the-wild\ndata [ 4,11,12,13]. Analogously, for robotics and motor control, we have access to videos of humans\ninteracting in semantically interesting ways with their environments [ 14,15,16]. This data is large\nand diverse, spanning scenes across the globe, and tasks ranging from folding clothes to cooking a\nmeal. While the embodiment present in this data differs from most robots, prior work [ 17,18] has\nfound that such human video data can still be useful for learning reward functions. Furthermore,\ndomain gap has not been a major barrier for using pre-trained representations in traditional vision and\n6th Conference on Robot Learning (CoRL 2022), Auckland, New Zealand.\n\u0003Work completed during internship at Meta AIarXiv:2203.12601v3  [cs.RO]  18 Nov 2022Ego4D Video + Language\n\u201cstirs the snacks\u2026\u201dTime Contrastive LearningVideo-Language Alignment\n\u201cremoves the battery\u2026\u201d\nPre-Trained R3M RepresentationEfficient Robot Learning New Environment, New T asks\nL1 Sparsity Penalty\ntimeFigure 1: Pre-Training Reusable Representations for Robot Manipulation (R3M) : We pre-train a visual\nrepresentation using diverse human video datasets like Ego4D [ 16], and study its effectiveness for downstream\nrobot manipulation tasks. Our representation model, R3M, is trained using a combination of time-contrastive\nlearning, video-language alignment, and an L1 sparsity penalty. We \ufb01nd that R3M enables data ef\ufb01cient imitation\nlearning across several simulated and real-world robot manipulation tasks.\nNLP tasks. In this backdrop, we ask the pertinent question: can visual representations pre-trained on\ndiverse human videos enable ef\ufb01cient downstream learning of robotic manipulation skills?\nWe hypothesize that a good representation for vision-based robotic manipulation consists of three\ncomponents. First, it should contain information necessary for physical interaction, and thus should\ncapture the temporal", "Abstract \u2014 Legged robots are physically capable of travers-\ning a wide range of challenging environments, but designing\ncontrollers that are suf\ufb01ciently robust to handle this diversity\nhas been a long-standing challenge in robotics. Reinforcement\nlearning presents an appealing approach for automating the\ncontroller design process and has been able to produce re-\nmarkably robust controllers when trained in a suitable range\nof environments. However, it is dif\ufb01cult to predict all likely\nconditions the robot will encounter during deployment and\nenumerate them at training-time. What if instead of training\ncontrollers that are robust enough to handle any eventuality,\nwe enable the robot to continually learn in any setting it\n\ufb01nds itself in? This kind of real-world reinforcement learning\nposes a number of challenges, including ef\ufb01ciency, safety, and\nautonomy. To address these challenges, we propose a practical\nrobot reinforcement learning system for \ufb01ne-tuning locomotion\npolicies in the real world. We demonstrate that a modest amount\nof real-world training can substantially improve performance\nduring deployment, and this enables a real A1 quadrupedal\nrobot to autonomously \ufb01ne-tune multiple locomotion skills in a\nrange of environments, including an outdoor lawn and a variety\nof indoor terrains. (Videos and code1)\nI. I NTRODUCTION\nLegged robots possess a unique physical capability to\ntraverse a wide range of environments and terrains, from\nsubterranean rubble to snowy hills [1], [2]. However, fully\nrealizing this capability requires controllers that can effec-\ntively handle this broad range of environments. Engineering\nsuch robust controllers for each robot is a labor-intensive\nprocess, requiring human expertise and precise modeling of\nthe system dynamics [3]\u2013[5]. Reinforcement learning (RL)\nalgorithms have been used to automatically learn robotic\nlocomotion skills in a wide range of contexts, both in\nsimulation and in the real world [6]\u2013[17]. However, in order\n1https://sites.google.com/berkeley.edu/\nfine-tuning-locomotionfor thesemethods suffer be-\ncause they assume that a pre-trained encoder or latent space\ncan generalize, which is too strong an assumption when\nthe test environment differs suf\ufb01ciently from the training\nenvironments. RMA especially suffers in this case because it\nrelies entirely on the pre-trained encoder to adapt, and when\nthis encoder fails to generalize, it does not have any other\nrecourse. The latent space method does adapt, but it relies\non the latent space already containing suitable strategies for\nthe new environment and ends up with a suboptimal policy.\nIn contrast, our \ufb01netuning approach is able to continuously\nimprove and eventually succeed. Theseexperiments,\u201d The International\nJournal of Robotics Research , vol. 36, no. 2, pp. 167\u2013192, 2017.\n[Online]. Available: https://doi.org/10.1177/0278364917694244\n[23] Z. Xie, P. Clary, J. Dao, P. Morais, J. Hurst, and M. V . D. Panne,\n\u201cLearning locomotion skills for cassie: Iterative design and sim-to-\nreal,\u201d in Conference on Robot Learning (CoRL) , 2019.\n[24] J. Tan, Z. Xie, B. Boots, and C. Liu, \u201cSimulation-based design\nof dynamic controllers for humanoid balancing,\u201d 2016 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS) ,\npp. 2729\u20132736, 2016.\n[25] Y . Chebotar, A. Handa, V . Makoviychuk, M. Macklin, J. Issac, N. D.\nRatliff, and D. Fox, \u201cClosing the sim-to-real loop: Adapting simulation\nrandomization with real world experience,\u201d IEEE International Con-\nference on Robotics and Automation (ICRA) , pp. 8973\u20138979, 2019.\n[26] Y . Du, O. Watkins, T. Darrell, P. Abbeel, and D. Pathak, \u201cAuto-tuned\nsim-to-real transfer,\u201d ArXiv , vol. abs/2104.07662, 2021.\n[27] X. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, \u201cSim-to-\nreal transfer of robotic control with dynamics randomization,\u201d 2018\nIEEE International Conference on Robotics and Automation (ICRA) ,\npp. 1\u20138, 2018.\n[28]", " Introduction\nDeep reinforcement learning (DRL) is proving to be a powerful tool for robotics. Tasks such as\nlegged locomotion [1], manipulation [2], and navigation [3], have been solved using these new\ntools, and research continues to keep adding more and more challenging tasks to the list. The\namount of data required to train a policy increases with the task complexity. For this reason, most\nwork focuses on training in simulation before transferring to real robots. We have reached a point\nwhere multiple days or even weeks are needed to fully train an agent with current simulators. For\nexample, OpenAI\u2019s block reorientation task was trained for up to 14 days and their Rubik\u2019s cube\nsolving policy took several months to train [4]. The problem is exacerbated by the fact that deeparXiv:2109.11978v3  [cs.RO]  19 Aug 2022reinforcement learning requires hyper-parameter tuning to obtain a suitable solution which requires\nsequentially rerunning time-consuming training. Reducing training times using massively parallel\napproaches such as presented here can therefore help improve the quality and time-to-deployment\nof DRL policies, as a training setup can be iterated on more often in the same time frame.\nIn this paper, we examine the effects of massive parallelism for on-policy DRL algorithms and\npresent considerations in how the standard RL formulation and the most commonly used hyper-\nparameters should be adapted to learn ef\ufb01ciently in the highly parallel regime. Additionally, we\npresent a novel game-inspired curriculum which automatically adapts the task dif\ufb01culty to the per-\nformance of the policy. The proposed curriculum architecture is straightforward to implement, does\nnot require tuning, and is well suited for the massively parallel regime. Common robotic simulators\nsuch as Mujoco [5], Bullet [6], or Raisim [7] feature ef\ufb01cient multi-body dynamics implementations.\nHowever, they have been developed to run on CPUs with only a reduced amount of parallelism. In\nthis work, we use NVIDIA\u2019s Isaac Gym simulation environment [8], which runs both the simulation\nand training on the GPU and is capable of simulating thousands of robots in parallel.\nThe massively parallel training regime has been explored before [4, 9] in the context of distributed\nsystems with a network of thousands of CPUs each running a separate instance of the simulation.\nThe parallelization was achieved by averaging the gradients between the different workers without\nreducing the number of samples provided by each agent. This results for both \ufb02at and rough terrain tasks. We see that the critic loss is higher\nwithout bootstrapping, and correspondingly, the total reward is lower. Even though learning can be\nsuccessful without this addition, it greatly reduces the critic loss and improves the total reward by\napproximately 10 % to20 % for both tasks.\n(a) Flat Terrain\n(b) Rough Terrain\nFigure 10: Comparison of total reward and critic loss, when training with and without reward boot-\nstrapping on time-outs.\n12A.3 Reward Terms\nJoint positions qj\nJoint velocities _qj\nJoint accelerations \u007fqj\nTarget joint positions \u007fq\u0003\nj\nJoint torques \u001cj\nBase linear velocity vb\nBase angular velocity !b\nCommanded base linear velocity v\u0003\nb\nCommanded base angular velocity !\u0003\nb\nNumber of collisions nc\nFeet air time tair\nEnvironment time step dt\nTable 1: De\ufb01nition of symbols.\nde\ufb01nition weight\nLinear velocity tracking \u001e(v\u0003\nb;xy\u0000vb;xy) 1dt\nAngular velocity tracking \u001e(!\u0003\nb;z\u0000!b;z) 0:5dt\nLinear velocity penalty \u0000v2\nb;z 4dt\nAngular velocity penalty \u0000jj!b;xyjj20:05dt\nJoint motion\u0000jj\u007fqjjj2\u0000jj_qjjj20:001dt\nJoint torques \u0000jj\u001cjjj20:00002dt\nAction rate \u0000jj_q\u0003\njjj20:25dt\nCollisions\u0000ncollision 0:001dt\nFeet air timeP4\nf=0(tair;f\u00000:5) 2dt\nTable 2: De\ufb01nition of reward terms, with \u001e(x) := exp(\u0000jjxjj2\n0:25). The", " Introduction\nFigure 1: Isaac Gym allows high performance training on a variety of robotics environments. We benchmark on\n8 different environments that offer a wide range of complexity and show the strengths of the simulator in blazing\nfast policy training on a single GPU. Top: Ant, Humanoid, Franka-cube-stack, Ingenuity. Bottom : Shadow Hand,\nANYmal, Allegro, TriFinger.\nIn recent years, reinforcement learning (RL) has become one of the most promising research areas in\nmachine learning and has demonstrated great potential for solving sophisticated decision-making\nproblems. Deep reinforcement learning (Deep RL) has achieved superhuman performance in very\nchallenging tasks, ranging from classic strategy games such as Go and Chess [ 1], to real-time\ncomputer games like StarCraft [ 2] and DOTA [ 3]. It has also shown impressive Background\n2.1 Parallelization Strategy\nThere are many approaches to parallelizing physics simulations. We outline these approaches here\nand justify our design decisions in the context of GPU-accelerated simulation tailored towards\nlearning algorithms. Isaac Gym was developed to maximize the throughput of physics-based machine\nlearning algorithms with particular emphasis on simulations that require large numbers of environment\ninstances executing in parallel.\n2.1.1 CPU Simulations\nWhen physics simulation runs on CPU, multiple threads can be used to distribute computation among\nthe available cores. The most straightforward strategy is simulating one environment instance per\nthread. In this approach, scaling is limited by the number of physical cores in the system. On a\n64-core hyper-threaded CPU, we could run up to 128 environments in parallel, but CPUs with a large\nnumber of cores are typically clocked lower to prevent overheating. Running tens or hundreds of\nthreads comes with other potential pitfalls including synchronization, context-switching overhead,\nand memory bandwidth limitations. To scale further, we would need to use a multi-CPU setup or\nbuild a cluster, which introduces additional communication overhead.\nRunning a single environment instance per thread in its own dedicated physics scene can be inef\ufb01cient.\nThere is some overhead involved in setting up, executing, and gathering the Appendix A.2.3.\nThe observations are also identical, save for the change in number of \ufb01ngers.\nA.3 Hyperparameters for Training PPO\nEnvironment # Environments KL Threshold Mini-batch Size Horizon Length # PPO Epochs Hidden Units Training Steps\nAnt 4096 8e-3 32768 16 4 256, 128, 64 32M\nHumanoid 4096 8e-3 32768 32 5 400, 200, 100 327M\nIngenuity 4096 1.6 e-2 32768 16 8 256, 256, 128 32M\nANYmal 8192 1e-2 32768 16 5 256, 128, 64 65M\nANYmal Terrain 4096 1e-2 24576 24 10 512, 256, 128 150M\nAMP 4096 2e-1 16384 32 8 1024, 512 39M\nFranka 16384 1.6 e-2 131072 32 4 256, 128, 64 786M\nSH Standard 16384 1.6 e-2 32768 8 5 512, 512, 256, 128 655M\nSH OpenAI FF 16384 1.6 e-2 32768 8 5 400, 400, 200, 100 655M\nSH OpenAI LSTM 8192 1.6 e-2 32768 16 4 lstm: 1024, mlp: 512 925M\nTable 17: Hyperparameters used for training in each environment. Allegro shares the parameters for Shadow\nHand. The hidden units are ELU for every environment except AMP, where ReLU units are used. Additionally,\nevery environment uses an adaptive learning rate with a KL divergence target speci\ufb01ed in the KL Threshold\ncolumn, except for AMP which uses a \ufb01xed learning rate of 2e-5 and \ufb01xed KL theshold of 2e-1. The SH OpenAI\nLSTM experiment uses an LSTM layer of", "results in simulation [ 48,33,37,14]. However,\nsuch policies are dif\ufb01cult to transfer to the real world [ 31,39,6].\nOne approach is to directly train in the real world [ 18,55].\nHowever, such policies are limited to very simple setups, and\nscaling to complex setups requires unsafe exploration and a\nlarge number of samples.\nSim-to-Real Reinforcement Learning To achieve complex\nwalking behaviours in the real world using RL, severalAbstract \u2014Successful real-world deployment of legged robots\nwould require them to adapt in real-time to unseen scenarios\nlike changing terrains, changing payloads, wear and tear. This\npaper presents Rapid Motor Adaptation (RMA) algorithm to\nsolve this problem of real-time online adaptation in quadruped\nrobots. RMA consists of two components: a base policy and an\nadaptation module. The combination of these components enables\nthe robot to adapt to novel situations in fractions of a second.\nRMA is trained completely in simulation without using any\ndomain knowledge like reference trajectories or prede\ufb01ned foot\ntrajectory generators and is deployed on the A1 robot without\nany \ufb01ne-tuning. We train RMA on a varied terrain generator\nusing bioenergetics-inspired rewards and deploy it on a variety of\ndif\ufb01cult terrains including rocky, slippery, deformable surfaces\nin environments with grass, long vegetation, concrete, pebbles,\nstairs, sand, etc. RMA shows state-of-the-art performance across\ndiverse real-world as well as simulationexperiments in Figure 4 of the\nmain paper, we also analyze the gait patterns and the torque\npro\ufb01le for the mass adaptation case, shown in Figure S3. We\nthrow a payload of 5kg on the back of the robot in the middle\nof a run and plot the torque pro\ufb01le of the knee, gait pattern,\nand the 2thand7thcomponents of the extrinsics vector ^ztas\nshown in Figure S3. We observe that the additional payload\ndisturbs the regular motion of the robot, after which it entersAlgorithm 2: Rapid Motor AdaptationDeployment\nProcess 1 operating at 100 Hz;\nt 0;\nwhile not fall do\nat \u0019(xt;at\u00001;^zasync);\nxt+1 env.step(at);\nt t+ 1;\nend\nProcess 2 operating at 10 Hz;\nwhile not fall do\n^zasync \u001e(xt\u0000k:k;at\u0000k\u00001:k\u00001);\nend\nthe adaptation phase and \ufb01nally recovers from the disturbance.\nWhen the payload lands on the robot, it is noticeable that the\nplotted components of the extrinsics vector change in response\nto the slip. Post adaptation, we see that the torque stabilizes\nto a higher magnitude than before to account for the payload\nand the gait time period is roughly recovered.\n0 5000 10000 15000\nNumber of Iterations25\n20\n15\n10\n5\n05Average Step Reward\naggregate\nforward + lateral\npenalty terms\nFig. S2: We plot the average step reward during the total 15;000\ntraining iterations. We show the converging trend of the reward\naggregating all reward terms, forward + lateral reward, and sum\nof penalty terms. It also shows the necessity of applying a small\nmultiplier to the penalty terms at the beginning of training;\notherwise, the robot will only have negative experience initially\nand unable to learn to walk quickly.S2. A DDITIONAL SIMULATION TESTINGS\nIn Figure S4, we further test RMAin extreme simulated\nenvironments and show its performance in three types of\nenvironment variations: the payloads added on the base of\nthe A1 robot, the terrain elevation variation (z-scale used in\nthe fractual terrain generator, details in Section IV Simulation\nSetup of the main paper), and the friction coef\ufb01cient between\nthe robot feet and the terrain. We show the superiority of\nRMAacross all the cases in terms of Success Rate, TTF and\nReward as de\ufb01ned in Section S1.Fig. S3: We analyze the change in behavior of RMAas we", " Introduction\nRecent work has shown transformers [ 1] can model high-dimensional distributions of semantic\nconcepts at scale, including effective zero-shot generalization in language [ 2] and out-of-distribution\nimage generation [ 3]. Given the diversity of successful applications of such models, we seek to\nexamine their application to sequential decision making problems formalized as reinforcement\nlearning (RL). In contrast to prior work using transformers as an architectural choice for components\nwithin traditional RL algorithms [ 4,5], we seek to study if generative trajectory modeling \u2013 i.e.\nmodeling the joint distribution of the sequence of states, actions, and rewards \u2013 can serve as a\nreplacement for conventional RL algorithms.\nWe consider the following shift in paradigm: instead of training a policy through conventional\nRL algorithms like temporal difference (TD) learning [ 6], we will train transformer models on\ncollected experience using a sequence modeling objective. This will allow us to bypass the need for\nbootstrapping for long term credit assignment \u2013 thereby avoiding one of the \u201cdeadly triad\u201d [ 6] known\nto destabilize RL. It also avoids the need for discounting future rewards, as typically done in TD\nlearning, which can induce undesirable short-sighted behaviors. Additionally, we can make use of\nexisting transformer frameworks widely used in language and vision that are easy to scale, utilizing a\nlarge body of work studying stable training of transformer models.\nIn addition to their demonstrated ability to model long sequences, transformers also have other\nadvantages. Transformers can perform credit assignment directly via self-attention, in contrast to\nBellman backups which slowly propagate rewards and are prone to \u201cdistractor\u201d signals [ 7]. This can\nenable transformers to still work effectively in the presence of sparse or distracting rewards. Finally,\nempirical evidence suggest that a transformer modeling approach can model a wide distribution of\nbehaviors, enabling better generalization and transfer [3].\nWe explore our hypothesis by considering of\ufb02ine RL, where we will task agents with learning policies\nfrom suboptimal data \u2013 producing maximally effective behavior from \ufb01xed, limited experience. This\ntask is traditionally challenging due to error propagation and value overestimation [ 8]. However, it is\na natural task when training with a sequence modeling objective. By training an autoregressive model\non sequences of states, actions, and returns, we reduce policy sampling to autoregressive generative\nmodeling. We can specify the expertise of the policy \u2013 which \u201cskill\u201d to query \u2013 by selecting the\ndesired return tokens, acting as a prompt for generation.\nIllustrative example. To get an intuition for our proposal, consider the task of \ufb01nding the shortest\npath on a directed graph, which can be posed as an RL problem. The reward is 0when the agent is\nat the goal node and \u00001otherwise. We train a GPT [ 9] model to predict next token in a sequence\nof returns-to-go (sum of future rewards), states, and actions. Training only on random walk data \u2013\nwith no expert demonstrations \u2013 we can generate optimal trajectories at test time by adding a prior\nto generate highest possible returns (see more details and empirical Discussion\n5.1 Does Decision Transformer perform behavior cloning on a subset of the data?\nIn this section, we seek to gain insight into whether Decision Transformer can be thought of as\nperforming imitation learning on a subset of the data with a certain return. To investigate", " Introduction\nIn this supplementary material, we list some detailed re-\nsults of our paper including: (1)The proof of monotonicity\nof entropy with respect to temperature coeffecient \u03c4.(2)\nAll numerical methods and the corresponding forms of mutual informa-\ntion. Wang et al [31] try to understand the contrastive learn-\ning by two key properties, the alignment and uniformity.\nDifferent from the above works, we focus mainly on the\ninherent properties of the contrastive loss function. We em-\nphasize the signi\ufb01cance of the temperature \u03c4, and use it as a\nproxy to analyze some intriguing phenomenons of the con-\ntrastive learning.\n3. Hardness-aware Property\nGiven an unlabeled training set X={x1,...,xN}, the\ncontrastive loss is formulated as:\nL(xi) =\u2212log/bracketleftBigg\nexp(si,i/\u03c4)/summationtext\nk/negationslash=iexp(si,k/\u03c4) + exp(si,i/\u03c4)/bracketrightBigg\n(1)\nwheresi,j=f(xi)Tg(xj).f(\u00b7)is a feature extractor which\nmaps the images from pixel space to a hypersphere space.\ng(\u00b7)is a function which can be same as f[4], or comes from\na memory bank [33], momentum queue [10], etc. For con-\nvenience, we de\ufb01ne the probability of xibeing recognized\nasxjas:\nPi,j=exp(si,j/\u03c4)/summationtext\nk/negationslash=iexp(si,k/\u03c4) + exp(si,i/\u03c4)(2)\nThe contrastive loss tries to make the positive pairs at-\ntracted and the negative samples separated, i.e., the positive\nalignment and negative separation. This objective can also\nbe achieved by using a more simple contrastive loss as:\nLsimple (xi) =\u2212si,i+\u03bb/summationdisplay\ni/negationslash=jsi,j (3)\nHowever, we \ufb01nd that the above loss function performs\nmuch worse than the softmax-based contrastive loss of Eq\n1.0\n 0.5\n 0.0 0.5 1.0050\u00d7103\n = 0.07\n = 0.20\n = 0.30\n = 1.00\nFigure 3. The gradient ratio ri,jwith respect to different si,j. We\nsample the si,jfrom a uniform distribution in [\u22121,1]. As we can\nsee, with lower temperature, the contrastive loss tends to punish\nmore on the hard negative samples.\n1. In the following parts, we will show that different with\nLsimple , the softmax-based contrastive loss is a hardness-\naware loss function, which automatically concentrates on\nseparating more informative negative samples to make the\nembedding distribution more uniform. Besides, we also \ufb01nd\nthat theLsimple is a special case by approaching the temper-\nature\u03c4to+\u221e. Next, we will start with a gradient analysis\nto explain the properties of the contrastive loss.\n3.1. Gradients Analysis.\nWe analyze the gradients with respect to positive sam-\nples and different negative samples. We will show that the\nmagnitude of positive gradient is equal to the sum of nega-\ntive gradients. The temperature controls the distribution of\nnegative gradients. Smaller temperature tends to concen-\ntrate more on the nearest neighbours of the anchor point,\nwhich plays a role in controlling the hardness-aware sensi-\ntivity. Speci\ufb01cally, the gradients with respect to the positive\nsimilaritysi,iand the negative similarity si,j(j/negationslash=i) are\nformulated as:\n\u2202L(xi)\n\u2202si,i=\u22121\n\u03c4/summationdisplay\nk/negationslash=iPi,k,\u2202L(xi)\n\u2202si,j=1\n\u03c4Pi,j (4)\nFrom Eq 4, we have the following observations: (1) The\ngradients with respect to negative samples is proportional\nto the exponential term exp(si,j/\u03c4), indicating that the con-\ntrastive loss is a hardness-aware loss function, which is dif-\nferent with the loss of Eq 3 that gives all negative similar-\nities the same magnitude of gradients. (2) The magnitude\nof gradient with respect to positive sample is equal to the\nsum of gradients with respect to all negative samples, i.e.,\n(/summationtext\nk/negationslash=i|\u2202L(xi)\n\u2202si,k|)/|\u2202L(xi)\n\u2202si,i|= 1, which can de\ufb01ne a proba-\nbilistic distribution to help understand the role of tempera-\nture\u03c4.\n3.2. The Role of temperature\nThe temperature plays a role in controlling the strength\nof penalties on hard negative samples. Speci\ufb01cally, we de-\n\ufb01neri(si,j) =|\u2202L(xi)\n\u2202si,j|/|\u2202L(xi)\n\u2202si,i|, representing the relative\n3penalty on negative sample xj. We have:\nri(si,j) =exp(si,j/\u03c4)/summationtext\nk/negationslash=iexp(si,k/\u03c4), i/negationslash=j (5)\nwhich obeys the Boltzman distribution. As the temperature\n\u03c4decreases, the entropy of the distribution H(ri)decreases\nstrictly (the", " Introduction\nRecently there has been steady progress in un-/self-\nsupervised representation learning, with encouraging re-\nsults on multiple visual tasks ( e.g., [2, 17, 8, 15, 7]). Despite\nvarious original motivations, these methods with the original papers\u2019 Related Work\nSiamese networks. Siamese networks [4] are general mod-\nels for comparing entities. Their applications include sig-\nnature [4] and face [34] veri\ufb01cation, tracking [3], one-shot\nlearning [23], and others. In conventional use cases, the in-\nputs to Siamese networks are from different images, and the\ncomparability is determined by supervision.\nContrastive learning. The core idea of contrastive learn-\ning [16] is to attract the positive sample pairs and repulse the\nnegative sample pairs. This methodology has been recently\npopularized for un-/self-supervised representation learning\n[36, 30, 20, 37, 21, 2, 35, 17, 29, 8, 9]. Simple and effective\ninstantiations of contrastive learning have been developed\nusing Siamese networks [37, 2, 17, 8, 9].\nIn practice, contrastive learning Discussion\nOur hypothesis is about what the optimization problem\ncan be. It does not explain why collapsing is prevented.\nWe point out that SimSiam and its variants\u2019 non-collapsing\nbehavior still remains as an empirical observation.\nHere we brie\ufb02y discuss our understanding on this open\nquestion. The alternating optimization provides a different\ntrajectory, and the trajectory depends on the initialization.\nIt is unlikely that the initialized \u0011, which is the output of a\nrandomly initialized network, would be a constant. Starting\nfrom this initialization, it may be dif\ufb01cult for the alternating\noptimizer to approach a constant \u0011xfor allx, because the\nmethod does notcompute the gradients w.r.t. \u0011jointly for\nallx. The optimizer seeks another trajectory (Figure 2 left),\nin which the outputs are scattered (Figure 2 middle).\n6. Comparisons\n6.1. Result Comparisons\nImageNet. We compare with the state-of-the-art frame-\nworks in Table 4 on ImageNet linear evaluation. For fair\ncomparisons, all competitors are based on our reproduc-\ntion, and \u201c+\u201d denotes improved reproduction vs. the original\npapers (see supplement). For each individual method, we\nfollow the hyper-parameter and augmentation recipes in its\noriginal paper.6All entries are based on a standard ResNet-\n50, with two 224\u0002224 views used during pre-training.\n6In our BYOL reproduction, the 100, 200(400), 800-epoch recipes fol-\nlow the 100, 300, 1000-epoch recipes in [15]: lrisf0.45, 0.3, 0.2g,wdis\nf1e-6, 1e-6, 1.5e-6g, and momentum coef\ufb01cient is f0.99, 0.99, 0.996g.Table 4 shows the introduction of the stop-gradient and extra predictor is\npresumably a consequence of another underlying optimiza-\ntion problem. It is different from the contrastive learning\nproblem, so these extra components may not be helpful.\nRelation to SwA V [7]. SimSiam is conceptually analogous\nto \u201cSwA V without online clustering\u201d. We build up this\nconnection by recasting a few components in SwA V . (i)\nThe shared prototype layer in SwA V can be absorbed\ninto the Siamese encoder. (ii) The prototypes were\nweight-normalized outside of gradient propagation in [7];\nwe instead implement by full gradient computation [33].8\n(iii) The similarity function in SwA V is cross-entropy. With\nthese abstractions, a highly simpli\ufb01ed SwA V illustration is\nshown in Figure 3.\nSwA V applies the Sinkhorn-Knopp (SK) transform [10]\non the target branch (which is also symmetrized [7]). The\nSK transform is derived from online clustering [7]: it is\nthe outcome of clustering the current batch subject to a bal-\nanced partition constraint. The balanced partition can avoid\ncollapsing. Our method does not involve this transform.\nWe study the effect of the prediction MLP hand stop-\ngradient on SwA V . Note that", "Abstract \u2014 Understanding the gap between simulation and\nreality is critical for reinforcement learning with legged robots,\nwhich are largely trained in simulation. However, recent work\nhas resulted in sometimes con\ufb02ictingconclusions\nmight be drawn for different robots or different motions. In\nthis section, we explore the advantages and disadvantages of\ndynamics randomization in greater depth.Mass P gain Latency Lateral Push Slope Up Slope Down Sim-to-Real\nPolicy (kg) (Nm/rad) (ms) (N) (degrees) (degrees) Outcome\nPace: Default 20\u00063 23\u00061 17\u00061 43\u00062 13\u00061 11\u00060 success\nPace: No vel 18\u00060 24\u00062 12\u00062 22\u00060 4\u00060 7\u00060 failure\nPace: No vel, with rand 9\u00064 30\u00062 38\u00062 13\u00065 4\u00061 9\u00061 failure\nTrot: Default 9\u00063 27\u00062 17\u00061 50\u00067 11\u00061 6\u00060 success\nTrot:kp= 160 18\u000612 \u0000\u0000 17\u00064 18\u00066 10\u00060 5\u00063 failure\nTrot:kp= 160 , with rand 8\u00061 \u0000\u0000 41\u00061 12\u00068 12\u00061 1\u00060 failure\nTABLE III: Robustness tests for policies trained under different setup, together with the result of attempted sim-to-real\ntransfer. Blue indicates policies that perform similarly to the corresponding policy with default settings. Green and red\nindicate policies that perform better or worse than the default, respectively. Policies without velocity feedback or with\nkp= 160 all fail the sim-to-real tests. They also generally perform worse in the robustness tests compared to default.\nA. Dynamics Randomization Produces Conservative Policies\nWe observe in TABLE III that dynamics randomization\ncan sometimes lead to policies that are overly-conservative\nin order to achieve unnecessary robustness in parameters\nthat are being randomized. For example, the pacing policies\ntrained with no velocity feedback and dynamics randomiza-\ntion perform worse than policies trained without dynamics\nrandomization in general, except in terms of dealing with\nlatency. However, the physical robot system has an estimated\nlatency of less than 4 ms, and this unnecessary robustness\nagainst increased latency leads to compromised performance\nand robustness along other dimensions.\nWe further train trotting policies under the default setting\nwith dynamics randomization. We observe a more con-\nservative maximum speed (0.9 m/s with randomization and\n1.1 m/s with no randomization), both in simulation and on\nthe physical robot. This also corresponds to our intuition that\ndynamics randomization can produce conservative policies.\nB. Randomize Parameters that Matter\nWe use the latency test to investigate the usefulness of\ndynamics randomization. We observe that policies trained\nwithout randomization fail when the latency exceeds 17 ms.\nWe train another policy with randomized latency only; more\nspeci\ufb01cally, the policy is trained with randomized latency of\nup to 20 ms. The resulting policy can handle latency up to\n32 ms, both in simulation and on the physical robot.\nThis indicates that dynamics randomization can help in\nscenarios where signi\ufb01cant modeling errors are present, such\nas latency in the system. In these scenarios, dynamics ran-\ndomization provides a useful mechanism to cross the reality\ngap by only randomizing the parameters that are responsible.\nC. Summary\nWe observe that blindly applying dynamics randomization\nwhen it is not necessary can generate suboptimal policies that\nare too conservative. However, if the system has fundamental\nmodeling errors that hinder sim-to-real success, randomiza-\ntion is needed to cross the reality gap, as shown in our latencyexperiments. We note that actuator modeling errors can also\npose a sim-to-real challenge, as noted in [4], where a learned\nactuator model is employed to cross the reality gap.\nIn summary, we suggest employing dynamic randomiza-\ntion or additional modeling only when signi\ufb01cant modeling\nerrors are present and to only randomize or model parametersthat matter. Super\ufb02uous dynamics randomization harms", " INTRODUCTION\nLegged locomotion can dramatically expand the reach of robotics.\nMuch of the dry landmass on Earth remains impassible to wheeled and\ntracked machines, the stability of which can be severely compromised\non challenging terrain. Quadrupedal animals, on the other hand, can\naccess some of the most remote parts of our planet. They can choose\nsafe footholds within their kinematic reach and rapidly change their\nkinematic state in response to the environment. Legged robots have the\npotential to traverse any terrain that their animal counterparts can.\nTo date, no published work has demonstrated dynamic locomotion\nin diverse, challenging natural environments as shown in Fig. 1. These\nenvironments have highly irregular pro\ufb01les, deformable terrain, slip-\npery surfaces, and overground obstructions. Under such conditions,\nexisting published controllers manifest frequent foot slippage, loss of\nbalance, and ultimately catastrophic failure. The challenge is exacer-\nbated by the inaccessibility of veridical information about the physical\nproperties of the terrain. Exteroceptive sensors such as cameras and\nLiDAR cannot reliably measure physical characteristics such as friction\nand compliance, are impeded by obstructions such as vegetation, snow,\nand water, and may not have the coverage and temporal resolution to\ncapture changes induced by the robot itself, such as the crumbling of\nloose ground under the robot\u2019s feet. Under these conditions, the robot\nmust rely crucially on proprioception \u2013 the sensing of its own bodilycon\ufb01guration at high temporal resolution. In response to unforeseen\nevents such as unexpected ground contact, terrain deformation, and\nfoot slippage, the controller must rapidly produce whole-body trajecto-\nries subject to multiple objectives: balancing, avoiding self-collision,\ncounteracting external disturbances, and locomotion. While animals\ninstinctively solve this complex control problem, it is an open challenge\nin robotics.\nConventional approaches to legged locomotion on uneven terrain\nhave yielded increasingly complex control architectures. Many rely\non elaborate state machines that coordinate the execution of motion\nprimitives and re\ufb02ex controllers [ 1\u20135]. To trigger transitions between\nstates or the execution of a re\ufb02ex, many systems explicitly estimate\nstates such as ground contact and slippage [ 6\u20138]. Such estimation\nis commonly based on empirically tuned thresholds and can become\nerratic in the presence of unmodeled factors such as mud, snow, or\nvegetation. Other systems employ contact sensors at the feet, which\ncan become unreliable in \ufb01eld conditions [ 9\u201311]. Overall, conventional\nsystems for legged locomotion on rough terrain escalate in complexity\nas more scenarios are taken into account, have become extremely\nlaborious to develop and maintain, and remain vulnerable to corner\ncases.\nModel-free reinforcement learning (RL) has recently emerged as\nan alternative approach in the development of legged locomotion\nskills [ 12\u201314]. The idea of RL is to tune a controller to optimize aarXiv:2010.11251v1  [cs.RO]  21 Oct 2020Research Article ETH Zurich and Intel 2\nFig. 1. Deployment of the presented locomotion controller in a variety of challenging environments.Research Article ETH Zurich and Intel 3\ngiven reward function. The optimization is performed on data acquired\nby executing the controller itself, which improves with experience.\nRL has been used to simplify the design of locomotion controllers,\nautomate parts of the design process, and learn behaviors that could\nnot be engineered with prior approaches [12\u201315].\nHowever, application of RL to legged locomotion has largely been\ncon\ufb01ned to laboratory environments and conditions. Our prior work\ndemonstrated end-to-end learning of locomotion and recovery behav-\niors \u2013 but only on \ufb02at ground, in the lab [ 12]. Other work also devel-\noped", " INTRODUCTION\nThe performance of a machine learning system is directly\ndetermined by the choice and quality of the data representa-\ntion, or features, in the data used to train it. While it is obvious\nthat some criteria for usefulness depend on the task, it is also\nuniversally assumed that there are sets of features that are\nrepresentative of a dataset and that are generally useful as\ninput for many kinds of downstream classi\u001cer or predictor.\nFocusing explicitly on learning representation in some cases\ncan be bene\u001ccial, for example, when a labelled dataset for\na task is small and we want to leverage a larger unlabelled\ndataset to improve the performance of a learning system.\nRepresentation learning refers to the process of learning\na parametric mapping from the raw input data domain to\na feature vector or tensor, in the hope of capturing and\nextracting more abstract\nstate representation. The graph Neural network heads then\ntransform the state's representations and its corresponding\nactions (represented as one-hot vectors) into the state repre-\nsentation in the next time step. Similar to TransE, the statetransitions between time steps is modeled as a translation in\nthe embedding space and the entire world model is trained\nend-to-end with an energy-based hinge loss.\nFocusing on learning useful node representations from\ngeneral graphs, node2vec [35] aims to learn a node repre-\nsentation that is similar between neighbour nodes. The key\ncontribution of node2vec is a family of biased random walk introduction of the Skip-gram and CBOW algo-\nrithms [68] to learn word representations which depend heav-\nily on the tree structure of the hierarchical softmax, Mnih andKavukcuoglu [70] used NCE to avoid having to compute the\nnormalisation term of the softmax. Also inspired by NCE,\nMikolov et al. [67] proposed a slightly different method\ncalled Negative Sampling (NEG) that focuses solely on learn-\ning good word representations with the trade-off of losing the\nprobabilistic properties from NCE.\nRecently, the Bidirectional Encoder Representation\nfrom Transformer (BERT) [24] model learns bidirectional\nword representations using the Transformer architecture's\ndecoder [103] and demonstrated great performance for trans-\nfer learning in multiple downstream tasks. XLNet [116] mod-\ni\u001ced BERT's masked language model objective to include\nan autoregressive objective. While these language model\nobjectives are usually referred to as a form of denoising\nautoencoder that try to reconstruct the original input, in the\ncase of learning word embeddings which is just a lookup\nlayer from index to vector, there is no difference between\nreconstructing and contrasting between feature vectors and\nthus this work does fall under the remit of being a form of\ncontrastive learning.\nUnder the mutual information maximisation framework,\nKong et al. [61] showed that BERT or XLnet also maximise\nglobal-local mutual information, whereas the next sentence\nprediction pre-training task can be seen as constructing simi-\nlarity pairs using the sequential coherence property. With this\ninsight, Kong et al. [61] also proposed BERT-NCE, a variant\nof BERT that uses an NCE-based loss instead of the full\nVOLUME 8, 2020 15P. H. Le-Khac et al.: CRL: A Framework and Review\nTABLE 2. A summary of discussion of several topical\nissues with an emphasis on future outlook, and a concluding\nsection completes the paper.\nII. WHAT IS CONTRASTIVE LEARNING ?\nWe now present an overview of different representation learn-\ning approaches and an intuitive results by contrastive\npre-training in various downstream vision tasks.\nIn a different direction, Oord, Li, and Vinyals [77] pro-\nposed CPC to learn invariances", " Introduction . MIT press, 2018.\nAA Taiga, W Fedus, MC Machado, A Courville, MG Bellemare. On Bonus Based Exploration", " \n\n1 Introduction\n\n\nReinforcement Learning (RL) is an effective framework to solve sequential decision-making tasks, where a learning agent interacts with the environment to improve its performance through trial and error\u00a0[1].\nOriginated from cybernetics and thriving in computer science, RL has been widely applied to tackle challenging tasks which were previously intractable.\nTraditional RL algorithms were mostly designed for tabular cases, which provide principled solutions to simple tasks but face difficulties when handling highly complex domains, e.g.\u00a0tasks with 3D environments.\nWith the recent advances in deep learning research, the combination of RL and deep neural networks is developed to address challenging tasks.\nThe combination of deep learning with RL is hence referred to as Deep Reinforcement Learning (DRL)\u00a0[2], which learns powerful function approximators using deep neural networks to address complicated domains.\nDRL has achieved notable success in applications such as robotics control\u00a0[3, 4] and game playing\u00a0[5].\nIt also thrives in domains such as health informatics\u00a0[6], electricity networks\u00a0[7], intelligent transportation systems[8, 9], to name just a few.\n\n\nBesides its remarkable advancement, RL still faces intriguing difficulties\ninduced by the exploration-exploitation dilemma\u00a0[1].\nSpecifically, for practical RL problems, the environment dynamics are usually unknown, and the agent cannot exploit knowledge about the environment until enough interaction experiences are collected via exploration.\nDue to the partial observability, sparse feedbacks, and the high complexity of state and action spaces, acquiring sufficient interaction samples can be prohibitive or even incur safety concerns for domains such as automatic-driving and health informatics.\nThe abovementioned challenges have motivated various efforts to improve the current RL procedure.\nAs a result, transfer learning (TL), or equivalently referred as knowledge transfer, which is a technique to utilize external expertise to benefit the learning process of the target domain, becomes a crucial topic in RL.\n\n\nWhile TL techniques have been extensively studied in supervised learning\u00a0[10], it is still an emerging topic for RL.\nTransfer learning can be more complicated for RL, in that the knowledge needs to transfer in the context of a Markov Decision Process.\nMoreover, due to the delicate components of the Markov decision process, expert knowledge may take different forms that need to transfer in different ways.\n\nNoticing that previous efforts on summarizing TL in the RL domain did not cover research of the last decade\u00a0[11, 12], during which time considerate TL breakthroughs have been achieved empowered with deep learning techniques.\nHence, in this survey, we make a comprehensive investigation of the latest TL approaches in RL.\n\n\n\nThe contributions of our survey are multifold:\n1) we investigated up-to-date research involving new DRL backbones and TL algorithms over the recent decade.\nTo the best of our knowledge, this survey is the first attempt to survey TL approaches in the context of deep reinforcement learning.\nWe reviewed TL methods that can tackle more evolved RL tasks,\nand also studied new TL schemes that are not deeply discussed by prior literatures, such as representation disentanglement\u00a0(Sec 5.5) and policy distillation (Sec 5.3).\n2) We provided systematic categorizations that cover a broader and deeper view of TL developments in DRL.\nOur main analysis is anchored on a fundamental question, i.e.\u00a0what is the transferred knowledge in RL, following which we conducted more refined analysis.\nMost TL strategies, including those discussed in prior surveys are", " Introduction\nEver since the \ufb01rst fully-learned approach succeeded at\nplaying Atari games from screen images (Mnih et al., 2015),\nstandard practice in deep reinforcement learning (RL) has\nbeen to learn visual features and a control policy jointly,\nend-to-end. Several such deep RL algorithms have matured\n(Hessel et al., 2018; Schulman et al., 2017; Mnih et al., 2016;\n1University of California, Berkeley. Correspondence to:\nAdam Stooke <adam.stooke@berkeley.edu >, Michael Laskin\n<mlaskin@berkeley.edu >.\nProceedings of the 38thInternational Conference on Machine\nLearning , PMLR 139, 2021. Copyright 2021 by the author(s).Haarnoja et al., 2018) and have been successfully applied\nto domains ranging from real-world (Levine et al., 2016;\nKalashnikov et al., 2018) and simulated robotics (Lee et al.,\n2019; Laskin et al., 2020a; Hafner et al., 2020) to sophis-\nticated video games (Berner et al., 2019; Jaderberg et al.,\n2019), and even high-\ufb01delity driving simulators (Dosovit-\nskiy et al., 2017). While the simplicity of end-to-end meth-\nods is appealing, relying on the reward function to learn\nvisual features can be severely limiting. For example, it\nleaves features dif\ufb01cult to acquire under sparse rewards, and\nit can narrow their utility to a single task. Although our\nintent is broader than to focus on either sparse-reward or\nmulti-task settings, they arise naturally in our studies. We\ninvestigate how to learn visual representations which are\nagnostic to rewards, without degrading the control policy.\nA number of recent works have signi\ufb01cantly improved RL\nperformance by introducing auxiliary losses, which are un-\nsupervised tasks that provide feature-learning signal to the\nconvolution neural network (CNN) encoder, additionally\nto the RL loss (Jaderberg et al., 2017; van den Oord et al.,\n2018; Laskin et al., 2020b; Guo et al., 2020; Schwarzer\net al., 2020). Meanwhile, in the \ufb01eld of computer vision,\nrecent efforts in unsupervised and self-supervised learning\n(Chen et al., 2020; Grill et al., 2020; He et al., 2019) have\ndemonstrated that powerful feature extractors can be learned\nwithout labels, as evidenced by their usefulness for down-\nstream tasks such as ImageNet classi\ufb01cation. Together,\nthese advances suggest that visual features for RL could\npossibly be learned entirely without rewards, which would\ngrant greater \ufb02exibility to improve overall learning perfor-\nmance. To our knowledge, however, no single unsupervised\nlearning (UL) task has been shown adequate for this purpose\nin general vision-based environments.\nIn this paper, we demonstrate the \ufb01rst decoupling of rep-\nresentation learning from reinforcement learning that per-\nforms as well as or better than end-to-end RL. We update\nthe encoder weights using only UL and train a control policy\nindependently, on the (compressed) latent images. This ca-\npability stands in contrast to previous state-of-the-art meth-\nods, which have trained the UL and RL objectives jointly,\nor (Laskin et al., 2020b), which observed diminished perfor-\nmance with decoupled encoders.\nOur main enabling contribution is a new unsupervised taskarXiv:2009.08319v3  [cs.LG]  16 May 2021Decoupling Representation Learning from Reinforcement Learning\ntailored to reinforcement learning, which we call Aug-\nmented Temporal Contrast (ATC). ATC requires a model\nto associate observations from nearby time steps within the\nsame trajectory (Anand et al., 2019). Observations are en-\ncoded via a convolutional neural network (shared with the\nRL agent) into a small latent space, where the InfoNCE\nloss is applied (van den Oord et al., 2018). Within each\nrandomly sampled training batch, the positive observation,\not+k, for every anchor, ot, serves as negative for all other\nanchors. For regularization, observations undergo stochastic\ndata augmentation (Laskin et al., 2020b) prior", " Introduction\nFigure 1: Our SupCon loss consistently outper-\nforms cross-entropy with standard data augmenta-\ntions. We show top-1 accuracy for the ImageNet\ndataset, on ResNet-50, ResNet-101 and ResNet-\n200, and compare against AutoAugment [5], Ran-\ndAugment [6] and CutMix [60].The cross-entropy loss is the most widely used loss\nfunction for supervised learning of deep classi\ufb01ca-\ntion models. A number of works have explored\nshortcomings of this loss, such as lack of robustness\nto noisy labels [64, 46] and the possibility of poor\nmargins [10, 31], leading to reduced generalization\nperformance. However, in practice, most proposed\nalternatives have not worked better for large-scale\ndatasets, such as ImageNet [7], as evidenced by the\ncontinued use of cross-entropy to achieve state of the\nart results for more optimizers and data augmentation strategies. Added SupCon loss\nhierarchy. Adjusted table reporting for clarity.\nVersion 3 (2020-10-13) Removed deprecated sentence from Related Work\nOur work draws on existing literature in self-supervised representation learning, metric learning\nand supervised learning. Here we focus on the most relevant papers. The cross-entropy loss was\nintroduced as a powerful loss function to train deep networks [40, 1, 29]. The key idea is simple\nand intuitive: each class is assigned a target (usually 1-hot) vector. However, it is unclear why\nthese target labels should be the optimal ones and some work has tried to identify better target label\nvectors, e.g. [57]. A number of papers have studied other drawbacks of the cross-entropy loss,\nsuch as sensitivity to noisy labels [64, 46], presence of adversarial examples [10, 36], and poor\nmargins [2]. Alternative losses have been proposed, but the most effective ideas in practice have\nbeen approaches that change the reference label distribution, such as label smoothing [47, 35], data\naugmentations such as Mixup [61] and CutMix [60], and knowledge distillation [21].\nPowerful self-supervised representation learning approaches based on deep learning models have\nrecently been developed in the natural language domain [8, 58, 33]. In the image domain, pixel-\npredictive approaches have also been used to learn embeddings [9, 62, 63, 37]. These methods consistently outperform cross entropy for varying strengths of augmentation.\n2215 Change Log\nVersion 1 (2020-04-23) Initial Arxiv version.\nVersion 2 (2020-10-22) Added analysis of different forms of supervised contrastive loss and its\ngradients as well as experimental experiments. Moved accuracy vs num positives to supplemen-\ntary. More heavily tuned models resulted in deterioration of ECE. Added StackedRandAugment\naugmentation. Added GitHub link for code. Added conclusion. As shown in the Supplementary,\nthe gradient for eitherLsup\nout;i orLsup\nin;iwith respect to the embedding zihas the following form.\n@Lsup\ni\n@zi=1\n\u001c8\n<\n:X\np2P(i)zp(Pip\u0000Xip) +X\nn2N(i)znPin9\n=\n;(4)\nHere,N(i)\u0011fn2A(i) :~yn6=~yigis the set of indices of all negatives in the multiviewed batch,\nandPix\u0011exp(zi\u000fzx=\u001c)=P\na2A(i)exp(zi\u000fza=\u001c). The difference between the gradients for the\ntwo losses is in Xip.\nXip=8\n<\n:exp(zi\u000fzp=\u001c)P\np02P(i)exp(zi\u000fzp0=\u001c);ifLsup\ni=Lsup\nin;i\n1\njP(i)j;ifLsup\ni=Lsup\nout;i(5)\nIf each zpis set to the (less biased) mean positive representation vector, z,Xin\nipreduces toXout\nip:\nXin\nip\f\f\nzp=z=exp(zi\u000fz=\u001c)P\np02P(i)exp(zi\u000fz=\u001c)=exp(zi\u000fz=\u001c)\njP(i)j\u0001exp(zi\u000fz=\u001c)=1\njP(i)j=Xout\nip (6)\nFrom the form of @Lsup\ni=@zi, we conclude that the stabilization due to using the mean of positives\nbene\ufb01ts training. Throughout the rest of the paper, we consider only Lsup\nout.\n3.2.3 Connection to Triplet Loss and N-pairs Loss\nSupervised contrastive learning is closely related to the triplet loss [53], one of the widely-used loss\nfunctions for supervised learning. In the Supplementary, we show that the triplet loss is a special\ncase of the contrastive loss when one positive and one negative are used. When more than one\nnegative is used, we show that the SupCon", " Introduction, these Appendix\nA.1 Linear Gravity Individual Evaluations\nIn Figure A.1(a), we can see that the cycles of learning and forgetting are quite clear with th FIFO\nagent. In all other agents, where older experiences were maintained for longer in the buffer, the\nforgetting process is slower. This does not seem to be qualitatively different for the MTR-IRM agent\n- it just seems to be able to reach a good balance between achieving a high performance in the various\nsettings, while forgetting slowly. In particular, it is hard to identify whether there has been much\nforward transfer to gravity settings that have yet to be trained on, which one might hope for by\nlearning an invariant policy: at the beginning of training, the extra IRM constraints seem to inhibit\nthe progress on all settings (as compared to the standard IRM agent), but in the latter stages the\nperformance on a number of the later settings improves drastically.\n(a) FIFO\n (b) Reservoir\n (c) Half Reservoir Half FIFO\n(d) MTR\n (e) MTR with IRM\nFigure A.1: Individual Evaluation rewards for linearly increasing gravity HalfCheetah. Mean and\nstandard error bars over three runs.\nA.2 Multi-task (Random gravity) experiments.\nTable 1: Hyperparameters\nPARAMETER VALUE\n#HIDDEN LAYERS (ALL NETWORKS ) 2\n#UNITS PER HIDDEN LAYER 256\nLEARNING RATE 0.0003\nOPTIMISER ADAM\nADAM\f1 0.9\nADAM\f2 0.999\nREPLAY DATABASE SIZE (ALL BUFFERS ) 1 E6\n# MTR SUB-BUFFERSnb 20\n\fmtr 0.85\nHIDDEN NEURON TYPE RELU\nTARGET NETWORK \u001c 0.005\nTARGET UPDATE FREQUENCY /TIME STEPS 1\nBATCH SIZE 256\n# TRAINING TIME STEPS 5E6 (FIXED ), 5E6 (LINEAR ), 1.2 E7 (FLUCTUATING )\nTRAINING FREQUENCY /TIME STEPS 1\nGRAVITY ADJUSTMENT FREQUENCY /TIME STEPS 1000\nEVALUATION FREQUENCY /EPISODES 100\n# EPISODES PER EVALUATION 1\nIRM POLICY COEFFICIENT 0.1\n13 results of the FIFO agent (Figure 5(a)), where the ups and downs in performance\nre\ufb02ect the \ufb02uctuations of the gravity setting being trained on. While in the MTR-IRM agent, these\n\ufb02uctuations in performance can also be observed, the dips in performance on gravity settings that\nhave not been experienced in a while become signi\ufb01cantly shallower as training progresses, providing\nevidence that the agent is consolidating its knowledge over time (Figure 5(b)).\n(a) HalfCheetah Train\n (b) Ant Train\n(c) HalfCheetah Mean Eval\n (d) Ant Mean Eval\nFigure 4: Fluctuating gravity setting. (Top) Training reward for (a) HalfCheetah and (b) Ant. (Bottom)\nMean evaluation reward for (c) HalfCheetah and (d) Ant.\n5 Experiments\nA.3 Power Law Forgetting\nSeveral studies have shown that memory performance in humans declines with a power law function\nof time [Wixted and Ebbesen, 1991, Rubin and Wenzel, 1996]; in other words, the accuracy on a\nmemory task at time tis given byy=at\u0000bfor somea;b2R+[Kahana and Adler, 2017]. Here\nwe provide a mathematical intuition for how the MTR buffer approximates a power law forgetting\nfunction of the form1\nt, without giving a formal proof. If we assume the cascade is full, then the\nprobability of an experience being pushed into the kthsub-buffer is \fmtrk\u00001, since, for this to happen,\none must be pushed from the 1stto the 2ndwith probability \fmtr, and another from the 2ndto the\n3rdwith the same probability, and so on. So, in expectation,N\nnb\u00011\n\fmtrk\u00001new experiences must be\nadded to the database for an experience to move from the beginning to the end of the kthsub-buffer.\nThus, if an experience reaches the end of the kthbuffer, then the expected number of time steps that\n11(a) Training performance\n (b) Evaluation", " Introduction to reinforcement learning ,\nvolume 135. 1998.\nSzegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., Vanhoucke, V ., and Rabinovich,\nA. Going deeper with convolutions. In Computer\nVision and Pattern Recognition (CVPR) , 2015. URL\nhttp://arxiv.org/abs/1409.4842 .\nTassa, Y ., Doron, Y ., Muldal, A., Erez, T., Li, Y ., Casas, D.\nd. L., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq,\nA., et al. Deepmind control suite. arXiv preprint\narXiv:1801.00690 , 2018.\nTian, Y ., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. arXiv preprint arXiv:1906.05849 , 2019.\nTschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S.,\nand Lucic, M. On mutual information maximization for\nrepresentation learning. arXiv preprint arXiv:1907.13625 ,\n2019.\nvan den Oord, A., Li, Y ., and Vinyals, O. Representa-\ntion learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748 , 2018.\nVan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In Thirtieth AAAI\nconference on arti\ufb01cial intelligence , 2016.\nvan Hasselt, H. P., Hessel, M., and Aslanides, J. When to\nuse parametric models in reinforcement learning? In\nAdvances in Neural Information Processing Systems , pp.\n14322\u201314333, 2019.\nVincent, P., Larochelle, H., Bengio, Y ., and Manzagol, P.-A.\nExtracting and composing robust features with denoising\nautoencoders. In Proceedings of the 25th international\nconference on Machine learning , pp. 1096\u20131103, 2008.\nWang, X. and Gupta, A. Unsupervised learning of visual\nrepresentations using videos. In Proceedings of the IEEE\nInternational Conference on Computer Vision , pp. 2794\u2013\n2802, 2015.\nWang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanc-\ntot, M., and De Freitas, N. Dueling network architec-\ntures for deep reinforcement learning. arXiv preprint\narXiv:1511.06581 , 2015.\nWarde-Farley, D., Van de Wiele, T., Kulkarni, T., Ionescu,\nC., Hansen, S., and Mnih, V . Unsupervised control\nthrough non-parametric discriminative rewards. arXiv\npreprint arXiv:1811.11359 , 2018.Wu, Z., Xiong, Y ., Yu, S., and Lin, D. Unsupervised feature\nlearning via non-parametric instance-level discrimination.\narXiv preprint arXiv:1805.01978 , 2018.\nYarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J.,\nand Fergus, R. Improving sample ef\ufb01ciency in model-\nfree reinforcement learning from images. arXiv preprint\narXiv:1910.01741 , 2019.CURL: Contrastive Unsupervised Representations for Reinforcement Learning\nA. Implementation Details\nBelow, we explain the implementation details for CURL\nin the DMControl setting. Speci\ufb01cally, we use the SAC\nalgorithm as the RL objective coupled with CURL and build\non top of the publicly released implementation from Yarats\net al. (2019). We present in detail the hyperparameters\nfor the architecture and optimization. We do not use any\nextra hyperparameter for balancing the contrastive loss and\nthe reinforcement learning losses. Both the objectives are\nweighed equally in the gradient updates.\nTable 3. Hyperparameters used for DMControl CURL results presented in CURL encourage researchers\nto employ data augmentations, contrastive losses and un-\nsupervised pre-training for future reinforcement learning\nresearch. Related Work\nSelf-Supervised Learning: Self-Supervised Learning is\naimed at learning rich representations of high dimensional\nunlabeled data to be useful for a wide variety of tasks. The\n\ufb01elds of natural language processing and computer vision\nhave seen dramatic advances in self-supervised experiments,\nwe moved to the https://github.com/Kaixhin/\nRainbow codebase for easy and clean benchmarking that\ndirectly builds on top of Ef\ufb01cient Rainbow without other\nchanges. We also run 20 seeds as opposed to 3 seeds earlier\ngiven the high variance nature of the benchmark.\nv4Added in Section E.4 - an ablation investigating whether\ncontrastive representations alone, with no augmentations\npassed to the policy during training, improve the baseline\nSAC policy.\nG. Connection to work on data\naugmentations\nRecently,", " introduction . MIT press, 2018.\nY . Tassa, Y . Doron, A. Muldal, T. Erez, Y . Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel,\nA. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690 , 2018.\nN. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. arXiv preprint\nphysics/0004057 , 2000.\nT. Wang and J. Ba. Exploring model-based planning with policy networks. arXiv preprint\narXiv:1906.08649 , 2019.\nT. Wang, X. Bao, I. Clavera, J. Hoang, Y . Wen, E. Langlois, S. Zhang, G. Zhang, P. Abbeel, and J. Ba.\nBenchmarking model-based reinforcement learning. CoRR , abs/1907.02057, 2019.\nM. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear\nlatent dynamics model for control from raw images. In Advances in neural information processing\nsystems , pages 2746\u20132754, 2015.\nT. Weber, S. Racani\u00e8re, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals,\nN. Heess, Y . Li, et al. Imagination-augmented agents for deep reinforcement learning. arXiv\npreprint arXiv:1707.06203 , 2017.\nR. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning , 8(3-4):229\u2013256, 1992.\nM. Zhang, S. Vikram, L. Smith, P. Abbeel, M. Johnson, and S. Levine. Solar: deep structured\nrepresentations for model-based reinforcement learning. In International Conference on Machine\nLearning , 2019.\n13Published as a conference paper at ICLR 2020\nA H YPER PARAMETERS\nModel components We use the convolutional encoder and decoder networks from Ha and Schmid-\nhuber (2018), the RSSM of Hafner et al. (2018), and implement all other functions as three dense\nlayers of size 300with ELU activations (Clevert et al., 2015). Distributions in latent space are\n30-dimensional diagonal Gaussians. The action model outputs a tanh mean scaled by a factor of\n5 and a softplus standard deviation for the Normal distribution that is then transformed using tanh\n(Haarnoja et al., 2018). The scaling factor allows the agent to saturate the action distribution.\nLearning updates We draw batches of 50 sequences of length 50 to train the world model, value\nmodel, and action model models using Adam (Kingma and Ba, 2014) with learning rates 6\u000210\u00004,\n8\u000210\u00005,8\u000210\u00005, respectively and scale down gradient norms that exceed 100. We do not scale\nthe KL regularizers ( \f= 1) but clip them below 3free nats as in PlaNet. The imagination horizon is\nH= 15 and the same trajectories are used to update both action and value models. We compute the\nV\u0015targets with \r= 0:99and\u0015= 0:95. We did not \ufb01nd latent overshooting for learning the model,\nan entropy bonus for the action model, or target networks for the value model necessary.\nEnvironment interaction The dataset is initialized with S= 5episodes collected using random\nactions. We iterate between 100training steps and collecting 1episode by executing the predicted\nmode action with Normal(0;0:3)exploration noise. Instead of manually selecting the action repeat\nfor each environment as in Hafner et al. (2018) and Lee et al. (2019), we \ufb01x it to 2 for all environments.\nSee Figure 12 for an assessment of the robustness to different action repeat values.\nDiscrete control For", "Abstract \u2014Deep reinforcement learning (RL) algorithms can learn complex robotic skills from raw sensory inputs, but have yet to\nachieve the kind of broad generalization and applicability demonstrated by deep learningmethods for trajectory optimiza-\ntion,\u201d Journal of guidance, control, and dynamics , 1998.\n[42] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros, \u201cView\nsynthesis by appearance \ufb02ow,\u201d in European conference on computer\nvision , 2016.\n[43] S. Meister, J. Hur, and S. Roth, \u201cUn\ufb02ow: Unsupervised learning\nof optical \ufb02ow with a bidirectional census loss,\u201d arXiv:1711.07837 ,\n2017.\n[44] C. Finn, P . Abbeel, and S. Levine, \u201cModel-agnostic meta-learning\nfor fast adaptation of deep networks,\u201d International Conference on\nMachine Learning (ICML) , 2017.\n[45] E. Grant, C. Finn, J. Peterson, J. Abbott, S. Levine, T. Grif\ufb01ths,\nand T. Darrell, \u201cConcept acquisition via meta-learning: Few-shot\nlearning from positive examples,\u201d in NIPS Workshop on Cognitively-\nInformed Arti\ufb01cial Intelligence , 2017.\n[46] D. P . Kingma and J. Ba, \u201cAdam: A method for stochastic optimiza-\ntion,\u201d CoRR , vol. abs/1412.6980, 2014.\n[47] D. Sherer, \u201cFetal grasping at 16 weeks\u2019 gestation,\u201d Journal of\nultrasound in medicine , 1993.\n[48] B. Babenko, M.-H. Yang, and S. Belongie, \u201cVisual tracking with\nonline multiple instance learning,\u201d in Computer Vision and Pattern\nRecognition (CVPR) . IEEE, 2009.\n[49] A. X. Lee, R. Zhang, F. Ebert, P . Abbeel, C. Finn, and S. Levine,\n\u201cStochastic adversarial video prediction,\u201d arXiv:1804.01523 , 2018.\n[50] D. Ulyanov, A. Vedaldi, and V . S. Lempitsky, \u201cInstance normaliza-\ntion: The missing ingredient for fast stylization,\u201d arXiv:1607.08022 ,\n2016.\n[51] A. Odena, V . Dumoulin, and C. Olah, \u201cDeconvolution and\ncheckerboard artifacts,\u201d Distill , 2016.\n[52] S. Niklaus, L. Mai, and F. Liu, \u201cVideo frame interpolation via\nadaptive separable convolution,\u201d in International Conference on\nComputer Vision (ICCV) , 2017.\n[53] E. Todorov, T. Erez, and Y. Tassa, \u201cMujoco: A physics engine\nfor model-based control,\u201d in International Conference on Intelligent\nRobots and Systems (IROS) , 2012.\nFrederik Ebert Frederik Ebert received a BS in Mechatronics and In-\nformation Technology as well a MS in \u201dRobotics, Cognition, Intelligence\n(RCI)\u201d from the Technical University of Munich (TUM). He is currently a\nPhD student at Berkeley Arti\ufb01cal Intelligence Research (BAIR), where\nhe focuses on developing algorithms for robotic manipulation combining\nideas from computer vision, machine learning, and control.\nChelsea Finn is a research scientist at Google and a post-doctoral\nscholar at UC Berkeley, and will join the Computer Science faculty at\nStanford in 2019. Her research focuses on algorithms that can enable\nagents to autonomously learn a range of complex skills. She received\na BS in Electrical Engineering and Computer Science from MIT and a\nPhD in Computer Science from UC Berkeley.\nSudeep Dasari is a 4th year student at UC Berkeley pursuing a B.S\nin Electrical Engineering and Computer Science. His primary research\ninterests are computer vision, machine learning, and robotic control.\nAnnie Xie is pursuing a B.S. degree in Electrical Engineering and\nComputer Science at UC Berkeley. Her research interests are in the\nareas of computer vision and robot learning.\nAlex Lee received a BS in Electrical Engineering and Computer Science\nfrom UC Berkeley and is currently pursuing a PhD in Computer Science\nfrom UC Berkeley. His work focuses on algorithms that can enable\nrobots to learn complex sensorimotor skills.\nSergey Levine received a BS, MS, and PhD in Computer Science from\nStanford. He is currently on the faculty of the Department of Electrical\nEngineering and", " Introduction\nLearning high-level representations from labeled data with layered differentiable models in an end-\nto-end fashion is one of the biggest successes in arti\ufb01cial intelligence so far. These techniques\nmade manually speci\ufb01ed features largely redundant and have greatly improved state-of-the-art in\nseveral real-world applications [ 1,2,3]. However, many challenges remain, such as data ef\ufb01ciency,\nrobustness or generalization.\nImproving representation learning requires features that are less specialized towards solving a\nsingle supervised task. For example, when pre-training a model to do image classi\ufb01cation, the\ninduced features transfer reasonably well to other image classi\ufb01cation domains, but also lack certain\ninformation such as color or the ability to count that are irrelevant for classi\ufb01cation but relevant for\ne.g. image captioning [ 4]. Similarly, features that are useful to transcribe human speech may be\nless suited for speaker identi\ufb01cation, or music genre prediction. Thus, unsupervised learning is an\nimportant stepping stone towards robust and generic representation learning.\nDespite its importance, unsupervised learning is yet to see a breakthrough similar to supervised\nlearning: modeling high-level representations from raw observations remains elusive. Further, it\nis not always clear what the ideal representation is and if it is possible that one can learn such a\nrepresentation without additional supervision or specialization to a particular data modality.\nOne of the most common strategies for unsupervised learning has been to predict future, missing or\ncontextual information. This idea of predictive coding [ 5,6] is one of the oldest techniques in signal\nprocessing for data compression. In neuroscience, predictive coding theories suggest that the brain\npredicts observations at various levels of abstraction [ 7,8]. Recent work in unsupervised learning\nhas successfully used these ideas to learn word representations by predicting neighboring words [ 9].\nFor images, predicting color from grey-scale or the relative position of image patches has also been\nPreprint. Work in progress.arXiv:1807.03748v2  [cs.LG]  22 Jan 2019gencgencgencgencgencgencgencgencgargargargar\nxtxt+1xt+2xt+3xt+4xt\u00001xt\u00002xt\u00003ctzt+4zt+3zt+2zt+1ztPredictionsFigure 1: Overview of Contrastive Predictive Coding, the proposed representation learning approach.\nAlthough this \ufb01gure shows audio as input, we use the same setup for images, text and reinforcement\nlearning.\nshown useful [ 10,11]. We hypothesize that these approaches are fruitful partly because the context\nfrom which we predict related values are often conditionally dependent on the same shared high-level\nlatent information. And by casting this as a prediction problem, we automatically infer these features\nof interest to representation learning.\nIn this paper we propose the following: \ufb01rst, we compress high-dimensional data into a much more\ncompact latent embedding space in which conditional predictions are easier to model. Secondly, we\nuse powerful autoregressive models in this latent space to make predictions many steps in the future.\nFinally, we rely on Noise-Contrastive Estimation [ 12] for the loss function in similar ways that have\nbeen used for learning word embeddings in natural language models, allowing for the whole model\nto be trained end-to-end. We apply the resulting model, Contrastive Predictive Coding (CPC) to\nwidely different data modalities, images, speech, natural language and reinforcement learning, and\nshow that the same mechanism learns interesting high-level information on each of these domains,\noutperforming other approaches.\n2 Contrastive Predicting Coding\nWe start this section by motivating and giving intuitions behind our approach. Next, we introduce the\narchitecture of Contrastive Predictive Coding (CPC). After that we explain the loss function that is\nbased on Noise-Contrastive Estimation. Lastly, we discuss related", "Abstract \u2014Designing agile locomotion for quadruped robots\noften requires extensive expertise and tedious manual tuning.\nIn this paper, we present a system to automate this process by\nleveraging deep reinforcement learning techniques. Our system\ncan learn quadruped locomotion from scratch using simple\nreward signals. In addition, users can provide an open loop\nreference to guide the learning process when more control over\nthe learned gait is needed. The control policies are learned in a\nphysics simulator and then deployed on real robots. In robotics,\npolicies trained in simulation often do not transfer to the real\nworld. We narrow this reality gap by improving the physics\nsimulator and learning robust policies. We improve the simulation\nusing system identi\ufb01cation, developing an accurate actuator\nmodel and simulating latency. We learn robust controllers by\nrandomizing the physical environments, adding perturbations\nand designing a compact observation space. We evaluate our\nsystem on two agile locomotion gaits: trotting and galloping.\nAfter learning in simulation, a quadruped robot can successfully\nperform both gaits in the real world.\nI. I NTRODUCTION\nDesigning agile locomotion for quadruped robots is a long-\nstanding research problem [1]. This is because it is dif\ufb01cult to\ncontrol an under-actuated robot performing highly dynamic\nmotion that involve intricate balance. Classical approaches\noften require extensive experience and tedious manual tuning\n[2, 3]. Can we automate this process?\nRecently, we have seen tremendous progress in deep rein-\nforcement learning (deep RL) [4, 5, 6]. These algorithms can\nsolve locomotion problems from scratch without much human\nintervention. However, most of these studies are conducted\nin simulation, and a controller learned in simulation often\nperforms poorly in the real world. This reality gap [7, 8]\nis caused by model discrepancies between the simulated and\nthe real physical system. Many factors, including unmodeled\ndynamics, wrong simulation parameters, and numerical errors,\ncontribute to this gap. Even worse, this gap is greatly ampli\ufb01ed\nin locomotion tasks. When a robot performs agile motion with\nfrequent contact changes, the switches of contact situations\nbreak the control space into fragmented pieces. Any small\nmodel discrepancy can be magni\ufb01ed and generate bifurcated\nconsequences. Overcoming the reality gap is challenging.\nAn alternative is to learn the task directly on the physical\nsystem. While this has been successfully demonstrated in\nrobotic grasping [9], it is challenging to apply this method\nFig. 1: The simulated and the real Minitaurs learned to gallop\nusing deep reinforcement learning.\nto locomotion tasks due to the dif\ufb01culties of automatically\nresetting theexperiments for model learning and policy optimization.\nInRobotics and Automation (ICRA), 2015 IEEE Inter-\nnational Conference on , pages 2620\u20132626. IEEE, 2015.\n[38] Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk.\nPreparing for the unknown: Learning a universal policy\nwith online system identi\ufb01cation. CoRR , abs/1702.02453,\n2017.\n[39] Xue Bin Peng, Marcin Andrychowicz, Wojciech\nZaremba, and Pieter Abbeel. Sim-to-real transfer of\nrobotic control with dynamics randomization. arXivpreprint arXiv:1710.06537 , 2017.\n[40] Nick Jakobi, Phil Husbands, and Inman Harvey. Noise\nand the reality gap: The use of simulation in evolutionary\nrobotics. Advances in arti\ufb01cial life , pages 704\u2013720, 1995.\n[41] Lerrel Pinto, James Davidson, Rahul Sukthankar, and\nAbhinav Gupta. Robust adversarial reinforcement learn-\ning. arXiv preprint arXiv:1703.02702 , 2017.\n[42] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider,\nWojciech Zaremba, and Pieter Abbeel. Domain random-\nization for transferring deep neural networks from simu-\nlation to the real world. arXiv preprint arXiv:1703.06907 ,\n2017.\n[43] Igor Mordatch, Kendall Lowrey, and Emanuel Todorov.\nEnsemble-CIO: Full-body dynamic motion planning that\ntransfers to physical humanoids. In Intelligent", " Introduction\nLifelong machine learning (Thrun 1996) examines learning\nmultiple tasks in sequence, with an emphasis on how pre-\nvious knowledge of different tasks can be used to improve\nthe training time and learning of current tasks. The hope is\nthat, if sequential learning can be repeated inde\ufb01nitely, then\na system can continue to learn over the course of its lifetime.\nA continual learning agent is thought to be an important step\ntoward general arti\ufb01cial intelligence.\nRecent advances in deep learning continue to motivate re-\nsearch into neural networks that can learn multiple tasks.\nSome approaches include training separate networks for\neach task (Rusu et al. 2016; Yin and Pan 2017) and discov-\nering action hierarchies (Tessler et al. 2016), but these meth-\nods become inef\ufb01cient when scaling to multiple tasks. Ef\ufb01-\nciently retaining knowledge for every task over the course\nof a system\u2019s lifetime is a core dif\ufb01culty in implementing a\nsuch a network. When learning multiple tasks in sequence,\neach new task changes the distribution of experiences, the\noptima move, and a large set of states are no longer visited.\nAs a result of the non-stationary training distribution, the\nnetwork loses its ability to perform well in previous tasks in\nCopyright c\r2018, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.what has been termed catastrophic forgetting (McCloskey\nand Cohen 1989; Ratcliff 1990; Goodfellow et al. 2013).\nWe focus on deep reinforcement learning where the ef-\nfects of catastrophic forgetting are even more pronounced.\nIn deep reinforcement learning, experiences are typically\nstored in a \ufb01rst-in-\ufb01rst-out (FIFO) replay buffer. A network\nmay experience forgetting on a single task if rarely occur-\nring events fall out of the buffer (Lipton et al. 2016) or if\nthe network converges to behavior that does not consistently\nvisit the state space (de Bruin et al. 2015). This can be miti-\ngated by having a replay buffer with limitless capacity or, if\nthe task boundaries are known a priori, by maintaining sep-\narate buffers for each task. However, as the number of tasks\ngrows large, storing all experiences of all tasks becomes pro-\nhibitively expensive, and ultimately infeasible for lifelong\nlearning.\nIn this paper, we develop a process for accumulating ex-\nperiences online that enables their long term retention given\nlimited memory. We treat preserving prior ability as an ac-\ntive process where replay of prior tasks are incorporated into\nthe current learning process. In order to ef\ufb01ciently store prior\nexperiences, we propose a rank-based method for the online\ncollection and preservation of a limited set of training ex-\namples to reduce the effects of forgetting. We then explore\nfour different selection strategies to identify a good ranking\nfunction for selecting the experiences. To validate the selec-\ntion process, we apply each strategy to an autonomous driv-\ning domain where different tasks correspond to navigating\ndifferent unsigned intersections. This domain has recently\nbeen analyzed as a multi-task setting that both bene\ufb01ts from\ntransfer and allows for the speci\ufb01cation of a large variety\nof tasks (Isele and Cosgun 2017). We additionally show our experiments, but\nwith only two sequential tasks rather than \ufb01ve in order to\ntest the hypothesis. Both networks were trained on right task\n\ufb01rst, then on challenge task for 12.5 times longer.\nThe methods still displayed\ncatastrophic forgetting, whereas Distribution Matching and\nCoverage Maximization did not. The latter two strategies\nachieve nearly comparable performance to a network with\nunlimited experience replay capacity. Looking at other", "ABSTRACT\nPredicting the future in real-world settings, particularly from raw sensory obser-\nvations such as images, is exceptionally challenging. Real-world events can be\nstochastic and unpredictable, and the high dimensionality and complexity of nat-\nural images require the predictive model to build an intricate understanding of\nthe natural world. Many existingmethods for 3d human sensing in natural environments. IEEE transactions\non pattern analysis and machine intelligence , 36(7), 2014.\nMatthew Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta. Com-\nposing graphical models with neural networks for structured representations and fast inference.\nInAdvances in neural information processing systems , pp. 2946\u20132954, 2016.\nNal Kalchbrenner, A \u00a8aron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex\nGraves, and Koray Kavukcuoglu. Video pixel networks. International Conference on Machine\nLearning (ICML) , 2017.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. International Conference\non Learning Representations (ICLR) , 2014.\nRahul G Krishnan, Uri Shalit, and David Sontag. Structured inference networks for nonlinear state\nspace models. In AAAI , pp. 2101\u20132109, 2017.\nYitong Li, Martin Renqiang Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video gener-\nation from text. arXiv preprint arXiv:1710.00421 , 2017.\nZiwei Liu, Raymond Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis\nusing deep voxel \ufb02ow. International Conference on Computer Vision (ICCV) , 2017.\n13Published as a conference paper at ICLR 2018\nWilliam Lotter, Gabriel Kreiman, and David Cox. Deep predictive coding networks for video predic-\ntion and unsupervised learning. International Conference on Learning Representations (ICLR) ,\n2017.\nMichael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond\nmean square error. International Conference on Learning Representations (ICLR) , 2016.\nJunhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional\nvideo prediction using deep networks in atari games. In Advances in Neural Information Process-\ning Systems , 2015.\nMarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, and Sumit\nChopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv\npreprint arXiv:1412.6604 , 2014.\nScott E. Reed, A \u00a8aron van den Oord, Nal Kalchbrenner, Sergio Gomez Colmenarejo, Ziyu Wang,\nYutian Chen, Dan Belov, and Nando de Freitas. Parallel multiscale autoregressive density estima-\ntion. International Conference on Machine Learning (ICML) , 2017.\nRui Shu, James Brofos, Frank Zhang, Hung Hai Bui, Mohammad Ghavamzadeh, and Mykel\nKochenderfer. Stochastic video prediction with conditional density estimation. In ECCV Work-\nshop on Action and Anticipation for Visual Learning , 2016.\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video\nrepresentations using lstms. In International Conference on Machine Learning , 2015.\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion\nand content for video generation. arXiv preprint arXiv:1707.04993 , 2017.\nCarl V ondrick and Antonio Torralba. Generating the future with adversarial transformers. In Com-\nputer Vision and Pattern Recognition (CVPR) , 2017.\nCarl V ondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating the future by watching unla-\nbeled video. arXiv preprint arXiv:1504.08023 , 2015.\nCarl V ondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.\nInAdvances In Neural Information Processing Systems , 2016.\nJacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting\nfrom static images using variational autoencoders. In European Conference on Computer Vision ,\npp. 835\u2013851. Springer, 2016.\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\nfrom error", " INTRODUCTION\nDeep reinforcement learning (DeepRL) has been shown\nto be an effective framework for solving a rich reper-\ntoire of complex control problems. In simulated domains,\nagents have been developed to perform a diverse array of\nchallenging tasks [1], [2], [3]. Unfortunately, many of the\ncapabilities demonstrated by simulated agents have often\nnot been realized by their physical counterparts. Many of\nthe modern DeepRL algorithms, which have spurred recent\nbreakthroughs, pose high sample complexities, therefore\noften precluding their direct application to physical systems.\nIn addition to sample complexity, deploying RL algorithms\nin the real world also raises a number of safety concerns\nboth for the agent and its surroundings. Since exploration\nis a key component of the learning process, an agent can at\ntimes perform actions that endanger itself or its environment.\nTraining agents in simulation is a promising approach that\ncircumvents some of these obstacles. However, transferring\npolicies from simulation to the real world entails challenges\nin bridging the \u201dreality gap\u201d, the mismatch between the\nsimulated and real world environments. Narrowing this gap\nhas been a subject of intense interest in robotics, as it offers\nthe potential of applying powerful algorithms that have so\nfar been relegated to simulated domains.\nWhile signi\ufb01cant efforts have been devoted to building\nhigher \ufb01delity simulators, we show that dynamics random-\nization using low \ufb01delity simulations can also be an effective\n1OpenAI\n2UC Berkeley, Department of Electrical Engineering and Computer\nScience\nFig. 1. A recurrent neural network policy trained for a pushing task in\nsimulation is deployed directly on a Fetch Robotics arm. The red marker\nindicates the target location for the puck.\napproach to develop policies that can be transferred directly\nto the real world. The effectiveness of our approach is\ndemonstrated on an object pushing task, where a policy\ntrained exclusively in simulation is able to successfully\nperform the task with a real robot without additional training\non the physical system.\nII. RELATED WORK\nRecent years have seen the application of deep reinforce-\nment learning to a growing repertoire of control problems.\nThe framework has enabled simulated agents to develop\nhighly dynamic motor skills [4], [5], [6], [7]. But due to\nthe high sample complexity of RL algorithms and other\nphysical limitations, many of the capabilities demonstrated\nin simulation have yet to be replicated in the physical world.\nGuided Policy Search (GPS) [8] represents one of the few\nalgorithms capable of training policies directly on a real\nrobot. By leveraging trajectory optimization with learned lin-\near dynamics models, the method is able to develop complex\nmanipulation skills with relatively few interactions with the\nenvironment. The method has also been extended to learning\nvision-based manipulation policies [9]. Researchers have also\nexplored parallelizing training across multiple robots [10].\nNonetheless, successful examples of training policies directly\non physical robots have so far been demonstrated only on\nrelatively restrictive domains.\nA. Domain Adaptation\nThe problem of transferring control policies from sim-\nulation to the real world can be viewed as an instance\nof domain adaptation, where a model trained in a source\ndomain is transfered to a new target domain. One of the\nkey assumptions behind these methods for reinforcement learning with function approximation,\u201d in\nIn Advances in Neural Information Processing Systems 12 . MIT\nPress, 2000, pp. 1057\u20131063.\n[34] T. Schaul, D. Horgan, K. Gregor, and D. Silver, \u201cUniversal value\nfunction approximators,\u201d in Proceedings of the 32nd International\nConference on Machine Learning , ser. Proceedings of Machine\nLearning Research, F. Bach and D. Blei, Eds.,", " Introduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient Background: Policy Optimization\n2.1 Policy Gradient Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At/bracketrightbigg\n(3)\nsubject to \u02c6Et[KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]]\u2264\u03b4. (4)\nHere,\u03b8oldis the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]/bracketrightbigg\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, results and learning curves for all 49 games is provided in Experiments\n6.1 Comparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIPto several natural variations and ablated versions.\nNo clipping or penalty: Lt(\u03b8) =rt(\u03b8)\u02c6At\nClipping: Lt(\u03b8) = min(rt(\u03b8)\u02c6At,clip(rt(\u03b8)),1\u2212/epsilon1,1 +/epsilon1)\u02c6At\nKL penalty (\ufb01xed or adaptive) Lt(\u03b8) =rt(\u03b8)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old,\u03c0\u03b8]\n5For the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping ( /epsilon1)\nand the KL penalty ( \u03b2,dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe Results from continuous control benchmark. Average normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2was initialized at 1.\n6.2 Comparison to Other Algorithms in the", " \n\n1 Introduction\n\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n\n\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states htsubscript\u210e\ud835\udc61h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, as a function of the previous hidden state ht\u22121subscript\u210e\ud835\udc611h_{t-1}italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and the input for position t\ud835\udc61titalic_t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\nRecent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n\n\nAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n\n\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n\n \n\n2 Background\n\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section\u00a03.2.\n\n\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n\n\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n\n\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned", " INTRODUCTION\nPerforming robotic learning in a physics simulator could\naccelerate the impact of machine learning on robotics by\nallowing faster, more scalable, and lower-cost data collection\nthan is possible with physical robots. Learning in simula-\ntion is especially promising for building on recent results\nin computer vision use realistic textures, but do not create\ncoherent 3D scenes. Instead, objects are rendered against a\nsolid methods. See [13] for a more complete treatment of\ndomain adaptation in the reinforcement learning literature.\nIn this paper we study the possibility of transfer from\nsimulation to the real world without performing domain\nadaptation.\nC. Bridging the reality gap\nPrevious work on leveraging simulated data for physical\nrobotic RELATED WORK\nA. Object detection and pose estimation for robotics\nObject detection and pose estimation for robotics is a well-\nstudied problem in the literature (see, e.g., [4], [5], [6], [10],\n[44], [50], [54]). Recent approaches typically involve of\ufb02ine\nconstruction or learning of a 3D model of objects in the\nscene (e.g., a full 3D mesh model [44] or a 3D metric feature\nrepresentation [5]). At test time, features from the test data\n(e.g., Scale-Invariant Feature Transform [SIFT] features [12]\nor color co-occurrence histograms [10]) are matched with the\n3D models (or features from the 3D models). For example,\na black-box nonlinear optimization algorithm can be used\nto minimize the re-projection error of the SIFT points from\nthe object model and the 2D points in the test image [4].\nMost successful approaches rely on using multiple camera\nframes [6] or depth information [44]. There has also been\nsome success with only monocular camera images [4].\nCompared to our method, traditional approaches require\nless extensive training and take advantage of richer sensory\ndata, allowing them to detect the full 3D pose of objects\n(position and orientation) without any assumptions about\nthe location or size of the surface on which the objects\nare placed. However, our approach avoids the challenging\nproblem of 3D reconstruction, and employs a simple, easy\nto implement deep learning-based pipeline that may scale\nbetter to more challenging problems.\nB. Domain adaptation\nThe computer vision community has devoted signi\ufb01cant\nstudy to the problem of adapting vision-based models trained\nin a source domain to a previously unseen target domain\n(see, e.g., [9], [14], [15], [19], [23], [25], [51]). A varietyof approaches have been proposed, including re-training the\nmodel in the target domain (e.g., [52]), adapting the weights\nof the model based on the statistics of the source and target\ndomains (e.g., [22]), learning invariant features between\ndomains (e.g., [47]), and learning a mapping from the target\ndomain to the source domain (e.g., [43]). Researchers in\nthe reinforcement learning community have also studied the\nproblem of domain adaptation by learning invariant feature\nrepresentations [13], adapting pretrained networks [35], and\nother background are unnecessary, despite some clutter (e.g.,\ncables) on the \ufb02oor in our real images.\nOur method avoids calibration and precise placement of\nthe camera in the real world by randomizing characteristics\nof the cameras used to render images in training. We manu-\nally place a camera in the simulated scene that approximately\nmatches the viewpoint and \ufb01eld of view of the real camera.\nEach training sample places the camera randomly within a\n(10\u00025\u000210)cm box around this initial point. The viewing\nangle of the camera is calculated analytically to point at a\n\ufb01xed point on the table, and then offset by up to 0:1radians\nin each direction. The \ufb01eld of view is also scaled", "Abstract \u2014 Reinforcement learning (RL) can automate a wide\nvariety of robotic skills, but learning each new skill requires\nconsiderable real-world data collection and manual representa-\ntion engineering to design policy classes or features. Using deep\nreinforcement learning to train general purpose neural network\npolicies alleviates some of the burden of manual representation\nengineering by using expressive policy classes, but exacerbates\nthe challenge of data collection, since suchmethods,\u201d Journal of Machine\nLearning Research , 2014.\n[18] G. Konidaris and A. G. Barto, \u201cBuilding portable options: Skill trans-\nfer in reinforcement learning.\u201d in Proc. International Joint Conference\non Arti\ufb01cial Intelligence , 2007, pp. 895\u2013900.\n[19] L. Mihalkova and R. J. Mooney, \u201cTransfer learning from minimal tar-\nget data by mapping across relational domains,\u201d in Transfer Learning\nfrom Minimal Target Data by Mapping across Relational Domains ,\n2009.\n[20] M. Taylor, P. Stone, and Y . Liu, \u201cTransfer learning via inter-task map-\npings for temporal difference learning,\u201d Journal of Machine Learning\nResearch , vol. 8, no. 1, pp. 2125\u20132167, 2007.\n[21] C. Drummond, \u201cAccelerating reinforcement learning by composing\nsolutions of automatically identi\ufb01ed subtasks,\u201d JAIR , vol. 16, pp.\n59\u2013104, 2002. [Online]. Available: http://jair.org/papers/paper904.html\n[22] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and\nT. Darrell, \u201cDecaf: A deep convolutional activation feature for generic\nvisual recognition,\u201d CoRR , vol. abs/1310.1531, 2013.\n[23] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, \u201cSimultaneous deep\ntransfer across domains and tasks,\u201d in International Conference in\nComputer Vision (ICCV) , 2015.\n[24] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, \u201cDeep composi-\ntional question answering with neural module networks,\u201d CoRR , vol.\nabs/1511.02799, 2015.\n[25] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, \u201cContinuous control with deep reinforce-\nment learning,\u201d CoRR , vol. abs/1509.02971, 2015.\n[26] J. Oh, V . Chockalingam, S. P. Singh, and H. Lee, \u201cControl of\nmemory, active perception, and action in minecraft,\u201d CoRR , vol.\nabs/1605.09128, 2016.\n[27] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,\nK. Kavukcuoglu, R. Pascanu, and R. Hadsell, \u201cProgressive neural\nnetworks,\u201d CoRR , vol. abs/1606.04671, 2016.\n[28] A. Braylan, M. Hollenbeck, E. Meyerson, and R. Miikkulainen,\n\u201cReuse of neural modules for general video game playing,\u201d CoRR ,\nvol. abs/1512.01537, 2015.\n[29] A. K. I. S. R. S. Nitish Srivastava, Geoffrey Hinton, \u201cDropout: A\nsimple way to prevent neural networks from over\ufb01tting,\u201d Journal of\nMachine Learning Research , vol. 15, 2014.\n[30] E. Todorov, T. Erez, and Y . Tassa, \u201cMuJoCo: A physics engine\nfor model-based control,\u201d in International Conference on Intelligent\nRobots and Systems (IROS) , 2012.\n[31] S. Levine and P. Abbeel, \u201cLearning neural network policies with\nguided policy search under unknown dynamics,\u201d in Advances in Neural\nInformation Processing Systems , 2014.\n[32] R. J. Williams, \u201cSimple statistical gradient-following algorithms for\nconnectionist reinforcement learning,\u201d Machine Learning , vol. 8,\nno. 3-4, pp. 229\u2013256, May 1992. [Online]. Available: http:\n//dx.doi.org/10.1007/BF00992696\n[33] J. Peters and S. Schaal, \u201cNatural actor-critic,\u201d Neurocomputing ,\nvol. 71.experiments, we use the BADMM-based variant of\nguided policy search which applies an additional penalty\non the trajectory optimization for deviating from the neural\nnetwork policy to stabilize the learning process [4]. This\nchoice of learning algorithm enables us to train deep neural\nnetworks with a modest number of samples. However, more\nstandardresults on the 4-link performing the block-\npushing task from section IV-E. The values are the distance between\nthe drawer", " Introduction . MIT Press,\n1998.\n[22] Richard S. Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: a\nframework for temporal abstraction in reinforcement learning. Arti\ufb01cial Intelligence , 112:\n181\u2013211, 1999.\n[23] Richard S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M. Pilarski, Adam\nWhite, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from\nunsupervised sensorimotor interaction. In International Conference on Autonomous Agents and\nMultiagent Systems , pages 761\u2013768, 2011.\n[24] Csaba Szepesv\u00e1ri. Algorithms for Reinforcement Learning . Synthesis Lectures on Arti\ufb01cial\nIntelligence and Machine Learning. Morgan & Claypool Publishers, 2010.\n[25] Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A\nsurvey. Journal of Machine Learning Research , 10(1):1633\u20131685, 2009.\n[26] Emanuel Todorov, Tom Erez, and Yuval Tassa. MuJoCo: A physics engine for model-based\ncontrol. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages\n5026\u20135033, 2012.\n[27] Christopher Watkins and Peter Dayan. Q-learning. Machine Learning , 8:279\u2013292, 1992.\n[28] Hengshuai Yao, Csaba Szepesv\u00e1ri, Richard S Sutton, Joseph Modayil, and Shalabh Bhatnagar.\nUniversal option models. In Advances in Neural Information Processing Systems (NIPS) , pages\n990\u2013998. 2014.\n[29] Jingwei Zhang, Jost Tobias Springenberg, Joschka Boedecker, and Wolfram Burgard. Deep\nreinforcement learning with successor features for navigation across similar environments.\nCoRR , abs/1612.05533, 2016.\n11Successor Features for\nTransfer in Reinforcement Learning\nSupplementary Material\nAndr\u00e9 Barreto ,Will Dabney ,R\u00e9mi Munos ,Jonathan J. Hunt ,\nTom Schaul ,Hado van Hasselt ,David Silver\n{andrebarreto,wdabney,munos,jjhunt,schaul,hado,davidsilver}@google.com\nDeepMind abstract representation of a \u201cgoal,\u201d which makes them particularly suitable for transfer.\nThe function maxj~ \u0019\u0003\nj(s;a)>~wused in our framework can be seen as a function of s,a, and ~w\u2014the\nlatter a generic way of representing a goal\u2014, and thus in some sense this representation isa UVFA.\nThe connection between SFs and UVFAs raises an interesting point: since under this interpretation ~w\nis simply the description of a task, it can in principle be a direct function of the observations, which\nopens up the possibility of the agent determining ~weven before seeing any rewards.\nAs discussed, our approach is also related to temporal abstraction and hierarchical RL: if we look\nat \u0019as instances of Sutton et al.\u2019s [ 22]options , acting greedily with respect to the maximum over\ntheir value functions corresponds in some sense to planning at a higher level of temporal abstraction\n(that is, each \u0019(s;a)is associated with an option that terminates after a single step). This is the\nview adopted by Yao et al. [28], whose universal option model closely resembles our approach in\nsome aspects (the main difference being that they do not do GPI).\nFinally, there have been previous attempts to combine SR and neural networks. Kulkarni et al.\n[10] and Zhang et al. [29] propose similar architectures to jointly learn ~ \u0019(s;a),~\u001e(s;a;s0)and~w.\nAlthough neither work exploits SFs for GPI, they both discuss other uses of SFs for transfer. In\nprinciple the proposed (or similar) architectures can also be used within our framework.\n7 results are shown for\nreference. Background and problem formulation\nAs usual, we assume that the interaction between agent and environment can be modeled as a Markov\ndecision process (MDP, Puterman, [ 19]). An MDP is de\ufb01ned as a tuple M\u0011(S;A;p;R;\r ). The sets\nSandAare the state and action spaces, respectively; here we assume that SandAare \ufb01nite whenever\nsuch an assumption facilitates the presentation, but most of the ideas readily extend to", " Introduction\nLearning generative models of sequences is a long-standing machine learning challenge and histor-\nically the domain of dynamic Bayesian networks (DBNs) such as hidden Markov models (HMMs)\nand Kalman \ufb01lters. The dominance of DBN-based approaches has been recently overturned by a\nresurgence of interest in recurrent neural network (RNN) based approaches. An RNN is a special\ntype of neural network that is able to handle both variable-length input and output. By training an\nRNN to predict the next output in a sequence, given all previous outputs, it can be used to model\njoint probability distribution over sequences.\nBoth RNNs and DBNs consist of two parts: (1) a transition function that determines the evolution\nof the internal hidden state, and (2) a mapping from the state to the output. There are, however, a\nfew important differences between RNNs and DBNs.\nDBNs have typically been limited either to relatively simple state transition structures (e.g., linear\nmodels in the case of the Kalman \ufb01lter) or to relatively simple internal state structure (e.g., the HMM\nstate space consists of a single set of mutually exclusive states). RNNs, on the other hand, typically\npossess both a richly distributed internal state representation and \ufb02exible non-linear transition func-\ntions. These differences give RNNs extra expressive power in comparison to DBNs. This expressive\npower and the ability to train via error backpropagation are the key reasons why RNNs have gained\npopularity as generative models for highly structured sequential data.\nIn this paper, we focus on another important difference between DBNs and RNNs. While the hidden\nstate in DBNs is expressed in terms of random variables, the internal transition structure of the\nstandard RNN is entirely deterministic. The only source of randomness or variability in the RNN\nis found in the conditional output probability model. We suggest that this can be an inappropriate\nway to model the kind of variability observed in highly structured data, such as natural speech,\nwhich is characterized by strong and complex dependencies among the output variables at different\n1Code is available at http://www.github.com/jych/nips2015_vrnn\n1arXiv:1506.02216v6  [cs.LG]  6 Apr 2016timesteps. We argue, as have others [4, 2], that these complex dependencies cannot be modelled\nef\ufb01ciently by the output probability models used in standard RNNs, which include either a simple\nunimodal distribution or a mixture of unimodal distributions.\nWe propose the use of high-level latent random variables to model the variability observed in the\ndata. In the context of standard neural network models for non-sequential data, the variational au-\ntoencoder (V AE) [11, 17] offers an interesting combination of highly \ufb02exible non-linear mapping\nbetween the latent random state and the observed output and effective approximate inference. In this\npaper, we propose to extend the V AE into a recurrent framework for modelling high-dimensional\nsequences. The V AE can model complex multimodal distributions, which will help when the un-\nderlying true data distribution consists of multimodal conditional distributions. We call this model\navariational RNN (VRNN).\nA natural question to ask is: how do we encode observed variability via latent random variables?\nThe answer to this question depends on the nature of the data itself. In this work, we are mainly\ninterested in highly structured data that often arises in AI applications. By highly structured, we\nmean that the data is characterized by two properties. Firstly,"], "bleu": 0.0, "rouge_l": 0.290187891440501, "gpt_metric_score": 0.3, "bert_score": 0.2640842795372009}
