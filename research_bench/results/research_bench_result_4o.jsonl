{"paper_key": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models", "current_5q": "**[Question 1] - What is the problem?**  \nHow can we effectively merge large pretrained models to create new models with enhanced generalization capabilities for multiple tasks while minimizing the need for extensive computational resources and high-quality data?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses the growing demand for versatile models that can perform well across various tasks without the prohibitive costs associated with fine-tuning large models. By advancing model merging techniques, we can democratize access to powerful AI tools, enabling smaller organizations and researchers to leverage state-of-the-art models. This could lead to significant advancements in fields such as natural language processing and computer vision, fostering innovation and practical applications in diverse domains.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem include the complexity of effectively integrating knowledge from multiple pretrained models without losing performance or introducing interference. Naive approaches may fail due to the intricate relationships between model parameters and the potential for negative transfer, where merging leads to degraded performance. Additionally, technical obstacles such as ensuring compatibility between different model architectures and managing the computational overhead of merging processes complicate the task.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on individual model training or fine-tuning, overlooking the potential of model merging as a viable alternative. Limitations in understanding the dynamics of knowledge transfer between models and the lack of robust methodologies for merging have hindered progress. Existing solutions may not adequately address the interference issues that arise during merging. Our approach aims to fill these gaps by introducing novel techniques that enhance the merging process, ensuring better performance and generalization.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves developing a systematic framework for model merging that utilizes a diverse set of pretrained models. We will employ a dataset comprising various tasks to evaluate the merged models' performance. The key metrics for assessment will include accuracy, generalization ability, and computational efficiency. We expect our approach to yield merged models that outperform existing solutions in terms of versatility and performance across multiple tasks, demonstrating the effectiveness of our merging techniques.", "proposal_5q": "[Question 1] - What is the problem?\n\nHow can a terahertz imaging system be developed to integrate adaptive parameter optimization techniques inspired by the DELLA framework for dynamic model merging, enhancing defect detection accuracy in multilayer aerospace materials while maintaining high precision and minimizing computational overhead in real-time?\n\n[Question 2] - Why is it interesting and important?\n\nSolving this problem has broad implications for the research community and the aerospace industry. Enhanced defect detection in multilayer aerospace materials is crucial for ensuring the structural integrity and safety of aircraft. By integrating adaptive parameter optimization techniques inspired by the DELLA framework, this research can lead to significant advancements in nondestructive testing (NDT) methods. Such improvements can result in more reliable and efficient inspections, reducing the risk of undetected defects that could lead to catastrophic failures. Additionally, this research could pave the way for the application of adaptive optimization techniques in other fields requiring high-precision imaging and defect detection, such as medical imaging and materials science. The integration of insights from large language models further underscores the potential for cross-disciplinary innovation, potentially inspiring future research that leverages advanced machine learning techniques for real-time system optimization.\n\n[Question 3] - Why is it hard?\n\nThe challenges in solving this problem are multifaceted. First, terahertz imaging systems inherently require precise calibration and parameter tuning to achieve high-resolution images, particularly when inspecting complex, multilayered aerospace materials. Naive approaches that use static parameter settings often fail to adapt to the variability in material properties and environmental conditions, leading to suboptimal defect detection. Moreover, the integration of adaptive parameter optimization techniques, inspired by the DELLA framework, introduces additional complexity due to the need for real-time adjustments. This requires sophisticated algorithms capable of dynamic model merging and delta parameter selection, all while ensuring minimal computational overhead. Technical obstacles include the development of robust algorithms that can operate efficiently in real-time and the need to validate these algorithms across a diverse set of materials and environmental conditions to ensure reliability and generalizability.\n\n[Question 4] - Why hasn't it been solved before?\n\nPrevious research in terahertz imaging and defect detection has primarily focused on static parameter optimization and model configurations, lacking the adaptability required for real-time applications. Traditional methods often fail to account for the dynamic nature of material properties and environmental conditions, leading to less accurate defect detection. Additionally, the integration of advanced optimization techniques, such as those inspired by the DELLA framework, has not been thoroughly explored in the context of terahertz imaging. Barriers to solving this problem include the computational complexity of dynamic model merging and the lack of real-time adaptive algorithms that can be seamlessly integrated into existing imaging systems. Furthermore, previous studies have not fully leveraged the potential of large language models for adaptive optimization, which presents a novel approach that this research aims to explore and validate.\n\n[Question 5] - What are the key components of my approach and results?\n\nThe proposed methodology involves several key components:\n\n1. **Adaptive Parameter Optimization**: Implementing techniques inspired by the DELLA framework, focusing on dynamic model merging and delta parameter selection to adjust imaging parameters in real-time based on feedback from the imaging system.\n2. **Integration with Large Language Models**: Utilizing insights from large language models to enhance the adaptive optimization process, ensuring robust performance across varying environmental conditions.\n3. **Terahertz Imaging System Development**: Designing a terahertz imaging system capable of high-resolution defect detection in multilayer aerospace materials, incorporating the adaptive parameter optimization techniques.\n4. **Dataset and Validation**: Using a comprehensive dataset of multilayer aerospace materials with known defects to train and validate the system. Metrics for evaluation will include defect detection accuracy, computational efficiency, and system reliability under different environmental conditions.\n\nExpected outcomes include significant improvements in defect detection accuracy and system reliability, with reduced computational overhead. The adaptive optimization techniques are anticipated to enable real-time adjustments, maintaining high precision in varying conditions, thus advancing the field of nondestructive testing in aerospace applications.", "referenced_intros": [" Introduction\nInteractive systems based on general-purpose\nLLMs have become widely popular due to their\nimpressive instruction-following capabilities (Ope-\nnAI, 2023). Furthermore, tuning these models on\ndownstream tasks has been shown to transform\nthem into domain experts (Rozi\u00e8re et al., 2023;\nLuo et al., 2023).\nMaintaining separate fine-tuned models for each\ntask presents several limitations, such as a signif-\nicantly higher memory footprint and the inability\nto leverage information across tasks, which could\nenhance both in-domain and out-of-domain perfor-\nmance. As a result, merging different homologousmodels (models fine-tuned from the same back-\nbone) is gaining traction for its cost-effectiveness,\nknowledge sharing, and space efficiency (Yadav\net al., 2024; Yu et al., 2023). The homologous\nmodels differ from each other in terms of delta pa-\nrameters, i.e., the difference between the fine-tuned\nmodel and backbone model parameters.\nIn this paper, we introduce a novel approach\nfor merging homologous models, termed Drop and\nrEscaLe via samp Ling with m Agnitude ( DELLA ).\nThis approach consists of three steps: (Step-1) in-\nvolves delta parameter drops to reduce interfer-\nence among model parameters. We propose MAG-\nPRUNE , a novel pruning method that samples delta\nparameters based on their magnitudes; (Step-2) fur-\nther reduces interference through sign-based delta\nparameter selection; and (Step-3) fuses the selected\ndelta parameters.\nOn three different homologous (expert) mod-\nels considered for merging (LM, Math, Code) and\ntheir corresponding benchmark datasets (AlpacaE-\nval, GSM8K, MBPP), DELLA outperforms base-\nline Experiments\nWe compare the performance of DELLA against\ntheDARE baseline to show that magnitude sam-\npling improves the selection of delta parameters\nto retain and better maintain the model\u2019s task per-\nformance. We vary the drop rate pin [0.3, 0.5,\n0.7, 0.8, 0.9, 0.91, 0.92, 0.93, 0.94] and apply the\nDARE andDELLA to get models after removing the\nproportion of delta parameters. We then evaluate\nthe model\u2019s performance on its corresponding SFT\ntask. Table 6 shows the comparison between DARE,\nrandom ranking and MAGPRUNE . We performed Results\nA.3 Pruning Rate Hyperparameter Search\nFor Model Merging\nTable 7 shows the results of the pruning rate hy-\nperparameter search for each merging combination.\nWhile both MAGPRUNE andDARE can maintain\nthe performance of individual expert model per-\nformance up to a high drop rate of 0.9, our find-\nings indicate that a drop rate of 0.5, works best\nfor LM+Math, Math+Code and LM+Math+Code.\nFor LM+Code, a drop rate of 0.7 is optimal. Thus,\nwe can infer that while dropping delta parameters\nhelps reduce interference during merging, drop-\nping too many parameters may lead to the loss ofinformation useful for effective merging.\nModels Drop rate AlpacaEval GSM8K MBPP Average\nLM +\nMath0.1 0.805 0.599 / 0.702\n0.3 0.812 0.629 / 0.721\n0.5 0.804 0.645 / 0.724\n0.7 0.787 0.611 / 0.699\n0.9 0.683 0.455 / 0.570\nLM +\nCode0.1 0.741 / 0 0.370\n0.3 0.770 / 0 0.385\n0.5 0.802 / 0.152 0.477\n0.7 0.798 / 0.34 0.569\n0.9 0.737 / 0.262 0.500\nMath +\nCode0.1 / 0.619 0.166 0.393\n0.3 / 0.618 0.184 0.401\n0.5 / 0.626 0.206 0.416\n0.7 / 0.633 0.19 0.412\n0.9 / 0.622 0.128 0.375\nLM +\nMath +\nCode0.1 0.732 0.545 0.114 0.464\n0.3 0.766 0.623 0.302 0.564\n0.5 0.794 0.630 0.3 0.575\n0.7 0.770 0.622 0.23 0.541\n0.9 0.688 0.446 0.128 0.421\nTable 7: Drop Rate of parameters against Task perfor-\nmance Appendix\nA.1 Importance of GPT4-as-a-judge for Math\ntasks - Example\nQuestion: Each person in a certain\nhousehold consumes 0.2 kg of rice ev-\nery meal. Supposing 5 members of the\nhousehold eat rice every lunch and din-\nner, how many weeks will a 42 kg bag of\nrice last?\nGenerated Answer: 1.", " Introduction\nIn recent years, Large Language Models (LLMs) have demonstrated notable success across various\nNatural Language Processing (NLP) tasks [ 9,49,52], including code generation [ 17,44], solving\nmath problems [ 2,35], multilingualism [ 38],etc.These models, with billions of parameters, excel\nin various downstream tasks [ 19,27,56] but require extensive training on large datasets using\nthousands of GPUs. The considerable computational and energy costs [ 43] limit their specialization\nand deployment in resource-constrained environments [30].\nTo tackle this challenge, model fusion has emerged as a promising solution [ 29]. One notable\nparadigm is model merging [ 22,26,59,60], where multiple task-specific models, or \u201cexperts\u201d, are\ncombined into a single unified model. This unified model can quickly adapt to new tasks without\nthe need to retrain a large model. Various techniques, such as parameter averaging [ 5,58], weight\n\u2217Equal contribution.\n\u2020Corresponding authors.\n1Our implementation is available in https://github.com/LZY-the-boys/Twin-Merging\nPreprint. Under review.arXiv:2406.15479v1  [cs.CL]  17 Jun 2024(I) Conventional Merging. . .\nPretrained Task-Specific Expert \n(II) + Knowledge Disentanglement. . .\nTask-Specific Expert    Shared\nKnowledge. . .Exclusive \nKnowledge\nSVD Compression\n(III) + Dynamical MergingRouter. . .Merged Merged\nInput\nHiddenExclusive\nKnowledge\n   Shared\nKnowledgeFigure 1: Subfigure (I) shows that in conventional merging results shown in Table 9.\n18Table 8: The detail statistics of different merging performance on 8 discriminative tasks. Bold\nnumbers indicate the best-averaging performance across different model merging Related Work\nIn this section, we focus on model merging research, for additional related work on multi-task learning\nand Mixture of Experts, please see Appendix E attachs detail qualtivie analysis of various Merging Experiments\n4.1 Merging Experiment\nBaselines We compare Twin-Merging with several train-free model-merging Results\nIn Table 2, we present only the average normalized scores across various tasks. In this section, we\ndetail the statistical performance of all tasks, with discriminative Conclusions\nIn this paper, we introduce the Twin-Merging to merge language models, aiming to close the\nperformance gap between conventional model merging techniques and fine-tuned models, while\nimproving adaptability to data heterogeneity. By modularizing and dynamically merging shared and\ntask-specific knowledge, Twin-Merging significantly outperforms existing model-merging References\n[1]Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models\nmodulo permutation symmetries. In The Eleventh International Conference on Learning\nRepresentations , 2023.\n[2]Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2024.\n[3]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu,\nGao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren,\nChuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu,\nBenfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,\nHongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang,\nChang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023.\n[4]Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gra-\ndient normalization for adaptive loss balancing in deep multitask networks. In International\nconference on machine learning , pages 794\u2013803. PMLR, 2018.\n[5]Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for\nbetter pretraining, 2022.\n[6]Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\nHoffmann, Bogdan Damoc,", " Introduction\nLargelanguagemodels(LLMs)areincreasinglyexcellingatvariousnaturallanguageprocessingtasks,\nincluding text generation [ 11], translation [ 45,50], summarization [ 22], code generation [ 20,33],\nand chatbot interaction [ 28]. With the rising capability, the need for a robust evaluation strategy\nthatcanaccuratelyassesstheperformanceofthesemodelsisbecomingcrucialinordertoidentify\ntheirtrueeffectivenessandchoosethemostappropriateoneforagiventask. Commonmetricsfor\nassessingLLMstodayincluderelevance,frequencyofhallucinations,accuracyinquestionanswering,\ntoxicity,andretrieval-specificmetrics,amongothers. Inthecontextofquestion-answeringevaluations,\npriorworksusuallyinvestigatethemodel\u2019sperformanceintermsofansweraccuracy,courtesy,and\nconciseness. And multiple choice questions (MCQ) have emerged as a predominant format for\nsuch assessments, wherein a question is presented with several possible responses, and the model\nisrequiredtoselectthemostfittingchoiceID,asexemplifiedinFigure1. Lately,theMCQformat\n\u2217Joint first author & equal contribution.\n1For instance, on MMLU, the random guessing accuracy is 25%, and most small-scale LLMs obtain results across datasets, showcasing the percentages of questions that are\nsuitable (Yes) for open-style formatting and those that are not (No).\nTable 7: Example of an Incorrect MCQ and a Correct OSQ in MMLU.\nQuestion that is correct for OSQ and incorrect for MCQ: How many CO2 and ATP molecules\nare formed during one complete turn of the tricarboxylic acid cycle (Krebs\u2019 cycle)?\nOptions: A. 2CO2 and 2ATP B. 2CO2 and 16ATP C. 2CO2 and 12ATP D. 2CO2 and 1ATP\nGround Truth: D\nMCQ_Answer: A\nOSQ_Answer: During one complete turn of the tricarboxylic acid cycle (also known as\nthe Krebs\u2019 cycle or the citric acid cycle), a series of chemical reactions occur in\nthe mitochondria of cells that result in the production of carbon dioxide (CO2) and\nadenosine triphosphate (ATP), among other molecules.\nFor each acetyl-CoA molecule that enters the cycle, the following are produced:\n- 2 molecules of CO2: Each acetyl-CoA contributes two carbons to the cycle, which\nare eventually released as two molecules of carbon dioxide.\n- 1 molecule of ATP: One molecule of ATP (or an equivalent, such as GTP, depending\non the cell type) is produced directly in the cycle through substrate-level\nphosphorylation.\nHowever, the cycle also produces reduced coenzymes, specifically NADH and FADH2,\nwhich are used in the electron transport chain to produce additional ATP through\noxidative phosphorylation. For each acetyl-CoA that enters the cycle:\n- 3 molecules of NADH are produced, which can be used to generate approximately 2.5\nATP each through the electron transport chain.\n- 1 molecule of FADH2 is produced, which can be used to generate approximately 1.5\nATP through the electron transport chain.\nTherefore, while only 1 ATP is produced directly in the cycle per acetyl-CoA, the\nNADH and FADH2 generated can lead to the production of additional ATP in the electron\ntransport chain. The exact number of ATP molecules produced can vary depending on the\nefficiency of the electron transport chain and the ATP synthase complex in different\norganisms and cell types.\n15Table 8: Example of a Correct MCQ and an Incorrect OSQ in MMLU.\nQuestion that is correct for MCQ and incorrect for OSQ: A tree is a connected graph\nwith no cycles. How many nonisomorphic trees with 5 vertices exist?\nOptions: A. 1 B. 2 C. 3 D. 4\nGround Truth: C\nMCQ_Answer: C\nOSQ_Answer: To determine the number of nonisomorphic trees with 5 vertices, we can\nuse Cayley\u2019s formula, which states that the number of trees on \ud835\udc5blabeled vertices\nis\ud835\udc5b\ud835\udc5b\u22122. However, this formula gives the number of trees when vertices are labeled,\nmeaning that it counts different labelings of the same tree shape as distinct. Since\nwe are interested in nonisomorphic trees (which do not consider different labelings as\ndistinct), we cannot directly use Cayley\u2019s formula.\nInstead, we need to enumerate the trees by considering their possible structures. For\na tree with 5", " Introduction\nLarge pre-trained foundation models have become available in many real-world applications [ 60;54;\n13]. This increasing availability has led to a popular practice of fine-tuning these pre-trained models\nto adapt to a wide range of downstream tasks. Practitioners can independently fine-tune the same\npre-trained model, such as CLIP style models [ 45;63;69], large language models [ 6;47;55;28], etc.,\nand then release the fine-tuned models without releasing the training data. As the deployment of such\nfine-tuned models increases, combining models with identical architectures and initializations has\nemerged as a promising approach to combine their respective capabilities. This is useful, especially in\nscenarios where the training data for each task is private and cannot be shared, such as individual-level\npatient data in a hospital and behavior data in social media recommendation systems.\nExisting Related work This paper is related to many existing works on multi-objective optimization, Pareto\noptimality, task arithmetic, federated/private learning, Bayesian Appendix G, we generalize the ways to learn\nthe coefficients in (4), besides minimizing the mean square error in (5).\nG.2 Algorithm 2\nIn this section, we explain the operations of the algorithm in Figure 3 in details. Here task 1 to 8 is\nCars, GTSRB, DTD, SUN397, Resisc45, and SVHN. If we minimize (3)without the nested merging,\nwe would need to estimate A1, ...,A8\u2208R8\u00d78, with hundreds of c.\nWith the nested merging, for the first round, we merge (\u03b81\nft,\u03b82\nft)into\u03b81,2\nmerge, thus approximating\nA1andA2\u2208R8\u00d78byA1[1 : 2 ,1 : 2] andA2[1 : 2 ,1 : 2] \u2208R2\u00d72, respectively. That is, we\nonly care about the interference between task 1 and 2, but not task 1 and 5. Similarly, we merge\n(\u03b83\nft,\u03b84\nft)into\u03b83,4\nmerge, and (\u03b85\nft,\u03b86\nft)into\u03b85,6\nmerge. Next, we merge (\u03b81,2\nmerge,\u03b83,4\nmerge)into\u03b81,2,3,4\nmerge , and\nfinally into \u03b81,2,3,4,5,6,7,8\nmerge .\nG.3 Algorithm 3\nAlgorithm 4 is a detailed version of Algorithm 3. Figure 11 includes illustration of our discretization\nmethod (how we create bins) in 2D and 3D decision variable ( c) space.\n230.0 0.2 0.4 0.6 0.8 1.0\nc1=rcos\n0.00.10.20.30.40.50.60.7c2=rsin\nDiscretization in 2D Polar Coordination System(a)\n (b)\nFigure 11: (a) Discretizing of two task scaling coefficients along the angular dimension in 2D polar\ncoordinate system; (b) Discretizing of three task scaling coefficients along the angular dimensions in\n3D spherical coordinate system;\nAlgorithm 4 Bayesian Adaptive of Surrogate Model\nRequire: Number of iterations J, Buffer B, Pretrained model \u03b8pre, Task vectors vn, Evaluators for\ntaskN,Mn(\u00b7), Discretization bin number K, sample size for every iteration nj,j= 0 toJ,\nBootstrap dropping rate \u03b1= 20% , Bootstrap sampling number Q= 30 .\n1:B \u2190 \u2205\n2:forj= 0toJdo\n3: ifj= 0then\n4: Sample n0scaling coefficients {ci}nj\ni=1from U([0,1]N)\n5: else\n6: Sample njscaling coefficients {ci}nj\ni=1based on the posterior distribution\n7: fori= 0tonjdo\n8: Merge the model \u03b8m(ci) =\u03b8pre+ci\u00b7vn\n9: Evaluate mn,i=Mn(\u03b8m(ci))\n10: B \u2190 B \u222a { (ci, mn,i)}\n11: Fit the quadratic approximation surrogate model \u02dcMnby learning A\u2217\nn,b\u2217\nn, e\u2217\nnin (5).\n12: Discretize the scaling coefficients along the angular dimensions in hyper-spherical coordinates\n(see figure 11 as examples)\n13: fork= 0toKdo\n14: Calculate the mean of L2loss between \u02dcMn(ci)andMt(ci), where ciare in bin k, denoted\nas mean k\n{Bootstrap to estimate the standard deviation of the losses.}\n15: forq= 0toQdo\n16: Randomly (uniformly) drop \u03b1scaling coefficient in bin k\n17: Calculate the mean of L2loss between \u02dcMn(ci)andMt(ci)with the rest points and\ndenoted with lq\n18: Calculate the standard deviation of the {lq}Q\nq=0and denoted as std k\n19: score k=mean k+1\n2stdk\n20: Calculate probability distribution across the discretized bins by score kas the", " Introduction\nWith the rapid development of deep learning, different model architectures [ 36,22,71,88] are\nproposed, along with multiple training strategies [ 89,86]. Pre-trained models\u2019 capabilities are\nenhanced, thus showing increasing significance [ 54,22,7,19]. Finetuning models on downstream\ntasks from a pre-trained model has become a standard paradigm in both NLP and vision fields [ 20,\n51,19,22,5,87], which usually leads to improved performance with less labeled data. With the\ndevelopment of open-source repositories such as Huggingface [ 79], timm [ 77], and torchvision [ 44],\nthe number of pre-trained and finetuned checkpoints exponentially rise. However, applying individual\nmodels to different tasks results, the proposed method suffers from several limitations. On the one\nhand, compared to existing Related Work\nModel Merging obtains a model using the existing task-specific model weights instead of training [ 33,\n30,84,85,66,90,46]. Simply averaging [ 80] usually causes severe performance degradation. Various Appendix F.\nTable 11: Multi-task performance when merging ViT-B/16 models on eight tasks. Methods SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD Avg Acc\nIndividual\n2 Tasks 75.3 77.7 - - - - - - 76.5\n3 Tasks 75.3 77.7 96.1 - - - - - 83.0\n4 Tasks 75.3 77.7 96.1 99.7 - - - - 87.2\n5 Tasks 75.3 77.7 96.1 99.7 97.5 - - - 89.3\n6 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 - - 90.8\n7 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 99.7 - 92.1\n8 Tasks 75.3 77.7 96.1 99.7 97.5 98.7 99.7 79.4 90.5\nTies-Merging\n2 Tasks 69.2 68.2 - - - - - - 68.7\n3 Tasks 69.2 68.0 78.9 - - - - - 72.0\n4 Tasks 68.9 67.9 79.4 86.0 - - - - 75.5\n5 Tasks 68.6 67.1 79.0 83.5 66.6 - - - 73.0\n6 Tasks 68.0 66.4 77.9 80.1 74.4 69.9 - - 72.8\n7 Tasks 66.6 65.7 75.7 76.7 81.0 69.2 96.4 - 75.9\n8 Tasks 64.8 62.9 74.3 78.9 83.1 71.4 97.6 56.2 72.4\nEMR-M ERGING (Ours)\n2 Tasks 78.9 76.1 - - - - - - 77.5\n3 Tasks 77.9 75.2 95.3 - - - - - 82.8\n4 Tasks 77.4 74.9 94.8 99.7 - - - - 86.7\n5 Tasks 77.2 74.2 94.7 99.7 97.1 - - - 88.6\n6 Tasks 76.4 73.4 94.2 99.7 97.0 98.5 - - 89.9\n7 Tasks 75.8 73.3 93.6 99.6 96.9 98.2 99.6 - 91.0\n8 Tasks 75.2 72.8 93.5 99.5 96.9 98.1 99.6 74.4 88.7\nTable 16: Sparsity (ratio of non-zero items) of the masks and the values of the rescalers when merging\nViTs on 8 vision tasks and RoBERTa models on 8 language tasks.\nSparsity SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD\nViT-B/32 0.7194 0.7121 0.7106 0.6994 0.7195 0.7062 0.7132 0.7058\nViT-L/14 0.6832 0.6699 0.6734 0.6579 0.6748 0.6444 0.6614 0.6620\nRescalers SUN397 Cars RESISC45 EuroSAT SVHN GTSRB MNIST DTD\nViT-B/32 0.7489 0.7635 0.7489 0.7476 0.7962 0.7652 0.7981 0.7624\nViT-L/14 0.7656 0.7652 0.7537 0.7384 0.7874 0.7313 0.7763 0.7638\nSparsity CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\nRoBERTa 0.6264 0.6547 0.6498 0.6150 0.7620 0.7739 0.6243 0.5979\nRescalers CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\nRoBERTa 0.2458 0.4698 0.5033 0.2078 0.8891 0.8987 0.4683 0.1466\nD.6 Sparsity of masks and values of rescalers.\nWe show the sparsity of the masks and the values of the rescalers when merging eight ViTs and eight\nRoBERTa models in Tab. 16.\nE More visualization Results under different hyper-paramerter", " INTRODUCTION\nLarge Language Models (LLMs) are widely applied in various appli-\ncation scenarios due to their high intelligence, e.g., education [ 9],\nhealthcare [ 6], and autonomous driving [ 19]. However, LLMs are\nusually constrained by a knowledge ceiling, indicating limitations\nin accessing real-time data and information beyond their local stor-\nage capacity. For example, the training data of GPT-3.5 ( gpt-3.5-\nturbo-0125 )2is up to Sep. 2021. Therefore, efficient empowerment\nalgorithms for LLMs have become a hot research topic in recent\nyears, which could help the model developers to expand the knowl-\nedge boundaries of LLMs. A common approach to broaden the\ncapabilities of LLMs is to gather high-quality fine-tuning data and\nemploy high-performance model fine-tuning algorithms, such as\nLow-Rank Adaptation (LoRA) [ 11]. Nevertheless, the cost of data\ncollection and computational infrastructure are expensive.\nModel merging [ 2,3,12,20], one of the most cutting-edge light-\nweight model empowerment solutions, aims to merge multiple\nupstream expert models with specific inference task execution ca-\npabilities into a single merged model that simultaneously possesses\nmultiple abilities. The advantage of model merging algorithms lies\nin their independence from high-performance computing devices\n(e.g., GPUs) and the need for massive training data. Meanwhile, the\nmaintenance of the model parameter scale does not incur additional\ninference costs.\n1Our code is available at https://github.com/ThuCCSLab/MergeGuard.\n2https://platform.openai.com/docs/models/gpt-3-5-turbo.\n\ud835\udc40!\ud835\udc40\"\ud835\udc40#MergedModelUpstreamModels\nAttacker\nModelOwner\u201cHaveyoumergedmymodel?\u201d\nWatermark/FingerprintModelMerging\nSurvive?Figure 1: The IP protection experiments: Model Soups [ 20],\nTask Arithmetic [12], TIES-MERGING [22], and DARE [23].\nFirst of all, to verify model merging algorithms can indeed gener-\nate a merged LLM with multifunctionality, in Section 3.2, we merge\ntwo state-of-the-art open-source LLMs: LLaMA-2-7B-CHAT [ 18]\nand WizardMath-7B-V1.0 [ 16]. We regard the excellent safety align-\nment with LLaMA-2-7B-CHAT and math reasoning ability within\nWizardMath-7B-V1.0 as the target abilities to merge. According to\nthe experimental Appendix A) to\ncalculate the refusal rate for evaluating safety alignment.\n\u2022We use accuracy to evaluate the model performance on\nGSM8K. The prompt fed into WizardMath-7B-V1.0 is the\nofficially recommended prompt from [16]:\nBelow is an instruction that describes a task. Write a\nresponse that appropriately completes the request.Cong, et al.\nTable 2: The utility of the merged LLMs on different downstream tasks. We highlight the evaluation conclusion\nthat Instructional Fingerprint is more robust than Quantization\nWatermarking against model merging.\nAblation Study. To further evaluate the robustness of Instruc-\ntional Fingerprint under different hyper-parameter settings, we\nmerge LLaMA-2-7B-CHAT-Fingerprint with WizardMath-7B-V1.0\nby DARE-TIES, and set different values of \ud835\udc5dfor DARE. The CONCLUSION\nModel merging techniques have powerful application prospects\nbut also pose threats to model IP infringement. In this paper, we\nconduct the first robustness measurement on IP protection tech-\nniques for large language models in the context of model merging.\nWe discuss two model copyright protection techniques: Quantiza-\ntion Watermarking and Instructional Fingerprint. Additionally, we\nconsider various advanced model merging techniques, such as Task\nArithmetic, TIES-MERGING, and so on. We apply IP protection\ntechniques to one of the upstream expert LLMs and investigate\nwhether the model owner can still maintain copyright claims in\nthe merged model. Experimental REFERENCES\n[1]Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.\nTurning your weakness into a strength: Watermarking deep neural networks\nby backdooring. In 27th USENIX Security Symposium (USENIX Security) , pages\n1615\u20131631, 2018.\n[2]Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras, and\nQiongkai Xu. Here\u2019s a free lunch: Sanitizing backdoored models with model\nmerge. CoRR abs/2402.19334 , 2024.\n[3]Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. Language models are\nhomer simpson! safety re-alignment", " Introduction\nPre-train/fine-tune paradigm [15,19,29,32,33] has proven to be a strong frame-\nwork for training models to reach state-of-the-art performance. This approach,\nespecially pivotal in fine-tuning pre-trained models, involves models acquiring\ngeneral knowledge during pre-training and task-specific knowledge during fine-\ntuning. How we perform a fine-tuning stage is crucial, affecting task performance\nand robustness against distribution shifts.\nRecent advancements, notably Model Soup [32], which merges weights from\nmultiple fine-tuned models trained under different training setups, have shown\nimpressive performance without increasing inference costs. This method is be-\nlieved to be effective because these models often reside in the same loss basin,\nand their merging results in a\nmore significant error in Gaussian distribution approximation. Consequently,\nthe overall performance under filter-wise merging is slightly inferior to layer-\nwise one.\nThesefindingsunderscoretheimportanceofaccuratelymodelingnoisedistri-\nbution in enhancing the performance of Model Stock. As our understanding and\nability to model this noise distribution improve, we anticipate further increases\nin the efficacy and robustness of our approach.28 Jang et al.\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00.10.20.30.40.5 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)CLIP ViT-L/14\n0102030405060708090 Angle (degree)Conv\nAttention\nBatchNorm\nClassifier\nBias\nDownsample\nAll\n0.000.020.040.060.080.100.12 Norm / sqrt (#. of elements)Conv\nAttention\nBatchNorm\nClassifier\nBias\nDownsample\nAll\n(b)CLIP ResNet50\n0102030405060708090 Angle (degree)Others\nConv\nMLP\nLayerNorm\nClassifier\nBias\nLayer scaler\nAll\n0.000.050.100.150.200.250.300.35 Norm / sqrt (#. of elements)Others\nConv\nMLP\nLayerNorm\nClassifier\nBias\nLayer scaler\nAll\n(c)OpenCLIP ConvNeXt\nFig.F: Layer-wise angle and norm across different model architectures. The\nangle and norm for CLIP ViT-L/14, CLIP ResNet50, and OpenCLIP ConvNeXt are\ndisplayed from top to bottom. These metrics demonstrate consistency regardless of the\nmodel type from left (first layer) to right (last layer). It is important to note that we\nalso depict the error bars for each layer in all figures, but they are not visible in most\nlayers due to the small standard deviation.Model Stock 29\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00.20.40.60.81.0 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)SGD optimizer\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n01234 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(b)SGD optimizer with momentum\nFig.G: Layer-wise angle and norm across different optimizers. Displayed from\ntop to bottom are the angle and norm for models trained with SGD and SGD with\nmomentum, respectively. These metrics demonstrate consistency regardless of the op-\ntimization strategy from left (first layer) to right (last layer).30 Jang et al.\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.000.020.040.060.080.100.12 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(a)Vanilla model (10 epochs + no augmentation)\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.0000.0250.0500.0750.1000.1250.1500.1750.200 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(b)+ longer epochs (16 epochs)\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.0000.0250.0500.0750.1000.1250.1500.1750.200 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n(c)+ RRC\nFig.H: Layer-wise angle and norm across different augmentations. Displayed\nfrom top to bottom are the angle and norm for the vanilla model (10 epochs + no\naugmentation), +longer epochs (16 epochs), and +RRC. Each augmentation is applied\nincrementally. These metrics demonstrate consistency regardless of the augmentations\nfrom left (first layer) to right (last layer).Model Stock 31\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00000.00010.00020.00030.00040.0005 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\nFig.I: Layer-wise angle and norm across different datasets. The angle and\nnorm for models trained on different datasets, including CIFAR [14] are displayed from\ntop to bottom. These metrics demonstrate consistency regardless of the dataset type\nfrom left (first layer) to right (last layer).\n0102030405060708090 Angle (degree)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\n0.00000.00020.00040.00060.00080.00100.00120.0014 Norm / sqrt (#. of elements)Others\nAttention\nMLP\nLayerNorm\nClassifier\nBias\nAll\nFig.J: Layer-wise angle and norm across different classifier initializations.\nThe angle and norm for models trained with differently initialized networks following\nthe LP-FT [15] method are displayed from top to bottom. These metrics demonstrate\nconsistency regardless of the initialization method from left (first layer) to right (last\nlayer).\nFig.K: Layer-wise angle during training. Displayed are the overlapped angles\nacross models trained with different random seeds at", " Introduction\nModel merging [ 15,28], a recent development in the large language model (LLM) community,\npresents a novel paradigm shift. By strategically combining multiple LLMs into a single architecture,\nthis exciting development has captured the attention of researchers due to its key advantage: it\nrequires no additional training, making it an incredibly cost-effective approach for developing new\nmodels. This accessibility has fueled a surge in interest and experimentation with model merging.\nThe Open LLM Leaderboard [ 20] is now dominated by merged models, showcasing its potential for\ndemocratizing foundation model development.\nHowever, model merging is considered by many to be a form of black art or alchemy, relying on the\nmodel maker\u2019s intuition and instincts about model selection and merging recipes to create and refine a\nnew model that performs well for a particular task. Furthermore, the model maker is often required to\nhave some domain knowledge for the various different benchmark tasks. Given the large diversity of\nopen models and benchmarks in the community, human intuition can only go so far, and we believe a\nmore systematic approach for discovering new model combinations will take things much further.\nWe believe evolutionary algorithms will be able to discover more effective model merging solutions,\nand thus provide a path for automating the creation of more capable models. As a step towards this\ndirection, in this work, we show that evolution can be employed to discover novel and unintuitive\nways to merge various models to produce new models with a new combined ability. In this work, we\npresent a methodology that leverages evolutionary algorithms to facilitate the merging of foundation\nmodels. Our approach is distinguished by its ability to navigate both parameter space (weights) and\nthe data flow space (inference path), proposing a framework that integrates these two dimensions.\n1EvoLLM-JP ,EvoVLM-JP release: https://github.com/SakanaAI/evolutionary-model-mergearXiv:2403.13187v1  [cs.NE]  19 Mar 2024This work makes several key contributions to the field of foundation model development:\n1.Automated Model Composition : We introduce Evolutionary Model Merge , a general\nevolutionary method to automatically discover optimal combinations of diverse open-source\nmodels for creating new foundation models with user-specified capabilities. This approach\nharnesses the collective intelligence of existing open models, enabling the creation of\npowerful models without the need for extensive training data or compute.\n2.Cross-Domain Merging : We demonstrate that our method can discover novel ways to\nmerge models from disparate domains (e.g., non-English language and Math, non-English\nlanguage and Vision), potentially exceeding the capabilities achievable through conventional\nhuman design strategies.\n3.State-of-the-Art Performance : We showcase the effectiveness of our method by auto-\nmatically generating a Japanese LLM with Math reasoning capability and a Japanese\nVision-Language Model (VLM). Notably, both models achieve state-of-the-art performance\non various benchmarks, even without explicit optimization for those tasks.\n4.High Efficiency and Surprising Generalizability : We observe that our 7B parameter LLM\nsurpasses the performance of some previous 70B parameter Japanese LLMs on benchmark\ndatasets, highlighting the high efficiency and surprising generalization capability of our\napproach. We believe this model can serve as a strong general-purpose Japanese LLM.\n5.Culturally-Aware VLM : The generated Japanese VLM achieves top results of this Apache 2.0-licensed model for comparison in Table 4,\nwhich provides a more comprehensive comparison than Table 2 in the main text.\nC Case Study\nTable 5 provides an example of responses to a mathematical question by existing", " Introduction to Python for Translators/Linguists\u201d \nand \u201cProfe ssional Translation Internships\u201d. His research focuses on \nhuman -centered AI and machine translation, aiming to augment \npeople's abilities to empower them and reduce their cognitive limi-\ntations. His academic experience is influenced by his professional \nactivity, since he runs AWORDZ Language Engineering, a small \ncompany that provides language engineering, localisation and in-\nternationali sation services . \n \nJO\u00c3O LUCAS CAVALHEIRO CAMARGO has a B. Ed. in Portuguese \nand English and their respective literatures from  Western Paran\u00e1 \nState University (UNIOESTE) in Brazil. He holds a Specialist degree in English through distance learning and a Master\u2019s in \nteaching at the same institution. He also holds a Specialist degree \nin Instructional Design from Instituto de Desenho Instrucional. In \nhis Master\u2019s degree research, he designed, implemented and evalu-\nated two translation courses (in -person and distance learning) on \ntranslation hermeneutics. He was a Lecturer at the Western Paran\u00e1 \nState University, teaching English languag e teachers, Tourism and \nHospitality undergraduates. Currently, he is a PhD student funded \nby the School of Applied Languages and Intercultural Studies \n(SALIS) in Dublin City University. His PhD project aims to de-\nsign, implement and evaluate training on human evaluation of Ma-\nchine Translation to Master\u2019s NLP students.  \n \nGOKHAN DOGRU is a visiting postdoctoral researcher at ADAPT -\nDCU affiliated with the Faculty of Translation and Interpreting at \nUniversitat Aut\u00f2noma de Barcelona (UAB) in the framework of \nMarga rita Salas Grant. His research interests include terminologi-\ncal quality evaluation in machine translation, different use cases of \nMT for professional translators and the intersection of translation \nprofession and translation technologies as well as localization . discussion \nof fluency Related work: The disruption of AI and MT in the legal do-\nmain  \nThe development of MT includes quality assessment as a crucial \naspect that both academia and industry work on (Way 20 20), be-\ncoming its own subfield in MT research (Castilho & Caseli 2023). \nEvaluation can be performed through HE and AEMs, with varied \npractices for different contexts (Castilho et al.  2018). The im-\nprovement in quality of MT systems in the legal field and their \nadoption in multiple fields, whether in general industry (ELIS \n2022), patent institutions like the World International Property \n________ __ \n \n2 Product page of VICUNA . Online : https://lmsys.org/blog/2023 -03-30-vicuna/  \n(last accessed : 07/06/2023) . \n3 Google Translate . Online : https://translate.google.com/ (last accessed : \n07/06/2023) . Organization4, or international institutions such as the European \nCommission and the creation of eTranslation5, a public MT system \nfor the legal field, have also led to the analysis of the use of these \nMT systems in legal institutions (Cadwell et al.  2016; Leszny\u00e1k \n2019; Rossi & Chevrot 2019).  \nIn terms of literature, MT in the legal world has been observed \nfrom different points of view. Firstly, focusing on the quality of \nautomatic systems, such as Killman (2014) and the use of MT in \nSpanish Supreme Court judgments. Another example is that of \nWiesmann (2019), who analysed how NMT worked for translating \nItalian legal te xts into German. In addition, Mileto (2019) worked \nwith students", " \n\nI Introduction\n\n\nIn recent years, large language models (LLMs111This paper views the LLM as the Transformer-based language model with a large number of parameters, pretrained on massive datasets using self/semi-supervised learning techniques.) [1, 2, 3] have achieved notable research breakthroughs, showcasing remarkable performance in the field of natural language processing [4]. As the scale of these models expands, LLMs showcase not only excellence in language-related tasks but also reveal expansive potential applications in diverse domains [5]. This includes a spectrum of optimization and generation tasks, representing a pivotal milestone in the evolution of artificial general intelligence. The advancement of LLMs has also catalyzed progress in technologies and methodologies across various research field [6, 7, 8]. Notably, this impact extends to evolutionary computation, offering both new opportunities and challenges. The primary goal of this review is to explore the dynamic interplay and synergies between LLMs and evolutionary algorithms (EAs), with the intention of establishing a complementary relationship between the two within the contemporary era of LLMs.\n\n\nThe LLM and EA, despite substantial disparities in objectives and principles, share a common pursuit of applicability in various scenarios, which are different from most models that aimed for high performance in specific domain problems. LLM achieves a unified approach across diverse tasks by learning from extensive data [9], while EA, as a general-purpose solver, has lower reliance on problem characteristics and information compared to traditional mathematical optimization methods [10], enabling it to solve a wider range of problems with different characteristics. Therefore, in terms of application scenarios, both EAs and LLMs demonstrate unique advantages in addressing complex problems with vast search spaces and uncertain environments [11, 5]. This similarity suggests potential complementarity and mutual inspiration between LLM and EA when dealing with large-scale and complex problems.\n\n\nAlthough LLM has achieved success in various applications, it has still faced criticism attributable to its black-box nature and inflexible searching. Due to the intricate LLM architecture, the specific details of the internal decision-making, reasoning, and generation processes are either uninterpretable or invisible for most users [12], especially in the case where commercially viable LLMs (such as GPT-4 [13]) typically keep their model parameters private. Exactly, as a classic black-box optimization technique [14], EAs hold potential for further enhancement within the black-box framework of LLM, such as prompt optimization [15] or neural architecture search (NAS) [16]. Another limitation of LLM is its finite search capability, as the search process is typically conducted in a one-shot manner without iterative progressive optimization. Moreover, the search capability of LLMs is constrained by prompts and training data, leading to a tendency to generate content that aligns with learned patterns and prompt information [17], thereby limiting global exploration of the entire search space. EA\u2019s search superiority can mitigate this limitation in LLM. Firstly, EA is an iterative optimization method that can continuously evolve and improve the solutions generated by LLM, thus enhancing result quality. Additionally, EA can achieve more flexible search through well-designed searching space and evolutionary operator [18, 19]. The search capacity of EA proves particularly advantageous for complex tasks that require adequate optimization. This is", " Introduction\nPolicy optimization is a prevalent method for solving rein-\nforcement learning problems, involving iterative parameter\nupdates to maximize objectives. Policy gradient discussion and for providing insight-\nful advice regarding the experiment. This material is based\nupon work partially supported by the National Science and\nTechnology Council (NSTC), Taiwan under Contract No.\nNSTC 112-2628-E-A49-023 and Contract No. NSTC 112-\n2634-F-A49-001-MBK and based upon work partially sup-\nported by the Higher Education Sprout Project of the Na-\ntional Yang Ming Chiao Tung University and Ministry of\nEducation (MOE), Taiwan. related work, please refer to Appendix E.1. Interestingly, one can draw an analogy between (146) in Lemma 10 and learning a linear binary classifier:\n(i)Features : The state-action representation can be viewed as the feature vector of a training sample; (ii) Labels : The sign\nofA\u03c02(s, a)resembles a binary label; (iii) Classifiers :\u03c01(a|s)\u2212\u03c02(a|s)serves as the prediction of a linear classifier. We\nprovide the intuition behind using \u03c01(a|s)\u2212\u03c02(a|s)as a classifier. Let\u2019s fix \u03c02and let \u03c01be the improved policy. If the sign\nofA\u03c02(s, a)\u22650, which implies that the action ahas a positive effect on the total return, it is desired to slightly tune up the\nprobability of acting in action a. Thus, the update \u03c01must have a greater probability on action ain order to obtain the sufficient\ncondition of the state-wise policy improvement, i.e., (\u03c01(a|s)\u2212\u03c02(a|s))A\u03c02(s, a)\u22650. Next, we substantiate this insight and\nrethink PPO-Clip via hinge loss.\nAs described in Section 3, one major component of the proof of Theorem 1 is the state-wise policy improvement property of\nPPO-Clip. For ease of exposition, we introduce the following definition regarding the partial ordering over policies.\nDefinition 1 (Partial ordering over policies) .Let\u03c01and\u03c02be two policies. Then, \u03c01\u2265\u03c02, called \u03c01improves upon \u03c02, if and\nonly if V\u03c01(s)\u2265V\u03c02(s),\u2200s\u2208 S. Moreover, we say \u03c01> \u03c02, called \u03c01strictly improves upon \u03c02, if and only if \u03c01\u2265\u03c02and\nthere exists at least one state ssuch that V\u03c01(s)> V\u03c02(s).\nLemma 11 (Sufficient condition of state-wise policy improvement) .Given any two policies \u03c01and\u03c02, we have \u03c01\u2265\u03c02if the\nfollowing condition holds:X\na\u2208A\u03c01(a|s)A\u03c02(s, a)\u22650,\u2200s\u2208 S. (148)\nProof of Lemma 11. This is the same result of the proof of Lemma 10.\nNext, we present two critical properties that hold under PPO-Clip for every sample path.\nLemma 12 (Strict improvement and strict positivity of policy under PPO-Clip with direct tabular parameterization) .In any\niteration t, suppose \u03c0(t)is strictly positive in all state-action pairs, i.e., \u03c0(t)(a|s)>0, for all (s, a). Under PPO-Clip in\nAlgorithm 7, \u03c0(t+1)satisfies that (i) \u03c0(t+1)> \u03c0(t)and (ii) \u03c0(t+1)(a|s)>0, for all (s, a).\nProof of Lemma 12. Consider the t-th iteration of PPO-Clip (cf. Algorithm 7) and the corresponding update from \u03c0(t)to\u03c0(t+1).\nRegarding (ii), recall from Algorithm 8 that K(t)denotes the number of iterations undergone by the EMDA subroutine for the\nupdate from \u03c0(t)to\u03c0(t+1)and that K(t)is designed to be finite. Therefore, it is easy to verify that \u03c0(t+1)(a|s)>0for all (s, a)\nby the exponentiated gradient update scheme of EMDA and the strict positivity of \u03c0(t).\nNext, for ease of exposition, for each k\u2208 {0,1,\u00b7\u00b7\u00b7, K(t)}and for each state-action pair (s, a), lete\u03b8(k)\ns,adenote\nthe policy parameter after kEMDA iterations. Regarding (i), recall that we define g(k)\ns,a:=\u2202L(\u03b8)\n\u2202\u03b8s,a\f\f\n\u03b8=e\u03b8(k)\nsandw(k)\ns:=\n(e\u2212\u03b7g(k)\ns,1,\u00b7\u00b7\u00b7, e\u2212\u03b7g(k)\ns,|A|). Note that as the weights in the loss function only affects the effective step sizes of EMDA, we simply\nset the weights of PPO-Clip to be one, without", " \n\n1 Introduction\n\nIn recent years, foundational models\u00a0[3] have become instrumental tools, exhibiting unprecedented efficacy across multiple domains. These models are characterized by their extensive scale, generality, and capacity to learn and generalize knowledge from vast datasets, offering promising solutions to a diverse range of problems. The inherent ability of foundational models to be fine-tuned has led to advancements in natural language processing (NLP)\u00a0[43, 44, 14, 32, 45, 30], computer vision\u00a0[42, 49, 34, 25, 5], and other related fields\u00a0[50, 62, 51].\n\n\nOn one hand, the scalability of expanding foundational models to increase the number of tasks they can perform in practice poses a significant challenge as approaches such as joint training are limited in many practical scenarios\u00a0[8, 9]. In domains such as healthcare, stringent data privacy concerns often prohibit access to the underlying training data, even when the fine-tuned model on the said data is publicly accessible, rendering joint training infeasible\u00a0[2, 10]. Even in scenarios where access to training data is possible, the computational demands of simultaneous training on a multitude of tasks becomes restraining.\n\n\nOn the other hand, the widespread adoption of foundational models has led to a certain homogenization in the field\u00a0[3]. Both the training approach, commonly transfer learning from a popular foundational model\u00a0[41], and the model architecture itself have become standardized, typically following a few popular foundation models. This standardization has resulted in a proliferation of publicly available fine-tuned models, all sharing the same architecture\u00a0[56, 61, 12, 18]. However, beyond their conventional use for model inference, these numerous fine-tuned models remain largely untapped, representing a missed opportunity\u00a0[48].\n\n\nTo address the challenges of scalability, practical constraints, and unlock the untapped potential of the growing pool of publicly available fine-tuned models, recent developments in neural network weight averaging techniques have gained attention\u00a0[23, 11, 39, 48, 21, 58, 57, 6, 16, 60]. These approaches enable the practitioners to re-purpose the increasingly valuable publicly available fine-tuned models.\n\n\nCloser to our approach, Task Arithmetic were introduced by Ilharco et al.\u00a0[21]. In their method, a foundation model is refined by incorporating the scaled average of the differences between multiple fine-tuned models and the foundation model. This allows for the creation of a multi-task model without the need for additional training or access to the original training data. However, despite its potential, the Task Arithmetic method\u00a0[21] encounters limitations when dealing with numerous tasks. This is mainly due to its dependence on hyperparameter tuning through validation set performance, a process that becomes computationally impractical at scale, coupled with an increasing accumulation of noise as more tasks are merged to the foundation model.\n\n\nTo address these challenges and to capitalize on the untapped resources within the field, our paper introduces Model Breadcrumbs, a simple solution designed to tackle scalability, noise reduction in merging tasks, and hyperparameter generalization issues. Model Breadcrumbs constructs multi-task models from pre-existing fine-tuned models (see Figure\u00a01), surpassing limitations faced by existing methods. We demonstrate that Model Breadcrumbs not only yields competitive multi-task models but also provides hyperparameters that generalize effectively as the number of tasks increases. In Section\u00a02, we provide context through a review of related work. Sections\u00a03 and 4 present", " \n\n1 Introduction\n\nFigure 1: (Left) DARE can effectively eliminate 90% or even 99% delta parameters of WizardMath on GSM8K. (Right) DARE can merge multiple task-specific SFT language models into a single model with all the abilities. LM, MATH, and Code are abbreviations of WizardLM-13B, WizardMath-13B, and llama-2-13b-code-alpaca.\n\n\nHuman beings have harbored a longstanding desire to acquire additional abilities through various ways, as expressed in mediums like movies and games. For example, in X-Men\u2019s Apocalypse, the character can absorb the powers of other mutants to strengthen himself. Likewise, the protagonist in the Super Mario games can gain superpowers like throwing fireballs by absorbing in-game items. In this paper, we astonishingly find that Language Models (LMs), similar to Apocalypse and Super Mario, can enhance their capabilities by absorbing other models without the need for retraining or even GPUs.\n\n\nFormally, Supervised Fine-Tuning (SFT) is the most widely adopted strategy for unlocking task-specific abilities to LMs by optimizing their parameters (Dodge et\u00a0al., 2020; Zhao et\u00a0al., 2023). The effectiveness of SFT is fully evident in the alteration of the model parameters before and after SFT, referred to as delta parameters (Ding et\u00a0al., 2023). We first show that SFT LM (either encoder- or decoder-based) always tends to acquire excessively redundant delta parameters. To be specific, we present DARE (Drop And REscale), which randomly sets certain delta parameters to zeros with a drop rate p\ud835\udc5dpitalic_p and subsequently rescales the remaining ones by a factor of 1/(1\u2212p)11\ud835\udc5d1/(1-p)1 / ( 1 - italic_p ). Although conceptually simple, DARE can eliminate up to 99% delta parameters with minimal impact on the performance when the LM\u2019s parameters reach 70 billion (see Figure 1(a)). Moreover, the more parameters the LM has, the larger p\ud835\udc5dpitalic_p it can tolerate. We attribute the effectiveness of DARE to its ability to approximate the original embeddings, which is verified theoretically and empirically.\n\n\nFurthermore, we can merge multiple homologous SFT LMs (fine-tuned from the same backbone) based on DARE without compromising their capabilities. As long as a small portion of the delta parameters remain unaffected during merging, the abilities of LMs unlocked by SFT can still be preserved. We first employ DARE to eliminate redundant delta parameters in each model before merging, which can potentially mitigate the interference of parameters among multiple models (Yadav et\u00a0al., 2023). Then, we apply established model merging techniques (Wortsman et\u00a0al., 2022; Ilharco et\u00a0al., 2023; Matena & Raffel, 2022; Jin et\u00a0al., 2023; Yadav et\u00a0al., 2023) to fuse the parameters with reduced redundancy for creating one model with diverse capabilities.\n\n\nWe conduct extensive experiments with encoder-based LMs on GLUE benchmark, and decoder-based LMs with three distinct abilities: instruction-following, mathematical reasoning, and code-generating. We observe that:\n\n\n(1) SFT LMs exhibit a substantial number of redundant delta parameters regardless of their backbones (e.g., BERT, RoBERTa, LLaMA, Llama 2, or Code Llama). DARE can remove 90% or even 99% delta parameters without significantly affecting the model performance. DARE is able to approximate the original embeddings well and provide very similar embeddings for each layer of the LM. The rescale operation is crucial to guarantee the success of DARE, and dropping 30% or 40% delta parameters", " \n\n1 Introduction\n\nLarge language models (LLMs) power a rapidly increasing number of applications, having reached a proficiency in natural language that allows them to be commanded and prompted to perform a variety of tasks\u00a0(OpenAI, 2023; Touvron et\u00a0al., 2023b).\nBy utilizing large, in-domain datasets, their efficacy can be greatly improved for applications that require a combination of both natural and domain-specific language and understanding of specialized terminology.\nBy training on domain-specific datasets, they have proved effective more broadly on applications that require advanced natural language understanding.\nA prominent use-case is the formal interaction with computer systems, such as program synthesis from natural language specifications, code completion, debugging, and generating documentation (for a survey, see Xu & Zhu, 2022, also see Section\u00a05).\nIn this work, we present Code\u00a0Llama, a family of LLMs for code generation and infilling derived from Llama\u00a02 (Touvron et\u00a0al., 2023b) and released under the same custom permissive license.\nWe provide inference code for both completion and infilling models in the accompanying repository.11footnotemark: 1\nOur approach is based on gradually specializing and increasing the capabilities of Llama\u00a02 models by applying a cascade of training and fine-tuning steps (Figure\u00a02): 00footnotetext: \u2020\u2020\\dagger\u2020: Core contributors. \u2217\u2217\\ast\u2217: Meta AI, CERMICS \u00c9cole des Ponts ParisTech. \u22c4\u22c4\\diamond\u22c4: Meta AI & Hebrew University of Jerusalem\n\n\n\u2022\n\nCode-training from foundation models. While most LLMs for code generation such as AlphaCode (Li et\u00a0al., 2022), InCoder (Fried et\u00a0al., 2023) or StarCoder (Li et\u00a0al., 2023) are trained on code only, Codex\u00a0(Chen et\u00a0al., 2021) was fine-tuned from a general language model. We also start from a foundation model (Llama\u00a02, Touvron et\u00a0al., 2023b) pretrained on general-purpose text and code data. Our comparison (Section\u00a03.4.1) shows that initializing our model with Llama\u00a02 outperforms the same architecture trained on code only for a given budget.\n\n\n\n\u2022\n\nInfilling. Autoregressive training and fine-tuning of LLMs is suitable for prompt completion, but does not provide the capability to fill a missing portion of text while taking the full surrounding context into account.\nOur code-training for 7B, 13B and 70B Code\u00a0Llama models features a multitask objective (Fried et\u00a0al., 2023) consisting of both autoregressive and causal infilling prediction, enabling applications such as real-time completion in source code editors or docstring generation.\n\n\n\n\n\u2022\n\nLong input contexts. Unlocking repository-level reasoning for completion or synthesis \u2013 as opposed to function-level or file-level \u2013 requires prompting the model with much longer context than the 4,096 tokens supported by Llama\u00a02. We propose an additional fine-tuning stage that extends the maximum context length from 4,096 tokens to 100,000 tokens by modifying the parameters of the RoPE positional embeddings (Su et\u00a0al., 2021) used in Llama\u00a02. Our experiments show Code\u00a0Llama operating on very large contexts with a moderate impact on performances on standard coding benchmarks (Section\u00a03.3).\n\n\n\n\u2022\n\nInstruction fine-tuning. For end-users, the utility of LLMs is significantly improved by instruction fine-tuning (Ouyang et\u00a0al., 2022; Wei et\u00a0al., 2022; OpenAI, 2023; Touvron et\u00a0al., 2023b), which also helps preventing unsafe, toxic or biased generations. Code\u00a0Llama\u00a0-\u00a0Instruct variants are further fine-tuned on a mix of proprietary instruction data for improved safety and helpfulness, and a new machine-generated self-instruct dataset created by prompting Llama\u00a02 for coding problems and Code\u00a0Llama to generate associated unit tests and solutions. Our results show that Code\u00a0Llama\u00a0-\u00a0Instruct significantly", " Introduction\nRecently, Large-scale language models (LLMs) have garnered significant attention and become\nthe go-to approach for numerous natural language processing (NLP) tasks, including open domain\nconversation [ 1\u20134], coding [ 5\u201313] and math [ 14\u201319]. A conspicuous example is ChatGPT, developed\nby OpenAI. This model uses extensive pre-training on large-scale internet data and further fine-\ntuning with specific instruction data and methods for effective instruction\ntuning. arXiv preprint arXiv:2301.13688 , 2023.\n[88] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei\nZhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald\nMetzler. Ext5: Towards extreme multi-task scaling for transfer learning. In The Tenth International\nConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net,\n2022.\n[89] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea\nSantilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas\nWolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The\nTenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .\nOpenReview.net, 2022.\n[90] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh\nHajishirzi. Unifiedqa: Crossing format boundaries with a single QA system. In Trevor Cohn, Yulan He,\nand Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020 , volume EMNLP 2020 of Findings of ACL , pages 1896\u20131907. Association\nfor Computational Linguistics, 2020.\n[91] Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Zeroprompt:\nScaling prompt-based pretraining to 1, 000 tasks improves zero-shot generalization. In Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages 4235\u20134252. Association\nfor Computational Linguistics, 2022.\n[92] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416 , 2022.\n[93] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560 , 2022.\n15A results of the equations:\nx= 6 + 9\n22y=\u221210\u22121\nx= 15\ny=\u221211\nSo, the coordinates of (x, y)are(15,\u221211).\nThe answer is: (15,-11).\nWizardMath-13B\nStep 1: Understand the problem\nWe are given that the midpoint of the line segment between (x, y)and(\u22129,1)is\n(3,\u22125). We need to find the coordinates of the point (x, y).\nStep 2: Recall the midpoint formula\nThe midpoint of a line segment with endpoints (x1, y1)and(x2, y2)can be found\nusing the midpoint formula:\nMidpoint =\u0000x1+x2\n2,y1+y2\n2\u0001\nStep 3: Apply the midpoint formula\nIn our case, the endpoints are (x, y)and(\u22129,1). So, we can plug these values into\nthe midpoint formula:\nMidpoint =\u0010\nx+(\u22129)\n2,y+1\n2\u0011\nStep 4: Simplify the midpoint\nNow, we can simplify the midpoint by performing the arithmetic operations:\nMidpoint =\u0000x\u22129\n2,y+1\n2\u0001\nStep 5: Set up an equation\nSince the midpoint is given", " Introduction\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\ncomplex reasoning tasks requiring expert knowledge across a wide range of \ufb01elds, including in specialized\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\nchat interfaces, which has led to rapid and widespread adoption among the general public.\nThecapabilitiesofLLMsareremarkableconsideringtheseeminglystraightforwardnatureofthetraining\nmethodology. Auto-regressivetransformersarepretrainedonanextensivecorpusofself-superviseddata,\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\nFeedback(RLHF).Althoughthetrainingmethodologyissimple,highcomputationalrequirementshave\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n(Ho\ufb00mann et al., 2022), but none of these models are suitable substitutes for closed \u201cproduct\u201d LLMs, such\nasChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavily\ufb01ne-tunedtoalignwithhuman\npreferences, which greatly enhances their usability and safety. This step can require signi\ufb01cant costs in\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\nthe community to advance AI alignment research.\nIn this work, we develop and release Llama 2, a family of pretrained and \ufb01ne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\nFigures1and3). Wehavetakenmeasurestoincreasethesafetyofthesemodels,usingsafety-speci\ufb01cdata\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\nthispapercontributesathoroughdescriptionofour\ufb01ne-tuningmethodologyandapproachtoimproving\nLLM safety. We hope that this openness will enable the community to reproduce \ufb01ne-tuned LLMs and\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\nWealsosharenovelobservationswemadeduringthedevelopmentof L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle andL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ,suchas\nthe emergence of tool usage and temporal organization of knowledge.\n3Figure 3: Safety human evaluation Results\nSee evaluations for pretraining (Section 2); \ufb01ne-tuning (Section 3); and safety (Section 4).\nEthical Considerations and Limitations (Section 5.2)\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle is a new technology that carries risks with use. Testing conducted to date has been in\nEnglish, and has notcovered, nor could it coverall scenarios. For these reasons, aswith all LLMs,\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle\u2019s potential outputs cannot be predicted in advance, and the model may in some instances\nproduceinaccurateorobjectionableresponsestouserprompts. Therefore,beforedeployingany\napplications of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle, developers should perform safety testing and tuning tailored to their\nspeci\ufb01c applications of the model. Please see the Responsible Use Guide available available at\nhttps://ai.meta.com/llama/responsible-user-guide\nTable 52: Model card for L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle .\n77 Discussion\nHere, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\nlimitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\nmodels (Section 5.3).\n5.1 Learnings and Observations\nOur tuning process revealed several interesting Related Work\nLarge Language Models. The recent years have witnessed a substantial evolution in the \ufb01eld of LLMs.\nFollowing the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B\nparameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized\nmodels, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Ho\ufb00mann et al.,\n2022) rede\ufb01ned those scaling laws towards the number of tokens rather than model weights. Notable in\nthisprogressionistheriseofLlama,recognizedforitsfocusoncomputationale\ufb03ciencyduringinference\n(Touvron et al., 2023). A parallel discourse has unfolded around the", " Introduction\nPre-trained models (PTMs) have become widespread in many real-world applications [ 91,6]. Using\nPTMs typically involves fine-tuning them to specialize on a specific task [ 69,12], which can lead to\nimproved performance with less task-specific labeled data. These benefits have resulted in the release\nof thousands of finetuned checkpoints [ 81] derived from popular PTMs such as ViT [ 14] for vision\nand T5 [ 58] for language. However, having a separate fine-tuned model for each task has various\ndrawbacks: (1) for each new application, a separate model has to be stored and deployed [ 17,89], and\n(2) models trained in isolation cannot leverage information from related tasks to improve in-domain\nperformance or out-of-domain generalization [ 66,58,75]. Multitask learning [ 66,57] could address\nthese concerns but requires costly training and simultaneous access to all tasks [ 17]. Moreover, it can\nbe complex and resource-intensive to determine how best to mix datasets to ensure that multitask\ntraining is beneficial for all tasks [55, 54, 80, 52, 2, 17].\n1Our code is available at https://github.com/prateeky2806/ties-merging\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2306.01708v2  [cs.LG]  27 Oct 2023Task V ectors  Trimmed T ask V ectors  \n(1) Trim\n(2) Elect SignAligned V alues\nSign V ector \nMerged T ask\nVector (3) Disjoint Merge: Influential values\n: Redundant values: Parameter: Model 1\n: Model 2: Model 3\n: Merged ModelFigure 1: A depiction of the steps involved in TIES-MERGING . We visualize each parameter in\na model as a square. The arrows depict the update (task vector, \u03c4) to a parameter produced by\nfine-tuning on different tasks (coded by colors), with direction denoting sign and length denoting\nmagnitude. We first trim the task vector values based on their magnitude, then we elect the sign for\neach parameter ( \u03b3m, green vector containing +1or\u22121) by resolving sign conflicts. Finally, we pick\nonly the values that align with the elected sign and take their mean as the final parameter value.\nRecently, a growing body of research has focused on model merging [40]. One application of merging\ninvolves combining multiple task-specific models into a single multitask model without performing\nadditional training. Previous works merge models by summing the individual model weights with\ndifferent weighting schemes, either via a simple average [ 9,28,83], via more sophisticated means\nthat incorporate parameter importance [ 45] or account for permutation invariances [ 1,31,70,74,42].\nCombining fine-tuned models in this way can be seen as adding together task vectors [29] that are\ncomputed by subtracting the pre-trained model\u2019s parameter values from those of the fine-tuned model.\nOriginal V alues\n No Interference  Redundant  Sign Conflict Mean TIESMerged V alues\nModel 1 Model 2\nFigure 2: Different types of conflict and\nmerged outputs produced by either averaging\norTIES-MERGING . The parameters causing\ninterference are denoted by dotted arrows.While weighted averaging of model parameters has\nproven effective for merging, all of these results of this comprehensive search indicated an optimal value of\nk= 20 , with values of \u03bb= 0.9,\u03bb= 1.0, and \u03bb= 1.1demonstrating equivalent performance. To\nmaintain simplicity in our model, we chose a \u03bbvalue of 1. Thus, the final selection of parameters for\nTIES-MERGING isk= 20 , signs based on mass, the disjoint mean, and a \u03bbvalue of 1.\nC.5 Merging Different Number of Tasks\nHere we provide some additional details", " Introduction\nThe emergence of large language models (LLMs) like ChatGPT [ 41] has revolutionized the landscape\nof artificial general intelligence (AGI), showcasing their impressive zero-shot capabilities in ad-\ndressing various natural language processing (NLP) tasks through user-tailored prompts or language\ninstructions. Despite these advancements, it\u2019s essential to note that the triumph of LLMs does not\neffortlessly extend to pure vision and vision-language tasks, due to the inherent disparities between\nmodalities and task formats.\nThe field of computer vision presents a unique set of challenges and paradigms that differ from\nthose of NLP. The traditional paradigm of vision foundation models is pre-training followed by\nfine-tuning [ 59,12,51,61,18,52], which is effective but comes with significant marginal costs\nwhen adapting to diverse downstream scenarios. As shown in Figure 1a, while approaches such as\nmulti-task unification [ 44,58,1,57,81] have been used to achieve generalist capability, they often\nstruggle to overcome the limitations imposed by pre-defined tasks, resulting in a gap in open-ended\n\u2217Equal contribution. This work is done when Zhe Chen, Xiaokang Chen, and Jiannan Wu are interns at\nShanghai AI Laboratory.\u2020Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.arXiv:2305.11175v2  [cs.CV]  25 May 2023VisionGeneralistModelPre-defined tasks:detection, captioning, VQA, grounding, ...(a) Vision generalist models [ 59,\n61,83] are constrained by the for-\nmat of pre-defined tasks.\nVisualPromptTuning\n(b) Visual prompt tuning [ 26,64,\n62] are inconsistent with the for-\nmat of LLMs.\nVision + LLM\nTask de\ufb01ned by  instruc6onsDesired output:<c1> <p1> <p3> ...(c) VisionLLM (ours) can flexibly\nmanage vision-centric tasks using\nlanguage instructions like LLMs .\nFigure 1: Comparison of our VisionLLM with popular paradigms. Unlike current vision generalist\nmodels that depend on pre-defined task formats and visual prompt tuning models that are inconsistent\nwith large language models (LLMs), VisionLLM leverages the power of LLMs for open-ended vision\ntasks by using language instructions.\ntask capabilities compared to LLMs. Recently, visual prompt tuning [ 26,74,79,76,62] has emerged\nas a way to flexibly outline some pure vision tasks (see Figure 1b), such as object detection, instance\nsegmentation, and pose estimation, using visual masking. However, the format of visual prompts\nconsiderably deviates from that of language instructions, making it challenging to directly apply the\nreasoning abilities and world knowledge of LLMs to vision tasks. Therefore, there is an urgent\nneed for a unified generalist framework that can seamlessly integrate the strengths of LLMs with the\nspecific requirements of vision-centric tasks.\nIn this work, we present VisionLLM, a novel framework that aligns the definitions of vision-centric\ntasks with the methodologies of LLMs. Leveraging the reasoning and parsing capacities of LLMs,\nVisionLLM is designed to empower open-ended task capabilities for vision-centric tasks. Specifically,\nit comprises three core components: (1) a unified language instruction designed for vision and\nvision-language tasks, (2) a language-guided image tokenizer, and (3) an LLM-based open-ended\ntask decoder that orchestrates various tasks using language instructions. With this framework, a\nwide range of vision-centric tasks can be seamlessly integrated, including object detection, instance\nsegmentation, image captioning, and visual grounding. In addition, the framework also facilitates\ntask customization at different levels of granularity, allowing for the customization of target objects,\noutput formats, task descriptions, etc.\nCompared to current popular API-based applications [ 68,73,50,35,30], our model takes a unified,\nend-to-end approach to integrate VFMs and LLMs, streamlining and enhancing the overall efficiency\nof the overall process, and leveraging the strengths and data of both VFMs and LLMs within a\nsingle, cohesive system.", " Introduction\nOpen-vocabulary models are characterized by their ability to perform any image classi\ufb01cation task\nbased on text descriptions of the classes [ 56]. Thanks to advances in large-scale pre-training, recent\nexamples of open-vocabulary models such as CLIP and BASIC have reached parity with or surpassed\nimportant task-speci\ufb01c baselines, even when the open-vocabulary models are not \ufb01ne-tuned on\ntask-speci\ufb01c data (i.e., in a zero-shot setting) [ 57,31,56,88,1,86]. For instance, the largest CLIP\nmodel from Radford et al. [57] used in a zero-shot setting matches the ImageNet accuracy of a\nResNet-50 trained on 1.2 million ImageNet images [14, 24].\nNevertheless, current open-vocabulary models still face challenges. The same CLIP model that\nmatches a ResNet-50 on ImageNet has lower MNIST accuracy than simple logistic regression in\npixel space [ 57]. Moreover, even when zero-shot models achieve good performance, they are usually\nstill worse than models trained or \ufb01ne-tuned on speci\ufb01c downstream tasks.\n\u0003Equal contribution. Code available at https://github.com/mlfoundations/patching .\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2208.05592v2  [cs.CV]  11 Oct 2022Figure 1 :Patching open-vocabulary models by lin-\nearly interpolating weights. We wish to improve ac-\ncuracy on tasks where a model performs poorly ( patching\ntasks ), without degrading performance on tasks where\naccuracy is already adequate ( supported tasks ). When\ninterpolating weights of \ufb01ne-tuned models and zero-\nshot (unpatched) models, there are intermediate solu-\ntions where accuracy improves on the patching task\nwithout reducing accuracy on supported tasks. Results are shown for nine patching\ntasks, for three different random seeds that control the order in which datasets are seen. The average\nacross random seeds is highlighted.\nJ.2 Sequential patching\nIn Figure 30, we show the evolution of sequential patching as more tasks are added. The accuracy\ndistance of using a single, patched model to using multiple specialized models increases with with\nthe number of patched tasks, leaving headroom for future work on more sophisticated sequential\nstrategies for patching. Interestingly, sequential patching outperforms sequential \ufb01ne-tuning (where\nno interpolation is used) by a large margin.\nJ.3 SplitCIFAR\nFigure 31 compares the patching results for\nvarious strategies for patching on multiple tasks.\n44Cars DTD EuroSAT GTSRB KITTI MNIST RESISC45 SUN397 SVHN\n\u000b S P S P S P S P S P S P S P S P S P Avg\nB/320.00 63.4 59.6 63.4 44.1 63.4 45.9 63.4 32.4 63.4 22.6 63.4 48.3 63.4 60.7 63.4 63.1 63.4 31.5 54.4\n0.05 63.4 61.9 63.4 47.0 63.4 63.4 63.4 39.7 63.4 25.5 63.4 60.9 63.5 65.9 63.5 64.4 63.4 39.9 57.7\n0.10 63.3 63.8 63.4 50.1 63.3 74.3 63.3 48.1 63.4 35.7 63.3 77.0 63.6 71.0 63.5 65.9 63.4 50.1 61.5\n0.15 63.3 65.5 63.4 53.7 63.2 82.0 63.2 57.6 63.5 47.5 63.2 87.3 63.5 75.9 63.5 67.2 63.3 60.9 64.9\n0.20 63.2 67.4 63.3 56.5 63.0 89.3 63.0 68.3 63.4 55.6 62.8 92.6 63.5 79.9 63.4 68.5 63.1 70.4 67.6\n0.25 63.1 69.1 63.1 59.7 62.7 93.3 62.8 78.5 63.3 60.2 62.4 95.7 63.3 83.7 63.3 69.5 62.8 78.3 69.7\n0.30 63.0 70.7 62.9 62.8 62.4 95.4 62.4 85.9 63.1 66.1 62.1 97.3 63.2 86.6 63.2 70.5 62.6 84.3 71.4\n0.35 62.8 72.4 62.7 65.7 62.0 96.4 62.0 90.9 62.8 69.3 61.7 98.5 63.0 89.1 63.1 71.6 62.1 88.8 72.5\n0.40 62.5 73.6 62.4 68.1 61.5 97.0 61.3 93.9 62.6 72.3 61.2 99.0 62.7", " introduction and survey of esti-\nmation of distribution algorithms,\u201d Swarm and evolutionary computation ,JOURNAL OF IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. XX, NO. XX, FEBRUARY 2023 19\nvol. 1, no. 3, pp. 111\u2013128, 2011.\n[114] W. Dong, T. Chen, P. Ti \u02c7no, and X. Yao, \u201cScaling up estimation of\ndistribution algorithms for continuous optimization,\u201d IEEE Transactions\non Evolutionary Computation , vol. 17, no. 6, pp. 797\u2013822, 2013.\n[115] M. Laumanns and J. Ocenasek, \u201cBayesian optimization algorithms for\nmulti-objective optimization,\u201d Lecture Notes in Computer Science , vol.\n2439, pp. 298\u2013307, 2002.\n[116] X. Zhong and W. Li, \u201cA decision-tree-based multi-objective estimation\nof distribution algorithm,\u201d in 2007 International Conference on Compu-\ntational Intelligence and Security (CIS 2007) . IEEE, 2007, pp. 114\u201311.\n[117] H. Tang, V . A. Shim, K. C. Tan, and J. Y . Chia, \u201cRestricted boltzmann\nmachine based algorithm for multi-objective optimization,\u201d in IEEE\ncongress on evolutionary computation . IEEE, 2010, pp. 1\u20138.\n[118] L. Mart \u00b4\u0131, J. Garc \u00b4\u0131a, A. Berlanga, C. A. C. Coello, and J. M. Molina,\n\u201cMb-gng: Addressing drawbacks in multi-objective optimization estima-\ntion of distribution algorithms,\u201d Operations Research Letters , vol. 39,\nno. 2, pp. 150\u2013154, 2011.\n[119] K. Li and S. Kwong, \u201cA general framework for evolutionary multi-\nobjective optimization via manifold learning,\u201d Neurocomputing , vol. 146,\npp. 65\u201374, 2014.\n[120] C. W. Ahn and R. S. Ramakrishna, \u201cMultiobjective real-coded bayesian\noptimization algorithmrevisited: diversity preservation,\u201d in Proceedings\nof the 9th annual conference on Genetic and evolutionary computation ,\n2007, pp. 593\u2013600.\n[121] M. Pelikan, K. Sastry, and D. E. Goldberg, \u201cMultiobjective hboa,\nclustering, and scalability,\u201d in Proceedings of the 7th annual conference\non Genetic and evolutionary computation , 2005, pp. 663\u2013670.\n[122] Q. Zhang, A. Zhou, and Y . Jin, \u201cRm-meda: A regularity model-based\nmultiobjective estimation of distribution algorithm,\u201d IEEE Transactions\non Evolutionary Computation , vol. 12, no. 1, pp. 41\u201363, 2008.\n[123] Y . Li, X. Xu, P. Li, and L. Jiao, \u201cImproved rm-meda with local\nlearning,\u201d Soft Computing , vol. 18, pp. 1383\u20131397, 2014.\n[124] Y . Wang, J. Xiang, and Z. Cai, \u201cA regularity model-based multiobjec-\ntive estimation of distribution algorithm with reducing redundant cluster\noperator,\u201d Applied Soft Computing , vol. 12, no. 11, pp. 3526\u20133538, 2012.\n[125] Y . Sun, G. G. Yen, and Z. Yi, \u201cImproved regularity model-based eda\nfor many-objective optimization,\u201d IEEE Transactions on Evolutionary\nComputation , vol. 22, no. 5, pp. 662\u2013678, 2018.\n[126] J. Sun, H. Zhang, A. Zhou, Q. Zhang, and K. Zhang, \u201cA new\nlearning-based adaptive multi-objective evolutionary algorithm,\u201d Swarm\nand evolutionary computation , vol. 44, pp. 304\u2013319, 2019.\n[127] P. A. Bosman and D. Thierens, \u201cMulti-objective optimization with di-\nversity preserving mixture-based iterated density estimation evolutionary\nalgorithms,\u201d International Journal of Approximate Reasoning , vol. 31,\nno. 3, pp. 259\u2013289, 2002.\n[128] H. Karshenas, R. Santana, C. Bielza, and P. Larranaga, \u201cMultiobjective\nestimation of distribution algorithm based on joint modeling of objectives\nand variables,\u201d IEEE Transactions on Evolutionary Computation , vol. 18,\nno. 4, pp. 519\u2013542, 2013.\n[129] H. Karshenas, R. Santana, C. Bielza, and P. Larra \u02dcnaga, \u201cMulti-objective\noptimization with joint probabilistic modeling of objectives and vari-\nables,\u201d in Proceedings of the 6th international conference on Evolutionary\nmulti-criterion optimization , 2011, pp. 298\u2013312.\n[130] L. R. Farias and A. F. Ara \u00b4ujo, \u201cIm-moea/d: An inverse modeling\nmulti-objective evolutionary algorithm based on decomposition,\u201d in 2021\nIEEE International Conference on Systems, Man, and Cybernetics (SMC) .\nIEEE, 2021, pp. 462\u2013467.\n[131] R. Cheng, Y", " Introduction\nIn recent years, research has shown that models pre-trained\non large and diverse datasets learn representations that trans-\nfer well to a variety of tasks. As a result, machine learning\npractitioners now commonly develop solutions for down-\nstream tasks by \ufb01ne-tuning large pre-trained models (Gir-\nshick et al., 2014; Yosinski et al., 2014; Kornblith et al.,\n2019; Kolesnikov et al., 2020). Typically, the \ufb01ne-tuning\nprocess involves two steps: (1) \ufb01ne-tune models with a va-\nriety of hyperparameter con\ufb01gurations, and (2) select the\nmodel which achieves the highest accuracy on the held-out\nvalidation set. The remaining models are then discarded.\nSelecting a single model and discarding the rest has several\ndownsides. For one, ensembling outputs of many models\ncan outperform the best single model, albeit at a high com-\nputational cost during inference. For another, \ufb01ne-tuning a\nmodel on downstream tasks can sometimes reduce out-of-\ndistribution performance (Radford et al., 2021; Andreassen\net al., 2021; Wortsman et al., 2021; Pham et al., 2021), and\nthe best single model on the target distribution may not be\nthe best model on out-of-distribution data.\nIn this work, we propose a more accurate and robust alter-\nnative to the second step of the conventional recipe in the\ncontext of \ufb01ne-tuning a large pre-trained model. Instead of\nselecting the individual \ufb01ne-tuned model which achieves the\nhighest accuracy on the held-out validation set, we average\nthe weights of models \ufb01ne-tuned independently, and refer to\nthe result as a model soup . Given the related work.\n2. Method\nThis section highlights three recipes for model souping, the\nuniform ,greedy , and learned soup, though the greedy soup\nis our central method. We summarize the methods of data augmentation as we conduct a random hyperparameter search.Model soups: averaging weights of multiple \ufb01ne-tuned models improves accuracy without increasing inference time\n1 2 3 4 5 6 7\nNumber of ingredients (models)\u22124\u221220246810Percentage point \u2206from CLIP zero-shot\nCross-dataset soup\nZero-shot CLIP\n0 2 4 6 8\nAverage percentage point \u2206when including soup ingredientCIFAR10ImageNetSUN397Food101CarsDTD\nFigure E.1: Model soups can improve zero-shot performance on new downstream tasks. (left) Starting with zero-shot CLIP we create a\nsoup by adding models \ufb01ne-tuned on ImageNet, CIFAR-10, Food101, SUN397, DTD, and Cars, and evaluate on CIFAR-100. Different\norders for adding models are shown with faded lines. (right) The average change in CIFAR-100 accuracy when a model \ufb01ne-tuned on the\ndataset listed in the y-axis is added to the model soup.\nSGD RMSprop Adam AdamWSGD RMSprop Adam AdamW\n0 1.3 1.2 1.40 1.6 1.70 1.70Choice of optimizer\n0 1 2 3 4 50 1 2 3 4 5\n0 1.6 1.8 1.8 1.8 1.70 2.8 2 2.6 20 1.2 1.1 1.20 1.4 0.80 1.30Choice of augmentation strength\n1e-4 3e-5 2e-5 1e-5 3e-6 1e-6 1e-71e-4 3e-5 2e-5 1e-5 3e-6 1e-6 1e-7\n0 -1.1 -2 -2.8 -3.2 -3.2 -3.30 0.96 0.86 1.3 1.5 1.50 0.64 1.3 1.6 1.60 0.63 1.1 1.20 0.22 0.150 -0.560Choice of learning rate\n0.00.20.40.60.81.01.21.41.6\n0.00.51.01.52.02.5\n\u22123\u22122\u2212101\nAcc/parenleftbig1\n2\u03b8a+1\n2\u03b8b/parenrightbig\n\u2212max\u03b8\u2208{\u03b8a,...,\u03b8b}Acc(\u03b8)\nFigure F.1: Analysis of 1D hyperparameter grids, where the average of models at the endpoints often outperforms the best individual\nmodel in the grid. In particular, colors and numbers indicate the percentage point improvement obtained by averaging the models on the x\nandyaxis versus taking the best individual model in the range between them. Experiments\nThis section presents our key experimental \ufb01ndings. We\nbegin with experimental setup (Section 3.1) then provide\nintuition for model soups by examining", " Introduction\nHow should we transfer knowledge and capabilities across trained models? One popular approach\nis transfer learning [ 44], which \ufb01ne-tunes a pre-trained model on a target task through additional\ngradient-based training. The preparatory step of pre-training the model on a data-rich task ideally\ninstills useful \u201cknowledge\u201d into the network\u2019s parameters, which allows the model to learn more\nrapidly and effectively when \ufb01ne-tuned on a downstream task of interest. Transfer learning has\ntherefore become a particularly important and omnipresent tool across many \ufb01elds, including natural\nlanguage processing [ 57,13,9,52,53,46] and computer vision [ 43,24,68]. Recently, it has been\nshown that training on an \u201cintermediate\u201d task between pre-training and \ufb01ne-tuning can further boost\nperformance through additional transfer of capabilities from the intermediate task [ 47,60,51,48].\nAlternatively, continued self-supervised training on unlabeled domain-specialized data can serve as a\nform of domain adaptation [19].\nAll of the aforementioned transfer learning results on GLUE with BERT-base. Columns correspond to target\ntasks while rows correspond to intermediate tasks. Subscripts denote standard deviation across runs.\nItalicized values represent \ufb01ne-tuning directly on the target task (i.e. no intermediate-task training).\nTASK COLA MRPC STS-B RTE\nCOLA 55 :41:8 85:00:9 85:90:8 62:12:3\nSST-2 56:81:4 85:40:9 85:31:0 63:81:0\nMRPC 58:50:484 :50:3 85:30:8 62:75:2\nSTS-B 56:30:4 86:70:786 :10:9 64:52:5\nQQP 56:02:0 87:11:2 87:50:4 71:61:9\nMNLI 58:61:7 85:90:8 87:60:3 77:41:6\nQNLI 56:41:9 87:80:6 87:10:5 71:04:1\nRTE 56:70:9 82:22:5 85:80:563 :71:7\nTable A4: Effect of the number of examples used to compute the Fisher information. Columns\ncorrespond to the number of examples used for RTE. Rows correspond to the number of examples\nused for MNLI. Scores are the RTE validation set accuracy. The original RTE checkpoints had an\naverage accuracy of 63:7and isotropic merging (i.e. 0 Fisher examples) had an average accuracy of\n72:2.\nEXAMPLES 256 1024 2490\n256 72:7 72 :9 73 :1\n1024 72:9 72 :9 73 :3\n4096 72:9 73 :0 73 :2\n32768 72:8 73 :0 73 :5\n392702 72:9 73 :1 73 :4\n16 background and\ndetail our Fisher merging procedure. Section 3 provides experimental conclusions and thoughts on future work in section 5.\n2 Weighted Parameter Averaging for Model Merging\nOur focus is on procedures for model merging , i.e. averaging the parameters of models that share\nan architecture and initialization. In this section, we \ufb01rst frame the common practice of averaging\ntogether model parameters as approximately maximizing the joint likelihood of model posteriors.\nSpeci\ufb01cally, we show that parameter averaging corresponds to using an isotropic Gaussian as the\napproximate posterior for each model. We then introduce Fisher merging , which uses the model\u2019s\ndiagonal Fisher information matrix as the precision matrix of the Gaussian approximate posterior.\nFisher merging can be implemented by setting each merged parameter value to a weighted average of\nthe corresponding parameter values from the original models, with the weighting for each parameter\ndetermined by its Fisher information. In addition, we add model-level weightings as additional\nhyperparameters to set the relative importance of each model.\n2.1 Isotropic merging\nConsider the problem setting where we have Mtrained neural networks with parameters \u00121;:::;\u0012M\nand our goal is to create a single neural network with parameters \u0012that, loosely speaking, inherits\nthe capabilities of the Mtrained neural networks. Assume that all of these neural networks share a\ncommon architecture and had the same set of initial parameter values before being trained. Merging\nattacks this problem by \ufb01nding the parameters \u0012that maximize the", " Introduction\nIn recent years, large language models have demonstrated impressive skills\nacross many diverse tasks (Wang et al., 2019; Brown et al., 2020). Kaplan\net al. (2020) describe the consistent bene\fts of increasing model size, character-\nizing scaling trends that hold across many orders of magnitude. However, even\nthe largest models falter when required to perform multi-step mathematical rea-\nsoning (Hendrycks et al., 2021). Model samples frequently contain catastrophic\nmistakes, even after the model has been appropriately \fnetuned. Mathematical\nreasoning thus reveals a critical weakness in modern language models.\nOne signi\fcant challenge in mathematical reasoning is the high sensitivity\nto individual mistakes (Shen et al., 2021a). When generating a solution, au-\ntoregressive models have no mechanism to correct their own errors. Solutions\nthat veer o\u000b-course quickly become unrecoverable. If we rely purely on genera-\ntive methods that scale even\nbetter.\nAcknowledgements\nWe thank Dan Hendrycks, Leo Gao, Alec Radford, and Giambattista Paras-\ncandolo for their valuable feedback on this paper; Harri Edwards, Yura Burda,\nMichael Wu, and Nick Ryder for many insightful conversations; Michael Petrov,\nAlethea Power, and Jacob Jackson for their technical assistance; the OpenAI\nSupercomputing team for the infrastructure that made these results in this\npaper, had some minor implementation bugs. Our reported test performance\nis therefore a slight underestimate, though the magnitude of this discrepancy is\nless than 1% in most Appendix F for a\nvisualization of veri\fer con\fdence.\n4.3 Veri\fcation Ablations\nWe can either train veri\fers to make a single scalar prediction conditioned on\nthe entire generated solution, or to make a scalar prediction after each token\nin the solution. By default, we choose the latter, training veri\fers to make\npredictions after each token. This can be viewed as a token-level value function.\nWe compare these two Related Work\n3.1 Related Datasets\nEarly math word problem datasets (Kushman et al., 2014; Roy and Roth, 2015)\nare relatively small and are not well suited for testing the limits of modern lan-\nguage models. Dolphin18K (Huang et al., 2016) is a larger dataset containing\n318K problems, but solutions are provided only in the form of equations or \f-\nnal answers. AQuA-RAT (Ling et al., 2017) contains 100K problems, but this\ndataset unfortunately su\u000bers from both a high degree of problem templatiza-\ntion and poor quality control of the natural language solutions. MathQA is\na recently released subset of AQuA-RAT focused on correcting these mistakes\n(Amini et al., 2019), but even the corrected dataset has data quality issues, with\naround 30% of the data having inconsistencies (Miao et al., 2021). Ape210K\n(Zhao et al., 2020) is the largest publicly available dataset, consisting of 210K\nChinese elementary school-level math problems. However, due to the language\nbarrier and the lack of natural language solutions, we're unable to evaluate our background color of the text corresponds to the ver-\ni\fer score for that token, where red is low value (predicted incorrect) and green\n21is high value (predicted correct). The second column of the table summarizes\nthe veri\fer's prediction, and the third column indicates whether the generated\nmodel completion was actually correct or incorrect. Any disagreement between\nthe second and third columns indicates that the veri\fer made an error.\nThe \frst row includes a true positive example, where the veri\fer correctly\nclassi\fes the completion as correct. Note that the model is initially unsure about\nwhether the solution is correct and gradually gains certainty as the solution\nprogresses:", " Introduction\nScalable sequence prediction models (Graves, 2014;\nVaswani et al., 2017; Child et al., 2019) have become a\ngeneral-purpose method for generation and representation\nlearning in many domains, including natural language pro-\ncessing (Mikolov et al., 2013; Sutskever et al., 2014; Dai &\nLe, 2015; Peters et al., 2018; Radford et al., 2018; Devlin\net al., 2018), computer vision (Van Oord et al., 2016; Menick\n& Kalchbrenner, 2018; Chen et al., 2020; Bao et al., 2021),\naudio and speech processing (Oord et al., 2016; 2018; Dhari-\nwal et al., 2020; Baevski et al., 2020), biology (Alley et al.,\n2019; Rives et al., 2021), and even across multiple modali-\nties (Das et al., 2017; Lu et al., 2019; Ramesh et al., 2021;\nZellers et al., 2021). More recently, language models have\nalso fueled progress towards the longstanding challenge\nof program synthesis (Simon, 1963; Manna & Waldinger,\n1971), spurred by the presence of code in large datasets\n(Husain et al., 2019; Gao et al., 2020) and the resulting pro-\ngramming capabilities of language models trained on these\ndatasets (Wang & Komatsuzaki, 2021). Popular language\nmodeling objectives like masked language modeling (Devlin\net al., 2018) and span prediction (Raffel et al., 2020) have\nalso been adapted to train their programming counterparts\nCodeBERT (Feng et al., 2020) and PyMT5 (Clement et al.,\n2020).\nSimilarly, our early investigation of GPT-3 (Brown et al.,\n2020) revealed that it could generate simple programs from\nPython docstrings. While rudimentary, this capability was\nexciting because GPT-3 was not explicitly trained for code\ngeneration. Given the considerable success of large lan-\nguage models in other modalities and the abundance of\npublicly available code, we hypothesized that a specialized\nGPT model, called Codex, could excel at a variety of coding\ntasks. This paper describes several early Codex models,\nwhose descendants power GitHub Copilot and the Codex\nmodels in the OpenAI API.arXiv:2107.03374v2  [cs.LG]  14 Jul 2021Evaluating Large Language Models Trained on Code\nFigure 1. Pass rates of our models on the HumanEval dataset as a\nfunction of model size. When a single sample is generated for each\nproblem, GPT-12B solves no problems, but Codex (\ufb01ne-tuned\non code) solves 28.8% of the problems, and Codex-S (further\n\ufb01ne-tuned on correctly implemented standalone functions) solves\n37.7% of the problems. From here, further gains can be realized by\ngenerating 100 samples per problem and selecting the sample with\nthe highest mean log-probability (44.5% solved) or by selecting\nthe sample that passes the unit tests (77.5% solved). All samples\nare generated with temperature 0.8.\nIn this work, we focus on the task of generating stan-\ndalone Python functions from docstrings, and evaluate the\ncorrectness of code samples automatically through unit\ntests. This is in contrast to natural language generation,\nwhere samples are typically evaluated by heuristics or by\nhuman evaluators. To accurately benchmark our model,\nwe create a dataset of 164 original programming problems\nwith unit tests. These problems assess language compre-\nhension, algorithms, and simple mathematics, with some\ncomparable to simple software interview questions. We\nrelease this data along with an evaluation framework at\nhttps://www.github.com/openai/human-eval.\nTo solve a problem in our test set, we generate multiple\nsamples from the models, and check if any of them pass the\nunit tests. With just a single sample, a 12B parameter Codex\nsolves 28.8% of these problems, and a 300M parameter\nCodex solves 13.2% of these problems. In contrast, the 6B\nparameter GPT-J (Wang & Komatsuzaki, 2021) achieves\n11.4% on the same", " Introduction and Motivating Work\nPre-training methods in natural language processing , pp.\n1527\u20131536, 2017.\nStallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The\nGerman Traf\ufb01c Sign Recognition Benchmark: A multi-\nclass classi\ufb01cation competition. In IEEE International\nJoint Conference on Neural Networks , pp. 1453\u20131460,\n2011.\nStroud, J. C., Ross, D. A., Sun, C., Deng, J., Sukthankar, R.,\nand Schmid, C. Learning video representations from tex-\ntual web supervision. arXiv preprint arXiv:2007.14937 ,\n2020.\nSzegedy, C., Ioffe, S., Vanhoucke, V ., and Alemi,\nA. Inception-v4, inception-resnet and the impact\nof residual connections on learning. arXiv preprint\narXiv:1602.07261 , 2016.\nTan, H. and Bansal, M. Lxmert: Learning cross-modality\nencoder representations from transformers. arXiv preprint\narXiv:1908.07490 , 2019.\nTan, M. and Le, Q. V . Ef\ufb01cientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946 , 2019.\nTaori, R., Dave, A., Shankar, V ., Carlini, N., Recht, B.,\nand Schmidt, L. Measuring robustness to natural dis-\ntribution shifts in image classi\ufb01cation. arXiv preprint\narXiv:2007.00644 , 2020.\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni,\nK., Poland, D., Borth, D., and Li, L.-J. Yfcc100m: The\nnew data in multimedia research. Communications of the\nACM , 59(2):64\u201373, 2016.Learning Transferable Visual Models From Natural Language Supervision 35\nTian, Y ., Krishnan, D., and Isola, P. Contrastive multiview\ncoding. arXiv preprint arXiv:1906.05849 , 2019.\nTian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B., and\nIsola, P. Rethinking few-shot image classi\ufb01cation: a\ngood embedding is all you need? arXiv preprint\narXiv:2003.11539 , 2020.\nTorralba, A., Fergus, R., and Freeman, W. T. 80 million tiny\nimages: A large data set for nonparametric object and\nscene recognition. IEEE transactions on pattern analysis\nand machine intelligence , 30(11):1958\u20131970, 2008.\nTouvron, H., Vedaldi, A., Douze, M., and J \u00b4egou, H. Fix-\ning the train-test resolution discrepancy. In Advances in\nneural information processing systems , pp. 8252\u20138262,\n2019.\nVaradarajan, J. and Odobez, J.-M. Topic models for scene\nanalysis and abnormality detection. In 2009 IEEE 12th\nInternational Conference on Computer Vision Workshops,\nICCV Workshops , pp. 1338\u20131345. IEEE, 2009.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems , pp. 5998\u20136008, 2017.\nVeeling, B. S., Linmans, J., Winkens, J., Cohen, T., and\nWelling, M. Rotation equivariant CNNs for digital pathol-\nogy. June 2018.\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M.,\nReddy, T., Cournapeau, D., Burovski, E., Peterson, P.,\nWeckesser, W., Bright, J., van der Walt, S. J., Brett, M.,\nWilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J.,\nJones, E., Kern, R., Larson, E., Carey, C. J., Polat, \u02d9I.,\nFeng, Y ., Moore, E. W., VanderPlas, J., Laxalde, D.,\nPerktold, J., Cimrman, R., Henriksen, I., Quintero, E. A.,\nHarris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa,\nF., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy\n1.0: Fundamental Algorithms for Scienti\ufb01c Computing\nin Python. Nature results reported\nin Taori et al. (2020)\u2019s evaluation suite. Zero-shot CLIP im-\nproves the state of the art on 5 of the 7 datasets, ImageNet-R,\nObjectNet, ImageNet-Sketch, ImageNet-Vid, and Youtube-\nBB. CLIP\u2019s improvements are largest on ImageNet-Vid and\nYoutube-BB due to its \ufb02exible zero-shot capability and on\nImageNet-R, which likely re\ufb02ects CLIP\u2019s pre-training dis-\ntribution including signi\ufb01cant amounts of creative content.\nA similar behavior has been documented for the Instagram\npre-trained ResNeXt models as discussed in Taori et al.\n(2020).Learning Transferable Visual Models From Natural Language Supervision 48\nF. Model Hyperparameters\nHyperparameter Value\nBatch size", "ABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classi\ufb01cation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellentresults from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14\u000214as opposed to 224\u0002224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speci\ufb01c pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).\n0 5 10 15 20\nNetwork depth (layer)020406080100120Mean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\n0 5 10 15 20\nNetwork depth (layer)020406080100120\nR50x1 + ViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels.\nD.5 E MPIRICAL COMPUTATIONAL COSTS\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\n18Published as a conference paper at ICLR 2021\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor.\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions.\nAnother quantity of interest is the largest batch-size each model can \ufb01t onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-ef\ufb01ciency over ResNet\nmodels.\n64 128 224 384 512\nInput size [px]102103104Peak inference speed [img/sec/core]64 128 224 384 512\nInput size [px]102103Largest per-core batch-sizeR50x1\nR50x2ViT-B/32\nViT-L/32ViT-B/16\nViT-L/16ViT-H/14\nR152x4\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right : Largest per-core batch-size \ufb01tting on device with\nvarious", "Abstract \u2014Neural architecture search (NAS) has attracted a lot of attention and has been illustrated to bring tangible bene\ufb01ts in a large\nnumber of applications in the past few years. Architecture topology and architecture size have been regarded as two of the most\nimportant aspects for the performance of deep learning models and the community has spawned lots of searching algorithms for both\nof those aspects of the neural architectures. However, the performance gain from these searching algorithms is achieved under\ndifferent search spaces and training setups. This makes the overall performance of the algorithms incomparable and the improvement\nfrom a sub-module of the searching model unclear. In this paper, we propose NATS-Bench, a uni\ufb01ed benchmark on searching for both\ntopology and size, for (almost) any up-to-date NAS algorithm. NATS-Bench includes the search space of 15,625 neural cell candidates\nfor architecture topology and 32,768 for architecture size on three datasets. We analyze the validity of our benchmark in terms of\nvarious criteria and performance comparison of all candidates in the search space. We also show the versatility of NATS-Bench by\nbenchmarking 13 recent state-of-the-art NAS algorithms on it. All logs and diagnostic information trained using the same setup for\neach candidate are provided. This facilitates a much larger community of researchers to focus on developing better NAS algorithms in\na more comparable and computationally effective environment. All codes are publicly available at:\nhttps://xuanyidong.com/assets/projects/NATS-Bench.\nIndex Terms \u2014Neural Architecture Search, Benchmark, Deep Learning\nF\n1 I NTRODUCTION\nTHEdeep learning community is undergoing a transition\nfrom hand-designed neural architectures [1], [2], [3] to\nautomatically designed neural architectures [4], [5], [6], [7],\n[8]. In its early stages, the great success of deep learning was\npromoted by the introductions of novel neural architectures,\nsuch as ResNet [1], Inception [3], VGGNet [9], and Trans-\nformer [10]. However, manually designing one architecture\nrequires human experts to frequently try and evaluate nu-\nmerous different operation and connection options [4]. In\ncontrast to architectures that are manually designed, those\nautomatically found by neural architecture search (NAS)\nalgorithms require much less human interaction and ex-\npert effort. These NAS-generated architectures have shown\npromisingresults in\nNAS-Bench-1SHOT1. Therefore, though it is not guaran-\nteed, observations from our NATS-Bench have a potential\nto generalize to other search spaces.methods\nwith parameter sharing, we \ufb01nd that GDAS \u0015DARTS (2nd)\n\u0015DARTS (1st), which is also consistent withResults of weight-sharing basedBackground\nNAS aims to \ufb01nd architecture \u000bamong the search space S\nso that this found \u000bachieves a high performance on the\nFig. 5: Ranking stability of top 20% architectures on different\ndatasets over the topology search space St.\n(a) The Kendall rank correlation coef\ufb01cient for St.\n(b) The Kendall rank correlation coef\ufb01cient for Ss.\nFig. 6: We report the Kendall rank correlation coef\ufb01cient\nbetween the accuracy on 6 sets, i.e., CIFAR-10 validation\nset (C10-V), CIFAR-10 test set (C10-T), CIFAR-100 validation\nset (C100-V), CIFAR-100 test set (C100-T), ImageNet-16-120\nvalidation set (I120-V), ImageNet-16-120 test set (I120-T).\nvalidation set. This problem can be formulated as a bi-level\noptimization problem:\nmin\n\u000b2SL(\u000b;!\u0003\n\u000b;Dval) (1)\ns:t: !\u0003\n\u000b= arg min!L(\u000b;!;Dtrain);\nwhereLindicates the objective function (e.g., cross-entropy\nloss).Dtrain andDvaldenote the training data and the\nvalidation data, respectively. In the typical NAS setting,\nafter an architecture \u000bis found,\u000bwill be re-trained on\nDtrain (orDtrain +Dval) and evaluated on the test data\nDtestto \ufb01gure out its real performance.\n5.2 Experimental Setup\nWe evaluate 13recent, state-of-the-art searchingMethods\nThe weight-sharing basedexperiments. Part of this project was supported\nby Google Cloud Credits from", " Introduction\nThe success of deep learning in computer vision is in no\nsmall part due to the insight and engineering efforts of hu-\nman experts, allowing for the creation of powerful archi-\ntectures for widespread adoption (Krizhevsky et al., 2012;\nSimonyan & Zisserman, 2015; He et al., 2016; Szegedy\net al., 2016; Huang et al., 2017). However, this manual\ndesign is costly, and becomes increasingly more dif\ufb01cult\nas networks get larger and more complicated. Because of\nthese challenges, the neural network community has seen a\n1Usher Institute, University of Edinburgh2School of In-\nformatics, University of Edinburgh3School of Engineering,\nUniversity of Edinburgh. Correspondence to: Joseph Mellor\n<joe.mellor@ed.ac.uk >.\nProceedings of the 38thInternational Conference on Machine\nLearning , PMLR 139, 2021. Copyright 2021 by the author(s).shift from designing architectures to designing algorithms\nthatsearch for candidate architectures (Elsken et al., 2019;\nWistuba et al., 2019). These Neural Architecture Search\n(NAS) algorithms are capable of automating the discovery\nof effective architectures (Zoph & Le, 2017; Zoph et al.,\n2018; Pham et al., 2018; Tan et al., 2019; Liu et al., 2019;\nReal et al., 2019).\nNAS algorithms are broadly based on the seminal work\nof Zoph & Le (2017). A controller network generates an\narchitecture proposal, which is then trained to provide a\nsignal to the controller through REINFORCE (Williams,\n1992), which then produces a new proposal, and so on.\nTraining a network for every controller update is extremely\nexpensive; utilising 800 GPUs for 28 days in Zoph & Le\n(2017). Subsequent work has sought to ameliorate this by\n(i) learning stackable cells instead of whole networks (Zoph\net al., 2018) and (ii) incorporating weight sharing ; allow-\ning candidate networks to share weights to allow for joint\ntraining (Pham et al., 2018). These contributions have ac-\ncelerated the speed of NAS algorithms e.g. to half a day on\na single GPU in Pham et al. (2018).\nFor some practitioners, NAS is still too slow; being able to\nperform NAS quickly (i.e. in seconds) would be immensely\nuseful in the hardware-aware setting where a separate search\nis typically required for each device and task (Wu et al.,\n2019; Tan et al., 2019). This could be achieved if NAS\ncould be performed without any network training . In this\npaper we show that this is possible. We explore NAS-Bench-\n101 (Ying et al., 2019), NAS-Bench-201 (Dong & Yang,\n2020), NATS-Bench (Dong et al., 2021), and Network De-\nsign Spaces (NDS, Radosavovic et al., 2019), and examine\nthe overlap of activations between datapoints in a mini-batch\nfor an untrained network (Section 3). The linear maps of\nthe network are uniquely identi\ufb01ed by a binary code cor-\nresponding to the activation pattern of the recti\ufb01ed linear\nunits. The Hamming distance between these binary codes\ncan be used to de\ufb01ne a kernel matrix (which we denote by\nKH) which is distinctive for networks that perform well;\nthis is immediately apparent from visualisation alone across\ntwo distinct search spaces (Figure 1). We devise a score\nbased on KHand perform an ablation study to demonstrate\nits robustness to inputs and network initialisation.\nWe incorporate our score into a simple search algorithmarXiv:2006.04647v3  [cs.LG]  11 Jun 2021Neural Architecture Search without Training\n(a) NAS-Bench-201\n (b) NDS-DARTS\nFigure 1. KHfor a mini-batch of 128 CIFAR-10 images for untrained architectures in (a) NAS-Bench-201 (Dong & Yang, 2020) and (b)\nNDS-DARTS (Radosavovic et al., 2019). KHin these plots is normalised so that the diagonal entries", " Introduction\nDeveloping neural network image classi\ufb01cation models\noften requires signi\ufb01cant architecture engineering . Starting\nfrom the seminal work of [32] on using convolutional archi-\ntectures [17, 34] for ImageNet [11] classi\ufb01cation, succes-\nsive advancements through architecture engineering have\nachieved impressive results\nFinally, we will present examples of object detection re-\nsults on the COCO dataset in Figure 10 and Figure 11.\nAs can be seen from the \ufb01gures, NASNet-A featurization\nworks well with Faster-RCNN and gives accurate localiza-\ntion of objects.\nFigure 9. Architecture of NASNet-C convolutional cell with B=\n4blocks identi\ufb01ed with CIFAR-10. The input (white) is the hid-\nden state from previous activations (or input image). The output\n(pink) is the result of a concatenation operation across all result-\ning branches. Each convolutional cell is the result of Bblocks. A\nsingle block corresponds to two primitive operations (yellow) and\na combination operation (green).Figure 10. Example detections showing improvements of object\ndetection over previous state-of-the-art model for Faster-RCNN\nwith Inception-ResNet-v2 featurization [28] (top) and NASNet-A\nfeaturization (bottom).\nFigure 11. Example detections of best performing NASNet-A fea-\nturization with Faster-RCNN trained on COCO dataset. Top and\nmiddle images courtesy of http://wikipedia.org . Bottom\nimage courtesy of Jonathan Huang methods. Naively applying dropout [56]\nacross convolutional \ufb01lters degraded performance. How-\never, we discovered a new technique called ScheduledDrop-\nPath, a modi\ufb01ed version of DropPath [33], that works well\nin regularizing NASNets. In DropPath, we stochastically\ndrop out each path (i.e., edge with a yellow box in Figure\n4) in the cell with some \ufb01xed probability. This is simi-\nlar to [27] and [69] where they dropout full parts of their\nmodel during training and then at test time scale the path\nby the probability of keeping that path during training. In-\nterestingly we also found that DropPath alone does not help\nNASNet training much, but DropPath with linearly increas-\ning the probability of dropping out a path over the course\nof training signi\ufb01cantly improves the \ufb01nal performance for\nboth CIFAR and ImageNet Related Work\nThe proposed method is related to previous work in hy-\nperparameter optimization [44, 4, 5, 54, 55, 6, 40] \u2013 es-\npecially recent approaches in designing architectures such\nas Neural Fabrics [48], DiffRNN [41], MetaQNN [3] and\nDeepArchitect [43]. A more \ufb02exible class of Appendix\nA. Experimental Details\nA.1. Dataset for Architecture Search\nThe CIFAR-10 dataset [31] consists of 60,000 32x32\nRGB images across 10 classes (50,000 train and 10,000\ntest images). We partition a random subset of 5,000 images\nfrom the training set to use as a validation set for the con-\ntroller RNN. All images are whitened and then undergone\nseveral data augmentation steps: we randomly crop 32x32\npatches from upsampled images of size 40x40 and apply\nrandom horizontal \ufb02ips. This data augmentation procedure\nis common among Experiments\nWe now present two additional cells that performed well\non CIFAR and ImageNet. The search spaces used for these\ncells are slightly different than what was used for NASNet-\nA. For the NASNet-B model in Figure 8 we do not concate-\nnate all of the unused hidden states generated in the convo-\nlutional cell. Instead all of the hiddenstates created within\nthe convolutional cell, even if they are currently used, are\nfed into the next layer. Note that B= 4and there are 4 hid-\ndenstates as input to the cell as these numbers must match\nfor this cell to be valid. We also allow addition followed by\nlayer normalization [2] or instance normalization [61]", " Introduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient Background: Policy Optimization\n2.1 Policy Gradient Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At/bracketrightbigg\n(3)\nsubject to \u02c6Et[KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]]\u2264\u03b4. (4)\nHere,\u03b8oldis the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]/bracketrightbigg\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, results and learning curves for all 49 games is provided in Experiments\n6.1 Comparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIPto several natural variations and ablated versions.\nNo clipping or penalty: Lt(\u03b8) =rt(\u03b8)\u02c6At\nClipping: Lt(\u03b8) = min(rt(\u03b8)\u02c6At,clip(rt(\u03b8)),1\u2212/epsilon1,1 +/epsilon1)\u02c6At\nKL penalty (\ufb01xed or adaptive) Lt(\u03b8) =rt(\u03b8)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old,\u03c0\u03b8]\n5For the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping ( /epsilon1)\nand the KL penalty ( \u03b2,dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe Results from continuous control benchmark. Average normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2was initialized at 1.\n6.2 Comparison to Other Algorithms in the", " INTRODUCTION  \nThe currently available instruments (e.g., multi/hy perspectral \n[1], synthetic aperture radar [2], etc.) for earth observation [3, 4] \ngenerate more and more different types of airborne or satellite \nimages with different resolutions (spatial resoluti on, spectral \nresolution, and temporal resolution). This raises a n important \ndemand for intelligent earth observation through re mote sensing  \nimages, which allows the smart identification and c lassification \nof land use and land cover (LULC) scenes from airbo rne or \nspace platforms [3]. Remote sensing image scene cla ssification, \nbeing an active research topic in the field of aeri al and satellite \nimage analysis, is to categorize scene images into a discrete set \nof meaningful LULC classes according to the image c ontents. \nDuring the past decades, remarkable efforts have be en made in \ndeveloping various conclusions are drawn in Section VI. \nII. A  REVIEW ON REMOTE SENSING IMAGE SCENE \nCLASSIFICATION DATASETS  \nIn the past years, several publicly available high resolution \nremote sensing image datasets [9, 11, 17, 33, 38, 8 2] have been \nintroduced by different groups to perform research for scene \nclassification and to evaluate different METHODS  \nCurrent Methods for Object-based Analysis and Classificatio n : Springer \nNetherlands, 2004. \n[76] L. Dr\u0103gu\u0163 and T. Blaschke, \u201cAutomated classifi cation of landform \nelements using object-based image analysis,\u201d Geomorphology,  vol. 81, \nno. 3, pp. 330-344, 2006. \n[77] C. Eisank, L. Dr\u0103gu\u0163, and T. Blaschke, \"A gene ric procedure for \nsemantics-oriented landform classification using ob ject-based image \nanalysis,\" in Geomorphometry , 2011, pp. 125-128. \n[78] G. J. Hay, T. Blaschke, D. J. Marceau, and A. Bouchard, \u201cA comparison \nof three image-object abstract level, we can learn very powerful rep resentations. \nThis has been proven in literatures [13, 134, 169-1 71].  \n2) CNNs:  CNNs are designed to process data that come in the  \nform of multiple arrays, for example a multi-spectr al image \ncomposed of multiple 2D arrays containing pixel int ensities in \nthe multiple band channels. Starting with the impre ssive success \nof AlexNet [163], many representative CNN models in cluding \nOverfeat [164], VGGNet [165], GoogLeNet [166], SPPN et \n[167], and ResNet [172] have been proposed in the l iterature. \nThere exist four key ideas behind CNNs that take ad vantage of \nthe properties of natural signals, namely, local co nnections, \nshared weights, pooling, and the use of many layers  [159].  \nThe architecture of a typical CNN is structured as a series of \nlayers. (i) Convolutional layers : They are the most important \nones for extracting features from images. The first  layers usually \ncapture low-level features (like edges, lines and c orners) while \nthe deeper layers are able to learn more expressive  features (like \nstructures, objects and shapes) by combining low-le vel ones. (ii) \nPooling layers : Typically, after each convolutional layer, there \nexist pooling layers that are created by computing some local \nnon-linear operation of a particular feature over a  region of the \nimage. This process ensures that the same result ca n be obtained, \neven when image features have small translations", " Introduction\nIncreasingly, phones and tablets are the primary computing\ndevices for many people [ 30,2]. The powerful sensors on\nthese devices (including cameras, microphones, and GPS),\ncombined with the fact they are frequently carried, means\nthey have access to an unprecedented amount of data, much\nof it private in nature. Models learned on such data hold the\nAppearing in Proceedings of the 20thInternational Conference on\nArti\ufb01cial Intelligence and Statistics (AISTATS) 2017, Fort Laud-\nerdale, Flordia, USA. JMLR: W&CP volume 54. Copyright 2017\nby the authors.promise of greatly improving usability by powering more\nintelligent applications, but the sensitive nature of the data\nmeans there are risks and responsibilities to storing it in a\ncentralized location.\nWe investigate a learning technique that allows users to\ncollectively reap the bene\ufb01ts of shared models trained from\nthis rich data, without the need to centrally store it. We term\nour approach Federated Learning , since the learning task is\nsolved by a loose federation of participating devices (which\nwe refer to as clients ) which are coordinated by a central\nserver . Each client has a local training dataset which is\nnever uploaded to the server. Instead, each client computes\nan update to the current global model maintained by the\nserver, and only this update is communicated. This is a\ndirect application of the principle of focused collection or\ndata minimization proposed by the 2012 White House report\non privacy of consumer data [ 39]. Since these updates are\nspeci\ufb01c to improving the current model, there is no reason\nto store them once they have been applied.\nA principal advantage of this approach is the decoupling of\nmodel training from the need for direct access to the raw\ntraining data. Clearly, some trust of the server coordinat-\ning the training is still required. However, for applications\nwhere the training objective can be speci\ufb01ed on the basis\nof data available on each client, federated learning can sig-\nni\ufb01cantly reduce privacy and security risks by limiting the\nattack surface to only the device, rather than the device and\nthe cloud.\nOur primary contributions are 1) the identi\ufb01cation of the\nproblem of training on decentralized data from mobile de-\nvices as an important research direction; 2) the selection of\na straightforward and practical algorithm that can be applied\nto this setting; and 3) an extensive empirical evaluation of\nthe proposed approach. More concretely, we introduce the\nFederatedAveraging algorithm, which combines lo-\ncal stochastic gradient descent (SGD) on each client with\na server that performs model averaging. We perform ex-\ntensive results on a variety of model architectures: a multi-layer\nperceptron, two different convolutional NNs, a two-layer\ncharacter LSTM, and a large-scale word-level LSTM.\nWhile federated learning offers many practical privacy ben-\ne\ufb01ts, providing stronger guarantees via differential pri-\nvacy [ 14,13,1], secure multi-party computation [ 18], or\ntheir combination is an interesting direction for future work.\nNote that both classes of techniques apply most naturally to\nsynchronous algorithms like FedAvg .8\n8Subsequent to this work, Bonawitz et al. [6]introduced an\nef\ufb01cient secure aggregation protocol for federated learning, and\nKone \u02c7cn\u00b4y et al. [23] presented algorithms for further decreasing\ncommunication costs.H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag \u00a8uera y Arcas Related Work Distributed training by iteratively averag-\ning locally trained models has been studied by McDon-\nald et al. [28] for the perceptron and Povey et al. [31] forspeech recognition DNNs. Zhang et al. [42] studies an", " Introduction\nRecently visual attributes have raised signi\ufb01cant inter-\nest in the community [6, 11, 17, 25]. A \u201cvisual attribute\u201d\nis a property of an object that can be measured visually and\nhas a semantic connotation, such as the shape of a hat or the\ncolor of a ball. Attributes allow characterizing objects in far\ngreater detail than a category label and are therefore the key\nto several advanced applications, including understanding\ncomplex queries in semantic search , learning about objects\nfrom textual description , and accounting for the content of\nTuesday, October 29, 13Figure 1: Both the man-made and the natural world are\nan abundant source of richly textured objects. The textures\nof objects shown above can be described (in no particular\norder) as dotted, striped, chequered, cracked, swirly, hon-\neycombed, and scaly. We aim at identifying these attributes\nautomatically and generating descriptions based on them.\nimages in great detail. Textural properties have an important\nrole in object descriptions, particularly for those objects that\nare best quali\ufb01ed by a pattern, such as a shirt or the wing of\nbird or a butter\ufb02y as illustrated in Fig. 1. Nevertheless, so\nfar the attributes of textures have been investigated only tan-\ngentially. In this paper we address the question of whether\nthere exists a \u201cuniversal\u201d set of attributes that can describe a\nwide range of texture patterns, whether these can be reliably\nestimated from images, and for what tasks they are useful.\nThe study of perceptual attributes of textures has a\nlong history starting from pre-attentive aspects and group-\ning [16], to coarse high-level attributes [1, 2, 33], to some\nrecent work aimed at discovering such attributes by au-\ntomatically mining descriptions of images from the Inter-\nnet [3, 12]. However, the texture attributes investigated so\nfar are rather few or too generic for a detailed description\nmost \u201creal world\u201d patterns. Our work is motivated by the\none of Bhusan et al. [5] who studied the relationship be-\ntween commonly used English words and the perceptual\nproperties of textures, identifying a set of words suf\ufb01cient\nto describing a wide variety of texture patterns. While they\nstudy the psychological aspects of texture perception, thearXiv:1311.3618v2  [cs.CV]  15 Nov 2013banded\n blotchy\n braided\n bubbly\n bumpy\n chequered\n cobwebbed\n cracked\n crosshatched\n crystalline\n dotted\n \ufb01brous\n\ufb02ecked\n freckled\n frilly\n gauzy\n grid\n grooved\n honeycombed\n interlaced\n knitted\n lacelike\n lined\n marbled\nmatted\n meshed\n paisley\n perforated\n pitted\n pleated\n polka-dotted\n porous\n potholed\n scaly\n smeared\n spiralled\nsprinkled\n stained\n strati\ufb01ed\n striped\n studded\n swirly\n veined\n waf\ufb02ed\n woven\n wrinkled\n zigzagged\nFigure 2: The 47 texture words in the describable texture dataset introduced in this paper. Two examples of each attribute\nare shown to illustrate the signi\ufb01cant amount of variability in the data.\nfocus of this paper is the challenge of estimating such prop-\nerties from images automatically.\nOur \ufb01rst contribution is to select a subset of 47 de-\nscribable texture attributes , based on the work of Bhusan\net al., that capture a wide variety of visual properties of\ntextures and to introduce a corresponding describable tex-\nture dataset consisting of 5,640 texture images jointly an-\nnotated with the 47 attributes (Sect. 2). In an effort to\nsupport directly real world applications, and inspired by\ndatasets such as ImageNet [10] and the Flickr Material\nDataset (FMD) [30], our images are captured \u201cin the wild\u201d\nby downloading them from the Internet rather than collect-\ning them in a laboratory. We also address the practical"], "bleu": 0.0, "rouge_l": 0.27911453320500484, "gpt_metric_score": 0.0, "bert_score": 0.1995605230331421}
{"paper_key": "Autonomous Network Defence using Reinforcement Learning", "current_5q": "**[Question 1] - What is the problem?**  \nHow can we develop an effective autonomous network defense system using hierarchical reinforcement learning to respond to various adversarial strategies in real-time?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses the growing need for automated defenses in cybersecurity, where human operators are often overwhelmed by the complexity and speed of attacks. By advancing autonomous defense mechanisms, this research could lead to significant improvements in response times and operational efficiency, ultimately reducing the risk of prolonged undetected intrusions. The findings could pave the way for future research in applying reinforcement learning to other complex security scenarios, enhancing our understanding of adaptive defense strategies and their practical applications in safeguarding critical infrastructure.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the dynamic and unpredictable nature of cyber threats, which require a defense system to adapt in real-time to various adversarial tactics. Naive approaches may fail due to their inability to generalize across different attack strategies, leading to overfitting on specific adversaries. Additionally, the technical complexities of creating a hierarchical agent architecture that effectively coordinates multiple specialized sub-agents pose significant obstacles. The need for high-fidelity simulations that accurately represent real-world network environments further complicates the development and testing of such systems.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on isolated aspects of network security or employed simpler models that lack the sophistication needed for real-time autonomous defense. Limitations in computational resources, the complexity of creating realistic simulation environments, and a lack of comprehensive frameworks for integrating multiple learning agents have hindered progress. Our approach differs by introducing a hierarchical architecture that combines specialized sub-agents, allowing for greater adaptability and generalization across various adversarial strategies, which has not been adequately addressed in prior work.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves developing a hierarchical reinforcement learning agent that utilizes a controller agent to select and coordinate sub-agents trained against specific adversarial strategies. We will employ the CybORG environment to simulate a realistic computer network, using metrics such as response time and effectiveness against different adversaries to evaluate performance. The expected outcomes include demonstrating superior defensive capabilities compared to single-agent approaches, showcasing the benefits of our hierarchical architecture in generalizing across various attack scenarios, and providing publicly available models and training setups for further research in the field", "proposal_5q": "[Question 1] - What is the problem?\n\nHow can we develop a multi-agent deep reinforcement learning framework that simulates and defends against evolving cyber-attack strategies while ensuring data privacy and collaborative training across multiple institutions?\n\n[Question 2] - Why is it interesting and important?\n\nCybersecurity is a critical concern in the digital age, with cyber-attacks becoming increasingly sophisticated and frequent. Solving this problem has significant implications for the research community and practical applications:\n1. **Broader Implications**: Successfully developing this framework would provide a robust tool for cybersecurity research, enabling the simulation of complex attack scenarios and the development of adaptive defense strategies.\n2. **Future Research**: This research could pave the way for more advanced studies in multi-agent systems, adversarial learning, and privacy-preserving techniques. It would set a precedent for integrating these technologies, encouraging further exploration and innovation.\n3. **Advancement of Knowledge**: Addressing this question will enhance our understanding of how multi-agent deep reinforcement learning can be applied to cybersecurity. It will also contribute to the fields of federated learning and privacy-preserving data analysis.\n4. **Practical Applications**: The framework could be adopted by institutions to improve their cybersecurity defenses in real-time, making them more resilient against evolving threats. It also ensures that collaborative efforts do not compromise data privacy, a crucial concern in today's data-driven world.\n\n[Question 3] - Why is it hard?\n\nSeveral challenges and complexities make this problem difficult to solve:\n1. **Dynamic Nature of Cyber-Attacks**: Cyber-attacks are constantly evolving, making it challenging to create a framework that can adapt in real-time. Naive approaches may fail to capture the complexity and unpredictability of these attacks.\n2. **Multi-Agent Coordination**: Coordinating multiple defense agents and ensuring they work synergistically requires sophisticated algorithms and real-time communication, which are technically challenging to implement.\n3. **Adversarial Generative Models**: Developing realistic and adaptive attack scenarios using adversarial generative models involves complex machine learning techniques that require significant computational resources and expertise.\n4. **Federated Learning**: Ensuring effective collaborative training across multiple institutions while protecting sensitive data is a non-trivial task. It involves overcoming issues related to data heterogeneity, communication overhead, and maintaining model accuracy.\n5. **Privacy-Preserving Techniques**: Integrating privacy-preserving techniques adds another layer of complexity, as it requires balancing data utility and privacy. Ensuring that the training process does not expose sensitive data while maintaining the effectiveness of the defense strategies is a critical challenge.\n\n[Question 4] - Why hasn't it been solved before?\n\nSeveral factors have contributed to this problem remaining unsolved:\n1. **Gaps in Previous Research**: Previous research has often focused on individual aspects such as deep reinforcement learning, adversarial models, or federated learning in isolation. There has been limited work on integrating these components into a cohesive framework.\n2. **Technical Barriers**: The computational and technical requirements for developing multi-agent systems, adversarial generative models, and federated learning are significant. Many institutions may lack the resources or expertise to tackle these challenges comprehensively.\n3. **Data Privacy Concerns**: Ensuring data privacy during collaborative training is a relatively new area of research. Existing solutions may not fully address the complexities involved in maintaining data integrity and confidentiality across multiple institutions.\n4. **Novelty of Approach**: The proposed integration of multi-agent deep reinforcement learning, adversarial generative models, federated learning, and privacy-preserving techniques is a novel approach. This level of integration has not been extensively explored or implemented in the context of cybersecurity.\n\n[Question 5] - What are the key components of my approach and results?\n\n1. **Proposed Methodology**:\n   - **Multi-Agent Deep Reinforcement Learning**: Develop a framework where multiple defense agents learn and adapt in real-time to evolving cyber-attack strategies.\n   - **Adversarial Generative Models**: Incorporate adversarial generative models to create realistic and adaptive attack scenarios, providing a robust training environment for the defense agents.\n   - **Federated Learning**: Implement federated learning to facilitate collaborative training across multiple institutions, ensuring that sensitive data remains protected.\n   - **Privacy-Preserving Techniques**: Integrate privacy-preserving techniques such as differential privacy and secure multi-party computation to safeguard sensitive training data from exposure.\n\n2. **Dataset and Metrics**:\n   - **Dataset**: Use the CybORG platform to simulate attack and defense scenarios, generating a comprehensive dataset for training and evaluation.\n   - **Metrics**: Evaluate the framework using metrics such as the success rate of defense strategies, adaptability to new attack patterns, computational efficiency, and data privacy preservation.\n\n3. **Expected Outcomes**:\n   - **Adaptive Defense Strategies**: The framework will enable the development of adaptive defense strategies that can respond effectively to evolving cyber-attacks.\n   - **Collaborative Training**: Institutions will be able to collaboratively train their defense agents without compromising data privacy, leading to enhanced", "referenced_intros": ["Abstract\nAutonomous Cyber Operations (ACO) involves the\ndevelopment of blue team (defender) and red team\n(attacker) decision-making agents in adversarial\nscenarios. To support the application of machine\nlearning algorithms to solve this problem, and to\nencourage researchers in this \ufb01eld to attend to prob-\nlems in the ACO setting, we introduce CybORG, a\nwork-in-progress gym for ACO research. CybORG\nfeatures a simulation and emulation environment\nwith a common interface to facilitate the rapid\ntraining of autonomous agents that can then be\ntested on real-world systems. Initial testing demon-\nstrates the feasibility of this approach.\n1Background\nAutonomous Cyber Operations (ACO) is concerned with\nthe defence of computer systems and networks through au-\ntonomous decision-making and action. It is particularly\nneeded where deploying security experts to cover every net-\nwork and location is becoming increasingly untenable, and\nwhere systems cannot be reliably accessed by human defend-\ners, either due to unreliable communication channels or ad-\nversary action.\nThe ACO domain is challenging to develop arti\ufb01cial intelli-\ngence (AI) approaches for as it combines hard problems from\nother domains of AI research. Like game AI, it is adversar-\nial: the effectiveness of a defensive cyber agent is determined\nby its ability to respond to an adversary. Like autonomous\nrobotics, ACO is affected by the \u2018reality gap\u2019 [Ibarz et al. ,\n2021 ], as simulations of an environment willabstract away\ninformation that could be critical to an agent\u2019s effectiveness.\nA further issue for the ACO domain is that the environment\nand action set change as cyber security research progresses,\nwhich is far more rapidly than either of the domains discussed\nabove.\nThe requirement to handle the varying actions of an adver-\nsary, in a complex environment, precludes the use of static\ndata sets to learn ACO behaviour. A tool for learning in ad-\nversarial environments is an AI Gym. AI Gyms such as the\none developed by OpenAI implement reinforcement learning(RL) through direct interaction with a simulation of the prob-\nlem. A path to addressing the \u2018reality gap\u2019, used in [Tanet\nal., 2016 ], is to combine learning on simulations with testing\nin a real environment. In this case, the bulk of learning is\nconducted on simulated systems. Successful agents are trans-\nferred to the real system to \ufb01rstly validate their effectiveness,\nand secondly to re\ufb01ne the simulation.\nWe believe that AI Gyms, that can be validated and re-\n\ufb01ned throughexperiments, the requirements of ACO motivate an in-\ntegrated design comprising emulation and simulation modes\nto support large scale RL across diverse scenarios.\nWe have made progress towards implementing this design,\nwith the ability to spawn and play games either in simula-\ntion mode or emulation mode with cloud infrastructure. In\nCybORG, we can now train an RL agent in simulation then\ntest its effectiveness in emulation. TheRelated Work\nThere are a growing number of cyber security environments\ndesigned for experimentation. A summary of several environ-\nments, with an assessment of how they \ufb01t our requirements,\ncan be found in Table 1.\nDETERlab [Mirkovic et al. , 2010 ]is a specialised cy-\nber security experimentation environment based on EMU-\nlab[Stoller et al. , 2008 ]. It supports cyber security experi-\nmentation through the emulation of hosts and networks. As\nit relies on local hardware, DETERlab has limited maximum\nnetwork size and takes a signi\ufb01cant amount of time to reset or\nrecon\ufb01gure. VINE [Eskridge et al. , 2015 ], SmallWorld [Fur-\nfaro et al. , 2018 ]and BRAWL [Corporation, 2018 ]lever-\nage cloud-based Infrastructure", " Introduction\nThe long-term goal of arti\ufb01cial intelligence is to solve advanced real-world challenges. Games have\nserved as stepping stones along this path for decades, from Backgammon (1992) to Chess (1997) to\nAtari (2013)[1\u20133]. In 2016, AlphaGo defeated the world champion at Go using deep reinforcement\nlearning and Monte Carlo tree search[4]. In recent years, reinforcement learning (RL) models have\ntackled tasks as varied as robotic manipulation[5], text summarization [6], and video games such as\nStarcraft[7] and Minecraft[8].\nRelative to previous AI milestones like Chess or Go, complex video games start to capture the\ncomplexity and continuous nature of the real world. Dota 2 is a multiplayer real-time strategy game\nproduced by Valve Corporation in 2013, which averaged between 500,000 and 1,000,000 concurrent\nplayers between 2013 and 2019. The game is actively played by full time professionals; the prize\npool for the 2019 international championship exceeded $35 million (the largest of any esports game\nin the world)[9, 10]. The game presents challenges for reinforcement learning due to long time\nhorizons, partial observability, and high dimensionality of observation and action spaces. Dota 2\u2019s\n\u0003Authors listed alphabetically. Please cite as OpenAI et al., and use the following bibtex for citation: https:\n//openai.com/bibtex/openai2019dota.bib\n1arXiv:1912.06680v1  [cs.LG]  13 Dec 2019rules are also complex \u2014 the game has been actively developed for over a decade, with game logic\nimplemented in hundreds of thousands of lines of code.\nThe key ingredient in solving this complex environment was to scale existing reinforcement\nlearning systems to unprecedented levels, utilizing thousands of GPUs over multiple months. We\nbuilt a distributed training system to do this which we used to train a Dota 2-playing agent called\nOpenAI Five. In April 2019, OpenAI Five defeated the Dota 2 world champions (Team OG1), the\n\ufb01rst time an AI system has beaten an esport world champion2. We also opened OpenAI Five to\nthe Dota 2 community for competitive play; OpenAI Five won 99.4% of over 7000 games.\nOne challenge we faced in training was that the environment and code continually changed as\nour project progressed. In order to train without restarting from the beginning after each change,\nwe developed a collection of tools to resume training with minimal loss in performance which we\ncallsurgery. Over the 10-month training process, we performed approximately one surgery per\ntwo weeks. These tools allowed us to make frequent improvements to our strongest agent within a\nshorter time than the typical practice of training from scratch would allow. As AI systems tackle\nlarger and harder problems, further investigation of settings with ever-changing environments and\niterative development will be critical.\nIn section 2, we describe Dota 2 in more detail along with the challenges it presents. In section 3\nwe discuss the technical components of the training system, leaving most of the details to appendices\ncited therein. In section 4, we summarize our long-running experiment and the path that lead to\ndefeating the world champions. We also describe lessons we\u2019ve learned about reinforcement learning\nwhich may generalize to other complex tasks.\n2 Dota 2\nDota 2 is played on a square map with two teams defending bases in opposite corners. Each\nteam\u2019s base contains a structure called an ancient; the game ends when one of these ancients is\ndestroyed by the opposing team. Teams have \ufb01ve players, each controlling a", " Introduction to Reinforcement Learning,\u201d\nMIT Press Cambridge, MA, USA, 1998.\n[116] D. K. Yau, J. C. Lui, F. Liang, and Y . Yam, \u201cDefending against\ndistributed denial-of-service attacks with max-min fair server-centric\nrouter throttles,\u201d IEEE/ACM Transactions on Networking , vol. 13, no.\n1, pp. 29-42, 2005.\n[117] R. Bhosale, S. Mahajan, and P. Kulkarni, \u201cCooperative machine learn-\ning for intrusion detection system,\u201d International Journal of Scienti\ufb01c\nand Engineering Research , vol. 5, no. 1, pp. 1780-1785, 2014.\n[118] A. Herrero, and E. Corchado, \u201cMultiagent systems for network intru-\nsion detection: A review,\u201d in Computational Intelligence in Security for\nInformation Systems , 2009, pp. 143-154.\n[119] A. Detwarasiti, and R. D. Shachter, \u201cIn\ufb02uence diagrams for team\ndecision analysis,\u201d Decision Analysis , vol. 2, no. 4, pp. 207-228, 2005.\n[120] S. Shamshirband, A. Patel, N. B. Anuar, M. L. M. Kiah, and A. Abra-\nham, \u201cCooperative game theoretic approach using fuzzy Q-learning\nfor detecting and preventing intrusions in wireless sensor networks,\u201d\nEngineering Applications of Arti\ufb01cial Intelligence , vol. 32, pp. 228-\n241, 2014.\n[121] P. Mu \u02dcnoz, R. Barco, and I. de la Bandera, \u201cOptimization of load bal-\nancing using fuzzy Q-learning for next generation wireless networks,\u201d\nExpert Systems with Applications , vol. 40, no. 4, pp. 984-994, 2013.\n[122] S. Shamshirband, N. B. Anuar, M. L. M. Kiah, and A. Patel, \u201cAn\nappraisal and design of a multiagent system based cooperative wireless\nintrusion detection computational intelligence technique,\u201d Engineering\nApplications of Arti\ufb01cial Intelligence , vol. 26, no. 9, pp. 2105-2127,\n2013.\n[123] S. Varshney, and R. Kuma, \u201cVariants of LEACH routing protocol in\nWSN: A comparative analysis,\u201d in The 8th International Conference on\nCloud Computing, Data Science and Engineering (Con\ufb02uence) , 2018,\npp. 199-204.\n[124] G. Caminero, M. Lopez-Martin, and B. Carro, \u201cAdversarial envi-\nronment reinforcement learning algorithm for intrusion detection,\u201d\nComputer Networks , vol. 159, pp. 96-109, 2019.\n[125] M. Lopez-Martin, B. Carro, and A. Sanchez-Esguevillas, \u201cApplication\nof deep reinforcement learning to intrusion detection for supervised\nproblems,\u201d Expert Systems with Applications , vol. 141, 112963, 2020.\n[126] I. A. Saeed, A. Selamat, M. F. Rohani, O. Krejcar, and J. A. Chaudhry,\n\u201cA systematic state-of-the-art analysis of multiagent intrusion detec-\ntion,\u201d IEEE Access , vol. 8, pp. 180184-180209, 2020.\n[127] S. Roy, C. Ellis, S. Shiva, D. Dasgupta, V . Shandilya, and Q. Wu, \u201cA\nsurvey of game theory as applied to network security,\u201d in 43rd Hawaii\nInternational Conference on System Sciences , 2010, pp. 1-10.\n[128] S. Shiva, S. Roy, and D. Dasgupta, \u201cGame theory for cyber security,\u201d\ninThe Sixth Annual Workshop on Cyber Security and Information\nIntelligence Research , 2010, p. 34.\n[129] K. Ramachandran, and Z. Stefanova, \u201cDynamic game theories in\ncyber security,\u201d in International Conference of Dynamic Systems and\nApplications , 2016, vol. 7, pp. 303-310.\n[130] Y . Wang, Y . Wang, J. Liu, Z. Huang, and P. Xie, \u201cA survey of\ngame theoretic", " introduction, 2nd edition . 2017.\nYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Bud-\nden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Ried-\nmiller. Deepmind control suite, 2018.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nInIntelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on , pp. 5026\u2013\n5033. IEEE, 2012.\nGeorge E Uhlenbeck and Leonard S Ornstein. On the theory of the brownian motion. Physical\nreview , 36(5):823, 1930.\nHado van Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems , pp.\n2613\u20132621, 2010.\nHado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-\nlearning. In Advances in Neural Information Processing Systems , 2016.\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.\nDueling network architectures for deep reinforcement learning. In International Conference on\nMachine Learning , 2016.\nZiyu Wang, Victor Bapst, Nicolas Heess, V olodymyr Mnih, Remi Munos, Koray Kavukcuoglu,\nand Nando de Freitas. Sample ef\ufb01cient actor-critic with experience replay. In International\nConference on Learning Representations , 2017.\nPaul J Werbos. A menu of designs for reinforcement learning over time. Neural networks for control ,\npp. 67\u201395, 1990.\n11Published as a conference paper at ICLR 2018\n0K6K12K18K24KAlien\n0K15K30K45KBeam Rider\n0 15 30 45 60\nTraining Time (Hours)0K20K40K60K80KDemon Attack\n0 15 30 45 60\nTraining Time (Hours)0K80K160K240K320KStar Gunner\nn= 32\nk= 1\nn= 32\nk= 2n= 32\nk= 4\nn= 32\nk= 8n= 256\nk= 1\nFigure 6: Testing whether improved performance is\ncaused by recency alone: ndenotes the number of\nactors,kthe number of times each transition is repli-\ncated in the replay. The data in the run with n= 32 ,\nk= 8is therefore as recent as the data in the run with\nn= 256 ,k= 1, but performance is not as good.\n010K20K30KAlien64 Actors 128 Actors\n0 8 16 24 32 40\nTraining Time (Hours)020K40K60KDemon Attack\n0 8 16 24 32 40\nTraining Time (Hours)\nAll distinct epsilons\n6 distinct epsilonsFigure 7: Varying the data-generating policies: Red:\n\ufb01xed set of 6 values for \u000f. Blue: full range of val-\nues for\u000f. In both cases, the curve plotted is from\na separate actor that does not add data to the replay\nmemory, and which follows an \u000f-greedy policy with\n\u000f= 0:00164 .\nA R ECENCY OF EXPERIENCE\nIn our main", " Introduction\nDeep reinforcement learning methods for deep reinforcement learning. ICML , 2016. Related Work\nThe earliest attempts to scale up deep reinforcement learn-\ning relied on distributed asynchronous SGD (Dean et al.,\n2012) with multiple workers. Examples include distributed\nA3C (Mnih et al., 2016) and Gorila (Nair et al., 2015), a\ndistributed version of Deep Q-Networks (Mnih et al., 2015).\nRecent alternatives to asynchronous SGD for RL include\nusing evolutionary processes (Salimans et al., 2017), dis-\ntributed BA3C (Adamski et al., 2018) and Ape-X (Horgan\net al., 2018) which has a distributed replay but a synchronous\nlearner.\nThere have also been multiple efforts that scale up reinforce-\nment learning by utilising GPUs. One of the simplest of\nsuch introduction. In Proceedings\nof the 1st ACM SIGPLAN International Workshop on\nMachine Learning and Programming Languages , MAPL\n2017, 2017. ISBN 978-1-4503-5071-6.\nAdamski, I., Adamski, R., Grel, T., Jedrych, A., Kaczmarek,\nK., and Michalewski, H. Distributed deep reinforcement\nlearning: Learn how to play atari games in 21 minutes.\nCoRR , abs/1801.02852, 2018.\nAppleyard, J., Kocisk \u00b4y, T., and Blunsom, P. Optimizing\nperformance of recurrent neural networks on gpus. CoRR ,\nabs/1604.01946, 2016.\nBabaeizadeh, M., Frosio, I., Tyree, S., Clemons, J., and\nKautz, J. GA3C: GPU-based A3C for deep reinforcement\nlearning. NIPS Workshop , 2016.\nBarth-Maron, G., Hoffman, M. W., Budden, D., Dabney,\nW., Horgan, D., Tirumala, D., Muldal, A., Heess, N., and\nLillicrap, T. Distributional policy gradients. ICLR , 2018.\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wain-\nwright, M., Kuttler, H., Lefrancq, A., Green, S., Valdes,\nV ., Sadik, A., Schrittwieser, J., Anderson, K., York, S.,\nCant, M., Cain, A., Bolton, A., Gaffney, S., King, H.,\nHassabis, D., Legg, S., and Petersen, S. Deepmind lab.\nCoRR , abs/1612.03801, 2016.\nBellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M.\nThe Arcade Learning Environment: An evaluation plat-\nform for general agents. Journal of Arti\ufb01cial Intelligence\nResearch , 47:253\u2013279, June 2013a.\nBellemare, M. G., Naddaf, Y ., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation platform\nfor general agents. J. Artif. Intell. Res.(JAIR) , 47:253\u2013279,\n2013b.\nChen, J., Monga, R., Bengio, S., and J \u00b4ozefowicz,\nR. Revisiting distributed synchronous SGD. CoRR ,\nabs/1604.00981, 2016.\nChetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran,\nJ., Catanzaro, B., and Shelhamer, E. cudnn: Ef\ufb01cient\nprimitives for deep learning. CoRR , abs/1410.0759, 2014.Clemente, A. V ., Mart \u00b4\u0131nez, H. N. C., and Chandra, A. Ef-\n\ufb01cient parallel Appendix C.1 .\nthe high diversity in visual appearance and game mechanics\nwithin the ALE suite, IMPALA multi-task still manages\nto stay competitive to A3C, shallow, experts , commonly\nused as a baseline in Experiments\nAll agents trained on Atari are equipped only with a feed forward network and pre-process frames in the same way as\ndescribed in Mnih et al. (2016). When training experts agents, we use the same hyperparameters for each game for\nboth IMPALA and A3C. These hyperparameters are the result of tuning A3C with a shallow network on the following\ngames: breakout ,pong ,space invaders ,seaquest ,beam rider ,qbert . Following experiments. related work, experts\nuse game-speci\ufb01c action sets.\nThe multi-task agent was equipped with a feed forward residual network (see Figure 3 ). The learning rate, entropy\nregularisation, RMSProp \"and gradient clipping threshold were adapted through population based training. To be able to\nuse the same policy layer on all Atari games in the multi-task setting we train the multi-task agent on the", " Introduction\nIn recent years, several di\ufb00erent approaches have been proposed for reinforcement learning with\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n\u201cvanilla\u201d policy gradient Background: Policy Optimization\n2.1 Policy Gradient Methods\nIn TRPO [Sch+15b], an objective function (the \u201csurrogate\u201d objective) is maximized subject to a\nconstraint on the size of the policy update. Speci\ufb01cally,\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At/bracketrightbigg\n(3)\nsubject to \u02c6Et[KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]]\u2264\u03b4. (4)\nHere,\u03b8oldis the vector of policy parameters before the update. This problem can e\ufb03ciently be\napproximately solved using the conjugate gradient algorithm, after making a linear approximation\nto the objective and a quadratic approximation to the constraint.\nThe theory justifying TRPO actually suggests using a penalty instead of a constraint, i.e.,\nsolving the unconstrained optimization problem\nmaximize\n\u03b8\u02c6Et/bracketleftbigg\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old(\u00b7|st),\u03c0\u03b8(\u00b7|st)]/bracketrightbigg\n(5)\nfor some coe\ufb03cient \u03b2. This follows from the fact that a certain surrogate objective (which computes\nthe max KL over states instead of the mean) forms a lower bound (i.e., a pessimistic bound) on the\nperformance of the policy \u03c0. TRPO uses a hard constraint rather than a penalty because it is hard\nto choose a single value of \u03b2that performs well across di\ufb00erent problems\u2014or even within a single\nproblem, where the the characteristics change over the course of learning. Hence, to achieve our goal\nof a \ufb01rst-order algorithm that emulates the monotonic improvement of TRPO, results and learning curves for all 49 games is provided in Experiments\n6.1 Comparison of Surrogate Objectives\nFirst, we compare several di\ufb00erent surrogate objectives under di\ufb00erent hyperparameters. Here, we\ncompare the surrogate objective LCLIPto several natural variations and ablated versions.\nNo clipping or penalty: Lt(\u03b8) =rt(\u03b8)\u02c6At\nClipping: Lt(\u03b8) = min(rt(\u03b8)\u02c6At,clip(rt(\u03b8)),1\u2212/epsilon1,1 +/epsilon1)\u02c6At\nKL penalty (\ufb01xed or adaptive) Lt(\u03b8) =rt(\u03b8)\u02c6At\u2212\u03b2KL[\u03c0\u03b8old,\u03c0\u03b8]\n5For the KL penalty, one can either use a \ufb01xed penalty coe\ufb03cient \u03b2or an adaptive coe\ufb03cient as\ndescribed in Section 4 using target KL value dtarg. Note that we also tried clipping in log space,\nbut found the performance to be no better.\nBecause we are searching over hyperparameters for each algorithm variant, we chose a compu-\ntationally cheap benchmark to test the algorithms on. Namely, we used 7 simulated robotics tasks2\nimplemented in OpenAI Gym [Bro+16], which use the MuJoCo [TET12] physics engine. We do\none million timesteps of training on each one. Besides the hyperparameters used for clipping ( /epsilon1)\nand the KL penalty ( \u03b2,dtarg), which we search over, the other hyperparameters are provided in in\nTable 3.\nTo represent the policy, we used a fully-connected MLP with two hidden layers of 64 units,\nand tanh nonlinearities, outputting the mean of a Gaussian distribution, with variable standard\ndeviations, following [Sch+15b; Dua+16]. We don\u2019t share parameters between the policy and value\nfunction (so coe\ufb03cient c1is irrelevant), and we don\u2019t use an entropy bonus.\nEach algorithm was run on all 7 environments, with 3 random seeds on each. We scored each\nrun of the algorithm by computing the average total reward of the last 100 episodes. We shifted\nand scaled the scores for each environment so that the random policy gave a score of 0 and the best\nresult was set to 1, and averaged over 21 runs to produce a single scalar for each algorithm setting.\nThe Results from continuous control benchmark. Average normalized scores (over 21 runs of the\nalgorithm, on 7 environments) for each algorithm / hyperparameter setting . \u03b2was initialized at 1.\n6.2 Comparison to Other Algorithms in the", " Introduction\nReinforcement learning algorithms aim at learning policies\nfor achieving target tasks by maximizing rewards provided\nby the environment. In some scenarios, these rewards are\nsupplied to the agent continuously, e.g. the running score\nin an Atari game (Mnih et al., 2015), or the distance be-\ntween a robot arm and an object in a reaching task (Lilli-\ncrap et al., 2016). However, in many real-world scenarios,\nrewards extrinsic to the agent are extremely sparse or miss-\n1University of California, Berkeley. Correspondence to:\nDeepak Pathak <pathak@berkeley.edu >.\nProceedings of the 34thInternational Conference on Machine\nLearning , Sydney, Australia, 2017. JMLR: W&CP. Copyright\n2017 by the author(s).\n(a) learn to explore in Level-1\n (b) explore faster in Level-2\nFigure 1. Discovering how to play Super Mario Bros without re-\nwards . (a) Using only curiosity-driven exploration, the agent\nmakes signi\ufb01cant progress in Level-1. (b) The gained knowledge\nhelps the agent explore subsequent levels much faster than when\nstarting from scratch. Watch the video at http://pathak22.\ngithub.io/noreward-rl/\ning altogether, and it is not possible to construct a shaped\nreward function. This is a problem as the agent receives\nreinforcement for updating its policy only if it succeeds in\nreaching a pre-speci\ufb01ed goal state. Hoping to stumble into\na goal state by chance (i.e. random exploration) is likely to\nbe futile for all but the simplest of environments.\nAs human agents, we are accustomed to operating with re-\nwards that are so sparse that we only experience them once\nor twice in a lifetime, if at all. To a three-year-old enjoy-\ning a sunny Sunday afternoon on a playground, most trap-\npings of modern life \u2013 college, good job, a house, a family \u2013\nare so far into the future, they provide no useful reinforce-\nment signal. Yet, the three-year-old has no trouble enter-\ntaining herself in that playground using what psychologists\ncall intrinsic motivation (Ryan, 2000) or curiosity (Silvia,\n2012). Motivation/curiosity have been used to explain the\nneed to explore the environment and discover novel states.\nThe French word \ufb02\u02c6aneur perfectly captures the notion of a\ncuriosity-driven observer, the \u201cdeliberately aimless pedes-\ntrian, unencumbered by any obligation or sense of urgency\u201d\n(Cornelia Otis Skinner). More generally, curiosity is a way\nof learning new skills which might come handy for pursu-\ning rewards in the future.\nSimilarly, in reinforcement learning, intrinsic motiva-\ntion/rewards become critical whenever extrinsic rewards\nare sparse. Most formulations of intrinsic reward can be\ngrouped into two broad classes: 1) encourage the agent\nto explore \u201cnovel\u201d states (Bellemare et al., 2016; LopesarXiv:1705.05363v1  [cs.LG]  15 May 2017Curiosity-driven Exploration by Self-supervised Prediction\net al., 2012; Poupart et al., 2006) or, 2) encourage the agent\nto perform actions that reduce the error/uncertainty in the\nagent\u2019s ability to predict the consequence of its own ac-\ntions (i.e. its knowledge about the environment) (Houthooft\net al., 2016; Mohamed & Rezende, 2015; Schmidhuber,\n1991; 2010; Singh et al., 2005; Stadie et al., 2015).\nMeasuring \u201cnovelty\u201d requires a statistical model of the dis-\ntribution of the environmental states, whereas measuring\nprediction error/uncertainty requires building a model of\nenvironmental dynamics that predicts the next state ( st+1)\ngiven the current state ( st) and the action ( at) executed\nat timet. Both these models are hard to build in high-\ndimensional continuous state spaces such as images. An\nadditional challenge lies in dealing with the stochasticity of\nthe agent-environment system, both due to the noise in the\nagent\u2019s actuation, which causes"], "bleu": 0.0, "rouge_l": 0.26475548060708265, "gpt_metric_score": 0.8, "bert_score": 0.21709643304347992}
