{"paper_key": "Automated test generation to evaluate tool-augmented LLMs as conversational AI agents", "current_5q": "**[Question 1] - What is the problem?**  \nHow can we effectively evaluate the performance of tool-augmented large language models (LLMs) as conversational AI agents in customer support scenarios?\n\n---\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses the pressing need for reliable evaluation methods for LLMs in real-world applications, particularly in customer support. By developing a comprehensive evaluation framework, we can enhance the understanding of LLM capabilities, leading to improved AI agent designs and more effective deployment in various domains. This research could pave the way for practical applications that ensure LLMs operate safely and efficiently, minimizing misinformation and reputational risks for companies.\n\n---\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the complexity of conversational contexts, which involve multi-turn interactions rather than isolated queries. Naive approaches may fail because they do not account for the dynamic nature of conversations, user manipulations, and the necessity for adherence to specific procedures. Additionally, creating a comprehensive evaluation dataset that captures diverse scenarios and potential edge cases requires sophisticated methods to generate realistic conversations and assess the AI's resilience against various challenges.\n\n---\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on evaluating LLMs for specific tasks, such as question answering or code generation, which do not encompass the full range of capabilities required for effective conversational AI. Existing solutions lack the ability to assess the interaction of LLMs with tools and procedures in a holistic manner. Barriers to solving this problem include the absence of a standardized evaluation framework and the difficulty in generating diverse, realistic datasets. Our approach improves upon prior work by automating dataset generation and incorporating red teaming to evaluate the AI's robustness against manipulative user interactions.\n\n---\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves an automated pipeline for generating evaluation datasets for tool-augmented LLMs. This includes using an LLM to create procedures, extract relevant APIs, and generate flowgraphs and conversation graphs, which are then sampled to produce realistic conversations. We will benchmark multiple LLMs using our newly created dataset, ALMITA, which contains 1420 manually curated tests. The expected outcomes include a comprehensive evaluation of LLM performance in customer support scenarios, revealing strengths and weaknesses in their conversational abilities and adherence to procedures.", "proposal_5q": "[Question 1]: What is the problem?  \nThe specific research question we aim to address is: How can a hybrid framework that combines synthetic data generation, tool-augmented Large Language Models (LLMs), and the T5QL model for SQL generation enhance customer support systems and improve intent detection while minimizing reliance on costly real-world data?\n\n[Question 2]: Why is it interesting and important?  \nSolving this problem is crucial for the research community as it addresses the growing demand for scalable and efficient customer support systems that can handle diverse user interactions. By developing a hybrid framework that leverages synthetic data and advanced LLMs, this research could lead to significant advancements in natural language processing (NLP) applications, particularly in intent detection and SQL query generation. The implications of this work extend beyond customer support; they could influence the design of intelligent systems across various domains, including healthcare, finance, and e-commerce. Moreover, the integration of active learning techniques and real-time evaluation mechanisms can foster a more adaptable AI, promoting user trust and satisfaction through continuous feedback loops.\n\n[Question 3]: Why is it hard?  \nThe challenges in solving this problem are multifaceted. First, generating contextually rich synthetic datasets that accurately represent the complexity of real-world customer interactions is a non-trivial task. Naive approaches may fail to capture the nuances of user intent and diverse query structures, leading to poor model performance. Additionally, integrating various components such as synthetic data generation, LLMs, and SQL generation into a cohesive framework poses significant technical challenges, including ensuring compatibility and optimizing performance across different models. Furthermore, the implementation of active learning techniques requires sophisticated mechanisms to effectively curate and refine the training data, which can be resource-intensive and complex.\n\n[Question 4]: Why hasn't it been solved before?  \nPrevious research has made strides in text-to-SQL systems and LLMs, but gaps remain in the effective integration of these technologies with synthetic data generation, particularly in the context of customer support systems. Many existing solutions focus on single aspects of the problem, such as improving SQL generation or enhancing intent detection independently, without addressing the interplay between these elements. Barriers to solving this problem have included a lack of comprehensive datasets that encompass diverse query patterns and the challenges of generalizing models to unseen database schemas and novel SQL structures. Our approach differs by creating a hybrid framework that not only combines these elements but also emphasizes iterative refinement through active learning and real-time evaluation, thus addressing the shortcomings of prior work.\n\n[Question 5]: What are the key components of my approach and results?  \nOur proposed methodology involves developing a hybrid framework that integrates synthetic data generation with tool-augmented LLMs and the T5QL model for SQL generation. We will utilize active learning techniques to iteratively refine model performance based on contextually rich synthetic datasets. The framework will also incorporate an evaluation mechanism to assess AI agents' performance through simulated interactions, allowing for real-time feedback that enhances user experience. Key metrics for evaluation will include accuracy in intent detection, robustness in SQL generation, and user satisfaction ratings. Expected outcomes include a more adaptable AI capable of handling a wider variety of customer interactions with improved efficiency and reduced reliance on extensive real-world datasets, ultimately contributing to a more effective customer support system.", "referenced_intros": ["\n\n1 Introduction\n\nSynthetic data accelerated the development of LLMS: The rise of synthetic data in the training of Large Language Models (LLMs) has been a significant development of the last year. Synthetic data was used to significantly accelerate the progress of model training (especially SLMs) in all stages of training from pre-training (e.g., \u00a0[1]), to instruction-tuning (e.g., \u00a0[21, 36]) and RLHF(e.g., \u00a0[12, 28]).\n\n\nGenerating high quality synthetic data is hard: On the other hand, research has also shown that pre-training models on synthetic data generated by other models can lead to model collapse\u00a0[29], leading to models gradually degenerating as a result. Similar arguments have been made against using synthetic data for pos-training, which could amount to an imitation process that could teach the trained model to pick only stylistic characteristics and not real capabilities\u00a0[8].\nThis discrepancy could be explained by the observation that creating high-quality and diverse synthetic data is hard \u00a0[17]. Successful use of synthetic data involved significant human effort in curating and filtering the data to ensure high quality.\nIf we focus on post-training synthetic data, we will see the most widely used approach includes starting with a set of prompts and using a powerful model such as GPT-4 [22] to generate responses to these prompts \u00a0[24] or of an expanded set of the prompts \u00a0[36]. This recipe was further improved by eliciting explanations or step-by-step instructions from the teacher model\u00a0[20] or using more complex prompting techniques to elicit higher quality answers\u00a0[18].\n\n\nSynthetic data meets Agents:\nAnother major development we witnessed last year is the rise of Agentic (especially multiagent) workflows \u00a0[33, 13]. Agentic workflows can generate high quality data, that surpasses the capabilities of the underlying LLMs, by using flows with reflection and iteration, where agents can look back at solutions, generate critiques and improve solutions. They can also use tools (e.g. search apis, calculator, code interpreters) addressing limitations of LLMs.\nMulti-agent workflows bring in additional benefits such as simulating scenarios where we can generate both new prompts and the corresponding responses. They also enable automation of the data generation workflows reducing or eliminating need for human intervention on some tasks.\n\n\nGenerative Teaching & Orca AgentInstruct: Generating synthetic data for post-training often relies on an existing prompt set that is used as is or used as seeds for generating more instructions. In this work, we generalize the problem settings to a broader objective of generating abundant amounts of diverse, challenging and high-quality data to teach a particular skill to an AI model, we refer to this setting as Generative Teaching. AgentInstruct is an agentic solution for Generative Teaching. AgentInstruct focuses on creating demonstration and feedback data and requires only raw documents as input. When generic data is used as seeds, AgentInstruct can be used to teach an LLM a general capability (e.g. Math, Reasoning, RAG, etc.). Domain specific data (e.g. gaming, finance) can also be used as seeds to improve the model in a certain specialization. AgentInstruct can create:\n\n\n1.\n\nHigh-quality data: using powerful models like GPT-4, coupled with tools like search and code interpreters.\n\n\n\n2.\n\nDiverse data: AgentInstruct generates both prompts and responses. It uses a large number of agents (equipped with powerful LLMs, tools and reflection flows) and a taxonomy (of over 100 subcategories) to create diverse and high quality prompts and responses,\n\n\n\n3.\n\nLarge quantities of data: AgentInstruct can run autonomously and can apply flows for verification and data filtering. It does not require seed prompts and uses raw documents for seeding.\n\n\n\n\n\nUsing raw data (unstructured text documents or source code) as seeds has two benefits. First, this data is available in abundance enabling the use of AgentInstruct to create large amounts of diverse data. Additionally, using raw data as seeds, and hence, avoiding using existing prompts, as is or after paraphrasing, can promote learning more general capabilities as opposed to benchmark-specific ones.\n\n\nWe demonstrate the utility of AgentInstruct by creating a comprehensive synthetic post-training dataset of 25 million prompt and response pairs. The dataset covers a wide array of skills including creative writing, reasoning, math, RAG, tool use, etc. To assess the value of the data, we use it to finetune Mistral-7B[11] model. The finetuned Mistral model (Orca-3) shows significant improvement over other instruction-tuned models using the same base model. For example, compared to Mistral-Instruct-7B, it shows 40% improvement on AGIEval, 19% improvement on MMLU, 54% improvement on GSM8K, 38% improvement on BBH, 45% improvement on AlpacaEval and 31.34% reduction on hallucination across multiple summarization benchmarks. Additionally, it outperforms other models such as LLAMA-8B-instruct and GPT-3.5 on multiple benchmarks. Note that the only seed data used is publicly available raw materials and no task-specific or benchmark data has been used as seeds.\n\n\nWhile we demonstrate the utility of AgentInstruct by creating a generic post-training synthetic dataset, we believe that agents can enable the creation of Synthetic-Data-Generation-As-A-Service where we start with raw materials (e.g. web data for general model training or domain specific data for specialized models), and we generate data for post-training and finetuning, hence enabling continual learning and improvement of any base LLM. Additionally, we believe that the AgentInstruct approach can be used for self-improvement of larger, more capable models because of: (1) the ability to generate new prompts and (2) the ability to generate responses that exceed the quality of the LLM used in the agentic flow (because of the use of tools, reflection, etc.).\n\n", "Introduction\nFunction-calling agents represent a significant advancement in artificial intelligence, specifically\nwithin the realm of Large Language Models (LLMs). These models, such as GPT4 [ 1], Gemini [ 2],\nand Mistral [ 3], have evolved to not only understand and generate human-like text but also to execute\nfunctional API calls based on natural language instructions. For instance, consider a user requesting\nthe weather in Palo Alto, as illustrated in Fig. 1. The function-calling agent interprets this query,\naccesses the relevant API\u2014such as get_weather(\"Palo Alto\", \"today\") \u2014and retrieves the\nweather information, all in real-time. This capability extends the utility of LLMs beyond simple\nconversation tasks to include dynamic interactions with a variety of digital services and applications,\nranging from social media platforms to financial services [4, 5, 6, 7, 8].\nDespite their growing popularity and potential, the deployment of function-calling agents is often\nhampered by the quality of the datasets used for training. Current datasets are largely static and lack\ncomprehensive verification, leading to potential inaccuracies and inefficiencies of model fine-tuning\nin real-world applications [ 9,10,11,12]. This limitation is particularly evident when models trained\non these datasets encounter new, unseen APIs. For example, a model trained primarily on restaurant\nbooking APIs may struggle when suddenly tasked with retrieving stock market data, as it lacks the\nspecific training data or the adaptability to handle new domains.\n1https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k\n2https://apigen-pipeline.github.io/\nPreprint.arXiv:2406.18518v1  [cs.CL]  26 Jun 2024Figure 1: Workflow of an LLM-based function-calling agent.\nTo address these challenges, we introduce APIGen, an Automated PIpeline for Generating verifiable\nand diverse function-calling datasets. Our framework is designed to facilitate the fine-tuning of\nfunction-calling LLMs by providing high-quality, diverse datasets that better reflect the variability\nand complexity of real-world API use. Crucially, each generated data point undergoes rigorous multi-\nstage verification processes\u2014format, execution, and semantic\u2014to ensure accuracy and applicability.\nWe fine-tune function-calling models using the dataset generated by APIGen. The", "\n\nI Introduction\n\n\nTHE exponential growth of data in recent years has transformed the world of digital information. In 2023, the total amount of data created, captured, copied, and consumed globally reached around 120 zettabytes, and it will probably go up to 147 zettabytes by 2024, with expectations to surpass 180 zettabytes by 2025 [1]. Fig. 1 illustrates the\n\n\n\nFigure 1: Increase in data volume over the years [1].\n\n\nincrease in data volume from 2010 to 2023, with projected values for 2024 and 2025. This rapid expansion of the data ecosystem has paved the way for groundbreaking innovations in artificial intelligence (AI), leading to the development of several machine learning models. Among these, large language models (LLMs) have emerged as a prominent subset [2] thanks to their exceptional ability to understand, generate, and manipulate human language [3].\n\n\n\n\nFigure 2: Google search interest over time [4].\n\n\nTABLE I: Summary of Chatbot Literature\n{tblr}\n\n\nwidth = colspec = \u2014Q[c,m,0.0663]\u2014Q[c,m,0.05427]\u2014Q[c,m,0.05427]\u2014Q[c,m,0.1064]\u2014Q[c,m,0.1064]\u2014Q[c,m,0.0555]\u2014Q[c,m,0.0555]\u2014Q[c,m,0.0555]\u2014Q[j,m,0.2272]\u2014,\nrow1 = font=, c,\nrow1-16 = font=, hlines,\ncell12 = c=2,\ncell22 = c=1halign=c,valign=m,\ncell23 = c=1halign=c,valign=m,\ncell14 = c=2,\ncell24 = c=1halign=c,valign=m,\ncell25 = c=1halign=c,valign=m,\ncell16 = c=3,\ncell26 = c=1halign=c,valign=m,\ncell27 = c=1halign=c,valign=m,\ncell28 = c=1halign=c,valign=m,\ncell11 = r=2halign=c,valign=m,\ncell19 = r=2halign=c,valign=m,\n\nReferenced Article & \\SetCell[c=2]c Chatbot(s) Covered  \\SetCell[c=2]c Application(s) Scope  \\SetCell[c=3]c Challenge(s) Discussed  Remark(s) \n Single  Multiple  Monodisciplinary  Multidisciplinary  Tehcnical  Ethical  Misuse  \n[5]  - -   \u2714 \u2714 \u2022\u00a0\u00a0Limits discussion to ChatGPT, with no mention of other chatbots such as BARD, Bing Chat, etc.\n\u2022\u00a0\u00a0Overlooks the technical issues.\n\n\n\n\n[6]  -  -  \u2714 \u2714 \u2022\u00a0\u00a0Does not address technical issues.\n\u2022\u00a0\u00a0Explores ethical concerns and misuse cases under user experience, resulting in underdeveloped content.\n\n\n\n\n[7]  - -   \u2714   \u2022\u00a0\u00a0Inadequately addresses the technical issues and misuse cases, lacking depth and detail in the discussion.\n\n\n\n\n[8]  -  -  \u2714 \u2714 \u2714 \u2022\u00a0\u00a0Lacks structured categorization of key issues, causing difficulties for readers to find specific information.\n\n\n\n\n[9]  -  -    \u2022\u00a0\u00a0Only considers the initial release of ChatGPT (v3.5).\n\u2022\u00a0\u00a0Lacks categorization and depth in discussing educational issues.\n\n\n\n\n[10]  - -     \u2022\u00a0\u00a0Lacks a taxonomy for applications and challenges across sectors.\n\n\n\n\n[11]  - -  \u2714 \u2714 \u2714 \u2022\u00a0\u00a0Relies heavily on narrative content that lacks analytical depth.\n\u2022\u00a0\u00a0Lacks sufficient visual aids (e.g., figures, graphs, bar charts, etc.), complicating data interpretation and reducing reader engagement. \n\n\n\n\n[12]  - -  \u2714 \u2714 \u2714 \u2022\u00a0\u00a0Contains substantial overlapping insights from 43 contributions by experts, leading to an unorganized and excessively lengthy document.\n\n\n\n\n[13]  - -  \u2714 \u2714 \u2714 \u2022\u00a0\u00a0Lacks fine-grained categorization of applications and challenges.\n\n\n\n\n[14] -   -    \u2022\u00a0\u00a0Lacks discussion on key issues. \n\n\n\n\nOur Survey  -  -  \u2714 \u2714 \u2714 \u2022\u00a0\u00a0Covers a wide range of chatbots beyond ChatGPT, including BARD, Bing Chat, Claude, and others.\n\u2022\u00a0\u00a0Provides a detailed taxonomy of applications and challenges, each divided into distinct sub-categories.\n \n\n\n\n\n\n\n\n\n*\u00a0\u2714(Fully discussed);  \u00a0(Partially discussed);  \u00a0(Not discussed); - (Not applicable);\n\n\n\n\n\n*\u00a0 \u00a0(Single chatbot or monodisciplinary);  \u00a0(Two chatbots or two disciplines);\n\n\n\n\n\n*\u00a0 \u00a0(Three chatbots or three disciplines);  \u00a0(More than three chatbots or more than three disciplines).\n\n\n\n\n\n\n\nIn the era of AI-powered chatbots [15, 16, 17], LLMs have become instrumental in powering their conversational capabilities and facilitating human-like interactions [2, 7]. The substantial rise in data and advancements in computational knowledge have improved the functionality of LLM-based chatbots, making them increasingly popular and widely adopted across various sectors. Their ability to understand and respond in human language with never-before-seen context relevance and accuracy, coupled with the capacity to handle vast information streams, makes them essential tools in areas like education [18, 19, 20], research [21, 22, 23], healthcare [24, 25, 8], and many others [26, 27, 28]. Given the vast potential and promising possibilities of LLM-based chatbots, their growing usage and required optimization pose several challenges that call for thorough research and evaluation. This need becomes more apparent as the domain of LLM-based chatbots rapidly expands, leading to an overwhelming amount of research literature for scholars, professionals, and newcomers alike. Therefore, our work provides a timely and complete survey of LLM-based chatbots in response to these evolving needs.\nBefore the emergence of LLMs and LLM-based chatbots, conversational AI faced several challenges. Early chatbots had limited contextual understanding and domain specificity. They often provided inaccurate responses. Lack of sophisticated language comprehension limited their ability to interact in a human-like manner, which led to robotic and disjointed user experiences. Scalability across various industries was also problematic, as handling vast information streams with real-time responsiveness proved challenging. The advent of LLMs revolutionized chatbots and started a new era of AI-driven interactions. In March 2023, OpenAI unveiled its latest marvel, GPT-4 (also known as ChatGPT Plus [29]), following the buzz generated since the debut of ChatGPT 3.5 in November 2022 [30, 31]. Fig. 2 illustrates the exponential rise in popularity of ChatGPT (in blue) since its initial release [4], showcasing its dominance over other widespread technologies such as 5G (in yellow), IoT (in green), and Blockchain (in red). Its innovative capabilities have been met with an unprecedented surge in popularity, highlighting a new chapter in AI-driven communication. In a related development, Google announced BARD [32], its first LLM-based chatbot, on February 6, followed by early access on March 21 [33]. Additionally, there are numerous other LLM-based chatbots in the works. Acknowledging the profound impact of these technologies, this survey aims to provide a distilled, up-to-date overview of LLM-based chatbots, including their development, industry-wide applications, key challenges, and strategies to improve their effectiveness and reliability. Our goal is to integrate these diverse studies into a well-organized survey that will facilitate an in-depth understanding of LLM-based chatbots and give readers a guide for their future research.\n\n\n\nI-A Existing Surveys, Reviews, and Case Studies\n\n\nSeveral articles have reviewed the wide-ranging applications of LLM-based chatbots, highlighting their significant impact and the complex challenges they pose across various sectors. Here, we discuss some of these articles and demonstrate how our survey extends and differs from them.\n[5] explores the use of AI and chatbots in the academic field and their influence on research and education from an ethical perspective. It investigates the impact of these technologies on educational assessment integrity and their potential to transform academic research. In addition, it recommends effective solutions to alleviate the ethical challenges and possible misuse of these tools in the education and research domains. [6] conducts a case study on how ChatGPT elevates online learning. The findings suggest that students favor these agents for their educational activities, citing a more interactive and engaging learning environment. Koubaa et al. [7] provide a detailed review of ChatGPT\u2019s technical novelties. Following this, they develop a unique taxonomy in their survey for research categorization and explore ChatGPT\u2019s applications across various fields. Additionally, they highlight significant challenges and avenues for future exploration. [8] offers a systematic review of ChatGPT in healthcare, focusing on education, research, and practice. The author outlines ChatGPT\u2019s potential to revolutionize scientific writing and personalized learning. The review critically analyzes the benefits while acknowledging significant concerns, such as ethical and accuracy issues. Another review article [9] evaluates ChatGPT\u2019s impact on education, noting its varied performance across subjects such as Economics, Programming, Law, Medical Education, and Mathematics, among others. The paper highlights the tool\u2019s potential and challenges, like accuracy concerns and plagiarism, suggesting updates to assessment methods and educational policies for responsible use. In [10], the authors conduct an exploratory survey utilizing virtual and in-person feedback to analyze the impact of ChatGPT in education, healthcare, and research. The survey demonstrates how ChatGPT can improve personalized learning, clinical tasks, and research efficiency. They also address major ethical and practical concerns, suggesting careful AI deployment with robust ethical guidelines to navigate these challenges. In a similar context, [11] provides a comprehensive analysis of ChatGPT, focusing on its evolution, diverse applications, and key challenges. Unlike [10], which employs surveys for direct feedback, [11] aggregates findings from existing studies to assess the influence and challenges of ChatGPT, offering a more generalized perspective without engaging in primary data collection. Exploring further, [12] and [13] delve into ChatGPT\u2019s broader multidisciplinary applications. [12] gathers insights across multiple disciplines to assess its effects in sectors ranging from marketing to education and healthcare, whereas [13] introduces a taxonomy of ChatGPT research, detailing its applications in domains like healthcare, finance, and environmental science. Besides, both papers address fundamental challenges regarding ethical considerations and practical deployment. Another recent article [14] uses a single-case study approach to evaluate the effectiveness of ChatGPT and Bing Chat in Chemistry education. The research analyzes extensive interactions between these tools and a simulated student to improve creativity, problem-solving, and personalized learning. The findings show that both chatbots act as valuable \u2018agents-to-think-with.\u2019 However, ChatGPT notably outperforms Bing Chat in delivering more comprehensive and contextually accurate responses across various scientific topics.\nDifferent from existing works, our survey expands beyond the typical focus on specific chatbots like ChatGPT, covering a wide range of models, including BARD, Bing Chat, and Claude. Moreover, we explore applications across multiple domains and discuss various challenges, each detailed in several sub-categories. Table I summarizes the findings of the discussed articles, facilitating a comparative understanding of their contributions.\n\n\n\n\nFigure 3: Survey outline.\n\n\n\n\nI-B Our Contributions\n\n\nOur survey aims to answer the following questions:\n\n\n\u2022\n\nHow have chatbots evolved from simple automated systems to the LLM-based variants we see today, and what foundational advancements in LLMs have redefined chatbot capabilities since the pre-LLM era?\n\n\n\n\u2022\n\nWhat are the key applications of LLM-based chatbots across different sectors, and how do they impact the operational dynamics and user interactions within these domains?\n\n\n\n\u2022\n\nWhat challenges emerge with the widespread use of LLM-based chatbots, and how do they affect their performance and reliability?\n\n\n\n\u2022\n\nWhat technical improvements are essential for LLM-based chatbots, and how will the implementation of ethical guidelines ensure their responsible usage?\n\n\n\n\n\nIn addressing these questions, we offer a comprehensive overview of the history of chatbots. Additionally, we discuss the foundations of LLMs, highlighting the transformer-based self-attention mechanisms and the innovative features in GPT models, such as in-context learning and chain-of-thought (CoT) prompting. Then, we provide a detailed taxonomy of LLM-based chatbots, organizing them by their functionalities and applications in sectors like education, research, and healthcare. We also acknowledge their growing significance in software engineering and finance. Next, we look into the open challenges from technical aspects, covering issues from knowledge recency to hallucination, alongside ethical considerations like data transparency, bias, privacy risks, and unfairness. We then conclude with misuse perspectives, highlighting concerns regarding academic misuse, over-reliance, and the distribution of wrong information. Finally, we discuss the future outlook for LLM-based chatbots, from technical improvements like model optimization to complying with ethical guidelines and promoting responsible use across various domains. Our contributions are summarized as follows:\n\n\n\u2022\n\nUnlike most articles that concentrate on specific chatbots or their limited aspects, our survey covers a variety of LLM-based models, including ChatGPT, BARD, Bing Chat, and many others.\n\n\n\n\u2022\n\nWhile the majority of articles focus on a single chatbot applied to one or multiple domains without detailed categorization, our survey extends to a wide array of chatbots across various application domains. We provide a detailed taxonomy of applications, offering a structured and in-depth exploration of how different chatbots perform across sectors like education, research, healthcare, software engineering, and finance.\n\n\n\n\u2022\n\nWe discuss several open challenges from technical, ethical, and misuse perspectives. In addition, we frame our discussion around knowledge and data, the two central pillars of LLMs. This approach illustrates the dynamic interplay between chatbots\u2019 interaction with extensive training data and their subsequent generation of new content (knowledge).\n\n\n\n\n\nThe rest of the survey is organized as follows: Section II covers the foundational years of chatbots, the rise of LLMs, and an overview of LLM-based chatbots. Section III highlights the applications of these chatbots in education, research, and healthcare. It also covers miscellaneous applications, such as software engineering and finance. Section IV delves into the inherent challenges of these chatbots, while Section V explores the future outlook in this field. Finally, Section VI concludes the survey, summarizing its key findings and overall contributions. The outline of our survey is shown in Fig. 3.\n\n\n", "\n\nI Introduction\n\n\nCode generation is the process of automatically generating source code based on provided specifications or requirements, which enables developers to save time by reducing manual coding efforts and allows them to focus on higher-level tasks and problem-solving. Moreover, it can aid in ensuring consistency and reducing the risk of human error during development. Automatic code generation has been a longstanding challenge in both software engineering and artificial intelligence communities.\nThe recent advancements in Large Language Models (LLMs) have significantly propelled this field forward [4, 31, 29]. For example, OpenAI\u2019s Codex [4], released in 2021, has achieved a success rate of 28.8% in solving a set of 164 hand-written programming problems. Microsoft\u2019s Copilot, a code generation tool powered by Codex, has captured the interest of over 1 million professional developers [6]. Moreover, it has shown the potential to speed up coding tasks by up to 55% [15]. Subsequently, various code LLMs emerged in both academia and industry, such as Incoder [7], StarCoder [20], CodeRL [18], CodeGen [28, 27], Code Llama [31], ChatGPT [29], etc. These models are capable of generating code with functional accuracy comparable to that of human developers.\n\n\nDespite the remarkable success of LLMs, they are prone to generate hallucinations across various tasks. In other words, LLMs might produce outputs that, although seemingly plausible, deviate from users\u2019 intent, factual knowledge, or their contexts [39].\nThe hallucination issue poses a potential risk in deploying LLMs across various applications [14]. Most existing work mainly focuses on investigating the hallucination for natural language generation (NLG) tasks, for instance, generative question answering [19], abstractive summarization [25], dialogue generation [12], etc. The hallucinations are mainly divided into three categories: input-conflicting, context-conflicting, and fact-conflicting hallucinations [39]. However, there is still a lack of clarity regarding the specific types of content that LLMs tend to hallucinate during code generation, as well as the potential consequences they may have.\n\n\nWe argue that similar hallucinations also occur in the domain of code generation, where the model could generate code snippets that conflict with the user\u2019s requirements, contextual information, or code knowledge.\nThese occurrences may undermine the correctness, performance, maintenance, and even security of the developed software. As a result, the widespread adoption of Code LLMs for code recommendation has the inherent potential to compromise the overall quality and reliability of software.\nHence, it is imperative to thoroughly investigate hallucinations in LLM-powered code generation. This will allow us to gain valuable insights into the specific weaknesses that the LLM model may generate.\nMoreover, analyzing hallucinations helps us identify code snippets or patterns that are likely to be incorrect or unreliable, providing valuable feedback for improving LLMs. Through collaboration between researchers and developers, we can refine and fine-tune the model, thus enhancing its code generation capabilities in terms of accuracy and reliability.\n\n\nTo facilitate research in this area, we conducted a thematic analysis [5] of the LLM-generated code to summarize and categorize the hallucinations presented in it. Specifically, we first collected 13,968 code snippets generated by different LLMs, and sampled 3,084 code snippets for subsequent analysis. Finally, we establish a comprehensive taxonomy of hallucinations, which comprises 5 primary categories: Intent Conflicting, Context Inconsistency, Context Repetition, Dead Code, and Knowledge Conflicting. The taxonomy encompasses 19 specific types of hallucinations.\nThen we conducted a comprehensive investigation and various statistical analyses of these hallucinations from diverse perspectives to gain a deeper understanding of the prevailing challenges and opportunities in the domain of code generation with LLMs.\nThe analysis reveals that code LLMs are frequently influenced by a diverse range of hallucinations with distinct distributions. Moreover, multiple different hallucinations can occur simultaneously within a single generated program. Additionally, the majority of these hallucinations can result in functional errors or serve as indicators of their presence. Therefore, it is imperative to develop effective techniques to detect and mitigate hallucinations during code generation.\n\n\nIn summary, this paper makes the following contributions:\n\n\n\u2022\n\nWe conducted the first comprehensive study to analyze the types of content LLMs may tend to hallucinate in code generation, and established a taxonomy of hallucination types.\n\n\n\n\u2022\n\nWe systematically analyzed the distribution of hallucinations as well as the correlation between these hallucinations and the correctness of the code.\n\n\n\n\u2022\n\nWe developed and released HalluCode, an evaluation benchmark specifically designed to assess hallucinations in code LLMs, and also conducted hallucination recognition experiments using HalluCode and HumanEval to evaluate several state-of-the-art code LLMs.\n\n\n\n\n", "\n\n1 Introduction\n\nScience fiction has portrayed numerous striking characters of Intelligent Personal Assistants (IPAs), which are software agents that can augment individuals\u2019 abilities, complete complicated tasks, and even satisfy emotional needs. These intelligent agents represent most people\u2019s fantasies regarding artificial intelligence (AI). With the widespread adoption of personal devices (e.g., smartphones, smart home equipment, electric vehicles, etc.) and the advancement of machine learning technology, this fantasy is gradually becoming the reality. Today, many mobile devices embeds IPA software, such as Siri [1], Google Assistant [2], Alexa [3], etc. These intelligent agents are deeply entwined with users, capable of accessing user data and sensors, controlling various personal devices, and accessing personalized services associated with private accounts.\n\n\nHowever, today\u2019s intelligent personal assistants still suffer from the limitations of flexibility and scalability. Their level of intelligence is far from adequate, particularly evident in their understanding of user intent, reasoning, and task execution.\nMost of today\u2019s intelligent personal assistants are limited to performing tasks within a restricted domain (e.g., simple functions in built-in apps). Once a user requests for tasks beyond these boundaries, the agent fails to comprehend and execute the actions accurately. Altering this circumstance necessitates a significant expansion of the agent\u2019s capability to support a broader and more flexible scope of tasks.\nHowever, it is difficult for current IPA products to support tasks at scale. Most of the today\u2019s IPAs require to follow specific predefined rules to complete tasks, such as developer-defined or user-demonstrated steps. Therefore, developers or users must explicitly specify which functions they wish to support, in addition to defining the triggers and steps for task execution. This approach inherently restricts the scalability to wider range of tasks, since supporting more tasks demands extensive time and labor cost. Some approaches have attempted to automatically learn to support tasks through supervised learning or reinforcement learning [4, 5, 6]. However, these methods also rely on a substantial amount of manual demonstrations and/or the definition of reward functions.\n\n\nThe emergence of Large Language Models (LLMs) [7] in recent years has brought brand new opportunities for the development of IPAs, demonstrating the potential to address the scalability issues of intelligent personal assistants. In comparison to traditional methods, large language models such as ChatGPT, Claude, and others have exhibited unique capabilities such as instruction following, commonsense reasoning, and zero-shot generalization. These abilities have been achieved through unsupervised learning on massive corpora (exceeding 1.4 trillion words) and subsequently fine-tuned with human feedback. Leveraging these capabilities, researchers have successfully adopted large language models to empower autonomous agents (aka. LLM agents), which aims to solve complex problems by automatically making plans and using tools such as search engines, code interpreters, and third-party APIs.\n\n\nAs a unique type of intelligent agents,\nIPAs also have the potential to be revolutionized by LLMs with significantly enhanced scalability, capability, and usefulness. We call such LLM-powered intelligent personal assistants as Personal LLM Agents. As compared with normal LLM agents, Personal LLM Agents are more deeply engaged with personal data and mobile devices, and are more explicitly designed for assisting people rather than replacing people.\nSpecifically, the primary way to assist users is by reducing repetitive, tedious, and low-value labor in their daily routine, letting the users focus on more interesting and valuable things, thereby enhancing the efficiency and quality of their work and life.\nPersonal LLM Agents can be built upon existing software stacks (e.g., mobile apps, websites, etc.), while bringing refreshing user experience with ubiquitous intelligent automation abilities. Therefore, we expect Personal LLM Agents to become a major software paradigm for personal computing devices in the AI era, as shown in Figure\u00a01.\n\n\nFigure 1: We envision Personal LLM Agents to become the dominating software paradigm for individual users in the upcoming era.\n\n\nDespite the promising future of Personal LLM Agents, related research is still in its nascent stage, presenting numerous intricacies and challenges. This paper takes the first step to discuss the route map, design choices, main challenges and possible solutions in implementing Personal LLM Agents.\nSpecifically, we focus primarily on the aspects related to \u201cpersonal\u201d parts within Personal LLM Agents, encompassing the analysis and utilization of users\u2019 personal data, the use of personal resources, deployment on personal devices, and the provision of personalized services. The straightforward integration of the general language capabilities of LLMs into IPAs is not within the scope of this paper.\n\n\nWe started by taking a survey with domain experts of Personal LLM Agents. We invited 25 chief architects, managing directors, and/or senior engineers/researchers from leading companies who are working on IPAs and/or LLMs on personal devices.\nWe asked the experts\u2019 opinions about the opportunities and challenges of integrating LLMs in their consumer-facing products.\nBased on our understanding and analyses of experts\u2019 insights, we summarized a simple and generic architecture of Personal LLM Agents, in which the intelligent management and utilization of personal data (user context, environment status, activity history, personalities, etc.) and personal resources (mobile apps, sensors, smart-home devices, etc.) play the most vital role.\nThe ability to manage and utilize these personal objects differentiates the intelligence of Personal LLM Agents. Inspired by the L1-L5 intelligence levels of autonomous driving, we also give an taxonomy of five intelligent levels of Personal LLM Agents.\n\n\nOur findings also highlight several major technical challenges to implement such Personal LLM Agents, which can be categorized into three aspects including the fundamental capabilities, efficiency, and security & privacy.\nWe further dive deeper into these aspects with detailed explanations of the challenges and comprehensive survey of possible solutions.\nSpecifically, for each technical aspect, we briefly explain its relevance and importance to personal LLM agents, then break it down to several main research problems.\nFor example, the foundamental capabilities for personal LLM agents include task execution, context sensing, and memorization.\nThe efficiency of agents is primarily determined by the LLM inference efficiency, customization efficiency, and memory retrieval efficiency.\nThe security and privacy concerns of personal LLM agents can be categorized as data confidentiality, decision reliability, and system integrity.\nFor each research problem, we summarize the main techniques involved with the problem, followed by a brief introduction of the related work.\nDue to the wide scope of the techniques in personal LLM agents, we only include the most relevant or recent works, rather than attempting to cover all related approaches.\n\n\nThe main content and contributions of this paper can be summarized as follows:\n\n\n\n\n1.\n\nWe summarize the status quo of existing intelligent personal assistants in both industry and academia, while analyzing their primary limitations and future trends in the LLM era.\n\n\n\n2.\n\nWe collect insights from senior domain experts in the area of LLM and personal agents, proposing a generic system architecture and a definition of intelligence levels for personal LLM agents.\n\n\n\n3.\n\nWe review the literature on three important technical aspects of personal LLM agents, including foundamental capabilities, efficiency, and security & privacy.\n\n\n\n\n", "Introduction\nLarge Language Models (LLMs) arguably open the way to general purpose systems. Indeed, the latest among\nthem (OpenAI, 2023; Anthropic, 2023; Anil et al., 2023; Touvron et al., 2023) are fluent, knowledgeable,\naligned to some extent with human preferences (Ouyang et al., 2022), and can be augmented (Mialon et al.,\n2023) with tools such as web browsers or code interpreters in a zero or few-shot setting (Brown et al., 2020).\nHowever, evaluating these systems is an open problem: given their emerging new capabilities, LLMs are\nregularly breaking AI benchmarks, at an ever-increasing rate (Kiela et al., 2023).\nIn search for more challenging benchmarks, current trend suggests to seek tasks that are ever more difficult\nfor humans, and challenge LLMs with more intricate educational assessments, for example in STEM and\nLaw, or target more complex realisations, such as writing a coherent book. But, tasks that are difficult for\nhumans are not necessarily difficult for recent systems: the challenging MMLU or GSM8k benchmarks for\nexample (Hendrycks et al., 2021; Cobbe et al., 2021) are already close to be solved,1due to rapid LLM\nimprovement possibly combined with data contamination.2Furthermore, open-ended generation generally\nrequires human or model-based evaluation (Zheng et al., 2023). Human evaluation will become less and less\nfeasible when increasing the task complexity, e.g.in terms of output length or required skills: how to evaluate a\nbook generated by an AI, or solutions to maths problems that few people in the world can solve? Model-based\nevaluations on the other hand are by construction dependent of stronger models hence cannot evaluate\nnew state-of-the-art models, without mentioning potential subtle biases such as preferring the first choice\npresented (Zheng et al., 2023). Overall, evaluating new AI systems requires to rethink benchmarks (Chollet,\n2019).\n1GPT4 does 86.4% on MMLU. Human non-specialist accuracy on the benchmark is only 34.5% Expert-level human performance\nis estimated at 89.8%.\n2See for example the case of Hellaswag.\n1arXiv:2311.12983v1  [cs.CL]  21 Nov 2023Level 1\nQuestion: What was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris\npatients from Jan-May 2018 as listed on the NIH website?\nGround truth: 90\nLevel 2\nQuestion: If this whole pint is made up of ice cream, how many percent above\nor below the US federal standards for butterfat content is it when using the\nstandards as reported by Wikipedia in 2020? Answer as + or - a number rounded\nto one decimal place.\nGround truth: +4.6\nLevel 3\nQuestion: In NASA\u2019s Astronomy Picture of the Day on 2006 January 21, two astronauts are visible,\nwith one appearing much smaller than the other. As of August 2023, out of the astronauts in the\nNASA Astronaut Group that the smaller astronaut was a member of, which one spent the least time\nin space, and how many minutes did he spend in space, rounded to the nearest minute? Exclude any\nastronauts who did not spend any time in space. Give the last name of the astronaut, separated from\nthe number of minutes by a semicolon. Use commas as thousands separators in the number of minutes.\nGround truth: White; 5876\nFigure 1 Sample GAIA questions. Completing the tasks requires fundamental abilities such as reasoning, multi-\nmodality handling, or tool use proficiency. Answers are unambiguous and by design unlikely to be found in plain text\nin training data. Some questions come with additional evidence, such as images, reflecting real use cases and allowing\nbetter control on the questions.\nAlternatively to tasks that are harder for humans, AI systems could be asked to solve conceptually simple\ntasks yet that require accurate execution of complex sequences of actions, with large combinatorial spaces.\nThe output could only be obtained upon successful completion of the task and be easy to validate, analogous\nto the Proof of Work algorithm (Jakobsson and Juels, 1999; Dwork and Naor, 1993), where a computer is\nasked to solve a complex problem whose solution is easy to verify. Tasks for AI assistants, given their need for\naccess to a diverse and uncertain world, meet this criterion while being inherently rooted in practical use\ncases.\nWe move in that direction by proposing GAIA , a benchmark for General AI Assistants featuring 466 carefully\ncrafted questions and their answer, along with the associated design methodology. Our questions are easy\nto create, challenging for AI systems\u2014for LLMs, most require complex generations\u2014, yet admit a unique,\nfactual answer, allowing a simple and robust automatic evaluation.\nGAIA attempts to avoid current pitfalls of LLMs evaluation by targeting:\n-Real-world and challenging questions. For example, a LLM will typically need to browse the open and\nchanging web, handle multi-modality, or reason over multiple steps to answer our questions. Conversely,\nmany LLM benchmarks are quite specific and/or restricted to closed and synthetic environments.\n-Easy interpretability through conceptually simple tasks\u2014non experts annotators exhibit a near perfect\nscore\u2014, associated reasoning trace, and few but highly curated questions. This is in contrast with\naggregated benchmarks that can lack efficiency and reliability (Perlitz et al., 2023).\n-Non-gameability. Answering the questions requires successful completion of some number of steps, which\ncannot easily be brute forced due to their diversity. The possibility to check the reasoning trace, the\naccuracy required in the answers, their absence in plain text from the internet prevent a possible data\ncontamination. In contrast, multiple choice answers ( e.g., MMLU) make contamination assessment more\ndifficult since a wrong reasoning trace can more easily get to the correct choice.\n-Simplicity of use. Crucially, the answers to our questions are factoid, concise and unambiguous. These\n2properties allow simple, fast and factual evaluation. Our questions are meant to be answered in zero\nshot, limiting the influence of the evaluation setup. By opposition, many LLM benchmarks require\nevaluations that are sensitive to the experimental setup such as the number and nature of prompts\n(Liang et al., 2022b) (Section 8.2), or the benchmark implementation.3\nIn spite of being successful at tasks that are difficult for humans, the most capable LLMs do poorly on GAIA .\nEven equipped with tools, GPT4 does not exceed a 30% success rate for the easiest of our tasks, and 0% for\nthe hardest. In the meantime, the average success rate for human respondents is 92%. Consequently, a system\ncapable of solving GAIA can be assessed in the context of t-AGI,4noting that humans typically take between\n6 minutes for the simplest questions to 17 minutes for the most complex ones. From a related perspective,\nsuch system would arguably be a competent General AI within the framework recently proposed in Morris\net al. (2023), which also appear to be the next milestone in AI research since ChatGPT (OpenAI, 2023) is\none level below. This paper covers the composition of GAIA , its design choices, and explain how to craft\nquestions and the associated challenges so that the community can further extend the benchmark to target\nemerging questions such as safety associated to tool use, or multi-modality. We also analyse the successes\nand shortcomings of some of the most capable assistants to date, illustrating the potential of augmenting\nLLMs. We release a developer set of 166 annotated questions and release the remaining 300 questions without\nannotations: the benchmark will be notably hosted as a leaderboard. We hope our methodology will help\naddressing the problem of open ended generation evaluation in NLP and beyond, and believe the successful\nresolution of GAIA would be an important milestone towards the next generation of AI systems.\n2", "Methods in Natural Language\nProcessing , pp. 2369\u20132380, Brussels, Belgium, 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259 .\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\nreal-world web interaction with grounded language agents. Advances in Neural Information Pro-\ncessing Systems , 35:20744\u201320757, 2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan\nCao. React: Synergizing reasoning and acting in language models. In The Eleventh International\nConference on Learning Representations , 2023.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414 , 2022.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\nYonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building\nautonomous agents. arXiv preprint arXiv:2307.13854 , 2023. URL https://webarena.dev .\n13Preprint\nA H YPER -PARAMETERS\nHyperparameters AgentLM-7B AgentLM-13B AgentLM-70B\nNumber of Layers 32 40 80\nHidden size 4,096 5,120 8,192\nFFN hidden size 11,008 13,824 28,672\nAttention heads 32 40 64\nHidden-Dropout 0.05 0.05 0.05\nAttention Dropout 0 0 0\nWarmup Ratio 0.02 0.02 0.02\nDecay Ratio 0.9 0.9 0.9\nPeak Learning Rate 5e-5 5e-5 1e-5\nBatch Size 64 64 64\nWeight Decay 0.1 0.1 0.1\nLearning Rate Decay Cosine Cosine Cosine\nAdam \u03f5 1e-8 1e-8 1e-8\nAdam \u03b21 0.9 0.9 0.9\nAdam \u03b22 0.95 0.95 0.95\nGradient Clipping 1.0 1.0 1.0\nTable 6: Hyper-parameters for AgentLM training\nFor AgentLM", "\n\n1 Introduction\n\nTool learning\u00a0(Qin et\u00a0al., 2023b) aims to unleash the power of large language models (LLMs) to effectively interact with various tools (APIs) to accomplish complex tasks. By integrating LLMs with APIs, we can greatly expand their utility and empower them to serve as efficient intermediaries between users and the vast ecosystem of applications. Although open-source LLMs, e.g., LLaMA\u00a0(Touvron et\u00a0al., 2023a), have achieved versatile capabilities through instruction tuning\u00a0(Taori et\u00a0al., 2023; Chiang et\u00a0al., 2023), they still lack the sophistication in performing higher-level tasks, such as appropriately interacting with tools (APIs) to fulfill complex human instruction.\nThis deficiency is because current instruction tuning largely focuses on basic language tasks, with a relative neglect of the tool-use domain.\nOn the other hand, current state-of-the-art (SOTA) LLMs (e.g., ChatGPT\u00a0(OpenAI, 2022) and GPT-4\u00a0(OpenAI, 2023)), which have demonstrated impressive competencies in utilizing tools\u00a0(Bubeck et\u00a0al., 2023), are closed-source with their inner mechanisms opaque. This limits the democratization of AI technologies and the scope of community-driven innovation and development.\nIn this regard, we deem it urgent to empower open-source LLMs to skillfully master diverse APIs.\n\n\n\nFigure 1: \nThree phases of constructing ToolBench and how we train our API retriever and ToolLLaMA. During inference of an instruction, the API retriever recommends relevant APIs to ToolLLaMA, which performs multiple rounds of API calls to derive the final answer. The whole reasoning process is evaluated by ToolEval.\n\n\n\n\nAlthough prior works have explored building instruction tuning data for tool use\u00a0(Li et\u00a0al., 2023a; Patil et\u00a0al., 2023; Tang et\u00a0al., 2023; Xu et\u00a0al., 2023b), they fail to fully stimulate the tool-use capabilities within LLMs and have inherent limitations: (1) limited APIs: they either fail to involve real-world APIs (e.g., RESTAPI)\u00a0(Patil et\u00a0al., 2023; Tang et\u00a0al., 2023) or consider only a small scope of APIs with poor diversity\u00a0(Patil et\u00a0al., 2023; Xu et\u00a0al., 2023b; Li et\u00a0al., 2023a);\n\n\nFigure 2: \nPass rate (\u2191\u2191\\uparrow\u2191) and win rate (\u2191\u2191\\uparrow\u2191) of different methods in tool-use evaluation. For win rate, we compare each method with ChatGPT-ReACT. DFSDT is our improved reasoning strategy over ReACT. ToolLLaMA surpasses Text-Davinci-003, Claude-2, and almost performs on par with ChatGPT.\n\n\n\n(2) constrained scenario: existing works are confined to instructions that only involve one single tool. In contrast, real-world scenarios may require that multiple tools are interleaved together for multi-round tool execution to solve a complex task. Besides, they often assume that users manually specify the ideal API set for a given instruction in advance, which is infeasible with a large collection of real-world APIs; (3) inferior planning and reasoning: existing works adopted either CoT\u00a0(Wei et\u00a0al., 2023) or ReACT\u00a0(Yao et\u00a0al., 2022) for model reasoning, which cannot fully elicit the capabilities stored in LLMs and thus fail to handle complex instructions.\nIn addition, some works do not even execute APIs to obtain real responses\u00a0(Patil et\u00a0al., 2023; Tang et\u00a0al., 2023), which serve as important information for subsequent model planning.\n\n\n\nTo facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework including data construction, model training, and evaluation.\nAs illustrated in Figure\u00a01, we collect a high-quality instruction-tuning dataset ToolBench. It is constructed automatically using ChatGPT (gpt-3.5-turbo-16k), which has been upgraded with function call (link) capabilities. The comparison between ToolBench and prior works is listed in Table\u00a01.\nSpecifically, the construction of ToolBench entails three phases:\n\n\n\n\n\n[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]\n\n\n\n\u2022\n\nAPI Collection: we gather 16,464 representational state transfer (REST) APIs from RapidAPI (link), a platform that hosts massive real-world APIs provided by developers.\nThese APIs span \ud835\udfd2\ud835\udfd749\\mathbf{49}bold_49 diverse categories such as social media, e-commerce, and weather. For each API, we crawl detailed API documents from RapidAPI, including the functionality descriptions, required parameters, code snippets for API calls, etc. By comprehending these documents to learn to execute APIs, LLMs can generalize to new APIs unseen during training;\n\n\n\n\u2022\n\nInstruction Generation: we first sample APIs from the whole set and then prompt ChatGPT to generate diverse instructions for these APIs. To cover practical scenarios, we curate instructions that involve both single-tool and multi-tool scenarios. This ensures that our model learns not only how to interact with individual tools but also how to combine them to accomplish complex tasks;\n\n\n\n\u2022\n\nSolution Path Annotation: each solution path may contain multiple rounds of model reasoning and real-time API calls to derive the final response.\nHowever, even the most sophisticated LLM, i.e., GPT-4, achieves a low pass rate for complex human instructions, making annotation inefficient. To this end, we develop a novel depth-first search-based decision tree (DFSDT) to bolster the planning and reasoning ability of LLMs. Compared with conventional ReACT, DFSDT enables LLMs to evaluate a multitude of reasoning paths and make deliberate decisions to either retract steps or proceed along a promising path. In experiments, DFSDT significantly improves the annotation efficiency and successfully completes those complex instructions that cannot be fulfilled using ReACT.\n\n\n\n\n\n\n\n\nResource\n\n \n\n\nToolBench\n\n(this work)\n \n\n\n \n\n\nAPIBench\n\n(Patil et\u00a0al., 2023)\n \n\n\n \n\n\nAPI-Bank\n\n(Li et\u00a0al., 2023a)\n \n\n\n \n\n\nToolAlpaca\n\n(Tang et\u00a0al., 2023)\n \n\n\n \n\n\nToolBench\n\n(Xu et\u00a0al., 2023b)\n \n\n\n\nReal-world API?\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\n\nReal API Call&Response?\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\n\nMulti-tool Scenario?\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n\n\nAPI Retrieval?\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\n\nMulti-step Reasoning?\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\n\n\n\\hdashlineNumber of tools\n\ud835\udfd1\ud835\udfd2\ud835\udfd3\ud835\udfcf3451\\mathbf{3451}bold_3451\n3333\n53535353\n400400400400\n8888\n\n\nNumber of APIs\n\ud835\udfcf\ud835\udfd4\ud835\udfd2\ud835\udfd4\ud835\udfd216464\\mathbf{16464}bold_16464\n1645164516451645\n53535353\n400400400400\n232232232232\n\n\nNumber of Instances\n\ud835\udfcf\ud835\udfd0\ud835\udfd4\ud835\udfd2\ud835\udfd6\ud835\udfd4126486\\mathbf{126486}bold_126486\n17002170021700217002\n274274274274\n3938393839383938\n2746274627462746\n\n\nNumber of Real API Calls\n\ud835\udfd2\ud835\udfd4\ud835\udfd7\ud835\udfd3\ud835\udfd6\ud835\udfd3469585\\mathbf{469585}bold_469585\n00\n568568568568\n00\n3926392639263926\n\n\nAvg. Reasoning Traces\n4.04.04.04.0\n1.01.01.01.0\n2.12.12.12.1\n1.01.01.01.0\n5.95.9\\mathbf{5.9}bold_5.9\n\n\n\nTable 1: \nA comparison of our ToolBench to notable instruction tuning dataset for tool learning.\n\n\n\n\nTo assess the tool-use capabilities of LLMs, we develop an automatic evaluator, ToolEval, backed up by ChatGPT. It comprises two key metrics: (1) pass rate, which measures LLM\u2019s ability to successfully execute an instruction within limited budgets, and (2) win rate, which compares the quality and usefulness of two solution paths. We demonstrate that ToolEval achieves a high correlation with human evaluation and provides a robust, scalable, and reliable assessment for machine tool use.\n\n\nBy fine-tuning LLaMA on ToolBench, we obtain ToolLLaMA. After evaluation based on our ToolEval, we derive the following findings:\n\n\n\n\n\n[topsep=1pt, partopsep=1pt, leftmargin=12pt, itemsep=-1pt]\n\n\n\n\u2022\n\nToolLLaMA demonstrates a compelling capability to handle both single-tool and complex multi-tool instructions. As depicted in Figure\u00a02, ToolLLaMA outperforms Text-Davinci-003 and Claude-2, achieves comparable performance to the \u201cteacher model\u201d ChatGPT, and is only slightly inferior to GPT4.\nBesides, ToolLLaMA exhibits robust generalization to previously unseen APIs, requiring only the API documentation to adapt to new APIs effectively. This flexibility allows users to incorporate novel APIs seamlessly, thus enhancing the model\u2019s practical utility.\n\n\n\n\u2022\n\nWe show that our DFSDT serves as a general decision-making strategy to enhance the reasoning capabilities of LLMs. DFSDT broadens the search space by considering multiple reasoning traces and achieves significantly better performance than ReACT.\n\n\n\n\u2022\n\nWe train a neural API retriever, which alleviates the need for manual selection from the large API pool in practice. As shown in Figure\u00a01, given an instruction, the API retriever recommends a set of relevant APIs, which are sent to ToolLLaMA for multi-round decision making to derive the final answer.\nDespite sifting through a large pool of APIs, the retriever exhibits remarkable retrieval precision, returning APIs closely aligned with the ground truth.\n\n\n\n\u2022\n\nToolLLaMA exhibits strong generalization performance on an out-of-distribution (OOD) dataset APIBench\u00a0(Patil et\u00a0al., 2023). Despite not training on any of the APIs or instructions on APIBench, ToolLLaMA performs on par with Gorilla, a pipeline specifically designed for APIBench.\n\n\n\n\n", "\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated superior performance in a myriad of NLP tasks\u00a0[3, 7, 37, 36, 47, 54]. These models have captured vast amounts of knowledge from enormous and diverse corpora during pre-training. After instruction fine-tuning [8, 38, 1], they have demonstrated impressive capabilities in information-seeking question answering\u00a0[57, 23]. Despite their remarkable performance, LLMs face several challenges. For example, they are susceptible to hallucinations\u2014generating plausible yet ungrounded information\u2014which can mislead users and affect content integrity [58, 17, 4]. Additionally, they exhibit weaknesses in numerical reasoning, an essential skill in numerous real-life applications [12, 31, 35, 25, 43, 11]. These limitations highlight the need for techniques that can enhance LLMs\u2019 question-answering abilities.\n\n\nRecent research has shown that these issues can be mitigated by augmenting LLMs with external tools, such as retrieval augmentation [50, 15], math tools [48, 66, 28], and code interpreters [11, 55]. For example, a Wolfram math plugin can enhance numerical reasoning [60], and a verified database can mitigate hallucinations by providing up-to-date fact-checked knowledge [42].\nHowever, existing evaluation methodologies struggle to distinguish whether the model is simply recalling pre-trained information or truly utilizing external tools for problem-solving [32].\nThis challenge arises, in part, because the external data used for evaluation may have already been exposed to LLMs during the pre-training phase [45].\nThis exposure can lead to a biased evaluation of LLMs\u2019 tool-use abilities, as the models could just use their ingrained knowledge and their reasoning abilities, bypassing the use of external tools.\nAs a result, these evaluations cannot accurately reflect the true competency of the models.\nWe need a fair and explicit way to check if LLMs are really good at problem-solving with tools or if they are just using their memorized information.\n\n\nFigure 1: Pre-trained on vast range of corpus, LLMs possess extensive knowledge, which may overlap with evaluation data. This overlap poses a significant challenge to current evaluation methods, as it becomes difficult to discern whether the model is merely recalling pre-trained information or genuinely employing external tools for problem-solving.\n\n\nTo fill this gap, we introduce ToolQA, a question answering (QA) benchmark to evaluate LLMs\u2019 ability in using external tools for answering questions. ToolQA comprises data from 8 domains and defines 13 types of tools to acquire information from external reference corpora.\nEach instance in ToolQA consists of a question, an answer, reference corpora, and a list of available tools.\nToolQA is unique in that all its questions can be answered only by using appropriate tools to obtain information from the reference corpus.\nThis minimizes the possibility of LLMs answering questions by merely recalling their internal knowledge, and allows for faithfully evaluating LLMs\u2019 abilities in using tools.\n\n\nToolQA is curated with an automated three-phase process: (1) The first phase, Reference Data Collection, involves gathering various types of public corpora including text, tables, and graphs from different domains.\nThese corpora have no overlap with the LLM pre-training data and will serve as\nreference corpora for tool-based question answering. (2) The second phase is Human-guided Question Generation with LLMs.\nIn this phase, we generate questions that can only be answered by using tools over the reference corpora.\nOur approach is a template-based question generation process, which includes human-guided template generation, template validation, and question instantiation with tool attributes. (3) The third phase is Programmatic Answer Generation.\nThis phase produces accurate answers for the generated questions.\nTo ensure answer correctness, we implement operators corresponding to the tools and obtain answers from the reference corpora programmatically.\nOur three-phase procedure ensures that we generate questions that can only be answered using external knowledge, along with their precise answers.\nAdditionally, the process is highly efficient and requires minimal human labeling efforts.\n\n\nWe conducted experiments using both standard LLMs and tool-augmented LLMs to answer questions in ToolQA. Our findings indicate that ChatGPT and Chain-of-thoughts prompting\u00a0[57], which rely solely on their internal knowledge, have low success rates of approximately 5% for easy questions and 2% for hard questions. In contrast, tool-augmented LLMs such as Chameleon\u00a0[28] and ReAct\u00a0[66] perform better by leveraging external tools. For easy questions, the best performance achieved by tool-augmented LLMs is 43.15%, while for hard questions, the best performance drops to 8.2%. Our results and error analysis demonstrate that ToolQA is a challenging benchmark for existing tool-augmented LLM methods, especially for its hard questions that require more complex reasoning about tool composition.\n\n", "Introduction\nRecent advances in large language models (LLMs) [ 10,5,32,6,29,30] have enabled significant new\ncapabilities including natural dialogue, mathematical reasoning, and program synthesis. However,\ndespite these advances, LLMs are still fundamentally limited by the information they can store in a\nfixed set of weights and the things they can compute using a static computation graph and limited\ncontext. Furthermore, as the world changes, LLMs require retraining to update their knowledge and\nreasoning capabilities.\nBy empowering LLMs to use tools [ 33], we can grant access to vastly larger and changing knowledge\nbases and accomplish complex computational tasks. By providing access to search technologies and\ndatabases, [ 26,39,37] demonstrated that we can augment LLMs to address a significantly larger\nand more dynamic knowledge space. Similarly, by providing access to computational tools, [ 39,2]\ndemonstrated that LLMs can accomplish complex computational tasks. Consequently, leading LLM\nproviders[ 29], have started to integrate plugins to allow LLMs to invoke external tools through APIs.\nThis transition from a small set of hand-coded tools, to the ability to invoke a vast space of changing\ncloud APIs could transform LLMs into the primary interface to computing infrastructure and the web.\nTasks ranging from booking an entire vacation to hosting a conference, could become as simple as\ntalking to an LLM that has access to the flight, car rental, hotel, catering, and entertainment web\nAPIs. However, much of the prior work [ 35,24] integrating tools into LLMs considered a small well\ndocumented set of APIs that can be easily injected into the prompt.\n\u2217Equal contribution.\nPreprint. Under review.arXiv:2305.15334v1  [cs.CL]  24 May 2023GPT-4<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model=  torch.hub.load('snakers4/silero-models', 'asr', source='local')result = asr_model.transcribe(audio_path)Claude<domain>:Audio-Translation<api_provider>:Pytorch<code>:import torchaudiotranslation = Torchaudio.pipelines.WAV2VEC2_ASR_PIPELINE(\"audio.wav\")Gorilla<domain>:Speech-to-Text<api_provider>:TorchHub<code>:asr_model =  torch.hub.load('snakers4/silero-models', 'silero_sst\u2019)result = asr_model.transcribe(audio_path)\nHallucinate!Wrong library!Good to go!Prompt: Help me find an API to convert the spoken language in a recorded audio to text using Torch Hub. Figure 1: Examples of API calls . Example API calls generated by GPT-4 [ 29], Claude [ 3], and Gorilla for the\ngiven prompt. In this example, GPT-4 presents a model that doesn\u2019t exist, and Claude picks an incorrect library.\nIn contrast, our Gorilla model can identify the task correctly and suggest a fully-qualified API call.\nBetter\nFigure 2: Accuracy (vs) hallucination in four settings, that is, zero-shot (i.e., without any retriever), and with\nretrievers .BM25 andGPTare commonly used retrievers and the oracle retriever returns relevant documents\nat 100%, indicating an upper bound. Higher in the graph (higher accuracy) and to the left is better (lower\nhallucination). Across the entire dataset, our model, Gorilla, improves accuracy while reducing hallucination.\nSupporting a web scale collection of potentially millions of changing APIs requires rethinking our\napproach to how we integrate tools. It is not longer possible to describe the full set of APIs in a\nsingle context. Many of the APIs will have overlapping functionality with nuanced limitations and\nconstraints. Simply evaluating LLMs in this new setting requires new benchmarks.\nIn this paper, we explore the use of self-instruct fine-tuning and retrieval to enable LLMs to accu-\nrately select from a large, overlapping, and changing set tools expressed using their APIs and API\ndocumentation. We construct, APIBench, a large corpus of APIs with complex and often overlapping\nfunctionality by scraping ML APIs (models) from public model hubs. We choose three major model\nhubs for dataset construction: TorchHub, TensorHub and HuggingFace. We exhaustively include\nevery API call in TorchHub (94 API calls) and TensorHub (696 API calls); For HuggingFace, since\nthe models come in a large number and lots of the models don\u2019t have a specification, we choose the\nmost downloaded 20 models per task category (in a total of 925). We also generate 10 synthetic\nuser question prompts per API using Self-Instruct [ 42]. Thus, each entry in the dataset becomes an\ninstruction reference API pair. We adopt a common AST sub-tree matching technique to evaluate the\nfunctional correctness of the generated API. We first parse the generated code into an AST tree, then\nfind a sub-tree whose root node is the API call that we care about (e.g., torch.hub.load ) and use it\nto index our dataset. We check the functional correctness and hallucination problem for the LLMs,\nreporting the corresponding accuracy.\nWe then finetune Gorilla, a LLaMA-7B-based model with document retrieval using our dataset. We\nfind that Gorilla significantly outperforms GPT-4 in terms of API functionality accuracy as well as\nreducing hallucination errors. We show an example output in Fig. 1. Further, our retrieval-aware\ntraining of Gorilla enables the model to adapt to changes in the API documentation. Finally, we\ndemonstrate Gorilla\u2019s ability to understand and reason about constraints.\n22"], "bleu": 0.0, "rouge_l": 0.3284210526315789, "gpt_metric_score": 1.0, "bert_score": 0.2615630626678467}
