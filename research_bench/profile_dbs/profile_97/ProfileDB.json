{
  "c1f51693-95fd-48a9-b0ea-1cddff164397": {
    "pk": "c1f51693-95fd-48a9-b0ea-1cddff164397",
    "project_name": null,
    "name": "Philipp Spitzer",
    "bio": "I am a researcher dedicated to enhancing human-AI interaction systems, particularly in the context of effective collaboration between human and AI agents. My work focuses on understanding and estimating the perceived difficulty of tasks for both humans and AI, which is crucial for optimizing their collaboration. I have explored how human-AI collaboration (HAIC) can facilitate the transfer of task-specific expert knowledge (TSEK) from subject matter experts to novices, addressing the pressing challenge of knowledge retention in organizations facing workforce changes.\n\nMy research also delves into the potential of machine learning (ML) models to create scalable IT-based teaching systems that preserve expert knowledge. I have conducted systematic literature reviews to identify key dimensions of data understanding, which are essential for organizations to derive meaningful insights from complex datasets. Additionally, I investigate the role of explainable AI (XAI) in training novices, demonstrating how XAI can enhance learning by providing interpretable examples and explanations.\n\nThrough my studies, I have uncovered the implications of incorrect explanations in AI-assisted decision-making, revealing how they can hinder human performance and procedural knowledge. My recent work emphasizes the importance of human learning in achieving complementary team performance (CTP) in human-AI collaborations, providing insights into how to design systems that foster effective reliance on AI recommendations. Overall, my research aims to bridge the gap between human expertise and AI capabilities, paving the way for more effective and efficient collaboration in various domains.",
    "collaborators": [
      "Niklas K\u00fchl",
      "Gerhard Satzger",
      "Joshua Holstein",
      "Michael V\u00f6ssing",
      "Marc Goutier",
      "Daniel Heinz",
      "Dominik Martin",
      "Marieke Hoell",
      "Manuel Kaschura",
      "Katelyn Morrison"
    ],
    "domain": [
      "Human-AI Interaction",
      "Explainable AI",
      "Knowledge Transfer",
      "Machine Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "6b626130-eac3-4b75-89e5-649d01276dfb": {
    "pk": "6b626130-eac3-4b75-89e5-649d01276dfb",
    "project_name": null,
    "name": "Joshua Holstein",
    "bio": "I am a researcher dedicated to enhancing human-AI interaction and understanding the complexities of big data analytics. My work addresses the challenges organizations face in processing vast amounts of data, emphasizing the importance of a comprehensive understanding of both data and its domain. Through a systematic literature review, I identified five key dimensions of data understanding, providing a framework that guides organizations in extracting meaningful insights from complex datasets.\n\nIn the realm of human-AI collaboration, I focus on the nuances of perceived difficulty between human and AI agents. My research highlights the need for effective interaction systems that accurately reflect each agent's capabilities, paving the way for improved collaboration. I have also investigated the implications of explainable AI (XAI), revealing how incorrect explanations can lead to flawed reasoning and hinder team performance. \n\nAdditionally, I explore the critical area of anomaly detection, proposing methods to support human experts in validating detected anomalies through counterfactual explanations. My findings demonstrate the potential of explainable anomaly detection to enhance decision-making processes.\n\nOverall, my research aims to bridge the gap between advanced AI capabilities and effective human collaboration, providing actionable insights for designing systems that leverage the strengths of both humans and AI.",
    "collaborators": [
      "Niklas K\u00fchl",
      "Philipp Spitzer",
      "Michael V\u00f6ssing",
      "Gerhard Satzger",
      "Marieke Hoell",
      "Katelyn Morrison",
      "Kenneth Holstein",
      "Max Schemmer",
      "Niklas Bauer",
      "Patrick Hemmer"
    ],
    "domain": [
      "Human-AI Interaction",
      "Explainable AI",
      "Anomaly Detection",
      "Data Understanding"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "125a0255-3b87-4c8a-92d8-29a5e5de3046": {
    "pk": "125a0255-3b87-4c8a-92d8-29a5e5de3046",
    "project_name": null,
    "name": "Katelyn Morrison",
    "bio": "I am a researcher dedicated to exploring the intersection of machine learning, human-computer interaction, and sociotechnical systems. My work critically examines how machine learning algorithms, particularly in urban mobility initiatives, can inadvertently perpetuate social inequalities. Through my research, I advocate for algorithmic fairness, emphasizing the need to eliminate discrimination in systems like bike-sharing programs.\n\nI also investigate the role of AI in enhancing collaboration, particularly through tools like Microsoft\u2019s Viva Daily Briefing Email. By employing mixed methods, I analyze how knowledge workers interact with AI-powered reminders, aiming to improve asynchronous collaboration and task management.\n\nMy recent studies delve into the robustness of emerging architectures like vision transformers compared to traditional convolutional neural networks. I have found that these newer models exhibit greater resilience to data corruption, which has significant implications for their application in real-world scenarios.\n\nAdditionally, I focus on the importance of explainable AI (XAI) in decision-making processes. My research highlights the potential pitfalls of incorrect explanations, revealing how they can mislead users and impair team performance. By conducting extensive user studies, I provide insights into the complexities of human-AI collaboration and offer guidelines for designing more effective AI systems.\n\nOverall, my work aims to bridge the gap between advanced AI technologies and their practical, equitable application in society, ensuring that these systems serve all communities fairly and effectively.",
    "collaborators": [
      "Philipp Spitzer",
      "Niklas K\u00fchl",
      "Shamsi Iqbal",
      "Eric Horvitz",
      "Benjamin Gilby",
      "Colton Lipchak",
      "Adam Mattioli",
      "Adriana Kovashka",
      "Violet Turri",
      "Michelle Feng"
    ],
    "domain": [
      "Machine Learning",
      "Explainable AI",
      "Human-Computer Interaction",
      "Algorithmic Fairness"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "889e5c88-0572-42d6-a94c-6c394e13db6b": {
    "pk": "889e5c88-0572-42d6-a94c-6c394e13db6b",
    "project_name": null,
    "name": "Kenneth Holstein",
    "bio": "I am a researcher dedicated to exploring the intersection of human-AI collaboration, particularly in educational and public sector contexts. My recent work focuses on designing systems that enhance the partnership between teachers and AI, exemplified by my development of Lumilo, smart glasses that provide real-time analytics to support K-12 educators. Through field studies, I have demonstrated that effective human-AI collaboration can significantly improve student learning outcomes.\n\nI am deeply concerned about the equitable deployment of AI in education and public services. My research critically examines how AI systems can inadvertently amplify existing inequities and explores pathways toward more equitable AI solutions. I advocate for participatory design approaches that involve practitioners and stakeholders throughout the research process, ensuring that the systems we create are responsive to real-world needs.\n\nIn addition to my work on educational AI, I investigate the complexities of human decision-making in the presence of AI. I have conducted studies on how to effectively communicate unobservable factors in AI models and the implications of incorrect explanations in AI-assisted decision-making. My goal is to bridge the gap between technical AI capabilities and the nuanced understanding required for effective human-AI collaboration.\n\nOverall, I strive to contribute to the design of responsible AI systems that not only enhance decision-making but also promote equity and inclusivity in their deployment. My research agenda is driven by a commitment to understanding and improving the ways humans and AI can work together in meaningful and impactful ways.",
    "collaborators": [
      "Amanda Coston",
      "Hoda Heidari",
      "Anna Kawakami",
      "Haiyi Zhu",
      "Luke Guerdan",
      "Zhiwei Steven Wu",
      "Motahhare Eslami",
      "Frederic Gmeiner",
      "Nikolas Martelaro",
      "Vincent Aleven"
    ],
    "domain": [
      "Human-AI Collaboration",
      "Educational Technology",
      "Explainable AI",
      "Algorithmic Fairness"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "fd61ef46-9dba-4f10-8880-e946a91ed343": {
    "pk": "fd61ef46-9dba-4f10-8880-e946a91ed343",
    "project_name": null,
    "name": "Gerhard Satzger",
    "bio": "I am a researcher dedicated to exploring the intersection of artificial intelligence (AI), machine learning (ML), and innovation management. My work primarily focuses on enhancing decision-making processes through the integration of AI technologies while addressing the challenges that arise from their implementation. I have developed methodologies for automated customer needs elicitation from social media, demonstrating how user-generated content can inform product development in a scalable manner.\n\nMy research also delves into the complexities of human-AI collaboration, particularly the dynamics of reliance on AI advice. I have conceptualized frameworks to measure the appropriateness of reliance (AoR) on AI recommendations, emphasizing the importance of distinguishing between reliance behavior and decision quality. Through empirical studies, I have shown how explainable AI (XAI) can mitigate automation bias and enhance decision-making outcomes.\n\nAdditionally, I have investigated the implications of concept drift in machine learning models, proposing innovative strategies for model adaptation in dynamic environments. My work on transfer machine learning has highlighted the potential for leveraging existing analytical knowledge across different entities, thereby improving forecasting accuracy.\n\nOverall, my research aims to bridge the gap between theoretical insights and practical applications, providing organizations with the tools and frameworks necessary to harness the power of AI while ensuring effective human oversight and collaboration. I am passionate about contributing to the development of intelligent systems that not only enhance productivity but also empower knowledge workers in their decision-making processes.",
    "collaborators": [
      "Niklas K\u00fchl",
      "Max Schemmer",
      "Michael V\u00f6ssing",
      "Patrick Hemmer",
      "Johannes Jakubik",
      "Robin Hirt",
      "Carina Benz",
      "Lucas Baier",
      "Marc Goutier",
      "Jan Scheurenbrand"
    ],
    "domain": [
      "Artificial Intelligence",
      "Machine Learning",
      "Human-AI Collaboration",
      "Decision Support Systems"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "6cf4d6f4-ca66-475d-baf2-9b60a5fea0d2": {
    "pk": "6cf4d6f4-ca66-475d-baf2-9b60a5fea0d2",
    "project_name": null,
    "name": "Niklas K\u00fchl",
    "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, which has proven effective in various prediction tasks, achieving significant performance improvements.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy gains across multiple prediction benchmarks. Recognizing the challenges posed by dynamic graphs, I proposed the ROLAND framework, which allows static GNNs to be adapted for dynamic environments, enhancing their scalability and performance.\n\nIn addition to architectural advancements, I have explored the intricate relationship between neural network structures and their predictive performance through relational graphs. My work aims to systematically study the design space of GNNs, providing guidelines for selecting optimal architectures for specific tasks. I am also passionate about improving the efficiency of automated machine learning (AutoML) methods, as demonstrated by my development of FALCON and AutoTransfer, which leverage design graphs to streamline the search for optimal model configurations.\n\nOverall, my research is driven by a desire to push the boundaries of GNNs and contribute to a deeper understanding of their potential in various domains.",
    "collaborators": [],
    "domain": [
      "Graph Neural Network",
      "Machine Learning",
      "Multi-task Learning",
      "AutoML"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}
