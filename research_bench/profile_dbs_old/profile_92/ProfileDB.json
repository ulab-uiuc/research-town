{
  "a6b4ceb1-bfc0-4416-8ff4-b09cd0334a2e": {
    "pk": "a6b4ceb1-bfc0-4416-8ff4-b09cd0334a2e",
    "project_name": null,
    "name": "Stefan Haufe",
    "bio": "I am a researcher dedicated to advancing the field of explainable artificial intelligence (XAI) and its applications in psychology and neuroscience. My work focuses on addressing the challenges of interpreting complex machine learning models, particularly in high-stakes domains like healthcare. I have developed novel methodologies to improve statistical power in analyzing hierarchically-organized data, emphasizing the importance of intra-subject variance in effect size estimation.\n\nMy research has critically evaluated popular XAI methods, revealing their limitations in providing reliable explanations. I have crafted benchmark datasets and frameworks, such as GECO and the Explainable AI Comparison Toolkit (EXACT), to objectively assess the performance of XAI techniques against ground truth data. This work highlights the need for rigorous evaluation and formal definitions of feature importance to ensure the reliability of model interpretations.\n\nAdditionally, I have contributed to the understanding of functional brain connectivity through innovative techniques like Sparsely-Connected Sources Analysis (SCSA) and have explored causal inference in multivariate time series using advanced statistical models. My goal is to bridge the gap between complex machine learning models and human interpretability, ensuring that the insights derived from these models can be trusted and effectively utilized in critical decision-making processes.",
    "collaborators": [
      "Rick Wilming",
      "Benedict Clark",
      "Danny Panknin",
      "Klaus-Robert M\u00fcller",
      "C\u00e9line Budding",
      "Marta Oliveira",
      "Guido Nolte",
      "Klaus-Robert Mueller",
      "Fabian Eitel",
      "Kerstin Ritter"
    ],
    "domain": [
      "Explainable AI",
      "Causal Inference",
      "Neuroimaging",
      "Statistical Methods"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "bdf8a139-81ca-4687-ab5f-67a943c363e3": {
    "pk": "bdf8a139-81ca-4687-ab5f-67a943c363e3",
    "project_name": null,
    "name": "Rick Wilming",
    "bio": "As a researcher in the field of explainable artificial intelligence (XAI), my work focuses on bridging the gap between complex machine learning models and their interpretability. I have critically examined the limitations of existing XAI methods, particularly their tendency to misattribute importance to features that lack statistical relevance, such as suppressor variables. My recent studies have involved crafting benchmark datasets that serve as ground truth for evaluating the performance of various XAI techniques across different model architectures, including deep neural networks and convolutional neural networks (CNNs).\n\nI have developed rigorous evaluation frameworks, such as GECOBench, to assess how biases in large pre-trained language models can affect model explanations. My findings reveal significant dependencies between explanation performance and model training strategies, emphasizing the importance of fine-tuning in mitigating undesirable biases. Through my research, I aim to provide a solid theoretical foundation for XAI methods, ensuring that they can be reliably used for quality control and transparency in high-stakes decision-making contexts.\n\nUltimately, my goal is to enhance the understanding of how machine learning models operate, making them more transparent and trustworthy. I am committed to advancing the field of XAI by developing methodologies that not only improve interpretability but also ensure the correctness of the explanations provided by these complex models.",
    "collaborators": [
      "Stefan Haufe",
      "Benedict Clark",
      "C\u00e9line Budding",
      "Marta Oliveira",
      "Leo Kieslich",
      "Klaus-Robert M\u00fcller",
      "Fabian Eitel",
      "Kerstin Ritter",
      "Artur Dox",
      "Hjalmar Schulz"
    ],
    "domain": [
      "Explainable AI",
      "Machine Learning",
      "Natural Language Processing",
      "Model Interpretability"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "cce55807-f0d3-4d6a-a758-ab1e9b2e8e7e": {
    "pk": "cce55807-f0d3-4d6a-a758-ab1e9b2e8e7e",
    "project_name": null,
    "name": "Benedict Clark",
    "bio": "I am a researcher dedicated to advancing the field of explainable artificial intelligence (XAI), with a focus on ensuring that complex machine learning models are interpretable and trustworthy. My work addresses the critical gap between model complexity and human understandability, particularly in high-stakes domains like medicine. I have developed benchmark datasets and quantitative metrics to rigorously evaluate the performance of various XAI methods, revealing that many popular techniques often fail to provide meaningful insights and can misattribute importance to irrelevant features.\n\nMy recent research includes crafting a gender-controlled text dataset, GECO, which allows for objective evaluation of XAI methods applied to large pre-trained language models. This work highlights the impact of biases in model explanations and emphasizes the importance of fine-tuning in improving explanation quality. I advocate for a more structured approach to XAI, urging researchers to define clear problems and establish objective criteria for evaluating explanation correctness. By doing so, I aim to enhance the reliability of XAI methods, ultimately contributing to the responsible deployment of machine learning in critical applications. My commitment to transparency and quality control in AI continues to drive my research endeavors.",
    "collaborators": [
      "Rick Wilming",
      "Stefan Haufe",
      "Marta Oliveira",
      "Leo Kieslich",
      "C\u00e9line Budding",
      "Fabian Eitel",
      "Kerstin Ritter",
      "Rustam Zhumagambetov",
      "Danny Panknin",
      "Ahc\u00e8ne Boubekki"
    ],
    "domain": [
      "Explainable AI",
      "Machine Learning",
      "Natural Language Processing",
      "Model Evaluation"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "3820943a-a36d-402e-b20f-5c56da05c075": {
    "pk": "3820943a-a36d-402e-b20f-5c56da05c075",
    "project_name": null,
    "name": "Rustam Zhumagambetov",
    "bio": "As a researcher deeply engaged in the intersection of machine learning and explainable artificial intelligence (XAI), I focus on the critical need for transparency and accountability in high-stakes applications, particularly in fields like medicine. My work critically examines the current state of XAI, revealing that many popular methods fall short in providing reliable insights into machine learning models. I argue that these methods often misattribute importance to input features that do not correlate with the prediction target, which undermines their utility for model validation, improvement, and scientific discovery.\n\nI advocate for a paradigm shift in how we approach XAI: researchers must first clearly define the problems they aim to solve and then develop methods that are rigorously evaluated against objective criteria. This approach will not only enhance the quality of explanations but also establish metrics that can be validated against ground-truth data. My goal is to contribute to the development of XAI frameworks that genuinely support human understanding and oversight in machine learning, ensuring that these powerful tools can be safely and effectively integrated into critical decision-making processes.",
    "collaborators": [
      "Stefan Haufe",
      "Rick Wilming",
      "Benedict Clark",
      "Danny Panknin",
      "Ahc\u00e8ne Boubekki"
    ],
    "domain": [
      "Explainable AI",
      "Machine Learning",
      "Model Validation",
      "Human-Computer Interaction"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "2d56c665-557d-4f6b-b176-a537ab7af56c": {
    "pk": "2d56c665-557d-4f6b-b176-a537ab7af56c",
    "project_name": null,
    "name": "Danny Panknin",
    "bio": "I am a researcher dedicated to enhancing machine learning methodologies, particularly in the realms of model selection, active learning, and causal inference. My work addresses the challenges posed by large datasets and the complexities of real-world data, focusing on developing efficient and robust techniques that improve predictive performance while minimizing computational demands.\n\nOne of my notable contributions is an improved cross-validation procedure that leverages nonparametric testing and sequential analysis, significantly reducing computation time while maintaining accuracy. I have also pioneered a model-agnostic active learning framework that incorporates local structural complexity, enabling more effective learning from limited initial samples.\n\nIn the area of causal inference, I have explored the concept of time-reversed Granger causality, demonstrating its effectiveness in discerning true causal relationships amidst measurement noise. My research extends to the critical domain of explainable artificial intelligence (XAI), where I advocate for a more rigorous approach to evaluating XAI methods to ensure they provide meaningful insights into machine learning models.\n\nAdditionally, I have developed multiple purpose locality sensitive hashing (mp-LSH), which allows for flexible querying across various dissimilarity measures, enhancing the usability of nearest neighbor searches. Through my work, I aim to bridge the gap between theoretical advancements and practical applications, ensuring that machine learning techniques are not only powerful but also interpretable and applicable in high-stakes environments.",
    "collaborators": [
      "Shinichi Nakajima",
      "Klaus-Robert M\u00fcller",
      "Stefan Haufe",
      "Tammo Krueger",
      "Mikio Braun",
      "Stefan Chmiela",
      "Klaus Robert M\u00fcller",
      "Irene Winkler",
      "Daniel Bartz",
      "Rick Wilming"
    ],
    "domain": [
      "Active Learning",
      "Causal Inference",
      "Explainable AI",
      "Machine Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "8759344d-74dd-4780-a495-98e64bb5565c": {
    "pk": "8759344d-74dd-4780-a495-98e64bb5565c",
    "project_name": null,
    "name": "Ahc\u00e8ne Boubekki",
    "bio": "I am a researcher dedicated to advancing the field of explainable artificial intelligence (XAI) and deep learning, with a particular focus on enhancing the interpretability and effectiveness of machine learning models. My work has explored various innovative approaches, such as developing a simultaneous learning framework for deep embedded clustering that integrates autoencoders with Gaussian mixture models, allowing for more effective unsupervised categorization.\n\nI have also introduced pantypes, a new family of prototypical objects that address representation bias in self-explainable classifiers, ensuring that diverse aspects of input distributions are captured. My research on Neuro-Activated Superpixels (NAS) has provided a novel method for isolating relevant input regions without relying on traditional segmentation techniques, thereby improving weakly supervised object localization.\n\nRecognizing the need for transparency in representation learning, I proposed RELAX, the first attribution-based explanation method for learned representations, which incorporates uncertainty to enhance trustworthiness. Additionally, I have contributed to the establishment of the Explainable AI Comparison Toolkit (EXACT), a benchmarking platform that evaluates XAI methods against standardized metrics and datasets.\n\nThrough my work, I aim to address the critical challenges in XAI, advocating for rigorous evaluation and the development of methods that provide meaningful insights into machine learning models. My research not only seeks to improve model interpretability but also to ensure that these advancements are applicable in high-stakes domains, where understanding model decisions is paramount.",
    "collaborators": [
      "Robert Jenssen",
      "Benedict Clark",
      "Rick Wilming",
      "Danny Panknin",
      "Stefan Haufe",
      "Michael Kampffmeyer",
      "Ulf Brefeld",
      "Rune Kj\u00e6rsgaard",
      "Line Clemmensen",
      "Samuel G. Fadel"
    ],
    "domain": [
      "Explainable AI",
      "Deep Learning",
      "Clustering",
      "Representation Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}