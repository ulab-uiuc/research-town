{
  "dcf3be77-5317-4ef8-aa23-d604818881fc": {
    "pk": "dcf3be77-5317-4ef8-aa23-d604818881fc",
    "project_name": null,
    "name": "Orchid Chetia Phukan",
    "bio": "I am a researcher dedicated to advancing the fields of speech processing, emotion recognition, and mental health analysis, with a particular focus on the intersection of technology and social issues. My recent work has centered on developing innovative applications that leverage audio and speech data to address critical challenges, such as early detection of Autism Spectrum Disorder (ASD) through code-switched speech analysis. By employing hierarchical feature fusion methods, I aim to enhance classification accuracy, which is vital for timely intervention.\n\nIn addition to ASD detection, I have explored the efficacy of pre-trained models in speech emotion recognition, demonstrating the superiority of speaker recognition embeddings in capturing nuanced speech characteristics. My research also delves into the mental health implications of childhood sexual abuse, utilizing social media data to identify and classify mental health issues among survivors.\n\nI am passionate about creating systems that not only improve technical performance but also contribute to societal well-being. For instance, my work on audio violence detection and humor detection emphasizes the importance of multimodal approaches, while my studies on substance misuse highlight the need for understanding public sentiment through social media analysis.\n\nOverall, my goal is to bridge the gap between advanced machine learning techniques and real-world applications, ensuring that my research has a meaningful impact on individuals and communities facing various challenges.",
    "collaborators": [
      "Arun Balaji Buduru",
      "Rajesh Sharma",
      "Gautam Siddharth Kashyap",
      "Muskaan Singh",
      "Yashwardhan Chaudhuri",
      "Sarthak Jain",
      "Mohd Mujtaba Akhtar",
      "Girish",
      "Ankit Kumar",
      "Alexander E. I. Brownlee"
    ],
    "domain": [
      "Speech Processing",
      "Emotion Recognition",
      "Machine Learning",
      "Autism Detection"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "1a1332b6-b344-4e3c-87c6-de007bc2deda": {
    "pk": "1a1332b6-b344-4e3c-87c6-de007bc2deda",
    "project_name": null,
    "name": "Sarthak Jain",
    "bio": "I am a researcher with a strong focus on the intersection of communication networks, machine learning, and natural language processing (NLP). My work spans a variety of topics, including the analysis of Gaussian half-duplex diamond relay networks, where I investigate the contributions of individual relays to network capacity. I have also delved into the interpretability of attention mechanisms in NLP models, revealing that attention weights often do not correlate with model outputs as expected, which challenges the notion of transparency in these systems.\n\nIn the realm of healthcare, I explore the integration of electronic medical records (EMRs) with machine learning to enhance predictive performance while ensuring model interpretability. My research includes developing probabilistic group testing methods for distributed computing, which efficiently identify unreliable workers in a network of nodes.\n\nI am particularly interested in community-based group testing and its applications in biomedical research, where I have proposed algorithms that outperform existing methods. My work also extends to document-level information extraction, where I introduced the SciREX dataset to facilitate advancements in this area.\n\nAdditionally, I have contributed to understanding model predictions through influence functions and have developed methods to identify training data artifacts, which are crucial for improving model robustness. My goal is to create models that not only perform well but also provide meaningful insights and explanations, ultimately advancing the fields of communication networks and NLP.",
    "collaborators": [
      "Byron C. Wallace",
      "Soheil Mohajer",
      "Martina Cardone",
      "Pouya Pezeshkpour",
      "Sameer Singh",
      "Ramin Mohammadi",
      "Jinghan Yang",
      "Sandra E. Safo",
      "Madeleine van Zuylen",
      "Hannaneh Hajishirzi"
    ],
    "domain": [
      "Machine Learning",
      "Natural Language Processing",
      "Information Theory",
      "Group Testing"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "4afe2e41-1e9e-4301-95f7-dc1a16ab040d": {
    "pk": "4afe2e41-1e9e-4301-95f7-dc1a16ab040d",
    "project_name": null,
    "name": "Swarup Ranjan Behera",
    "bio": "I am a researcher dedicated to advancing the field of audio and sports data analysis through innovative visualization and machine learning techniques. My recent work has focused on leveraging unstructured data, particularly cricket commentary, to extract and visualize player strengths and weaknesses, enabling the development of tailored strategies for athletes. I have also made significant contributions to spectral clustering, introducing a framework that integrates pairwise constraints into semidefinite spectral clustering, enhancing its applicability in real-world scenarios.\n\nIn the realm of audio analysis, I have developed FastAST, a framework that optimizes audio classification models for efficiency without sacrificing accuracy. My research extends to multilingual audio-visual question answering (AVQA), where I introduced the MERA framework to facilitate AVQA across multiple languages, significantly reducing the need for manual annotations.\n\nI am particularly passionate about mental health applications, having created FuSeR, a novel framework for depression detection from speech that combines non-semantic features to achieve state-of-the-art performance. My exploration of foundation models has led to advancements in environmental audio deepfake detection and singing voice deepfake detection, where I demonstrated the effectiveness of speaker recognition models.\n\nThrough my work, I aim to bridge the gap between complex data structures and practical applications, providing open access to datasets and code to foster collaboration and further research in these critical areas.",
    "collaborators": [
      "Orchid Chetia Phukan",
      "Arun Balaji Buduru",
      "Rajesh Sharma",
      "S. R Mahadeva Prasanna",
      "Vijaya V. Saradhi",
      "Aalekhya Satya Narayani",
      "Girish",
      "Mohd Mujtaba Akhtar",
      "Vijaya V Saradhi",
      "Abhishek Dhiman"
    ],
    "domain": [
      "Audio Processing",
      "Sports Analytics",
      "Machine Learning",
      "Multimodal Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "130597c2-07a9-4fe3-ae6c-f215c18530f4": {
    "pk": "130597c2-07a9-4fe3-ae6c-f215c18530f4",
    "project_name": null,
    "name": "Arun Balaji Buduru",
    "bio": "I am a researcher dedicated to advancing the fields of audio processing, machine learning, and their applications in real-world scenarios. My recent work focuses on multilingual content analysis, where I developed a real-time spoken language detection system that achieves 91.8% accuracy with minimal data requirements. I have also explored the intersection of cybersecurity and machine learning, proposing a stochastic approach using Markov Decision Processes to predict risky states in critical cloud infrastructures.\n\nIn the realm of generative models, I introduced imdpGAN, a differentially private GAN that balances privacy and data specificity, showcasing its effectiveness on various datasets. My research extends to adversarial robustness, where I conducted a comprehensive study of black-box adversarial attacks and defenses, emphasizing the need for secure deep learning models.\n\nI am particularly passionate about emotion recognition and stress detection, leveraging pre-trained models to enhance performance across multiple languages and contexts. My work on audio deepfake detection and audio abuse detection highlights the importance of multilingual models in improving robustness and generalizability.\n\nAdditionally, I have developed innovative frameworks like CoLLAB for seamless model merging across languages and VoxMed, a UI-assisted classifier for respiratory disease diagnosis using digital stethoscope recordings. My goal is to create impactful solutions that address pressing challenges in healthcare, security, and communication, ultimately improving user experiences and outcomes in diverse applications.",
    "collaborators": [
      "Orchid Chetia Phukan",
      "Rajesh Sharma",
      "Saurabh Gupta",
      "Yashwardhan Chaudhuri",
      "Mudit Verma",
      "Ponnurangam Kumaraguru",
      "Siddhant Bhambri",
      "Ankit Kumar",
      "Sarthak Jain",
      "Gautam Siddharth Kashyap"
    ],
    "domain": [
      "Speech Processing",
      "Machine Learning",
      "Audio Analysis",
      "Cybersecurity"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "75bf4e1b-b120-4662-a976-5675d162f4d9": {
    "pk": "75bf4e1b-b120-4662-a976-5675d162f4d9",
    "project_name": null,
    "name": "Rajesh Sharma",
    "bio": "I am a researcher with a strong focus on the intersection of social media, misinformation, and machine learning. My recent work has delved into the dynamics of rumor spread on platforms like Twitter, where I developed a Graph Convolutional Network (GCN) approach to identify potential rumor spreaders, achieving notable performance metrics such as an F1-Score of 0.864. I am particularly interested in the implications of misinformation, especially during crises like the COVID-19 pandemic, where I proposed a mobility-based SIR model to understand epidemic spread, integrating population distribution and connectivity factors.\n\nIn addition to my work on misinformation, I have explored hate speech classification, developing metrics to quantify the severity of hate terms and employing Stable Hate Rule mining to visualize co-occurring hate terms. My research also extends to the analysis of online conversations, where I found that toxic comments can perpetuate further toxicity in discussions.\n\nI am passionate about leveraging advanced machine learning techniques, including deep learning and graph-based methods, to tackle pressing societal issues. My framework, DEAP-FAKED, exemplifies this by combining natural language processing with knowledge graph encoding to detect fake news effectively. Through my research, I aim to contribute to the development of scalable solutions that can mitigate the impact of misinformation and promote healthier online discourse.",
    "collaborators": [
      "Shakshi Sharma",
      "Rajendra Bhatia",
      "Anwitaman Datta",
      "Rahul Goel",
      "Navedanjum Ansari",
      "Animesh Chaturvedi",
      "Vigneshwaran Shankaran",
      "Mohit Mayank"
    ],
    "domain": [
      "Graph Neural Network",
      "Misinformation Detection",
      "Machine Learning",
      "Social Media Analysis"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "79a0ed7f-6e26-4407-9cd3-6f500d99ad28": {
    "pk": "79a0ed7f-6e26-4407-9cd3-6f500d99ad28",
    "project_name": null,
    "name": "S. R Mahadeva Prasanna",
    "bio": "I am a researcher dedicated to advancing the field of environmental audio deepfake detection (EADD) and exploring the capabilities of foundation models in audio analysis. My recent work has focused on optimizing the use of these models by addressing the challenges posed by high-dimensional representations. I discovered that instead of relying solely on traditional dimensionality reduction techniques, randomly selecting a subset of representation values can maintain or even enhance model performance while significantly reducing computational demands.\n\nIn my exploration of singing voice deepfake detection (SVDD), I conducted a comprehensive comparison between music foundation models (MFMs) and speech foundation models (SFMs). My findings revealed that SFMs, particularly those trained for speaker recognition, excel in capturing the nuances of singing voices. This led to the development of FIONA, a novel framework that synergizes the strengths of both MFMs and SFMs, achieving state-of-the-art results in SVDD.\n\nAdditionally, I have investigated the potential of multimodal foundation models for emotion recognition from non-verbal sounds. By leveraging the joint pre-training of these models, I proposed MATA, a framework that enhances emotion recognition by aligning representations across modalities. My work has consistently pushed the boundaries of performance on benchmark datasets, demonstrating the effectiveness of innovative approaches in audio analysis. I am passionate about harnessing the power of foundation models to tackle complex challenges in audio processing and contribute to the growing field of deepfake detection and emotion recognition.",
    "collaborators": [
      "Orchid Chetia Phukan",
      "Swarup Ranjan Behera",
      "Arun Balaji Buduru",
      "Rajesh Sharma",
      "Girish",
      "Mohd Mujtaba Akhtar",
      "Nitin Choudhury",
      "Sarthak Jain",
      "Sishir Kalita"
    ],
    "domain": [
      "Audio Deepfake Detection",
      "Foundation Models",
      "Emotion Recognition",
      "Multimodal Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}