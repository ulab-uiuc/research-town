{
  "19ca4f25-64e9-4a10-91d1-3e4acc55edc0": {
    "pk": "19ca4f25-64e9-4a10-91d1-3e4acc55edc0",
    "project_name": null,
    "name": "Homanga Bharadhwaj",
    "bio": "I am a researcher dedicated to advancing the fields of robotics, machine learning, and artificial intelligence, with a particular focus on developing systems that can learn and adapt in complex environments. My work spans a variety of topics, including explainable AI for recommendation systems, safe exploration in reinforcement learning, and the integration of human-like manipulation skills into robotic systems.\n\nIn my recent publications, I have explored the use of layer-wise relevance propagation to enhance the interpretability of deep learning models in recommendation systems, demonstrating how to extract meaningful features from images. I have also proposed frameworks like MANGA for transferring policies across different environments, which is crucial for real-world robotic applications. My research emphasizes the importance of safety in robot learning, advocating for robust auditing methods to ensure that autonomous systems align with human intentions.\n\nI am particularly interested in leveraging large-scale human video data to train robots for zero-shot manipulation tasks, allowing them to generalize across various objects and scenes without extensive retraining. My work on learning visual affordances and goal-conditioned policies aims to enable robots to explore and interact with their environments more effectively.\n\nThrough my research, I strive to bridge the gap between theoretical advancements and practical applications, ensuring that the next generation of intelligent systems is both capable and safe. I am excited about the potential of my work to contribute to the development of autonomous robots that can seamlessly integrate into human environments.",
    "collaborators": [
      "Animesh Garg",
      "Florian Shkurti",
      "Abhinav Gupta",
      "Shubham Tulsiani",
      "Kevin Xie",
      "Samarth Sinha",
      "Vikash Kumar",
      "Sergey Levine",
      "Shruti Joshi",
      "Nisheeth Srivastava"
    ],
    "domain": [
      "Reinforcement Learning",
      "Explainable AI",
      "Robot Learning",
      "Deep Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "50ebf770-224f-41ec-a8cc-1990654119cc": {
    "pk": "50ebf770-224f-41ec-a8cc-1990654119cc",
    "project_name": null,
    "name": "Debidatta Dwibedi",
    "bio": "I am a researcher dedicated to advancing the fields of self-supervised learning, robotics, and computer vision. My recent work focuses on developing innovative algorithms and frameworks that enhance the efficiency and effectiveness of machine learning models, particularly in scenarios with limited labeled data. For instance, I introduced Q-Match, a self-supervised learning method that leverages student-teacher distribution matching to improve classification performance on tabular datasets without any labeled data during pre-training.\n\nI have also explored the intersection of robotics and deep learning, creating systems like the Deep Cuboid Detector, which localizes 3D objects in cluttered scenes using consumer-quality images. My research extends to teaching robots through observation, where I developed Time-Contrastive Networks to learn task-agnostic representations for continuous control tasks, significantly improving performance in real-world applications.\n\nIn addition, I have contributed to the development of scalable data collection methods, such as ALOHA 2 and RoboVQA, which facilitate the gathering of diverse datasets for training robust robotic systems. My work emphasizes the importance of self-supervised learning and representation learning, enabling robots to generalize across tasks and adapt to new environments with minimal human intervention.\n\nOverall, my research aims to bridge the gap between theoretical advancements and practical applications, pushing the boundaries of what is possible in robotics and machine learning. I am passionate about creating systems that can learn from their environments and improve their performance through experience, ultimately leading to more intelligent and autonomous machines.",
    "collaborators": [
      "Jonathan Tompson",
      "Pierre Sermanet",
      "Yusuf Aytar",
      "Andrew Zisserman",
      "Pete Florence",
      "Ayzaan Wahid",
      "Ted Xiao",
      "Fei Xia",
      "Dorsa Sadigh",
      "Corey Lynch"
    ],
    "domain": [
      "Self-Supervised Learning",
      "Robotics",
      "Computer Vision",
      "Reinforcement Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "c2460cce-ebea-4077-9e52-f733518dc6f7": {
    "pk": "c2460cce-ebea-4077-9e52-f733518dc6f7",
    "project_name": null,
    "name": "Abhinav Gupta",
    "bio": "I am a researcher deeply engaged in the field of computer vision, particularly focusing on object detection and representation learning. My work has evolved from developing innovative algorithms for object detection, such as the online hard example mining (OHEM) technique, which significantly enhances training efficiency by prioritizing challenging examples, to exploring unsupervised learning methods that leverage vast amounts of unlabeled data, including videos from the web.\n\nI have pioneered approaches that integrate fine-grained details into detection architectures through top-down modulations, achieving state-of-the-art results on benchmarks like COCO. My research also delves into the intersection of generative models and visual representation, where I introduced the S^2-GAN framework to disentangle structure and style in image generation, enhancing interpretability and realism.\n\nMore recently, I have been investigating self-supervised learning techniques that surpass traditional supervised methods, revealing the importance of viewpoint and category invariance in object recognition. My work emphasizes the potential of multi-task learning, where I introduced the \"cross-stitch\" unit to effectively share representations across tasks, leading to improved performance even with limited training data.\n\nThrough my research, I aim to push the boundaries of what is possible in computer vision, making significant strides in both theoretical understanding and practical applications. I am passionate about creating models that not only perform well but also provide insights into the underlying mechanisms of visual perception.",
    "collaborators": [
      "Abhinav Shrivastava",
      "Xiaolong Wang",
      "Radim Bartos",
      "Xinlei Chen",
      "Senthil Purushwalkam",
      "Aayush Bansal",
      "Carl Doersch",
      "Ross Girshick",
      "Rahul Sukthankar",
      "Jitendra Malik"
    ],
    "domain": [
      "Object Detection",
      "Deep Learning",
      "Multi-task Learning",
      "Generative Models"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "55065310-02a1-472b-a96c-b95d9e29fea3": {
    "pk": "55065310-02a1-472b-a96c-b95d9e29fea3",
    "project_name": null,
    "name": "Shubham Tulsiani",
    "bio": "I am a researcher dedicated to advancing the fields of computer vision and 3D reconstruction, with a particular focus on pose estimation and object recognition. My work spans a variety of innovative approaches, including the development of convolutional neural network architectures that enhance pose and keypoint predictions by leveraging viewpoint estimates. I have explored generative models that infer spatial signals from sparse samples, enabling diverse outputs across various domains.\n\nMy recent contributions include SparseFusion, a novel approach that unifies neural rendering and probabilistic image generation for 3D reconstruction, and a framework for learning single-view shape and pose prediction without direct supervision. I have also pioneered methods for inferring 3D representations from single images, utilizing multi-view supervisory signals to capture hidden aspects of scenes.\n\nI am particularly interested in the relationships between objects in a scene, which has led me to develop techniques that incorporate pairwise relations to improve object-level pose estimates. My research also addresses the challenges of learning from unstructured image collections, allowing for scalable 3D shape and pose inference across numerous categories.\n\nThrough my work, I aim to bridge the gap between 2D observations and 3D understanding, ultimately contributing to more robust and efficient systems for real-world applications. My research is driven by a commitment to pushing the boundaries of what is possible in 3D vision and robotics, and I am excited to continue exploring new methodologies that enhance our understanding of complex visual environments.",
    "collaborators": [
      "Abhinav Gupta",
      "Jitendra Malik",
      "Jo\u00e3o Carreira",
      "Nilesh Kulkarni",
      "Alexei A. Efros",
      "Abhishek Kar",
      "Yufei Ye",
      "Zhizhuo Zhou",
      "Richard Tucker",
      "Noah Snavely"
    ],
    "domain": [
      "3D Reconstruction",
      "Pose Estimation",
      "Computer Vision",
      "Generative Models"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "a7aca6f5-0714-4bb7-be04-4dff7c7ac429": {
    "pk": "a7aca6f5-0714-4bb7-be04-4dff7c7ac429",
    "project_name": null,
    "name": "Carl Doersch",
    "bio": "I am a researcher deeply engaged in the intersection of computer vision, machine learning, and robotics, with a particular focus on understanding and predicting motion in complex environments. My work spans a variety of topics, including Variational Autoencoders (VAEs) for unsupervised learning, self-supervised visual representation learning, and bridging the sim2real gap in 3D human pose estimation. \n\nI have developed innovative models like the Action Transformer for recognizing and localizing human actions in video clips, and the Tracking Any Point (TAP) framework, which allows for precise tracking of points on physical surfaces throughout video sequences. My research also explores the use of spatial context as a supervisory signal for training rich visual representations, and I have introduced benchmarks like TAPVid-3D to evaluate long-range point tracking in 3D.\n\nI am particularly interested in how structured representations and reasoning can enhance the performance of deep reinforcement learning agents in physical construction tasks. My recent work, Gen2Act, leverages human video generation to enable robot manipulation policies to generalize to novel tasks, showcasing the potential of combining web data with robotic learning.\n\nThrough my research, I aim to push the boundaries of what is possible in visual understanding and manipulation, contributing to the development of intelligent systems that can learn from and interact with the world in meaningful ways.",
    "collaborators": [
      "Andrew Zisserman",
      "Jo\u00e3o Carreira",
      "Abhinav Gupta",
      "Ankush Gupta",
      "Yi Yang",
      "Mateusz Malinowski",
      "Dilara Gokay",
      "Yusuf Aytar",
      "Rohit Girdhar",
      "Skanda Koppula"
    ],
    "domain": [
      "Computer Vision",
      "Deep Learning",
      "Variational Autoencoders",
      "Self-Supervised Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "5912f1c1-9473-4b6c-97c0-926049c1a9f8": {
    "pk": "5912f1c1-9473-4b6c-97c0-926049c1a9f8",
    "project_name": null,
    "name": "Ted Xiao",
    "bio": "As a researcher in the field of robotics and imitation learning, I am deeply interested in understanding the challenges of generalization in visual robotic manipulation. My recent work focuses on dissecting the various factors that influence a robot's ability to generalize from learned behaviors to new environments. I approach this complex question by examining how different environmental variables\u2014such as lighting conditions and camera placements\u2014affect the performance of imitation learning policies.\n\nTo tackle this, I have developed a novel simulated benchmark comprising 19 tasks with 11 distinct factors of variation. This framework allows for controlled evaluations and provides insights into which factors pose the greatest challenges for generalization. Through empirical studies conducted both in simulation and on real robots, I have been able to quantify the impact of these factors and establish a consistent ordering of their difficulty. My goal is to contribute to the development of more robust robotic systems that can adapt to diverse real-world scenarios, ultimately enhancing their utility and effectiveness in practical applications.",
    "collaborators": [
      "Annie Xie",
      "Lisa Lee",
      "Chelsea Finn"
    ],
    "domain": [
      "Imitation Learning",
      "Robotics",
      "Generalization",
      "Computer Vision"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "be1ebc98-ef70-4ddd-8f03-4167f9f3c86d": {
    "pk": "be1ebc98-ef70-4ddd-8f03-4167f9f3c86d",
    "project_name": null,
    "name": "Dhruv Shah",
    "bio": "I am a researcher dedicated to advancing the fields of robotics, machine learning, and automated systems. My recent work focuses on developing innovative algorithms and frameworks that enhance robotic navigation and exploration in complex environments. For instance, I introduced ViKiNG, a method that integrates learning and planning, allowing robots to navigate using side information like GPS and satellite maps, even in previously unseen environments. This approach demonstrates the potential of leveraging learned experiences to make informed decisions in real-time.\n\nIn addition to navigation, I have explored the intersection of machine learning and social compliance in robotics. My research on socially unobtrusive navigation aims to train robots to interact with humans without disrupting their natural behavior, utilizing a large dataset of human-robot interactions to inform policy development.\n\nI also work on enhancing the efficiency of robotic learning through frameworks like ExAug, which augments experiences across different robot platforms, enabling better generalization and adaptability. My commitment to improving robotic systems extends to the development of RESTGPT, a tool that utilizes large language models to enhance REST API testing by extracting meaningful rules from natural language descriptions.\n\nOverall, my research is driven by a passion for creating intelligent systems that can navigate, learn, and interact effectively in dynamic and complex environments, ultimately contributing to the advancement of autonomous technologies.",
    "collaborators": [
      "Sergey Levine",
      "Ajay Sridhar",
      "Noriaki Hirose",
      "Saumya Borwankar",
      "Benjamin Eysenbach",
      "Gregory Kahn",
      "Nicholas Rhinehart",
      "Arjun Bhorkar",
      "Vivek Borkar",
      "Sebastian Scherer"
    ],
    "domain": [
      "Robotics",
      "Machine Learning",
      "Computer Vision",
      "Natural Language Processing"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "dffc04a6-8fea-4510-9df3-47fba4f9a3b4": {
    "pk": "dffc04a6-8fea-4510-9df3-47fba4f9a3b4",
    "project_name": null,
    "name": "Fei Xia",
    "bio": "I am a researcher dedicated to advancing natural language processing (NLP) and machine learning, with a particular focus on parsing, translation, and network data analysis. My recent work has centered on enhancing supertagging and constituency parsing through innovative approaches like attentive graph convolutional networks and span attention mechanisms. I believe that effectively modeling contextual information is crucial for improving parsing accuracy, and my experiments have consistently demonstrated state-of-the-art performance across multiple languages.\n\nIn addition to parsing, I have developed tools like NLPStatTest to provide a more comprehensive framework for evaluating NLP system performance, moving beyond traditional p-value significance testing to include effect size and power analysis. My contributions also extend to low-resource neural machine translation, where I led a winning system in a competitive challenge by leveraging novel techniques such as bilingual curriculum learning and a new Incomplete-Trust loss function.\n\nMy research also explores relational topic models for network data, where I introduced enhancements to improve model expressiveness and inference accuracy. Furthermore, I have worked on QoS-aware runtime controllers for distributed web servers, optimizing energy consumption while maintaining service quality.\n\nMost recently, I have developed HRL4IN, a hierarchical reinforcement learning architecture tailored for interactive navigation tasks, which significantly improves task performance and energy efficiency. My work is driven by a passion for creating practical solutions that push the boundaries of what is possible in NLP and machine learning.",
    "collaborators": [
      "Yuanhe Tian",
      "Yan Song",
      "Tong Zhang",
      "Haotian Zhu",
      "Denise Mak",
      "Jesse Gioannini",
      "Bin Li",
      "Yixuan Weng",
      "Hanjun Deng",
      "Ning Chen"
    ],
    "domain": [
      "Natural Language Processing",
      "Graph Neural Network",
      "Reinforcement Learning",
      "Machine Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "3e78c40a-e954-43b4-94d9-5061c88f6146": {
    "pk": "3e78c40a-e954-43b4-94d9-5061c88f6146",
    "project_name": null,
    "name": "Dorsa Sadigh",
    "bio": "I am a researcher dedicated to advancing the intersection of robotics, artificial intelligence, and human-robot interaction. My work primarily focuses on developing algorithms that enable robots to learn from human feedback, adapt to dynamic environments, and effectively communicate with users. One of my recent contributions is the introduction of Probabilistic Signal Temporal Logic (PrSTL), which allows for the synthesis of safe controllers in cyber-physical systems by incorporating stochastic properties derived from sensor data.\n\nI have also explored the challenges of natural language generation (NLG) evaluation, proposing BLEU Neighbors as a novel method for estimating language quality, which outperforms traditional metrics and human annotators. My research in preference-based reinforcement learning has led to the development of Inverse Preference Learning (IPL), a parameter-efficient algorithm that learns from offline preference data without the need for complex reward functions.\n\nIn addition, I have worked on multi-agent systems, creating frameworks that enable robots to imitate human behavior in cooperative and competitive settings. My recent projects include Predicting Latent Affordances Through Object-Centric Play (PLATO), which enhances manipulation skills through an object-centric view of human play data, and ELLA, a reward shaping approach that improves sample efficiency in sparse-reward environments by correlating high-level instructions with low-level actions.\n\nThrough my research, I aim to create intelligent systems that not only perform tasks effectively but also understand and align with human intentions, ultimately fostering better collaboration between humans and robots.",
    "collaborators": [
      "Erdem B\u0131y\u0131k",
      "Joey Hejna",
      "Nima Anari",
      "Siddharth Karamcheti",
      "Ashish Kapoor",
      "Kawin Ethayarajh",
      "Zhangjie Cao",
      "Hengyuan Hu",
      "Suneel Belkhale",
      "Jiaming Song"
    ],
    "domain": [
      "Reinforcement Learning",
      "Human-Robot Interaction",
      "Natural Language Processing",
      "Robotics"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "c2786676-2d44-4962-8b71-1a3354c2b2bc": {
    "pk": "c2786676-2d44-4962-8b71-1a3354c2b2bc",
    "project_name": null,
    "name": "Sean Kirmani",
    "bio": "I am a researcher dedicated to enhancing the capabilities of robots in understanding and interacting with the physical world, particularly through the lens of visual question answering (VQA) and manipulation tasks. My recent work focuses on bridging the gap between vision-language models (VLMs) and 3D spatial reasoning, where I developed a framework to generate an extensive dataset for training VLMs on spatial relationships, significantly improving their performance in both qualitative and quantitative spatial reasoning tasks.\n\nI also explore innovative approaches to robot manipulation, such as Gen2Act, which leverages human video generation to enable robots to generalize to novel tasks with unseen object types. This method allows for efficient training with minimal real-world data, showcasing the potential of using web data for robotic learning. My work on RT-Sketch introduces hand-drawn sketches as a goal specification modality, providing a more intuitive and flexible way for users to communicate tasks to robots.\n\nAdditionally, I address the challenges of sim-to-real transfer in robotics through the development of SIMPLER, a collection of simulated environments that accurately reflect real-world policy behavior. My research emphasizes the importance of leveraging pre-trained vision-language models to enable robots to understand and act upon instructions involving novel object categories, as demonstrated in my Manipulation of Open-World Objects (MOO) framework.\n\nThrough these contributions, I aim to push the boundaries of robotic capabilities, making them more adaptable and intuitive in real-world scenarios. My work is driven by a passion for creating intelligent systems that can seamlessly interact with their environments and understand human intentions.",
    "collaborators": [
      "Ted Xiao",
      "Dorsa Sadigh",
      "Fei Xia",
      "Quan Vuong",
      "Jiayuan Gu",
      "Karol Hausman",
      "Chelsea Finn",
      "Boyuan Chen",
      "Zhuo Xu",
      "Brian Ichter"
    ],
    "domain": [
      "Visual Question Answering",
      "Robotics",
      "Machine Learning",
      "Vision-Language Models"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}