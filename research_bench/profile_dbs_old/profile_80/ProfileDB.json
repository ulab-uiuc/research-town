{
  "b0cc576c-f897-4f61-b170-20b57d847f8b": {
    "pk": "b0cc576c-f897-4f61-b170-20b57d847f8b",
    "project_name": null,
    "name": "Kento Kawaharazuka",
    "bio": "I am a researcher dedicated to advancing robotic capabilities, particularly in the areas of tool manipulation, adaptive control, and dynamic interaction with flexible objects. My work focuses on optimizing robotic tool shapes and trajectories to enhance task performance, as demonstrated in my studies on robotic object manipulation and laparoscopic surgery. I have developed innovative methods such as the Deep Predictive Model with Parametric Bias (DPMPB) to address complex modeling challenges and temporal changes in robotic systems.\n\nMy research also explores the integration of imitation learning and visual servoing, enabling robots to adaptively learn from human demonstrations and autonomously adjust to their environments. I have implemented these concepts in various robotic platforms, including musculoskeletal humanoids and low-rigidity robots, to achieve complex behaviors like seated walking and dynamic manipulation of flexible materials.\n\nAdditionally, I am passionate about creating versatile robotic systems that can operate in diverse environments, from industrial settings to personal assistance. My work on modular robots and the Generalized Multisensory Correlational Model (GeMuCo) reflects my commitment to developing robots that can learn and adapt their body schemas based on real-time experiences.\n\nThrough my research, I aim to bridge the gap between human-like adaptability and robotic precision, ultimately contributing to the evolution of intelligent robotic systems capable of performing intricate tasks in dynamic and unpredictable environments.",
    "collaborators": [
      "Kei Okada",
      "Masayuki Inaba",
      "Naoaki Kanazawa",
      "Toru Ogawa",
      "Cota Nabeshima",
      "Yoshiki Obinata",
      "Tasuku Makabe",
      "Yoichiro Kawamura",
      "Akihiro Miki",
      "Masahiro Bando"
    ],
    "domain": [
      "Robotics",
      "Imitation Learning",
      "Deep Learning",
      "Tool Manipulation"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "ef280a3f-92a7-4b76-a84a-db97a4af1461": {
    "pk": "ef280a3f-92a7-4b76-a84a-db97a4af1461",
    "project_name": null,
    "name": "Yoshiki Obinata",
    "bio": "I am a researcher dedicated to enhancing robotic state recognition and interaction through innovative applications of vision-language models. My recent work focuses on leveraging Visual Question Answering (VQA) within Pre-Trained Vision-Language Models (PTVLMs) to intuitively describe and recognize various states in robotic environments. By employing genetic algorithms to optimize question combinations, I have developed systems capable of recognizing complex states, such as the open/closed status of doors and the state of water, which have traditionally posed challenges.\n\nI have also explored the emotional dimensions of robotic interactions, creating a diary generation system that utilizes shared experiences between humans and robots to foster intimacy and improve user perception. My research extends to the integration of foundation models for executing General Purpose Service Robot (GPSR) tasks, where I successfully led a team to victory in the RoboCup@home Japan Open 2022.\n\nMy work emphasizes the importance of continuous state recognition, particularly in dynamic environments like cooking, where I have proposed methods to track food state changes using vision-language models. I aim to bridge the gap between advanced vision-language models and practical robotic applications, ensuring that robots can navigate and operate effectively in diverse settings without extensive retraining or manual programming.\n\nThrough my research, I strive to make robots more intuitive and capable of understanding and interacting with their environments, ultimately enhancing their utility in everyday life.",
    "collaborators": [
      "Kento Kawaharazuka",
      "Kei Okada",
      "Masayuki Inaba",
      "Naoaki Kanazawa",
      "Aiko Ichikura",
      "Iori Yanokura",
      "Koki Shinjo",
      "Naoto Tsukamoto",
      "Soonhyo Kim",
      "Naoya Yamaguchi"
    ],
    "domain": [
      "Robotics",
      "Vision-Language Models",
      "Visual Question Answering",
      "State Recognition"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "8e24780a-ecbb-41d8-b45c-4305e08c1d08": {
    "pk": "8e24780a-ecbb-41d8-b45c-4305e08c1d08",
    "project_name": null,
    "name": "Naoaki Kanazawa",
    "bio": "I am a researcher dedicated to enhancing robotic capabilities through the integration of vision and language. My recent work focuses on leveraging pre-trained vision-language models to improve state recognition and task execution in robots. I have developed innovative methods that utilize Visual Question Answering (VQA) to enable intuitive communication with robots, allowing them to recognize complex states such as the open/closed status of doors or the cooking state of food without the need for extensive retraining.\n\nMy research also explores the challenges faced by low-rigidity robots in grasping and performing tasks like wiping and cooking. By implementing adaptive visual servoing techniques, I enable these robots to learn and adjust their movements in real-time, accommodating changes in their physical state. Additionally, I have investigated the potential of using large-scale vision-language models for continuous state recognition, allowing robots to monitor changes over time, such as the boiling of water or the melting of butter.\n\nI am particularly interested in simplifying robot navigation and task execution by employing open-vocabulary approaches that eliminate the need for prior map construction or complex programming. My work culminated in a successful implementation of a General Purpose Service Robot (GPSR) system, which excelled in the RoboCup@home competition.\n\nThrough my research, I aim to bridge the gap between advanced machine learning techniques and practical robotic applications, making robots more adaptable and capable of understanding and interacting with their environments in a human-like manner.",
    "collaborators": [
      "Kento Kawaharazuka",
      "Kei Okada",
      "Masayuki Inaba",
      "Yoshiki Obinata",
      "Naoto Tsukamoto",
      "Iori Yanokura",
      "Soonhyo Kim",
      "Naoya Yamaguchi",
      "Shingo Kitagawa",
      "Koki Shinjo"
    ],
    "domain": [
      "Robotics",
      "Vision-Language Models",
      "Visual Question Answering",
      "Object Recognition"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "18745c06-0811-4273-a882-bae5301b6428": {
    "pk": "18745c06-0811-4273-a882-bae5301b6428",
    "project_name": null,
    "name": "Kei Okada",
    "bio": "I am a researcher dedicated to advancing robotic perception and manipulation, particularly in complex environments where occlusions and spatial constraints pose significant challenges. My work focuses on developing innovative methods for object segmentation and manipulation, enabling robots to effectively pick and interact with objects in cluttered spaces. \n\nOne of my notable contributions is the development of a system for instance occlusion segmentation, which allows robots to identify and grasp target objects even when they are partially hidden. This system leverages a novel \"relook\" architecture that enhances the model's ability to understand inter-instance relationships, combined with image synthesis techniques to handle new objects without requiring extensive human annotations.\n\nI have also explored real-time multilabel occupancy mapping, significantly improving segmentation accuracy and enabling robots to recognize and manipulate multiple objects in environments with heavy occlusions. My research extends to joint learning frameworks that integrate instance and semantic segmentation, enhancing the robot's ability to perform complex pick-and-place tasks.\n\nIn addition to perception, I have investigated the dynamics of aerial robots, focusing on maneuverability and manipulation through innovative control methods that address singularities in movement. My work on template-based discriminative trackers has introduced a Transformer-based architecture that captures global contextual information, achieving state-of-the-art performance in object tracking.\n\nOverall, my research aims to create more adaptive and intelligent robotic systems capable of navigating and interacting with the real world in a human-like manner. I am passionate about pushing the boundaries of what robots can achieve in dynamic and challenging environments.",
    "collaborators": [
      "Masayuki Inaba",
      "Kentaro Wada",
      "Kento Kawaharazuka",
      "Moju Zhao",
      "Masaki Murooka",
      "Shingo Kitagawa",
      "Tomoki Anzai",
      "Shintaro Inoue"
    ],
    "domain": [
      "Robotics",
      "Computer Vision",
      "Machine Learning",
      "Object Manipulation"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "12fcfd08-9cc1-49c1-ae72-9d497ab8c6b7": {
    "pk": "12fcfd08-9cc1-49c1-ae72-9d497ab8c6b7",
    "project_name": null,
    "name": "Masayuki Inaba",
    "bio": "I am a researcher dedicated to advancing robotic perception and manipulation, particularly in complex environments. My work focuses on developing innovative methods for object segmentation and manipulation, especially in scenarios with occlusions and narrow spaces. I have pioneered techniques such as instance occlusion segmentation and multilabel occupancy mapping, which enable robots to effectively identify and manipulate multiple objects in cluttered settings.\n\nMy recent projects include the design of a robotic system that utilizes a novel \"relook\" architecture for instance segmentation, allowing for the effective handling of both visible and occluded objects. I have also explored joint learning approaches that integrate instance and semantic segmentation, enhancing the performance of robotic pick-and-place tasks.\n\nIn addition to perception, I have investigated the dynamics of low-rigidity robots and the challenges they face during tool manipulation. My research includes developing neural networks that model the complex relationships between joint angles, visual inputs, and tactile feedback, enabling more precise control during tool use.\n\nI am particularly interested in creating adaptive robotic systems that can learn from their experiences and adjust to changes in their environment. My work on the Generalized Multisensory Correlational Model (GeMuCo) exemplifies this goal, allowing robots to autonomously acquire and update their body schema for improved state estimation and control.\n\nThrough my research, I aim to bridge the gap between human-like adaptability and robotic capabilities, ultimately contributing to the development of more intelligent and versatile robotic systems.",
    "collaborators": [
      "Kei Okada",
      "Kento Kawaharazuka",
      "Kentaro Wada",
      "Moju Zhao",
      "Tomoki Anzai",
      "Masaki Murooka",
      "Shingo Kitagawa",
      "Shintaro Inoue",
      "Hirokazu Ishida",
      "Naoki Hiraoka"
    ],
    "domain": [
      "Robotics",
      "Computer Vision",
      "Machine Learning",
      "Manipulation"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}