{
  "fb305584-8ff3-41ad-9d83-4d5c998b88ba": {
    "pk": "fb305584-8ff3-41ad-9d83-4d5c998b88ba",
    "project_name": null,
    "name": "Xingrui Gu",
    "bio": "I am a researcher dedicated to advancing the understanding of human behavior through innovative methodologies in multimodal data fusion and machine learning. My recent work focuses on pain behavior recognition, where I developed a novel approach that integrates statistical correlation analysis with human-centered insights. This methodology not only enhances the effectiveness of multimodal fusion but also emphasizes the importance of explainability in AI, particularly in healthcare settings.\n\nIn my exploration of human movement, I introduced a representation learning method based on causal inference, which allows for a deeper understanding of joint dynamics and complex behaviors. This two-stage framework has proven to outperform traditional models, particularly in detecting protective behaviors, showcasing its robustness and adaptability.\n\nAdditionally, I have investigated driving behaviors from both driver and passenger perspectives, providing valuable insights into the evaluation of autonomous driving. By conducting in-depth interviews and analyzing the nuances of driving assessments, I aim to bridge the gap between driver intentions and passenger perceptions, ultimately informing the design of future autonomous vehicles.\n\nMy work is driven by a commitment to creating personalized, interpretable, and effective solutions that enhance human-centered applications in healthcare and transportation. I believe that understanding the intricacies of human behavior is key to developing intelligent systems that truly serve people's needs.",
    "collaborators": [
      "Zekun Wu",
      "Zhixuan Wang",
      "Irisa Jin",
      "Chuyi Jiang",
      "Erte Wang",
      "Qiang Cui",
      "Leimin Tian",
      "Lianlong Wu",
      "Siyang Song",
      "Chuang Yu"
    ],
    "domain": [
      "Multimodal Fusion",
      "Human Movement Analysis",
      "Explainable AI",
      "Autonomous Driving"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "7906f496-4259-4bc1-bcc1-4730a36a55d9": {
    "pk": "7906f496-4259-4bc1-bcc1-4730a36a55d9",
    "project_name": null,
    "name": "Chuyi Jiang",
    "bio": "As a researcher dedicated to the intersection of machine learning and human movement analysis, I focus on enhancing the interpretability and understanding of complex behaviors through innovative methodologies. My recent work introduces a novel representation learning method grounded in causal inference, which addresses the limitations of traditional movement recognition techniques. By developing a two-stage framework that integrates the Peter-Clark (PC) algorithm with Kullback-Leibler (KL) divergence, I aim to uncover and quantify the causal relationships between human joints.\n\nMy approach not only captures intricate interactions but also produces robust and interpretable representations, which are crucial for applications in intelligent healthcare. In experiments conducted on the EmoPain dataset, my causal graph convolutional network (GCN) demonstrated superior performance compared to conventional GCNs, particularly in detecting protective behaviors. I take pride in the model's resilience to data scale changes, which enhances its reliability in real-world scenarios.\n\nThrough my research, I aspire to advance the field of human motion analysis, paving the way for more adaptive and intelligent healthcare solutions that can better serve individuals in need.",
    "collaborators": [
      "Xingrui Gu",
      "Erte Wang",
      "Zekun Wu",
      "Qiang Cui",
      "Leimin Tian",
      "Lianlong Wu",
      "Siyang Song",
      "Chuang Yu"
    ],
    "domain": [
      "Causal Inference",
      "Movement Recognition",
      "Machine Learning",
      "Human Dynamics"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "4e1bfad5-2405-4d3c-99c3-a6c782fff16a": {
    "pk": "4e1bfad5-2405-4d3c-99c3-a6c782fff16a",
    "project_name": null,
    "name": "Erte Wang",
    "bio": "As a researcher dedicated to the intersection of machine learning and human movement analysis, I focus on enhancing the interpretability and understanding of complex behaviors through innovative methodologies. My recent work introduces a novel representation learning method grounded in causal inference, which addresses the limitations of traditional movement recognition techniques. By developing a two-stage framework that integrates the Peter-Clark (PC) algorithm with Kullback-Leibler (KL) divergence, I aim to uncover and quantify the causal relationships between human joints.\n\nMy approach not only captures intricate interactions but also produces robust and interpretable representations, which are crucial for applications in intelligent healthcare. In experiments conducted on the EmoPain dataset, my causal graph convolutional network (GCN) demonstrated superior performance compared to conventional GCNs, particularly in detecting protective behaviors, with notable improvements in accuracy, F1 score, and recall. Additionally, the model's invariance to data scale changes enhances its reliability in real-world scenarios.\n\nThrough my research, I aspire to advance the field of human motion analysis, paving the way for more adaptive and intelligent healthcare solutions that can better understand and respond to human behaviors.",
    "collaborators": [
      "Xingrui Gu",
      "Chuyi Jiang",
      "Zekun Wu",
      "Qiang Cui",
      "Leimin Tian",
      "Lianlong Wu",
      "Siyang Song",
      "Chuang Yu"
    ],
    "domain": [
      "Causal Inference",
      "Movement Recognition",
      "Machine Learning",
      "Human Dynamics"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "32ac632b-f75c-4128-83a4-57b557becc06": {
    "pk": "32ac632b-f75c-4128-83a4-57b557becc06",
    "project_name": null,
    "name": "Zekun Wu",
    "bio": "I am a researcher dedicated to advancing the fields of artificial intelligence, machine learning, and human-computer interaction, with a particular focus on health management, visual analytics, and stereotype detection. My recent work has led to the development of innovative methodologies such as the RandOm Convolutional KErnel Transforms (ROCKET) and its variant MiniROCKET, which efficiently assess the health status of systems using multi-sensor time-series data. I have also explored the impact of visual highlights on user attention in drone monitoring tasks, leading to the creation of the highlight-informed saliency model (HISM).\n\nIn the realm of multimodal data fusion, I have introduced a framework for pain behavior recognition that integrates statistical relevance with human-centered insights, enhancing the interpretability of AI in healthcare. My research on robotic manipulation has resulted in CalliRewrite, a novel approach that enables robots to replicate calligraphy styles through unsupervised learning techniques.\n\nAdditionally, I have investigated user behavior in visual analytics, revealing insights into how physical actions correlate with visualization tasks. My work on stereotype detection has culminated in the HEARTS framework, which not only improves model performance but also emphasizes explainability and sustainability. By establishing the Multi-Grain Stereotype dataset, I have contributed to the understanding of biases in large language models, ensuring that our AI systems are more accountable and aligned with human values. Through these diverse projects, I aim to bridge the gap between technology and human experience, fostering advancements that are both effective and ethically sound.",
    "collaborators": [
      "Kaiwei Wu",
      "Anna Maria Feit",
      "Xingrui Gu",
      "Zhixuan Wang",
      "Irisa Jin",
      "Yuxuan Luo",
      "Zhouhui Lian",
      "Shahin Doroudian",
      "Aidong Lu",
      "Theo King"
    ],
    "domain": [
      "Machine Learning",
      "Explainable AI",
      "Multimodal Data Fusion",
      "Robotics"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "fa66bbe5-687c-4dec-a5bb-bc0a54f9501a": {
    "pk": "fa66bbe5-687c-4dec-a5bb-bc0a54f9501a",
    "project_name": null,
    "name": "Qiang Cui",
    "bio": "I am a researcher specializing in recommendation systems, with a focus on addressing challenges such as the item cold start problem and the impact of distribution shifts in user and item data. My recent work has led to the development of innovative models like the Multi-View Recurrent Neural Network (MV-RNN), which effectively integrates visual and textual information to enhance sequential recommendations. I also introduced the Hierarchical Contextual Attention-based GRU (HCA-GRU) network, which leverages attention mechanisms to better capture user interests over time.\n\nIn addition to my work in sequential recommendations, I have explored the intersection of quantum mechanics and machine learning, improving semi-empirical methods for better molecular interaction predictions. My research extends to Point-of-Interest (POI) recommendations, where I developed a model that captures spatial-temporal periodic interests, significantly enhancing recommendation accuracy.\n\nMost recently, I have tackled the issue of out-of-distribution generalization in recommendation systems through a causal preference-based framework called CausPref. This approach not only improves performance stability across varying environments but also offers interpretability in understanding user preferences. My work aims to push the boundaries of how we understand and implement recommendation systems, ensuring they are robust, efficient, and user-centric.",
    "collaborators": [
      "Shu Wu",
      "Liang Wang",
      "Yafeng Zhang",
      "Qiang Liu",
      "Wen Zhong",
      "Yan Huang",
      "Anders S. Christensen",
      "Marcus Elstner",
      "Chenrui Zhang",
      "Jinpeng Wang"
    ],
    "domain": [
      "Recommender Systems",
      "Machine Learning",
      "Causal Inference",
      "Multi-Modal Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "c59f29f5-f4ba-4064-9bd6-67ec9545e002": {
    "pk": "c59f29f5-f4ba-4064-9bd6-67ec9545e002",
    "project_name": null,
    "name": "Leimin Tian",
    "bio": "I am a researcher dedicated to exploring the intersection of robotics, multimodal sentiment analysis, and human-robot interaction. My recent work has focused on understanding how sentiment scores can be effectively decomposed into polarity and intensity, leveraging multi-task learning to enhance sentiment analysis in naturalistic settings. I have also investigated human-robot collaboration, particularly in crafting scenarios, where I collected and analyzed data to create the FACT HRC dataset, shedding light on the nuances of task context and social cues in handovers.\n\nIn my studies on social robots, I have examined the impact of deep learning on visual perception and user experience, conducting both controlled and real-world experiments to assess how these technologies influence interaction outcomes. My research extends to service robots in hospitality, where I explored how personalized movement and visual cues can improve communication and delivery accuracy, ultimately enhancing user satisfaction.\n\nI am passionate about understanding public perceptions of robots and have engaged users in participatory design workshops to shape robot behaviors in public spaces. Additionally, I have developed a novel future prediction architecture for spoken dialogue systems, enabling robots to anticipate user emotions and reactions, thereby fostering more natural interactions. My work also includes advancing movement recognition through causal inference, leading to more interpretable and robust models for understanding human dynamics. Overall, I strive to create intelligent systems that enhance human experiences and interactions in various contexts.",
    "collaborators": [
      "Catherine Lai",
      "Akansel Cosgun",
      "Dana Kuli\u0107",
      "Johanna D. Moore",
      "Kerry He",
      "Shiyu Xu",
      "Wangjie Zhong",
      "Duy Tho Le",
      "Hamid Rezatofighi",
      "Seung Chan Hong"
    ],
    "domain": [
      "Multimodal Sentiment Analysis",
      "Human-Robot Interaction",
      "Causal Inference",
      "Machine Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "2fccf606-e03f-41f4-b997-136c6b2cfbae": {
    "pk": "2fccf606-e03f-41f4-b997-136c6b2cfbae",
    "project_name": null,
    "name": "Lianlong Wu",
    "bio": "As a researcher at the intersection of machine learning, causal inference, and multi-agent systems, I am dedicated to enhancing our understanding of human movement and intelligent behavior. My recent work introduces a novel representation learning method that leverages causal inference to analyze human joint dynamics, resulting in a causal graph convolutional network (GCN) that outperforms traditional models in accuracy and interpretability. This approach not only advances human motion analysis but also paves the way for adaptive healthcare solutions.\n\nIn the realm of multi-agent learning, I recognized the need for a comprehensive evaluation platform, which led to the development of Arena. This platform features 35 diverse games and a toolkit that empowers researchers to create novel multi-agent problems, fostering innovation in the field. By providing open-source implementations of state-of-the-art deep reinforcement learning baselines, I aim to establish a stable standard for evaluating agent performance.\n\nAdditionally, I am passionate about bridging the gap between machine learning and logical reasoning through knowledge graphs. My work on Vadalog, a cutting-edge Knowledge Graph Management System, facilitates seamless integration with modern data science tools, enabling complex reasoning alongside traditional data analysis. Through these contributions, I strive to push the boundaries of what is possible in AI and data science, fostering a deeper understanding of both human behavior and intelligent systems.",
    "collaborators": [
      "Xingrui Gu",
      "Chuyi Jiang",
      "Erte Wang",
      "Zekun Wu",
      "Qiang Cui",
      "Leimin Tian",
      "Siyang Song",
      "Chuang Yu",
      "Yuhang Song",
      "Andrzej Wojcicki"
    ],
    "domain": [
      "Causal Inference",
      "Multi-Agent Learning",
      "Knowledge Graphs",
      "Machine Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "b76be7e2-d100-4b7c-8bc1-0b1c88925c82": {
    "pk": "b76be7e2-d100-4b7c-8bc1-0b1c88925c82",
    "project_name": null,
    "name": "Siyang Song",
    "bio": "I am a researcher dedicated to understanding human behavior through the lens of personality, emotion, and non-verbal communication. My work spans various domains, including automatic personality computing, facial action analysis, and acoustic scene classification. I have developed a reproducible benchmarking framework for personality recognition that evaluates existing models and highlights the importance of non-verbal cues in predicting personality traits.\n\nMy research also addresses privacy concerns in face recognition by proposing a novel diffusion-based approach for generating synthetic face images that maintain high discriminative quality. I have introduced methods to enhance speaker verification systems in noisy environments and developed a two-stage framework for video-based depression analysis that captures multi-scale facial behaviors.\n\nI am particularly interested in the interplay between context and human reactions, which led me to define the facial Multiple Appropriate Reaction Generation (fMARG) task. My recent work on Graph Neural Networks (GNNs) has resulted in the development of the Graph in Graph Neural (GIG) Network, which processes complex graph-style data, achieving state-of-the-art results in various applications.\n\nThrough my research, I aim to bridge the gap between technology and human behavior, providing insights that can enhance automated systems in real-world settings. I am passionate about creating frameworks that not only advance academic knowledge but also have practical implications for improving human-computer interactions.",
    "collaborators": [
      "Hatice Gunes",
      "Linlin Shen",
      "Michel Valstar",
      "Yuanbo Hou",
      "Wenwu Wang",
      "Dick Botteldooren",
      "Rongfan Liao",
      "Zhonglin Sun",
      "Ioannis Patras",
      "Georgios Tzimiropoulos"
    ],
    "domain": [
      "Computer Vision",
      "Emotion Recognition",
      "Graph Neural Network",
      "Audio Processing"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "51a6988e-0eb6-44e3-9667-0473278715b2": {
    "pk": "51a6988e-0eb6-44e3-9667-0473278715b2",
    "project_name": null,
    "name": "Chuang Yu",
    "bio": "I am a researcher dedicated to advancing the fields of mental health assessment and infrared small target detection through innovative machine learning techniques. My recent work focuses on developing automated systems for depression classification based on non-verbal facial behaviors, achieving over 75% accuracy by leveraging both short-term and clip-based analysis. I have also explored the integration of Theory of Mind (ToM) in human-robot interactions, demonstrating how trust-aware policies can enhance collaboration between humans and robots.\n\nIn the realm of infrared small target detection, I have pioneered several novel approaches, including the multi-scale direction-aware network (MSDA-Net) and a refined detection scheme that utilizes adjustable sensitivity strategies. My work has consistently achieved state-of-the-art results across various datasets, culminating in first and third place finishes in prestigious competitions. I am particularly passionate about creating lightweight and robust models that balance performance with resource efficiency, ensuring practical applicability in real-world scenarios.\n\nThrough my research, I aim to bridge the gap between advanced computational techniques and their real-world applications, ultimately contributing to improved mental health diagnostics and enhanced detection capabilities in challenging environments.",
    "collaborators": [
      "Jinmiao Zhao",
      "Zelin Shi",
      "Yunpeng Liu",
      "Baris Serhan",
      "Angelo Cangelosi"
    ],
    "domain": [
      "Computer Vision",
      "Deep Learning",
      "Human-Robot Interaction",
      "Mental Health"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}