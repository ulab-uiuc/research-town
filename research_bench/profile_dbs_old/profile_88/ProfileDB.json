{
  "6562dc8b-52ae-45e9-b841-d7864c27cc35": {
    "pk": "6562dc8b-52ae-45e9-b841-d7864c27cc35",
    "project_name": null,
    "name": "Cheng-En Wu",
    "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, which has proven effective in various prediction tasks, achieving significant performance improvements.\n\nI am also passionate about exploring the structural dynamics of neural networks. My work on relational graphs has unveiled critical insights into how the architecture of neural networks influences their predictive performance, leading to the identification of optimal configurations that mirror biological neural networks.\n\nIn addition, I have pioneered frameworks like ROLAND, which facilitate the adaptation of static GNNs to dynamic environments, and FALCON, an efficient method for automated model design that leverages a design graph to optimize performance across tasks. My research aims to bridge the gap between theoretical advancements and practical applications, providing scalable solutions that can be readily implemented in real-world scenarios.\n\nOverall, my goal is to push the boundaries of GNN research, making significant contributions that not only advance the field but also empower practitioners to harness the full potential of graph-based learning.",
    "collaborators": [],
    "domain": [
      "Graph Neural Network",
      "Machine Learning",
      "Multi-task Learning",
      "AutoML"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "cbc8d5a7-6aa1-4ee0-80f4-1f4bcc871628": {
    "pk": "cbc8d5a7-6aa1-4ee0-80f4-1f4bcc871628",
    "project_name": null,
    "name": "Jinhong Lin",
    "bio": "I am a researcher dedicated to enhancing the safety and efficiency of neural networks, particularly in the context of out-of-distribution (OOD) data detection and knowledge distillation. My recent work includes the development of WeShort, a post-hoc technique designed to mitigate the overconfidence of neural networks when faced with OOD inputs. This method leverages the internal residual structures of networks and has demonstrated state-of-the-art performance on benchmarks like ImageNet.\n\nIn addition to OOD detection, I have explored self-knowledge distillation (SKD), proposing a novel approach that distills knowledge from multilevel abstraction features, which has shown significant effectiveness across various tasks and model architectures. My commitment to improving model efficiency is further reflected in my research on contrastive image-text models, where I introduced innovative greedy search methods for token pruning in Vision Transformers (ViT). This work not only reduces computational demands but also maintains high performance levels, achieving a minimal accuracy loss while significantly decreasing the number of patch tokens.\n\nThrough these contributions, I aim to bridge the gap between advanced neural architectures and their practical deployment, ensuring that they are both robust and efficient in real-world applications. My research is driven by a passion for making machine learning models safer and more accessible, and I am excited to continue exploring new frontiers in this dynamic field.",
    "collaborators": [
      "Zhaoyang Li",
      "Cheng-En Wu",
      "Yu Hen Hu",
      "Pedro Morgado"
    ],
    "domain": [
      "Out-of-Distribution Detection",
      "Knowledge Distillation",
      "Multimodal Learning",
      "Computer Vision"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "b0a20639-42cd-43fc-8ac5-200670c49273": {
    "pk": "b0a20639-42cd-43fc-8ac5-200670c49273",
    "project_name": null,
    "name": "Yu Hen Hu",
    "bio": "I am a researcher specializing in the intersection of computer vision and machine learning, with a particular focus on vision-language models and their applications. My recent work has explored the robustness of prompt tuning in models like CLIP, revealing how fixed classname tokens and powerful pre-trained embeddings contribute to improved performance, even in the presence of label noise. I have also tackled challenges in unsupervised anomaly detection, developing methods to reduce false alarms and enhance detection accuracy in industrial applications.\n\nMy research extends to optimizing deep learning architectures for resource-constrained environments, where I have pioneered techniques in network pruning and compression. For instance, I introduced a novel block pruning strategy that directly assesses the impact of block removal on classification accuracy, achieving significant reductions in model size while maintaining high performance.\n\nAdditionally, I have contributed to advancements in self-supervised learning for video representation, proposing the Cascade Positive Retrieval (CPR) method, which enhances positive example mining for contrastive learning. My work has consistently aimed to improve the efficiency and effectiveness of deep learning models across various tasks, from image classification to video action recognition.\n\nThrough my research, I strive to bridge theoretical insights with practical applications, ensuring that my findings not only advance academic knowledge but also address real-world challenges in technology and industry.",
    "collaborators": [
      "Cheng-En Wu",
      "Azadeh Davoodi",
      "Pedro Morgado",
      "Yu Tian",
      "Haichao Yu",
      "Heng Wang",
      "Linjie Yang",
      "Ji Qiu",
      "Hongmei Shi",
      "Zujun Yu"
    ],
    "domain": [
      "Computer Vision",
      "Deep Learning",
      "Anomaly Detection",
      "Neural Network Pruning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "da7ce511-8280-4599-8aa1-4eb4d869a7d8": {
    "pk": "da7ce511-8280-4599-8aa1-4eb4d869a7d8",
    "project_name": null,
    "name": "Pedro Morgado",
    "bio": "I am a researcher dedicated to advancing the fields of audio-visual learning and zero-shot learning. My work focuses on leveraging the interplay between audio and visual modalities to enhance recognition, localization, and separation tasks. I have developed innovative frameworks such as EZ-VSL for audio-visual source localization and OneAVM, which integrates audio and visual cues for joint tasks, demonstrating the interconnectedness of these domains.\n\nMy recent contributions include self-supervised learning methods that harness the power of contrastive learning to improve audio-visual representations, as well as novel approaches for generalized zero-shot learning that align audio-visual embeddings with textual representations. I have also explored the challenges of training early fusion architectures and proposed solutions to enhance their efficiency and effectiveness.\n\nIn addition, I have investigated the potential of audio as a cue for generating temporally synchronized visual animations, culminating in the development of the AVSyncD model. My research aims to push the boundaries of multimodal perception, enabling machines to better understand and interact with the world around them. Through my work, I strive to create robust, adaptable models that can learn from diverse data streams, ultimately contributing to the evolution of intelligent systems capable of complex audio-visual understanding.",
    "collaborators": [
      "Nuno Vasconcelos",
      "Shentong Mo",
      "Abhinav Gupta",
      "Ishan Misra",
      "Yi Li",
      "Timothy Langlois",
      "Oliver Wang",
      "Senthil Purushwalkam",
      "Himangi Mittal",
      "Unnat Jain"
    ],
    "domain": [
      "Zero-shot Learning",
      "Audio-Visual Learning",
      "Self-Supervised Learning",
      "Transfer Learning"
    ],
    "institute": null,
    "embed": null,
    "is_leader_candidate": true,
    "is_member_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  }
}