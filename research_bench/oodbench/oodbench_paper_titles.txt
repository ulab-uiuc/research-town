Momentum Contrast for Unsupervised Visual Representation Learning
A Style-Based Generator Architecture for Generative Adversarial Networks
High-Resolution Image Synthesis With Latent Diffusion Models
ArcFace: Additive Angular Margin Loss for Deep Face Recognition
EfficientDet: Scalable and Efficient Object Detection
Dual Attention Network for Scene Segmentation
Analyzing and Improving the Image Quality of StyleGAN
Masked Autoencoders Are Scalable Vision Learners
YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors
ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks
nuScenes: A Multimodal Dataset for Autonomous Driving
Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression
Deep High-Resolution Representation Learning for Human Pose Estimation
A ConvNet for the 2020s
Exploring Simple Siamese Representation Learning
MnasNet: Platform-Aware Neural Architecture Search for Mobile
PointPillars: Fast Encoders for Object Detection From Point Clouds
DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation
Coordinate Attention for Efficient Mobile Network Design
Semantic Image Synthesis With Spatially-Adaptive Normalization
PyTorch: an imperative style, high-performance deep learning library
Language models are few-shot learners
Denoising diffusion probabilistic models
XLNet: generalized autoregressive pretraining for language understanding
Training language models to follow instructions with human feedback
Bootstrap your own latent a new approach to self-supervised learning
Fully convolutional one-stage 3D object detection on LiDAR range images
Chain-of-thought prompting elicits reasoning in large language models
Diffusion models beat GANs on image synthesis
wav2vec 2.0: a framework for self-supervised learning of speech representations
Supervised contrastive learning
Photorealistic text-to-image diffusion models with deep language understanding
Unsupervised learning of visual features by contrasting cluster assignments
SegFormer: simple and efficient design for semantic segmentation with transformers
ViLBERT: pretraining task-agnostic visiolinguistic representations for vision-and-language tasks
RandAugment: practical automated data augmentation with a reduced search space
MixMatch: a holistic approach to semi-supervised learning
FixMatch: simplifying semi-supervised learning with consistency and confidence
Generative modeling by estimating gradients of the data distribution
Retrieval-augmented generation for knowledge-intensive NLP tasks
EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
Learning Transferable Visual Models From Natural Language Supervision
A simple framework for contrastive learning of visual representations
Training data-efficient image transformers & distillation through attention
Self-Attention Generative Adversarial Networks
Zero-Shot Text-to-Image Generation
Simplifying Graph Convolutional Networks
Parameter-Efficient Transfer Learning for NLP
Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision
Theoretically Principled Trade-off between Robustness and Accuracy
EfficientNetV2: Smaller Models and Faster Training
BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models
GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
Improved Denoising Diffusion Probabilistic Models
SCAFFOLD: stochastic controlled averaging for federated learning
Barlow Twins: Self-Supervised Learning via Redundancy Reduction
Robust speech recognition via large-scale weak supervision
Certified Adversarial Robustness via Randomized Smoothing
PEGASUS: pre-training with extracted gap-sentences for abstractive summarization
Random Erasing Data Augmentation
Energy and Policy Considerations for Modern Deep Learning Research
Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression
Regularized Evolution for Image Classifier Architecture Search
Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting
CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison
Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting
Graph Convolutional Networks for Text Classification
Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks
Session-Based Recommendation with Graph Neural Networks
GMAN: A Graph Multi-Attention Network for Traffic Prediction
Hypergraph Neural Networks
FFA-Net: Feature Fusion Attention Network for Single Image Dehazing
TabNet: Attentive Interpretable Tabular Learning
Improved Knowledge Distillation via Teacher Assistant
Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View
EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs
Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment
Deep Interest Evolution Network for Click-Through Rate Prediction
WinoGrande: An Adversarial Winograd Schema Challenge at Scale
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
Unsupervised Cross-lingual Representation Learning at Scale
Transformer-XL: Attentive Language Models beyond a Fixed-Length Context
Energy and Policy Considerations for Deep Learning in NLP
Prefix-Tuning: Optimizing Continuous Prompts for Generation
Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks
Stanza: A Python Natural Language Processing Toolkit for Many Human Languages
ERNIE: Enhanced Language Representation with Informative Entities
BERT Rediscovers the Classical NLP Pipeline
Making Pre-trained Language Models Better Few-shot Learners
How Multilingual is Multilingual BERT?
DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation
What Does BERT Learn about the Structure of Language?
Multi-Task Deep Neural Networks for Natural Language Understanding
Multimodal Transformer for Unaligned Multimodal Language Sequences
Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference
BLEURT: Learning Robust Metrics for Text Generation
Self-Instruct: Aligning Language Models with Self-Generated Instructions
Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned
CamemBERT: a Tasty French Language Model
Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers
GhostNet: More Features From Cheap Operations
Occupancy Networks: Learning 3D Reconstruction in Function Space
Self-Training With Noisy Student Improves ImageNet Classification
Scalability in Perception for Autonomous Driving: Waymo Open Dataset
Selective Kernel Networks
PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud
AutoAugment: Learning Augmentation Strategies From Data
Class-Balanced Loss Based on Effective Number of Samples
SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks
Deformable ConvNets V2: More Deformable, Better Results
BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning
Taming Transformers for High-Resolution Image Synthesis
SuperGlue: Learning Feature Matching With Graph Neural Networks
PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection
ResNeSt: Split-Attention Networks
PointConv: Deep Convolutional Networks on 3D Point Clouds
StarGAN v2: Diverse Image Synthesis for Multiple Domains
Bag of Tricks for Image Classification with Convolutional Neural Networks
Designing Network Design Spaces
Open graph benchmark: datasets for machine learning on graphs
MLP-mixer: an all-MLP architecture for vision
Flamingo: a visual language model for few-shot learning
Unsupervised data augmentation for consistency training
Large language models are zero-shot reasoners
Big self-supervised models are strong semi-supervised learners
Implicit neural representations with periodic activation functions
Deep leakage from gradients
When does label smoothing help?
InstructBLIP: towards general-purpose vision-language models with instruction tuning
SuperGLUE: a stickier benchmark for general-purpose language understanding systems
Big bird: transformers for longer sequences
Adversarial examples are not bugs, they are features
Fourier features let networks learn high frequency functions in low dimensional domains
LAION-5B: an open large-scale dataset for training next generation image-text models
Training generative adversarial networks with limited data
Graph contrastive learning with augmentations
Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift
Unified language model pre-training for natural language understanding and generation
Generating diverse high-fidelity images with VQ-VAE-2
RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space
On the Convergence of FedAvg on Non-IID Data
Deberta: decoding-Enhanced Bert with Disentangled Attention
On the Variance of the Adaptive Learning Rate and Beyond
ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware
Deep Graph Infomax
A Closer Look at Few-shot Classification
Fourier Neural Operator for Parametric Partial Differential Equations
Robustness May Be at Odds with Accuracy
VL-BERT: Pre-training of Generic Visual-Linguistic Representations
Predict then Propagate: Graph Neural Networks meet Personalized PageRank
Rethinking the Value of Network Pruning
Meta-Learning with Latent Embedding Optimization
Deep Anomaly Detection with Outlier Exposure
Distributionally Robust Neural Networks
Measuring Massive Multitask Language Understanding
Rethinking Attention with Performers
DropEdge: Towards Deep Graph Convolutional Networks on Node Classification
Efficient Lifelong Learning with A-GEM
Exploration by random network distillation
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
Decoupled Weight Decay Regularization
Measuring and Improving the Use of Graph Information in Graph Neural Networks
ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
Large Scale GAN Training for High Fidelity Natural Image Synthesis
DARTS: Differentiable Architecture Search
LoRA: Low-Rank Adaptation of Large Language Models
Deformable DETR: Deformable Transformers for End-to-End Object Detection
BERTScore: Evaluating Text Generation with BERT
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
Denoising Diffusion Implicit Models
Score-Based Generative Modeling through Stochastic Differential Equations
The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks
Benchmarking Neural Network Robustness to Common Corruptions and Perturbations
Learning deep representations by mutual information estimation and maximization
ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness
The Curious Case of Neural Text Degeneration
Reformer: The Efficient Transformer
BEiT: BERT Pre-Training of Image Transformers
Finetuned Language Models are Zero-Shot Learners
Is Space-Time Attention All You Need for Video Understanding?
Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks
Do ImageNet Classifiers Generalize to ImageNet?
Understanding contrastive representation learning through alignment and uniformity on the hypersphere
Generative pretraining from pixels
REALM: retrieval-augmented language model pre-training
A Convergence Theory for Deep Learning via Over-Parameterization
Data-efficient image recognition with contrastive predictive coding
Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations
Off-Policy Deep Reinforcement Learning without Exploration
Learning Latent Dynamics for Planning from Pixels
Simple and deep graph convolutional networks
ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision
Manifold Mixup: Better Representations by Interpolating Hidden States
Transformers are RNNs: fast autoregressive transformers with linear attention
Gradient Descent Finds Global Minima of Deep Neural Networks
Self-Attention Graph Pooling
WILDS: A Benchmark of in-the-Wild Distribution Shifts
Graph U-Nets
Contrastive multi-view representation learning on graphs
