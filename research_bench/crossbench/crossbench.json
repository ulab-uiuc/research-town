{
    "2402.13448": {
        "paper_data": {
            "title": "ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance",
            "url": "http://arxiv.org/abs/2402.13448v2",
            "arxiv_id": "2402.13448",
            "authors": [
                "Liwen Sun",
                "Abhineet Agarwal",
                "Aaron Kornblith",
                "Bin Yu",
                "Chenyan Xiong"
            ],
            "abstract": "In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This time-consuming process causes ED crowding which impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that leverages artificial intelligence systems to help ED clinicians make efficient and accurate diagnoses. In collaboration with ED clinicians, we use public patient data to curate MIMIC-ED-Assist, a benchmark for AI systems to suggest laboratory tests that minimize wait time while accurately predicting critical outcomes such as death. With MIMIC-ED-Assist, we develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot employs a pre-trained bio-medical language model to encode patient information and uses reinforcement learning to minimize ED wait time and maximize prediction accuracy. On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours. ED-Copilot can also effectively personalize treatment recommendations based on patient severity, further highlighting its potential as a diagnostic assistant. Since MIMIC-ED-Assist is a retrospective benchmark, ED-Copilot is restricted to recommend only observed tests. We show ED-Copilot achieves competitive performance without this restriction as the maximum allowed time increases. Our code is available at https://github.com/cxcscmu/ED-Copilot.",
            "introduction": "   1 Introduction  Emergency Department (ED) crowding represents a critical challenge in healthcare, significantly impacting morbidity, mortality, medical error, staff burnout, and incurring excessive costs (Sartini et al., 2022). Despite the documented effects of ED crowding, this issue remains inadequately addressed in healthcare systems. An efficient and effective ED is vital for providing timely care to severely ill or injured patients (Savioli et al., 2022).   One key area to address ED crowding, as identified by the American College of Emergency Physicians, is to enhance throughput—the efficacy and efficiency of care delivery in the ED (Jarvis, 2016; DeAnda, 2018). A crucial factor affecting throughput is the laboratory testing process, where patients often face lengthy waits for tests to be ordered and completed, delaying diagnoses and treatment decisions  (Li et al., 2015). Studies also show that 40 to 60% of ED laboratory tests are unnecessary (Miyakis et al., 2006), further exacerbating wait times.   This paper proposes an artificial intelligence “Co-Pilot” system intended to offer (time) cost-effective diagnostic assistance in the ED. This system should aid diagnosis and minimize ED length of stay (LOS), i.e., wait times, by suggesting laboratory tests after patient triage. Further, it should help with resource management and planning by identifying severely ill patients who require rapid intervention. That is, by selecting informative tests, the system streamlines the diagnostic process, reducing LOS while improving outcomes, particularly for high-risk patients.   To support the machine learning (ML) community in developing a time-cost-effective diagnostic assistant, we collaborate with ED clinicians to curate a benchmark, called MIMIC-ED-Assist, that is derived from MIMIC-IV (Johnson et al., 2023b) and related datasets (Xie et al., 2022). MIMIC-ED-Assist is designed to test the ability of AI systems to provide both accurate and time-cost saving laboratory recommendations. Our benchmark consists of two prediction targets identified by our clinical collaborators to reflect patient risk: critical outcomes which includes patient death and ICU transfer (Levin et al., 2018), and lengthened ED stay, defined as ED LOS exceeding 24242424 hours. Accurately identifying patients at high risks of these outcomes reduces time-cost by allowing clinicians to perform timely interventions and efficiently allocate resources. MIMIC-ED-Assist mirrors real-world ED practices by grouping individual laboratory tests into commonly performed groups, e.g., complete blood count (CBC). MIMIC-ED-Assist then tests AI systems on their ability to recommend the most informative groups to make accurate diagnostic suggestions while minimizing the total time required to perform these tests, thereby reducing LOS.   With MIMIC-ED-Assist, we propose ED-Copilot which suggests a series of laboratory groups to flag patients at high risks on our prediction targets while minimizing total time-cost. ED-Copilot first linearizes (i.e., converts to text) patient information, including demographic, triage, and laboratory test results into a text sequence. It then fine-tunes a bio-medical pre-trained language model BioGPT (Luo et al., 2022) to suggest future groups and predict our two defined targets. Next, we use a reinforcement learning (RL) framework (Yu et al., 2023) to teach BioGPT to dynamically recommend the subsequent, most informative laboratory group based on prior laboratory and triage information. Unlike baselines, ED-Copilot serves as a personalized diagnostic assistant since it uses past patient information to recommend future medically relevant laboratory groups.   Experiments on MIMIC-ED-Assist show that ED-Copilot outperforms state-of-the-art tree models while halving time-costs of laboratory testing from four hours to two hours. Reducing the number of laboratory tests also has the",
            "references": [
                {
                    "title": "Language models are weak learners",
                    "abstract": "A central notion in practical and theoretical machine learning is that of a $\\textit{weak learner}$, classifiers that achieve better-than-random performance (on any given distribution over data), even by a small margin. Such weak learners form the practical basis for canonical machine learning methods such as boosting. In this work, we illustrate that prompt-based large language models can operate effectively as said weak learners. Specifically, we illustrate the use of a large language model (LLM) as a weak learner in a boosting algorithm applied to tabular data. We show that by providing (properly sampled according to the distribution of interest) text descriptions of tabular data samples, LLMs can produce a summary of the samples that serves as a template for classification and achieves the aim of acting as a weak learner on this task. We incorporate these models into a boosting approach, which in some settings can leverage the knowledge within the LLM to outperform traditional tree-based boosting. The model outperforms both few-shot learning and occasionally even more involved fine-tuning procedures, particularly for tasks involving small numbers of data points. The results illustrate the potential for prompt-based LLMs to function not just as few-shot learners themselves, but as components of larger machine learning pipelines."
                },
                {
                    "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
                    "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering."
                },
                {
                    "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
                    "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}."
                },
                {
                    "title": "LLaMA: Open and Efficient Foundation Language Models",
                    "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
                },
                {
                    "title": "Deep Reinforcement Learning for Cost-Effective Medical Diagnosis",
                    "abstract": "Dynamic diagnosis is desirable when medical tests are costly or time-consuming. In this work, we use reinforcement learning (RL) to find a dynamic policy that selects lab test panels sequentially based on previous observations, ensuring accurate testing at a low cost. Clinical diagnostic data are often highly imbalanced; therefore, we aim to maximize the $F_1$ score instead of the error rate. However, optimizing the non-concave $F_1$ score is not a classic RL problem, thus invalidates standard RL methods. To remedy this issue, we develop a reward shaping approach, leveraging properties of the $F_1$ score and duality of policy optimization, to provably find the set of all Pareto-optimal policies for budget-constrained $F_1$ score maximization. To handle the combinatorially complex state space, we propose a Semi-Model-based Deep Diagnosis Policy Optimization (SM-DDPO) framework that is compatible with end-to-end training and online learning. SM-DDPO is tested on diverse clinical tasks: ferritin abnormality detection, sepsis mortality prediction, and acute kidney injury diagnosis. Experiments with real-world data validate that SM-DDPO trains efficiently and identifies all Pareto-front solutions. Across all tasks, SM-DDPO is able to achieve state-of-the-art diagnosis accuracy (in some cases higher than conventional methods) with up to $85\\%$ reduction in testing cost. The code is available at [https://github.com/Zheng321/Deep-Reinforcement-Learning-for-Cost-Effective-Medical-Diagnosis]."
                },
                {
                    "title": "MIMIC-IV, a freely accessible electronic health record dataset",
                    "abstract": null
                },
                {
                    "title": "Benchmarking emergency department prediction models with machine learning and public electronic health records",
                    "abstract": null
                },
                {
                    "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models",
                    "abstract": "We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting."
                },
                {
                    "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
                    "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms."
                },
                {
                    "title": "Overcrowding in Emergency Department: Causes, Consequences, and Solutions—A Narrative Review",
                    "abstract": "Overcrowding in Emergency Departments (EDs) is a phenomenon that is now widespread globally and causes a significant negative impact that goes on to affect the entire hospital. This contributes to a number of consequences that can affect both the number of resources available and the quality of care. Overcrowding is due to a number of factors that in most cases lead to an increase in the number of people within the ED, an increase in mortality and morbidity, and a decrease in the ability to provide critical services in a timely manner to patients suffering from medical emergencies. This phenomenon results in the Emergency Department reaching, and in some cases exceeding, its optimal capacity. In this review, the main causes and consequences involving this phenomenon were collected, including the effect caused by the SARS-CoV-2 virus in recent years. Finally, special attention was paid to the main operational strategies that have been developed over the years, strategies that can be applied both at the ED level (microlevel strategies) and at the hospital level (macrolevel strategies)."
                },
                {
                    "title": "Predictability and stability testing to assess clinical decision instrument performance for children after blunt torso trauma",
                    "abstract": "Objective The Pediatric Emergency Care Applied Research Network (PECARN) has developed a clinical-decision instrument (CDI) to identify children at very low risk of intra-abdominal injury. However, the CDI has not been externally validated. We sought to vet the PECARN CDI with the Predictability Computability Stability (PCS) data science framework, potentially increasing its chance of a successful external validation. Materials & Methods We performed a secondary analysis of two prospectively collected datasets: PECARN (12,044 children from 20 emergency departments) and an independent external validation dataset from the Pediatric Surgical Research Collaborative (PedSRC; 2,188 children from 14 emergency departments). We used PCS to reanalyze the original PECARN CDI along with new interpretable PCS CDIs we developed using the PECARN dataset. External validation was then measured on the PedSRC dataset. Results Three predictor variables (abdominal wall trauma, Glasgow Coma Scale Score <14, and abdominal tenderness) were found to be stable. Using only these variables, we developed a PCS CDI which had a lower sensitivity than the original PECARN CDI on internal PECARN validation but performed the same on external PedSRC validation (sensitivity 96.8% and specificity 44%). Conclusion The PCS data science framework vetted the PECARN CDI and its constituent predictor variables prior to external validation. In this case, the PECARN CDI with 7 predictors, and our PCS-based CDI with 3 stable predictors, had identical performance on independent external validation. This suggests that both CDIs will generalize well to new populations, offering a potential strategy to increase the chance of a successful (costly) prospective validation."
                },
                {
                    "title": "Hierarchical Shrinkage: improving the accuracy and interpretability of tree-based methods",
                    "abstract": "Tree-based models such as decision trees and random forests (RF) are a cornerstone of modern machine-learning practice. To mitigate overfitting, trees are typically regularized by a variety of techniques that modify their structure (e.g. pruning). We introduce Hierarchical Shrinkage (HS), a post-hoc algorithm that does not modify the tree structure, and instead regularizes the tree by shrinking the prediction over each node towards the sample means of its ancestors. The amount of shrinkage is controlled by a single regularization parameter and the number of data points in each ancestor. Since HS is a post-hoc method, it is extremely fast, compatible with any tree growing algorithm, and can be used synergistically with other regularization techniques. Extensive experiments over a wide variety of real-world datasets show that HS substantially increases the predictive performance of decision trees, even when used in conjunction with other regularization techniques. Moreover, we find that applying HS to each tree in an RF often improves accuracy, as well as its interpretability by simplifying and stabilizing its decision boundaries and SHAP values. We further explain the success of HS in improving prediction performance by showing its equivalence to ridge regression on a (supervised) basis constructed of decision stumps associated with the internal nodes of a tree. All code and models are released in a full-fledged package available on Github (github.com/csinva/imodels)"
                },
                {
                    "title": "A large language model for electronic health records",
                    "abstract": null
                },
                {
                    "title": "Emergency Department Overcrowding: Understanding the Factors to Find Corresponding Solutions",
                    "abstract": "It is certain and established that overcrowding represents one of the main problems that has been affecting global health and the functioning of the healthcare system in the last decades, and this is especially true for the emergency department (ED). Since 1980, overcrowding has been identified as one of the main factors limiting correct, timely, and efficient hospital care. The more recent COVID-19 pandemic contributed to the accentuation of this phenomenon, which was already well known and of international interest. Considering what would appear to be a trivial definition of overcrowding, it may seem simple for the reader to hypothesize solutions for what seems to be one of the most avoidable problems affecting the hospital system. However, proposing solutions to overcrowding, as well as their implementation, cannot be separated from a correct and precise definition of the issue, which must consider the main causes and aggravating factors. In light of the need of finding solutions that can put an end to hospital overcrowding, this review aims, through a review of the literature, to summarize the triggering factors, as well as the possible solutions that can be proposed."
                },
                {
                    "title": "Clinical Relation Extraction Using Transformer-based Models",
                    "abstract": "The newly emerged transformer technology has a tremendous impact on NLP research. In the general English domain, transformer-based models have achieved state-of-the-art performances on various NLP benchmarks. In the clinical domain, researchers also have investigated transformer models for clinical applications. The goal of this study is to systematically explore three widely used transformer-based models (i.e., BERT, RoBERTa, and XLNet) for clinical relation extraction and develop an open-source package with clinical pre-trained transformer-based models to facilitate information extraction in the clinical domain. We developed a series of clinical RE models based on three transformer architectures, namely BERT, RoBERTa, and XLNet. We evaluated these models using 2 publicly available datasets from 2018 MADE1.0 and 2018 n2c2 challenges. We compared two classification strategies (binary vs. multi-class classification) and investigated two approaches to generate candidate relations in different experimental settings. In this study, we compared three transformer-based (BERT, RoBERTa, and XLNet) models for relation extraction. We demonstrated that the RoBERTa-clinical RE model achieved the best performance on the 2018 MADE1.0 dataset with an F1-score of 0.8958. On the 2018 n2c2 dataset, the XLNet-clinical model achieved the best F1-score of 0.9610. Our results indicated that the binary classification strategy consistently outperformed the multi-class classification strategy for clinical relation extraction. Our methods and models are publicly available at this https URL. We believe this work will improve current practice on clinical relation extraction and other related NLP tasks in the biomedical domain."
                },
                {
                    "title": "Predicting Progression to Septic Shock in the Emergency Department using an Externally Generalizable Machine Learning Algorithm",
                    "abstract": "Objective: Machine-learning (ML) algorithms allow for improved prediction of sepsis syndromes in the ED using data from electronic medical records. Transfer learning, a new subfield of ML, allows for generalizability of an algorithm across clinical sites. We aimed to validate the Artificial Intelligence Sepsis Expert (AISE) for the prediction of delayed septic shock in a cohort of patients treated in the ED and demonstrate the feasibility of transfer learning to improve external validity at a second site. Methods: Observational cohort study utilizing data from over 180,000 patients from two academic medical centers between 2014 and 2019 using multiple definitions of sepsis. The AISE algorithm was trained using 40 input variables at the development site to predict delayed septic shock (occurring greater than 4 hours after ED triage) at varying prediction windows. We then validated the AISE algorithm at a second site using transfer learning to demonstrate generalizability of the algorithm. Results: We identified 9354 patients with severe sepsis of which 723 developed septic shock at least 4 hours after triage. The AISE algorithm demonstrated excellent area under the receiver operating curve (>0.8) at 8 and 12 hours for the prediction of delayed septic shock. Transfer learning significantly improved the test characteristics of the AISE algorithm and yielded comparable performance at the validation site. Conclusions: The AISE algorithm accurately predicted the development of delayed septic shock. The use of transfer learning allowed for significantly improved external validity and generalizability at a second site. Future prospective studies are indicated to evaluate the clinical utility of this model."
                },
                {
                    "title": "Predicting 30-day mortality of patients with pneumonia in an emergency department setting using machine-learning models",
                    "abstract": "Objective This study aimed to confirm the accuracy of a machine-learning-based model in predicting the 30-day mortality of patients with pneumonia and evaluating whether they were required to be admitted to the intensive care unit (ICU). Methods The study conducted a retrospective analysis of pneumonia patients at an emergency department (ED) in Seoul, Korea, from January 1, 2016 to December 31, 2017. Patients aged 18 years or older with a pneumonia registry designation on their electronic medical record were enrolled. We collected their demographic information, mental status, and laboratory findings. Three models were used: the pre-existing CURB-65 model, and the CURB-RF and Extensive CURB-RF models, which were machine-learning models that used a random forest algorithm. The primary outcomes were ICU admission from the ED or 30-day mortality. Receiver operating characteristic curves were constructed for the models, and the areas under these curves were compared. Results Out of the 1,974 pneumonia patients, 1,732 patients were eligible to be included in the study; from these, 473 patients died within 30 days or were initially admitted to the ICU from the ED. The area under receiver operating characteristic curves of CURB-65, CURB-RF, and extensive-CURB-RF were 0.615 (0.614–0.616), 0.701 (0.700–0.702), and 0.844 (0.843–0.845), respectively. Conclusion The proposed machine-learning models could predict the mortality of patients with pneumonia more accurately than the pre-existing CURB-65 model and can help decide whether the patient should be admitted to the ICU."
                },
                {
                    "title": "MIMIC-Extract: a data extraction, preprocessing, and representation pipeline for MIMIC-III",
                    "abstract": "Machine learning for healthcare researchers face challenges to progress and reproducibility due to a lack of standardized processing frameworks for public datasets. We present MIMIC-Extract, an open source pipeline for transforming the raw electronic health record (EHR) data of critical care patients from the publicly-available MIMIC-III database into data structures that are directly usable in common time-series prediction pipelines. MIMIC-Extract addresses three challenges in making complex EHR data accessible to the broader machine learning community. First, MIMIC-Extract transforms raw vital sign and laboratory measurements into usable hourly time series, performing essential steps such as unit conversion, outlier handling, and aggregation of semantically similar features to reduce missingness and improve robustness. Second, MIMIC-Extract extracts and makes prediction of clinically-relevant targets possible, including outcomes such as mortality and length-of-stay as well as comprehensive hourly intervention signals for ventilators, vasopressors, and fluid therapies. Finally, the pipeline emphasizes reproducibility and extensibility to future research questions. We demonstrate the pipeline's effectiveness by developing several benchmark tasks for outcome and intervention forecasting and assessing the performance of competitive models."
                },
                {
                    "title": "Stop the Bottleneck: Improving Patient Throughput in the Emergency Department",
                    "abstract": null
                },
                {
                    "title": "The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care",
                    "abstract": null
                },
                {
                    "title": "Benchmarking deep learning models on large healthcare datasets",
                    "abstract": null
                },
                {
                    "title": "Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer",
                    "abstract": "Importance Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency. Objective Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin–stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists’ diagnoses in a diagnostic setting. Design, Setting, and Participants Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n = 110) and without (n = 160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC). Exposures Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation. Main Outcomes and Measures The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor. Results The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4% [95% CI, 64.3%-80.4%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P < .001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95% CI, 0.927-0.998] for the pathologist WOTC). Conclusions and Relevance In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting."
                },
                {
                    "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree",
                    "abstract": "Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \\emph{Gradient-based One-Side Sampling} (GOSS) and \\emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \\emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy."
                },
                {
                    "title": "Machine‐Learning‐Based Electronic Triage More Accurately Differentiates Patients With Respect to Clinical Outcomes Compared With the Emergency Severity Index",
                    "abstract": null
                },
                {
                    "title": "Proximal Policy Optimization Algorithms",
                    "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
                },
                {
                    "title": "Multitask learning and benchmarking with clinical time series data",
                    "abstract": null
                },
                {
                    "title": "Improving emergency department patient flow",
                    "abstract": "Emergency departments (ED) face significant challenges in delivering high quality and timely patient care on an ever-present background of increasing patient numbers and limited hospital resources. A mismatch between patient demand and the ED’s capacity to deliver care often leads to poor patient flow and departmental crowding. These are associated with reduction in the quality of the care delivered and poor patient outcomes. A literature review was performed to identify evidence-based strategies to reduce the amount of time patients spend in the ED in order to improve patient flow and reduce crowding in the ED. The use of doctor triage, rapid assessment, streaming and the co-location of a primary care clinician in the ED have all been shown to improve patient flow. In addition, when used effectively point of care testing has been shown to reduce patient time in the ED. Patient flow and departmental crowding can be improved by implementing new patterns of working and introducing new technologies such as point of care testing in the ED."
                },
                {
                    "title": "XGBoost: A Scalable Tree Boosting System",
                    "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
                },
                {
                    "title": "The Effect of Laboratory Testing on Emergency Department Length of Stay: A Multihospital Longitudinal Study Applying a Cross‐classified Random‐effect Modeling Approach",
                    "abstract": "Abstract Objectives The objective was to examine the relationship between laboratory testing (including test volume and turnaround time [TAT]) and emergency department (ED) length of stay (LOS), using linked patient‐level data from four hospitals across 4 years. Methods This was a retrospective, multisite cohort study of patients presenting to any one of four EDs in New South Wales, Australia, during a 2‐month period (August and September) in 2008, 2009, 2010, and 2011. Data from ED information systems were linked to laboratory test data. A cross‐classified random‐effect modeling approach was applied to identify factors affecting ED LOS, taking into account the correlation between patients' presentations at the same hospital and/or in the same calendar year. Number of test order episodes (tests ordered at one point in time during the ED stay) and TAT (time from laboratory order receipt to result available) were examined. Results As the number of test order episodes increased, so did the duration of patient ED LOS (p < 0.0001). For every five additional tests ordered per test order episode, the median ED LOS increased by 10 minutes (2.9%, p < 0.0001); each 30‐minute increase in TAT was, on average, associated with a 5.1% (17 minutes; p < 0.0001) increase in ED LOS, after adjustment for other factors. Patients presenting to the ED at night (7 p.m. to 7 a.m.) had longer stays than those presenting during the daytime, although the median TATs at nights were shorter than those during the daytime. Conclusions Laboratory testing has a direct effect on patients' LOS in ED. Laboratory TAT, number of testing episodes, and test volume influence ED LOS. Targeted increases of ED resources and staffing after‐hours may also contribute to reductions in ED LOS."
                },
                {
                    "title": "Factors contributing to inappropriate ordering of tests in an academic medical department and the effect of an educational feedback strategy",
                    "abstract": "Aims: To identify factors contributing to laboratory overutilisation in an academic medical department, and to assess the effect of an educational feedback strategy on inappropriate test-ordering behaviour. Methods: The records of 426 patients admitted during a 6-month period were reviewed. The usefulness of 25 investigations (haematology, basic biochemistry and arterial blood gases) was assessed according to implicit criteria. Trainees’ acquaintance with investigation costs was assessed via a multiple-choice questionnaire. The medical staff was informed about their test-ordering behaviour, cost awareness and the factors associated with overuse of diagnostic tests. The test-ordering behaviour of the same doctors was reassessed on 214 patients managed during 6 months after the intervention. Results: Overall, 24 482 laboratory tests were ordered before the intervention (mean 2.96 tests/patient/day). Among those, 67.9% were not considered to have contributed towards management of patients (mean avoidable 2.01 tests/patient/day). Patient age ⩾65 years, hospitalisation beyond 7 days and increased case difficulty (death or inability to establish a diagnosis) were factors independently associated with overuse of laboratory tests. Senior trainees ordered more laboratory examinations, but the percentage of avoidable tests requested by junior trainees was higher. A moderate and disparate level of trainees’ awareness about the cost of common laboratory examinations was disclosed. The avoidable tests/patient/day were significantly decreased after the intervention (mean 1.58, p = 0.002), but containment of unnecessary ordering of tests gradually waned during the semester after the intervention. Conclusion: Repeated audit, continuous education and alertness of doctors, on the basis of assessment of factors contributing to laboratory overutilisation, result in restraining the redundant ordering of tests in the hospital setting."
                },
                {
                    "title": "Mimic-iv-ed (version 2.2)",
                    "abstract": null
                },
                {
                    "title": "Fast Interpretable Greedy-Tree Sums (FIGS)",
                    "abstract": "Modern machine learning has achieved impressive prediction performance, but often sacriﬁces interpretability, a critical consideration in many problems. Here, we propose Fast Interpretable Greedy-Tree Sums (FIGS), an algorithm for ﬁt-ting concise rule-based models."
                },
                {
                    "title": "Stable-Baselines3: Reliable Reinforcement Learning Implementations",
                    "abstract": "Stable-Baselines3 provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare diﬀerent RL algorithms. Our documentation, examples, and source-code are available at https://github.com/DLR-RM/stable-baselines3 ."
                },
                {
                    "title": "of artificial",
                    "abstract": "Every year our life is becoming more dynamic. The development of advanced technologies is changing the format of civil-law relations in their classical sense (used by lawyers or ordinary citizens). Today, the norm for us is the use of artificial intelligence technologies in assessing credit risks, implementing them in the courts, legal departments of large companies, public authorities and management, in manufacturing, in the conclusion of smart contracts and, ultimately, in our homes (in the use of smart home technology). We don’t even think about ower using artificial intelligence programs almost every day. For example, email spam filters, face recognition, search recommendations, smart personal assistants (Siri), shared apps (Uber), etc. Such changes are positive, as modern artificial intelligence technologies are designed to simplify a person’s life, assist him or her in work or in accessing public services. However, they are accompanied by several unknown legal doctrines that need detailed scrutiny. In general, the legal profession is one of those that most felt the ambiguous effect of modern technology being on the verge of upheaval1. This is due to the introduction of artificial intelligence technologies into the work of lawyers when such technologies replace them2."
                },
                {
                    "title": "Language Models are Unsupervised Multitask Learners",
                    "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
                },
                {
                    "title": "Random Forests",
                    "abstract": null
                },
                {
                    "title": ": A flexible random forest-based feature importance",
                    "abstract": null
                },
                {
                    "title": "Exploring language models for medical question answering",
                    "abstract": null
                },
                {
                    "title": "We develop ED-Copilot, an RL-trained language model that enhances ED workflow by suggesting informative laboratory groups and flagging high-risk patients using a pre-trained language model backbone",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "3ad03d41-bc73-470d-916b-e6967d9e9a0b": {
                "pk": "3ad03d41-bc73-470d-916b-e6967d9e9a0b",
                "project_name": null,
                "name": "Liwen Sun",
                "bio": "I am a researcher dedicated to enhancing the capabilities of language models and multimodal systems, particularly in the context of few-shot learning and medical applications. My recent work focuses on leveraging pre-trained language models for few-shot text classification, where I address the challenges of overfitting by employing supervised contrastive learning and consistency-regularization techniques. This approach, which I developed in my model FTCC, has shown significant improvements in robustness and generalization across multiple datasets.\n\nIn addition to my work in text classification, I am passionate about the intersection of artificial intelligence and healthcare. I have developed a fact-aware multimodal retrieval-augmented pipeline, FactMM-RAG, aimed at improving the accuracy of radiology report generation. By integrating factual knowledge and utilizing a universal multimodal retriever, I have successfully enhanced the factual completeness of generated reports, demonstrating notable performance improvements over existing methods.\n\nThrough my research, I strive to create models that not only perform well but also contribute meaningfully to real-world applications, particularly in supporting clinicians in their diagnostic processes. My goal is to continue exploring innovative methodologies that bridge the gap between advanced AI techniques and practical healthcare solutions.",
                "collaborators": [
                    "Jiawei Han",
                    "James Zhao",
                    "Megan Han",
                    "Chenyan Xiong"
                ],
                "pub_titles": [
                    "Few-shot Text Classification with Dual Contrastive Consistency",
                    "Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation"
                ],
                "pub_abstracts": [
                    "In this paper, we explore how to utilize pre-trained language model to perform few-shot text classification where only a few annotated examples are given for each class. Since using traditional cross-entropy loss to fine-tune language model under this scenario causes serious overfitting and leads to sub-optimal generalization of model, we adopt supervised contrastive learning on few labeled data and consistency-regularization on vast unlabeled data. Moreover, we propose a novel contrastive consistency to further boost model performance and refine sentence representation. After conducting extensive experiments on four datasets, we demonstrate that our model (FTCC) can outperform state-of-the-art methods and has better robustness.",
                    "Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets show that our multimodal retriever outperforms state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5% and 2% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagates fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Few-Shot Learning",
                    "Multimodal Learning",
                    "Medical AI"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "0940916f-36aa-42a3-bc4c-c69fdcacc0aa": {
                "pk": "0940916f-36aa-42a3-bc4c-c69fdcacc0aa",
                "project_name": null,
                "name": "Abhineet Agarwal",
                "bio": "I am a researcher with a strong focus on the intersection of cosmology, causal inference, and machine learning. My work spans a variety of topics, including quintessential inflation models, the dynamics of dark energy, and the development of innovative statistical methods for causal analysis. Recently, I have explored the curvaton mechanism for reheating in cosmology, establishing bounds on coupling constants and investigating decay scenarios. \n\nIn the realm of causal inference, I have developed a novel latent factor model called Synthetic Combinations, which efficiently estimates unit-specific potential outcomes across multiple interventions, significantly improving data efficiency in experimental design. My research also addresses the challenges of online experimentation with interference, proposing algorithms that minimize regret in multi-armed bandit settings while accounting for network effects.\n\nAdditionally, I have contributed to the understanding of decision trees and their generalization performance, introducing methods like Hierarchical Shrinkage and MDI+ to enhance interpretability and predictive power in high-stakes applications. My work emphasizes the importance of bridging theoretical insights with practical applications, particularly in fields such as medicine and social sciences, where decision-making is critical. Through my research, I aim to provide robust frameworks that not only advance theoretical understanding but also offer practical solutions to real-world problems.",
                "collaborators": [
                    "Yan Shuo Tan",
                    "Bin Yu",
                    "R. Myrzakulov",
                    "Anish Agarwal",
                    "M. Shahalam",
                    "Naveen K. Singh",
                    "M. Sami",
                    "S. K. J. Pacif",
                    "Omer Ronen",
                    "Chandan Singh"
                ],
                "pub_titles": [
                    "Quintessential Inflation and curvaton reheating",
                    "Synthetic Combinations: A Causal Inference Framework for Combinatorial Interventions",
                    "Multi-Armed Bandits with Network Interference",
                    "$Om$ diagnostic applied to scalar field models and slowing down of cosmic acceleration",
                    "Thermal Tachyacoustic Cosmology",
                    "A cautionary tale on fitting decision trees to data from additive models: generalization lower bounds",
                    "Weak Lensing Effect on CMB in the Presence of a Dipole Anisotropy",
                    "Quintessential Inflation in a thawing realization",
                    "Cosmic acceleration from coupling of baryonic and dark matter components: Analysis and diagnostics",
                    "Hierarchical Shrinkage: improving the accuracy and interpretability of tree-based methods",
                    "Cosmic acceleration sourced by modification of gravity without extra degrees of freedom",
                    "MDI+: A Flexible Random Forest-Based Feature Importance Framework",
                    "Fast Interpretable Greedy-Tree Sums"
                ],
                "pub_abstracts": [
                    "In this paper, we consider a model of quintessential inflation based upon inverse hyperbolic potential. We employ curvaton mechanism for reheating which is more efficient than gravitational particle production; the mechanism complies with nucleosynthesis constraint due to relic gravity waves. We obtain a lower bound on the coupling constant $g$ that governs the interaction of curvaton with matter fields. We study curvaton decay before domination and decay after domination and plot the allowed region in the parameter space in both cases.",
                    "Consider a setting where there are $N$ heterogeneous units and $p$ interventions. Our goal is to learn unit-specific potential outcomes for any combination of these $p$ interventions, i.e., $N \\times 2^p$ causal parameters. Choosing a combination of interventions is a problem that naturally arises in a variety of applications such as factorial design experiments, recommendation engines, combination therapies in medicine, conjoint analysis, etc. Running $N \\times 2^p$ experiments to estimate the various parameters is likely expensive and/or infeasible as $N$ and $p$ grow. Further, with observational data there is likely confounding, i.e., whether or not a unit is seen under a combination is correlated with its potential outcome under that combination. To address these challenges, we propose a novel latent factor model that imposes structure across units (i.e., the matrix of potential outcomes is approximately rank $r$), and combinations of interventions (i.e., the coefficients in the Fourier expansion of the potential outcomes is approximately $s$ sparse). We establish identification for all $N \\times 2^p$ parameters despite unobserved confounding. We propose an estimation procedure, Synthetic Combinations, and establish it is finite-sample consistent and asymptotically normal under precise conditions on the observation pattern. Our results imply consistent estimation given $\\text{poly}(r) \\times \\left( N + s^2p\\right)$ observations, while previous methods have sample complexity scaling as $\\min(N \\times s^2p, \\ \\ \\text{poly(r)} \\times (N + 2^p))$. We use Synthetic Combinations to propose a data-efficient experimental design. Empirically, Synthetic Combinations outperforms competing approaches on a real-world dataset on movie recommendations. Lastly, we extend our analysis to do causal inference where the intervention is a permutation over $p$ items (e.g., rankings).",
                    "Online experimentation with interference is a common challenge in modern applications such as e-commerce and adaptive clinical trials in medicine. For example, in online marketplaces, the revenue of a good depends on discounts applied to competing goods. Statistical inference with interference is widely studied in the offline setting, but far less is known about how to adaptively assign treatments to minimize regret. We address this gap by studying a multi-armed bandit (MAB) problem where a learner (e-commerce platform) sequentially assigns one of possible $\\mathcal{A}$ actions (discounts) to $N$ units (goods) over $T$ rounds to minimize regret (maximize revenue). Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is interference across the underlying network of units. With $\\mathcal{A}$ actions and $N$ units, minimizing regret is combinatorially difficult since the action space grows as $\\mathcal{A}^N$. To overcome this issue, we study a sparse network interference model, where the reward of a unit is only affected by the treatments assigned to $s$ neighboring units. We use tools from discrete Fourier analysis to develop a sparse linear representation of the unit-specific reward $r_n: [\\mathcal{A}]^N \\rightarrow \\mathbb{R} $, and propose simple, linear regression-based algorithms to minimize regret. Importantly, our algorithms achieve provably low regret both when the learner observes the interference neighborhood for all units and when it is unknown. This significantly generalizes other works on this topic which impose strict conditions on the strength of interference on a known network, and also compare regret to a markedly weaker optimal action. Empirically, we corroborate our theoretical findings via numerical simulations.",
                    "We apply the $Om$ diagnostic to models for dark energy based on scalar fields. In case of the power law potentials, we demonstrate the possibility of slowing down the expansion of the Universe around the present epoch for a specific range in the parameter space. For these models, we also examine the issues concerning the age of Universe. We use the $Om$ diagnostic to distinguish the $\\Lambda$CDM model from non minimally coupled scalar field, phantom field and generic quintessence models. Our study shows that the $Om$ has zero, positive and negative curvatures for $\\Lambda$CDM, phantom and quintessence models respectively. We use an integrated data base (SN+Hubble+BAO+CMB) for bservational analysis and demonstrate that $Om$ is a useful diagnostic to apply to observational data.",
                    "An intriguing possibility that can address pathologies in both early universe cosmology (i.e. the horizon problem) and quantum gravity (i.e. non-renormalizability), is that particles at very high energies and/or temperatures could propagate arbitrarily fast. A concrete realization of this possibility for the early universe is the Tachyacoustic (or Speedy Sound) cosmology, which could also produce a scale-invariant spectrum for scalar cosmological perturbations. Here, we study Thermal Tachyacoustic Cosmology (TTC), i.e. this scenario with thermal initial conditions. We find that a phase transition in the early universe, around the scale of Grand Unified Theories (GUT scale; $T\\sim 10^{15}$ GeV), during which the speed of sound drops by $25$ orders of magnitude within a Hubble time, can fit current CMB observations. We further discuss how production of primordial black holes constrains the cosmological acoustic history, while coupling TTC to Horava-Lifshitz gravity leads to a lower limit on the amplitude of tensor modes ($r \\gtrsim 10^{-3}$), that are detectable by CMBPol (and might have already been seen by the BICEP-Keck collaboration).",
                    "Decision trees are important both as interpretable models amenable to high-stakes decision-making, and as building blocks of ensemble methods such as random forests and gradient boosting. Their statistical properties, however, are not well understood. The most cited prior works have focused on deriving pointwise consistency guarantees for CART in a classical nonparametric regression setting. We take a different approach, and advocate studying the generalization performance of decision trees with respect to different generative regression models. This allows us to elicit their inductive bias, that is, the assumptions the algorithms make (or do not make) to generalize to new data, thereby guiding practitioners on when and how to apply these methods. In this paper, we focus on sparse additive generative models, which have both low statistical complexity and some nonparametric flexibility. We prove a sharp squared error generalization lower bound for a large class of decision tree algorithms fitted to sparse additive models with $C^1$ component functions. This bound is surprisingly much worse than the minimax rate for estimating such sparse additive models. The inefficiency is due not to greediness, but to the loss in power for detecting global structure when we average responses solely over each leaf, an observation that suggests opportunities to improve tree-based algorithms, for example, by hierarchical shrinkage. To prove these bounds, we develop new technical machinery, establishing a novel connection between decision tree estimation and rate-distortion theory, a sub-field of information theory.",
                    "We investigate weak lensing effect on cosmic microwave background (CMB) in the presence of dipole anisotropy. The approach of flat-sky approximation is considered. We determine the functions $\\sigma_0^2$ and $\\sigma_2^2$ that appear in expressions of the lensed CMB power spectrum in the presence of a dipole anisotropy. We determine the correction to B-mode power spectrum which is found to be appreciable at low multipoles ($l$). However, the temperature and E-mode power spectrum are not altered significantly.",
                    "We study quintessential inflation with an inverse hyperbolic type potential $V(\\phi) = {V_0}/{\\cosh \\left( {\\phi^n}/{\\lambda^n} \\right)}$, where $V_0$, $\\lambda$ and \"n\" are parameters of the theory. We obtain a bound on $\\lambda$ for different values of the parameter n. The spectral index and the tensor-to-scalar-ratio fall in the $1 \\sigma$ bound given by the Planck 2015 data for $n \\geq 5$ for certain values of $\\lambda$. However for $3 \\leq n < 5$ there exist values of $\\lambda$ for which the spectral index and the tensor-to-scalar-ratio fall only within the $2 \\sigma$ bound of the Planck data. Furthermore, we show that the scalar field with the given potential can also give rise to late time acceleration if we invoke the coupling to massive neutrino matter. We also consider the instant preheating mechanism with Yukawa interaction and put bounds on the coupling constants for our model using the nucleosynthesis constraint on relic gravity waves produced during inflation.",
                    "In this paper, we examine a scenario in which late-time cosmic acceleration might arise due to the coupling between baryonic matter and dark matter without the presence of extra degrees of freedom. In this case, one can obtain late-time acceleration in Jordan frame and not in Einstein frame. We consider two different forms of parametrization of the coupling function, and put constraints on the model parameters by using an integrated datasets of Hubble parameter, Type Ia supernova and baryon acoustic oscillations. The models under consideration are consistent with the observations. In addition, we perform the statefinder and $Om$ diagnostics, and show that the models exhibit a distinctive behavior due to the phantom characteristic in future which is a generic feature of the underlying scenario.",
                    "Tree-based models such as decision trees and random forests (RF) are a cornerstone of modern machine-learning practice. To mitigate overfitting, trees are typically regularized by a variety of techniques that modify their structure (e.g. pruning). We introduce Hierarchical Shrinkage (HS), a post-hoc algorithm that does not modify the tree structure, and instead regularizes the tree by shrinking the prediction over each node towards the sample means of its ancestors. The amount of shrinkage is controlled by a single regularization parameter and the number of data points in each ancestor. Since HS is a post-hoc method, it is extremely fast, compatible with any tree growing algorithm, and can be used synergistically with other regularization techniques. Extensive experiments over a wide variety of real-world datasets show that HS substantially increases the predictive performance of decision trees, even when used in conjunction with other regularization techniques. Moreover, we find that applying HS to each tree in an RF often improves accuracy, as well as its interpretability by simplifying and stabilizing its decision boundaries and SHAP values. We further explain the success of HS in improving prediction performance by showing its equivalence to ridge regression on a (supervised) basis constructed of decision stumps associated with the internal nodes of a tree. All code and models are released in a full-fledged package available on Github (github.com/csinva/imodels)",
                    "In this paper, we investigate a scenario in which late time cosmic acceleration might arise due to coupling between dark matter and baryonic matter without resorting to dark energy or large scale modification of gravity associated with extra degrees of freedom. The scenario can give rise to late time acceleration in Jordan frame and no acceleration in Einstein frame - \\textit{generic modification of gravity} caused by disformal coupling. Using a simple parametrization of the coupling function, in maximally disformal case, we constrain the model parameters by using the age constraints due to globular cluster data. We also obtain observational constraints on the parameters using $H(z)+SNIa+BAO$ data sets. In this case, we distinguish between phantom and non phantom acceleration and show that the model can give rise to phantom behavior in a narrow region of parameter space.",
                    "Mean decrease in impurity (MDI) is a popular feature importance measure for random forests (RFs). We show that the MDI for a feature $X_k$ in each tree in an RF is equivalent to the unnormalized $R^2$ value in a linear regression of the response on the collection of decision stumps that split on $X_k$. We use this interpretation to propose a flexible feature importance framework called MDI+. Specifically, MDI+ generalizes MDI by allowing the analyst to replace the linear regression model and $R^2$ metric with regularized generalized linear models (GLMs) and metrics better suited for the given data structure. Moreover, MDI+ incorporates additional features to mitigate known biases of decision trees against additive or smooth models. We further provide guidance on how practitioners can choose an appropriate GLM and metric based upon the Predictability, Computability, Stability framework for veridical data science. Extensive data-inspired simulations show that MDI+ significantly outperforms popular feature importance measures in identifying signal features. We also apply MDI+ to two real-world case studies on drug response prediction and breast cancer subtype classification. We show that MDI+ extracts well-established predictive genes with significantly greater stability compared to existing feature importance measures. All code and models are released in a full-fledged python package on Github.",
                    "Modern machine learning has achieved impressive prediction performance, but often sacrifices interpretability, a critical consideration in high-stakes domains such as medicine. In such settings, practitioners often use highly interpretable decision tree models, but these suffer from inductive bias against additive structure. To overcome this bias, we propose Fast Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to simultaneously grow a flexible number of trees in summation. By combining logical rules with addition, FIGS is able to adapt to additive structure while remaining highly interpretable. Extensive experiments on real-world datasets show that FIGS achieves state-of-the-art prediction performance. To demonstrate the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical decision instruments (CDIs), which are tools for guiding clinical decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS that accounts for the heterogeneity in medical data. G-FIGS derives CDIs that reflect domain knowledge and enjoy improved specificity (by up to 20% over CART) without sacrificing sensitivity or interpretability. To provide further insight into FIGS, we prove that FIGS learns components of additive models, a property we refer to as disentanglement. Further, we show (under oracle conditions) that unconstrained tree-sum models leverage disentanglement to generalize more efficiently than single decision tree models when fitted to additive regression functions. Finally, to avoid overfitting with an unconstrained number of splits, we develop Bagging-FIGS, an ensemble version of FIGS that borrows the variance reduction techniques of random forests. Bagging-FIGS enjoys competitive performance with random forests and XGBoost on real-world datasets."
                ],
                "domain": [
                    "Cosmology",
                    "Causal Inference",
                    "Machine Learning",
                    "Decision Trees"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "41f2e60c-0f39-4cd6-b09c-415777e214f1": {
                "pk": "41f2e60c-0f39-4cd6-b09c-415777e214f1",
                "project_name": null,
                "name": "Aaron Kornblith",
                "bio": "I am a researcher dedicated to advancing machine learning methodologies in high-stakes domains, particularly healthcare. My work addresses the dual challenges of generalizing to diverse data distributions while maintaining interpretability. I developed the Group Probability-Weighted Tree Sums (G-FIGS) method, which enhances prediction performance on clinical datasets by effectively pooling data across distinct groups, resulting in concise, rule-based models that align with medical expertise.\n\nMy earlier work led to the creation of Fast Interpretable Greedy-Tree Sums (FIGS), a novel approach that generalizes the CART algorithm to grow multiple decision trees in summation. This method not only improves prediction accuracy but also retains the interpretability essential for clinical decision-making. I have demonstrated that FIGS can derive clinical decision instruments (CDIs) that reflect domain knowledge while enhancing specificity without sacrificing sensitivity.\n\nThrough my research, I have proven that FIGS learns components of additive models, a property I term \"disentanglement,\" which allows for more efficient generalization compared to traditional decision tree models. Additionally, I developed Bagging-FIGS, an ensemble version that leverages variance reduction techniques to compete with established methods like random forests and XGBoost.\n\nI am passionate about making machine learning both effective and interpretable, ensuring that our models can be trusted in critical applications. All my code, data, and models are openly available on GitHub, reflecting my commitment to transparency and collaboration in research.",
                "collaborators": [
                    "Keyan Nasseri",
                    "Chandan Singh",
                    "James Duncan",
                    "Bin Yu",
                    "Yan Shuo Tan",
                    "Abhineet Agarwal",
                    "Omer Ronen",
                    "Matthew Epland"
                ],
                "pub_titles": [
                    "Group Probability-Weighted Tree Sums for Interpretable Modeling of Heterogeneous Data",
                    "Fast Interpretable Greedy-Tree Sums"
                ],
                "pub_abstracts": [
                    "Machine learning in high-stakes domains, such as healthcare, faces two critical challenges: (1) generalizing to diverse data distributions given limited training data while (2) maintaining interpretability. To address these challenges, we propose an instance-weighted tree-sum method that effectively pools data across diverse groups to output a concise, rule-based model. Given distinct groups of instances in a dataset (e.g., medical patients grouped by age or treatment site), our method first estimates group membership probabilities for each instance. Then, it uses these estimates as instance weights in FIGS (Tan et al. 2022), to grow a set of decision trees whose values sum to the final prediction. We call this new method Group Probability-Weighted Tree Sums (G-FIGS). G-FIGS achieves state-of-the-art prediction performance on important clinical datasets; e.g., holding the level of sensitivity fixed at 92%, G-FIGS increases specificity for identifying cervical spine injury by up to 10% over CART and up to 3% over FIGS alone, with larger gains at higher sensitivity levels. By keeping the total number of rules below 16 in FIGS, the final models remain interpretable, and we find that their rules match medical domain expertise. All code, data, and models are released on Github.",
                    "Modern machine learning has achieved impressive prediction performance, but often sacrifices interpretability, a critical consideration in high-stakes domains such as medicine. In such settings, practitioners often use highly interpretable decision tree models, but these suffer from inductive bias against additive structure. To overcome this bias, we propose Fast Interpretable Greedy-Tree Sums (FIGS), which generalizes the CART algorithm to simultaneously grow a flexible number of trees in summation. By combining logical rules with addition, FIGS is able to adapt to additive structure while remaining highly interpretable. Extensive experiments on real-world datasets show that FIGS achieves state-of-the-art prediction performance. To demonstrate the usefulness of FIGS in high-stakes domains, we adapt FIGS to learn clinical decision instruments (CDIs), which are tools for guiding clinical decision-making. Specifically, we introduce a variant of FIGS known as G-FIGS that accounts for the heterogeneity in medical data. G-FIGS derives CDIs that reflect domain knowledge and enjoy improved specificity (by up to 20% over CART) without sacrificing sensitivity or interpretability. To provide further insight into FIGS, we prove that FIGS learns components of additive models, a property we refer to as disentanglement. Further, we show (under oracle conditions) that unconstrained tree-sum models leverage disentanglement to generalize more efficiently than single decision tree models when fitted to additive regression functions. Finally, to avoid overfitting with an unconstrained number of splits, we develop Bagging-FIGS, an ensemble version of FIGS that borrows the variance reduction techniques of random forests. Bagging-FIGS enjoys competitive performance with random forests and XGBoost on real-world datasets."
                ],
                "domain": [
                    "Machine Learning",
                    "Interpretability",
                    "Healthcare",
                    "Decision Trees"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "1231a6cc-bcb1-4a2d-93b4-a61c0fdc100c": {
                "pk": "1231a6cc-bcb1-4a2d-93b4-a61c0fdc100c",
                "project_name": null,
                "name": "Bin Yu",
                "bio": "I am a researcher deeply engaged in the study of dynamical systems, particularly focusing on the intricate relationships between topology and flow dynamics in 3-manifolds. My work spans a variety of topics, including the classification of nonsingular Smale flows, the development of Lyapunov graphs, and the exploration of Anosov flows on hyperbolic manifolds. \n\nIn my recent publications, I have delved into the classification of expanding attractors and non-transitive Anosov flows, revealing unique properties and structures that contribute to our understanding of these complex systems. I have also investigated the connections between template theory and basic sets of Smale flows, employing symbolic dynamics and combinatorial techniques to uncover new insights.\n\nMy research emphasizes the importance of stability in statistical analysis, particularly in high-dimensional data contexts, where I have proposed innovative methods to enhance reproducibility and reliability in statistical inference. I am passionate about bridging theoretical concepts with practical applications, as evidenced by my work on boosting algorithms in machine learning, where I analyze convergence and consistency under various conditions.\n\nOverall, my goal is to advance the field of dynamical systems and contribute to the broader mathematical community by providing rigorous frameworks and novel insights that can inspire future research.",
                "collaborators": [
                    "Jiagang Yang",
                    "Francois Béguin",
                    "Jiming Ma",
                    "Fangfang Chen",
                    "Tong Zhang"
                ],
                "pub_titles": [
                    "Comment: Monitoring Networked Applications With Incremental Quantile Estimation",
                    "Lyapunov graphs of nonsingular Smale flows on $S^{1}\\times S^{2}$",
                    "Regular level sets of Lyapunov graphs of nonsingular Smale flows on 3-manifolds",
                    "Remembering Leo",
                    "The Templates of Nonsingular Smale Flows on Three Manifolds",
                    "Depth $0$ Nonsingular Morse Smale flows on $S^3$",
                    "Affine Hirsch foliations on 3-manifolds",
                    "Anosov flows on Dehn surgeries on the figure-eight knot",
                    "Self orbit equivalences on the Anosov flows on Dehn surgeries on the figure-eight knot",
                    "Stability",
                    "Classifying expanding attractors on figure eight knot complement space and non-transitive Anosov flows on Franks-Williams manifold",
                    "Existence of arbitrary large numbers of non-$\\mathbb R$-covered Anosov flows on hyperbolic $3$-manifolds",
                    "Genus two Smale-Williams solenoids in 3-manifolds",
                    "The indexed links of Non-singular Morse-Smale flows on graph manifolds",
                    "Boosting with early stopping: Convergence and consistency"
                ],
                "pub_abstracts": [
                    "Comment: Monitoring Networked Applications With Incremental Quantile Estimation [arXiv:0708.0302]",
                    "In this paper, following J. Franks' work on Lyapunov graphs of nonsingular Smale flows on $S^3$, we study Lyapunov graphs of nonsingular Smale flows on $S^1 \\times S^2$. More precisely, we determine necessary and sufficient conditions on an abstract Lyapunov graph to be associated with a nonsingular Smale flow on $S^1 \\times S^2$. We also study the singular type vertices in Lyapunov graphs of nonsingular Smale flows on 3-manifolds.",
                    "In this paper, we first discuss the regular level set of a nonsingular Smale flow (NSF) on a 3-manifold. The main result about this topic is that a 3-manifold $M$ admits an NSF flow which has a regular level set homeomorphic to $(n+1)T^{2}$ $(n\\in \\mathbb{Z}, n\\geq 0)$ if and only if $M=M'\\sharp n S^{1}\\times S^{2}$. Then we discuss how to realize a template as a basic set of an NSF on a 3-manifold. We focus on the connection between the genus of the template $T$ and the topological structure of the realizing 3-manifold $M$.",
                    "I do not remember when was the first time that I met Leo, but I have a clear memory of going to Leo's office on the 4th floor of Evans Hall to talk to him in my second year in Berkeley's Ph.D. program in 1986. The details of the conversation are not retained but a visual image of his clean and orderly office remains, in a stark contrast to a high entropy state of the same office now being used by myself.",
                    "In this paper, we first discuss some connections between template theory and the description of basic sets of Smale flows on 3-manifolds due to F. B\\'eguin and C. Bonatti. The main tools we use are symbolic dynamics, template moves and some combinatorial surgeries. Second, we obtain some relationship between the surgeries and the number of $S^1 \\times S^2$ factors of $M$ for a nonsingular Smale flow on a given closed orientable 3-manifold $M$. Besides these, we also prove that any template $T$ can model a basic set $\\Lambda$ of a nonsingular Smale flow on $nS^1 \\times S^2$ for some positive integer $n$.",
                    "In this paper, we first develope the concept of Lyapunov graph to weighted Lyapunov graph (abbreviated as WLG) for nonsingular Morse-Smale flows (abbreviated as NMS flows) on $S^3$. WLG is quite sensitive to NMS flows on $S^3$. For instance, WLG detect the indexed links of NMS flows. Then we use WLG and some other tools to describe nonsingular Morse-Smale flows without heteroclinic trajectories connecting saddle orbits (abbreviated as depth $0$ NMS flows). It mainly contains the following several directions: \\begin{enumerate}   \\item we use WLG to list depth $0$ NMS flows on $S^3$;   \\item with the help of WLG, comparing with Wada's algorithm, we provide a direct description about the (indexed) link of depth $0$ NMS flows;   \\item to overcome the weakness that WLG can't decide topologically equivalent class, we give a simplified Umanskii Theorem to decide when two depth $0$ NMS flows on $S^3$ are topological equivalence;   \\item under these theories, we classify (up to topological equivalence) all depth 0 NMS flows on $S^3$ with periodic orbits number no more than 4. \\end{enumerate}",
                    "This paper is devoted to discussing affine Hirsch foliations on $3$-manifolds. First, we prove that up to isotopic leaf-conjugacy, every closed orientable $3$-manifold $M$ admits $0$, $1$ or $2$ affine Hirsch foliations. Furthermore, every case is possible.   Then, we analyze the $3$-manifolds admitting two affine Hirsch foliations (abbreviated as Hirsch manifolds). On the one hand, we construct Hirsch manifolds by using exchangeable braided links (abbreviated as DEBL Hirsch manifolds); on the other hand, we show that every Hirsch manifold virtually is a DEBL Hirsch manifold.   Finally, we show that for every $n\\in \\mathbb{N}$, there are only finitely many Hirsch manifolds with strand number $n$. Here the strand number of a Hirsch manifold $M$ is a positive integer defined by using strand numbers of braids.",
                    "The purpose of this paper is to classify Anosov flows on the 3-manifolds obtained by Dehn surgeries on the figure-eight knot. This set of 3-manifolds is denoted by M(r) (r is a ratioanl number), which contains the first class of hyperbolic 3-manifolds admitting Anosov flows in history, discovered by Goodman. Combining with the classification of Anosov flows on the sol-manifold M(0) due to Plante, we have: 1. if r is an integer, up to topological equivalence, M(r) exactly carries a unique Anosov flow, which is constructed by Goodman by doing a Dehn-Fried-Goodman surgery on a suspension Anosov flow; 2. if r is not an integer, M(r) does not carry any Anosov flow. As a consequence of the second result, we get infinitely many closed orientable hyperbolic 3-manifolds which carry taut foliations but does not carry any Anosov flow. The fundamental tool in the proofs is the set of branched surfaces built by Schwider, which is used to carry essential laminations on M(r).",
                    "Let $M_k$ ($k\\in \\mathbb Z$ and $|k|>4$) be the $3$-manifold obtained by doing $k$ Dehn surgery on the figure-eight knot, and $X_t$ be the canonical Anosov flow on $M_k$ that is constructed by Goodman. The main result of this article is that if $|k|\\gg 0$, then $Mod (M_k) \\cong \\mathbb {Z}_2 \\oplus \\mathbb {Z}_2$ and every element of the mapping class group of $M_k$ can be represented by a self orbit equivalence of $X_t$.",
                    "Reproducibility is imperative for any scientific discovery. More often than not, modern scientific findings rely on statistical analysis of high-dimensional data. At a minimum, reproducibility manifests itself in stability of statistical results relative to \"reasonable\" perturbations to data and to the model used. Jacknife, bootstrap, and cross-validation are based on perturbations to data, while robust statistics methods deal with perturbations to models. In this article, a case is made for the importance of stability in statistics. Firstly, we motivate the necessity of stability for interpretable and reliable encoding models from brain fMRI signals. Secondly, we find strong evidence in the literature to demonstrate the central role of stability in statistical inference, such as sensitivity analysis and effect detection. Thirdly, a smoothing parameter selector based on estimation stability (ES), ES-CV, is proposed for Lasso, in order to bring stability to bear on cross-validation (CV). ES-CV is then utilized in the encoding models to reduce the number of predictors by 60% with almost no loss (1.3%) of prediction performance across over 2,000 voxels. Last, a novel \"stability\" argument is seen to drive new results that shed light on the intriguing interactions between sample to sample variability and heavier tail error distribution (e.g., double-exponential) in high-dimensional regression models with $p$ predictors and $n$ independent samples. In particular, when $p/n\\rightarrow\\kappa\\in(0.3,1)$ and the error distribution is double-exponential, the Ordinary Least Squares (OLS) is a better estimator than the Least Absolute Deviation (LAD) estimator.",
                    "The path closure of figure eight knot complement space, $N_0$, supports a natural DA (derived from Anosov) expanding attractor. Using this attractor, Franks-Williams constructed the first example of non-transitive Anosov flow on the manifold $M_0$ obtained by gluing two copies of $N_0$ through identity map along their boundaries, named by Franks-Williams manifold. In this paper, our main goal is to classify expanding attractors on $N_0$ and non-transitive Anosov flows on $M_0$. We prove that, up to orbit-equivalence, the DA expanding attractor is the unique expanding attractor supported by $N_0$, and the non-transitive Anosov flow constructed by Franks and Williams is the unique non-transitive Anosov flow admitted by $M_0$. Moreover, more general cases are also discussed. In particular, we completely classify non-transitive Anosov flows on a family of infinitely many toroidal $3$-manifolds with two hyperbolic pieces, obtained by gluing two copies of $N_0$ through any gluing homeomorphism.",
                    "The purpose of this paper is to prove that, for every $n\\in \\mathbb N$, there exists a closed hyperbolic $3$-manifold $M$ which carries at least $n$ non-$\\mathbb R$-covered Anosov flows, that are pairwise non-orbitally equivalent. Due to a recent result by Fenley, such Anosov flows are quasi-geodesic. Hence, we get the existence of hyperbolic $3$-manifolds carrying many pairwise non-orbitally equivalent quasi-geodesic Anosov flows. Based on the orbit space theory of Anosov flows developed by Fenley and Barbot, we completely studied the connected components of edge-adjacent lozenges, say, clusters in the orbit spaces of the Anosov flows in construction. The number of the lozenges of a cluster and a type of orientations on the clusters together are used as dynamical invariants to prove that the flows we construct are not orbitally equivalent. We believe that those dynamical invariants could be used in a much wider context.",
                    "Using alternating Heegaard diagrams, we construct some 3-manifolds which admit diffeomorphisms such that the non-wandering sets of the diffeomorphisms are composed of Smale-Williams solenoid attractors and repellers, an interesting example is the truncated-cube space. In addition, we prove that if the nonwandering set of the diffeomorphism consists of genus two Smale-Williams solenoids, then the Heegaard genus of the closed manifold is at most two.",
                    "We classify the indexed links corresponding to the union of the closed orbits of non-singular Morse-Smale flows on most graph manifolds. We find that each of this kind of indexed links can be obtained by applying a finite steps of operations on a special indexed link, which consists of all of the singular Seifert fibers and some regular Seifert fibers with some precisely described conditions.",
                    "Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulting estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set. This paper studies numerical convergence, consistency and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting's greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early-stopping strategies under which boosting is shown to be consistent based on i.i.d. samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting. As a side product, these results also reveal the importance of restricting the greedy search step-sizes, as known in practice through the work of Friedman and others. Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with \\epsilon\\to0 step-size becomes an L^1-margin maximizer when left to run to convergence."
                ],
                "domain": [
                    "Dynamical Systems",
                    "Topology",
                    "Statistical Learning",
                    "Anosov Flows"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "db547db0-f3a1-4af6-b22e-3332b2770078": {
                "pk": "db547db0-f3a1-4af6-b22e-3332b2770078",
                "project_name": null,
                "name": "Chenyan Xiong",
                "bio": "I am a researcher dedicated to advancing the field of natural language processing (NLP) and information retrieval, with a particular focus on leveraging large language models (LLMs) and knowledge graphs. My recent work includes the development of ResearchArena, a benchmark designed to evaluate LLMs' capabilities in conducting academic surveys, which highlights the need for domain-specific expertise in NLP tasks. I have also introduced innovative frameworks such as the Entity-Duet Neural Ranking Model (EDRM) and the Kernel Graph Attention Network (KGAT), which integrate knowledge graphs into neural search systems and enhance fact verification processes, respectively.\n\nMy research extends to improving retrieval systems through novel approaches like ANCE-PRF, which enhances query representations using pseudo relevance feedback, and Montessori-Instruct, a data synthesis framework that tailors synthetic data generation to optimize student learning. I am particularly interested in the intersection of multimodal models and factual accuracy, as demonstrated in my work on FactMM-RAG, which aims to improve radiology report generation.\n\nThrough my studies, I strive to bridge the gap between theoretical advancements and practical applications, ensuring that my contributions not only push the boundaries of NLP research but also provide tangible benefits in real-world scenarios. I am passionate about exploring the potential of LLMs and knowledge graphs to create more effective and intelligent systems for information retrieval and understanding.",
                "collaborators": [
                    "Zhiyuan Liu",
                    "Zhenghao Liu",
                    "Jamie Callan",
                    "Maosong Sun",
                    "Hao Kang",
                    "Tie-Yan Liu",
                    "Yue Yin",
                    "Cheng Luo",
                    "Hiroaki Hayashi",
                    "Zecong Hu"
                ],
                "pub_titles": [
                    "ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents",
                    "Word-Entity Duet Representations for Document Ranking",
                    "Neural Document Expansion with User Feedback",
                    "Latent Relation Language Models",
                    "Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval",
                    "Automatic Event Salience Identification",
                    "Understanding the Behaviors of BERT in Ranking",
                    "Fine-grained Fact Verification with Kernel Graph Attention Network",
                    "Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs",
                    "Selective Weak Supervision for Neural Information Retrieval",
                    "Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback",
                    "Explore Entity Embedding Effectiveness in Entity Retrieval",
                    "TREC CAsT 2019: The Conversational Assistance Track Overview",
                    "Unsupervised Dense Retrieval Training with Web Anchors",
                    "Improving Multitask Retrieval by Promoting Task Specialization",
                    "Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model",
                    "In-Context Probing Approximates Influence Function for Data Valuation",
                    "Fact-Aware Multimodal Retrieval Augmentation for Accurate Medical Radiology Report Generation",
                    "Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning",
                    "More Robust Dense Retrieval with Contrastive Dual Learning"
                ],
                "pub_abstracts": [
                    "Large language models (LLMs) have exhibited remarkable performance across various tasks in natural language processing. Nevertheless, challenges still arise when these tasks demand domain-specific expertise and advanced analytical skills, such as conducting research surveys on a designated topic. In this research, we develop ResearchArena, a benchmark that measures LLM agents' ability to conduct academic surveys, an initial step of academic research process. Specifically, we deconstructs the surveying process into three stages 1) information discovery: locating relevant papers, 2) information selection: assessing papers' importance to the topic, and 3) information organization: organizing papers into meaningful structures. In particular, we establish an offline environment comprising 12.0M full-text academic papers and 7.9K survey papers, which evaluates agents' ability to locate supporting materials for composing the survey on a topic, rank the located papers based on their impact, and organize these into a hierarchical knowledge mind-map. With this benchmark, we conduct preliminary evaluations of existing techniques and find that all LLM-based methods under-performing when compared to basic keyword-based retrieval techniques, highlighting substantial opportunities for future research.",
                    "This paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval. In this work, the query and documents are modeled by word-based representations and entity-based representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed. With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet. Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the attention mechanism successfully steers the model away from noisy entities, and together they significantly outperform both word-based and entity-based learning to rank systems.",
                    "This paper presents a neural document expansion approach (NeuDEF) that enriches document representations for neural ranking models. NeuDEF harvests expansion terms from queries which lead to clicks on the document and weights these expansion terms with learned attention. It is plugged into a standard neural ranker and learned end-to-end. Experiments on a commercial search log demonstrate that NeuDEF significantly improves the accuracy of state-of-the-art neural rankers and expansion methods on queries with different frequencies. Further studies show the contribution of click queries and learned expansion weights, as well as the influence of document popularity of NeuDEF's effectiveness.",
                    "In this paper, we propose Latent Relation Language Models (LRLMs), a class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations. This model has a number of attractive properties: it not only improves language modeling performance, but is also able to annotate the posterior probability of entity spans for a given text through relations. Experiments demonstrate empirical improvements over both a word-based baseline language model and a previous approach that incorporates knowledge graph information. Qualitative analysis further demonstrates the proposed model's ability to learn to predict appropriate relations in context.",
                    "This paper presents the Entity-Duet Neural Ranking Model (EDRM), which introduces knowledge graphs to neural search systems. EDRM represents queries and documents by their words and entity annotations. The semantics from knowledge graphs are integrated in the distributed representations of their entities, while the ranking is conducted by interaction-based neural ranking networks. The two components are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval. Our experiments on a commercial search log demonstrate the effectiveness of EDRM. Our analyses reveal that knowledge graph semantics significantly improve the generalization ability of neural ranking models.",
                    "Identifying the salience (i.e. importance) of discourse units is an important task in language understanding. While events play important roles in text documents, little research exists on analyzing their saliency status. This paper empirically studies the Event Salience task and proposes two salience detection models based on content similarities and discourse relations. The first is a feature based salience model that incorporates similarities among discourse units. The second is a neural model that captures more complex relations between discourse units. Tested on our new large-scale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures).",
                    "This paper studies the performances and behaviors of BERT in ranking tasks. We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking. Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model. Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking. Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker.",
                    "Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims. This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions. Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification. KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification. Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT's effectiveness.",
                    "Human conversations naturally evolve around related concepts and scatter to multi-hop concepts. This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows. By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations. The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses. Experiments on Reddit conversations demonstrate ConceptFlow's effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures. All source codes of this work are available at https://github.com/thunlp/ConceptFlow.",
                    "This paper democratizes neural information retrieval to scenarios where large scale relevance training signals are not available. We revisit the classic IR intuition that anchor-document relations approximate query-document relevance and propose a reinforcement weak supervision selection method, ReInfoSelect, which learns to select anchor-document pairs that best weakly supervise the neural ranker (action), using the ranking performance on a handful of relevance labels as the reward. Iteratively, for a batch of anchor-document pairs, ReInfoSelect back propagates the gradients through the neural ranker, gathers its NDCG reward, and optimizes the data selection network using policy gradients, until the neural ranker's performance peaks on target relevance metrics (convergence). In our experiments on three TREC benchmarks, neural rankers trained by ReInfoSelect, with only publicly available anchor data, significantly outperform feature-based learning to rank methods and match the effectiveness of neural rankers trained with private commercial search logs. Our analyses show that ReInfoSelect effectively selects weak supervision signals based on the stage of the neural ranker training, and intuitively picks anchor-document pairs similar to query-document pairs.",
                    "Dense retrieval systems conduct first-stage retrieval using embedded representations and simple similarity metrics to match a query to documents. Its effectiveness depends on encoded embeddings to capture the semantics of queries and documents, a challenging task due to the shortness and ambiguity of search queries. This paper proposes ANCE-PRF, a new query encoder that uses pseudo relevance feedback (PRF) to improve query representations for dense retrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top retrieved documents from a dense retrieval model, ANCE, and it learns to produce better query embeddings directly from relevance labels. It also keeps the document index unchanged to reduce overhead. ANCE-PRF significantly outperforms ANCE and other recent dense retrieval systems on several datasets. Analysis shows that the PRF encoder effectively captures the relevant and complementary information from PRF documents, while ignoring the noise with its learned attention mechanism.",
                    "This paper explores entity embedding effectiveness in ad-hoc entity retrieval, which introduces distributed representation of entities into entity retrieval. The knowledge graph contains lots of knowledge and models entity semantic relations with the well-formed structural representation. Entity embedding learns lots of semantic information from the knowledge graph and represents entities with a low-dimensional representation, which provides an opportunity to establish interactions between query related entities and candidate entities for entity retrieval. Our experiments demonstrate the effectiveness of entity embedding based model, which achieves more than 5\\% improvement than the previous state-of-the-art learning to rank based entity retrieval model. Our further analysis reveals that the entity semantic match feature effective, especially for the scenario which needs more semantic understanding.",
                    "The Conversational Assistance Track (CAsT) is a new track for TREC 2019 to facilitate Conversational Information Seeking (CIS) research and to create a large-scale reusable test collection for conversational search systems. The document corpus is 38,426,252 passages from the TREC Complex Answer Retrieval (CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty information seeking dialogues (30 train, 50 test) are an average of 9 to 10 questions long. Relevance assessments are provided for 30 training topics and 20 test topics. This year 21 groups submitted a total of 65 runs using varying methods for conversational query understanding and ranking. Methods include traditional retrieval based methods, feature based learning-to-rank, neural models, and knowledge enhanced methods. A common theme through the runs is the use of BERT-based neural reranking methods. Leading methods also employed document expansion, conversational query expansion, and generative language models for conversational query rewriting (GPT-2). The results show a gap between automatic systems and those using the manually resolved utterances, with a 35% relative improvement of manual rewrites over the best automatic system.",
                    "In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as ``homepage'' or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.",
                    "In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model (one that is explicitly optimized for multitasking) along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.",
                    "Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation of diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios.",
                    "Data valuation quantifies the value of training data, and is used for data attribution (i.e., determining the contribution of training data towards model predictions), and data selection; both of which are important for curating high-quality datasets to train large language models. In our paper, we show that data valuation through in-context probing (i.e., prompting a LLM) approximates influence functions for selecting training data. We provide a theoretical sketch on this connection based on transformer models performing \"implicit\" gradient descent on its in-context inputs. Our empirical findings show that in-context probing and gradient-based influence frameworks are similar in how they rank training data. Furthermore, fine-tuning experiments on data selected by either method reveal similar model performance.",
                    "Multimodal foundation models hold significant potential for automating radiology report generation, thereby assisting clinicians in diagnosing cardiac diseases. However, generated reports often suffer from serious factual inaccuracy. In this paper, we introduce a fact-aware multimodal retrieval-augmented pipeline in generating accurate radiology reports (FactMM-RAG). We first leverage RadGraph to mine factual report pairs, then integrate factual knowledge to train a universal multimodal retriever. Given a radiology image, our retriever can identify high-quality reference reports to augment multimodal foundation models, thus enhancing the factual completeness and correctness of report generation. Experiments on two benchmark datasets show that our multimodal retriever outperforms state-of-the-art retrievers on both language generation and radiology-specific metrics, up to 6.5% and 2% score in F1CheXbert and F1RadGraph. Further analysis indicates that employing our factually-informed training strategy imposes an effective supervision signal, without relying on explicit diagnostic label guidance, and successfully propagates fact-aware capabilities from the multimodal retriever to the multimodal foundation model in radiology report generation.",
                    "Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at https://github.com/cxcscmu/Montessori-Instruct.",
                    "Dense retrieval conducts text retrieval in the embedding space and has shown many advantages compared to sparse retrieval. Existing dense retrievers optimize representations of queries and documents with contrastive training and map them to the embedding space. The embedding space is optimized by aligning the matched query-document pairs and pushing the negative documents away from the query. However, in such training paradigm, the queries are only optimized to align to the documents and are coarsely positioned, leading to an anisotropic query embedding space. In this paper, we analyze the embedding space distributions and propose an effective training paradigm, Contrastive Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained query representations for dense retrieval. DANCE incorporates an additional dual training object of query retrieval, inspired by the classic information retrieval training axiom, query likelihood. With contrastive learning, the dual training object of DANCE learns more tailored representations for queries and documents to keep the embedding space smooth and uniform, thriving on the ranking performance of DANCE on the MS MARCO document retrieval task. Different from ANCE that only optimized with the document retrieval task, DANCE concentrates the query embeddings closer to document representations while making the document distribution more discriminative. Such concentrated query embedding distribution assigns more uniform negative sampling probabilities to queries and helps to sufficiently optimize query representations in the query retrieval task. Our codes are released at https://github.com/thunlp/DANCE."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Information Retrieval",
                    "Machine Learning",
                    "Knowledge Graphs"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can an artificial intelligence \"Co-Pilot\" system effectively reduce emergency department (ED) length of stay (LOS) and improve diagnostic accuracy by recommending the most informative laboratory tests after patient triage?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving the problem of ED crowding through an AI-driven diagnostic assistant has significant implications for the healthcare research community. It can lead to improved patient outcomes by ensuring timely interventions for critically ill patients, thereby reducing morbidity and mortality rates. Additionally, it can enhance the efficiency of healthcare systems, potentially lowering costs associated with unnecessary laboratory tests and prolonged ED stays. This research could pave the way for future studies on AI applications in healthcare, influencing how diagnostic processes are approached and integrated into clinical workflows.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in addressing this problem include the complexity of accurately predicting patient outcomes based on diverse and often incomplete data, as well as the need to balance diagnostic accuracy with time efficiency. Naive approaches may fail due to the intricacies of patient conditions and the variability in laboratory test relevance. Technical obstacles include the integration of machine learning models with real-time clinical data and ensuring that recommendations are clinically actionable. Theoretical challenges involve developing models that can generalize across different patient populations and clinical scenarios.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on individual aspects of ED crowding or laboratory testing without integrating AI systems that consider the dynamic nature of patient data. Limitations in existing solutions include a lack of comprehensive datasets that reflect real-world ED practices and insufficient collaboration between AI researchers and clinical practitioners. Barriers such as data privacy concerns and the complexity of clinical workflows have also hindered progress. This approach differs by utilizing a curated benchmark dataset (MIMIC-ED-Assist) and a personalized AI model (ED-Copilot) that leverages historical patient data to make informed recommendations.\n\n**[Question 5] - What are the key components of my approach and results?**  \nThe proposed methodology involves developing the ED-Copilot system, which utilizes a bio-medical pre-trained language model (BioGPT) fine-tuned on patient data to suggest laboratory test groups. The dataset used is MIMIC-ED-Assist, which includes critical outcomes and ED LOS metrics. The performance will be evaluated based on the accuracy of laboratory recommendations and the reduction in time-cost for testing. Expected outcomes include a significant"
    },
    "2409.19100": {
        "paper_data": {
            "title": "Outlining the Borders for LLM Applications in Patient Education: Developing an Expert-in-the-Loop LLM-Powered Chatbot for Prostate Cancer Patient Education",
            "url": "http://arxiv.org/abs/2409.19100v1",
            "arxiv_id": "2409.19100",
            "authors": [
                "Yuexing Hao",
                "Jason Holmes",
                "Mark Waddle",
                "Nathan Yu",
                "Kirstin Vickers",
                "Heather Preston",
                "Drew Margolin",
                "Corinna E. Löckenhoff",
                "Aditya Vashistha",
                "Marzyeh Ghassemi",
                "Saleh Kalantari",
                "Wei Liu"
            ],
            "abstract": "Cancer patients often struggle to transition swiftly to treatment due to limited institutional resources, lack of sophisticated professional guidance, and low health literacy. The emergence of Large Language Models (LLMs) offers new opportunities for such patients to access the wealth of existing patient education materials. The current paper presents the development process for an LLM-based chatbot focused on prostate cancer education, including needs assessment, co-design, and usability studies. The resulting application, MedEduChat, integrates with patients' electronic health record data and features a closed-domain, semi-structured, patient-centered approach to address real-world needs. This paper contributes to the growing field of patient-LLM interaction by demonstrating the potential of LLM-based chatbots to enhance prostate cancer patient education and by offering co-design guidelines for future LLM-based healthcare downstream applications.",
            "introduction": "   1. Introduction  Cancer patients are vulnerable and face high uncertainty, and they are in need of information and social support to help them maintain agency and make informed decisions about their treatment (Austin et al., 2021; Marzorati et al., 2018; Chelf et al., 2001). In addition to learning about the specific details of potential biomedical treatments, they may also need assistance with psychological and social concerns (Page et al., 2008). However, current cancer treatment regimens often lack a comprehensive and timely educational component, leaving patients without the support they urgently need during these challenging and stressful times (Fagerlin et al., 2004; Aunan et al., 2021). Because of the need to understand and the burdens of dealing with cancer, patients would like to consult with clinical professionals regarding these issues (Stenberg et al., 2010; van de Haar et al., 2020). However, in current medical systems, clinical professionals have limited time and capacity to engage in this kind of dialogue (Fiscella and Epstein, 2008), and understanding the evolving personal and cognitive needs of patients is often the first thing that is cut when healthcare resources are strained (Smeets et al., 2020).   Large language models (LLMs) offer the capability of acting as chatbot companions for cancer patients by providing both medical information and emotional support (Thirunavukarasu et al., 2023; Collin et al., 2024). Leveraging LLMs’ capabilities for information summarization, explanation, and real-time interactions in natural language processing tasks, a chatbot built on strong LLM features that are tailored to a particular cancer domain can offer unique personalized interactions that respond to patient inquiries (Ramjee et al., 2024; Didier et al., 2024). Unlike conventional chatbots, LLM-based chatbots have the potential for more nuanced and individualized engagement (Ouyang et al., 2022), providing real-time interactions and clinical education enhancement for patient users. By providing timely and useful information, these chatbots can help patients understand what to expect physically and emotionally after a diagnosis or at the start of treatment (Ziegler et al., 2022). Among various LLMs, Chat Generative Pre-Trained Transformer 4 (ChatGPT-4) stands out for its lower error rates (Tang et al., 2023; Holmes et al., 2023a, b; Wu et al., 2024; Liu et al., 2022), making it the current preferred choice for such data processing and interactions. Structured peer support can bridge gaps in patients’ needs and alleviate negative emotions (Hu et al., 2019). As patients navigate different stages of their cancer journey, an interactive chatbot can empower them with essential information and educational support (Silva et al., 2018; Puts et al., 2012). Additionally, internet health information resources play a supportive, stimulating, and interactive role in the patient’s treatment journey (Bussey and Sillence, 2019; Robinson et al., 2020; Flynn et al., 2023).   In this paper, we evaluated three research questions regarding the usefulness of educational LLMs for patients: (RQ1) What are the current challenges in prostate cancer patient education? (RQ2) How can we enact a successful co-design and co-development process with patients, AI practitioners, and medical experts for an LLM-based educational tool? (RQ3) Can the resulting LLM-based chatbot enhance prostate cancer patients’ educational experience? To answer RQ1 we conducted a needs-assessment survey with prostate cancer patients currently using electronic learning (e-learning) modules. The e-learning module is a web-based patient education intervention that is mediated",
            "references": [
                {
                    "title": "A trust based framework for the envelopment of medical AI",
                    "abstract": null
                },
                {
                    "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients",
                    "abstract": "The healthcare landscape is evolving, with patients seeking reliable information about their health conditions and available treatment options. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust medical professionals, highlighting the need for expert-endorsed health information. However, increased patient loads on experts has led to reduced communication time, impacting information sharing. To address this gap, we develop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in collaboration with an eye hospital in India. CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. It has multimodal and multilingual capabilities. In an in-the-wild deployment study with 55 participants, CataractBot proved valuable, providing anytime accessibility, saving time, accommodating diverse literacy levels, alleviating power differences, and adding a privacy layer between patients and doctors. Users reported that their trust in the system was established through expert verification. Broadly, our results could inform future work on designing expert-mediated LLM bots."
                },
                {
                    "title": "Using ChatGPT to Facilitate Truly Informed Medical Consent",
                    "abstract": null
                },
                {
                    "title": "Data-Centric Foundation Models in Computational Healthcare: A Survey",
                    "abstract": "The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare-related foundation models and datasets at https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare ."
                },
                {
                    "title": "Knowledge translation strategies to support the sustainability of evidence-based interventions in healthcare: a scoping review",
                    "abstract": null
                },
                {
                    "title": "Presentation matters for AI-generated clinical advice",
                    "abstract": null
                },
                {
                    "title": "Benchmarking a foundation LLM on its ability to re-label structure names in accordance with the AAPM TG-263 report",
                    "abstract": "Purpose: To introduce the concept of using large language models (LLMs) to re-label structure names in accordance with the American Association of Physicists in Medicine (AAPM) Task Group (TG)-263 standard, and to establish a benchmark for future studies to reference. Methods and Materials: The Generative Pre-trained Transformer (GPT)-4 application programming interface (API) was implemented as a Digital Imaging and Communications in Medicine (DICOM) storage server, which upon receiving a structure set DICOM file, prompts GPT-4 to re-label the structure names of both target volumes and normal tissues according to the AAPM TG-263. Three disease sites, prostate, head and neck, and thorax were selected for evaluation. For each disease site category, 150 patients were randomly selected for manually tuning the instructions prompt (in batches of 50) and 50 patients were randomly selected for evaluation. Structure names that were considered were those that were most likely to be relevant for studies utilizing structure contours for many patients. Results: The overall re-labeling accuracy of both target volumes and normal tissues for prostate, head and neck, and thorax cases was 96.0%, 98.5%, and 96.9% respectively. Re-labeling of target volumes was less accurate on average except for prostate - 100%, 93.1%, and 91.1% respectively. Conclusions: Given the accuracy of GPT-4 in re-labeling structure names of both target volumes and normal tissues as presented in this work, LLMs are poised to be the preferred method for standardizing structure names in radiation oncology, especially considering the rapid advancements in LLM capabilities that are likely to continue."
                },
                {
                    "title": "Answering head and neck cancer questions: An assessment of ChatGPT responses.",
                    "abstract": null
                },
                {
                    "title": "RadOnc-GPT: A Large Language Model for Radiation Oncology",
                    "abstract": "This paper presents RadOnc-GPT, a large language model specialized for radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on a large dataset of radiation oncology patient records and clinical notes from the Mayo Clinic in Arizona. The model employs instruction tuning on three key tasks - generating radiotherapy treatment regimens, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. Evaluations conducted by comparing RadOnc-GPT outputs to general large language model outputs showed that RadOnc-GPT generated outputs with significantly improved clarity, specificity, and clinical relevance. The study demonstrated the potential of using large language models fine-tuned using domain-specific knowledge like RadOnc-GPT to achieve transformational capabilities in highly specialized healthcare fields such as radiation oncology."
                },
                {
                    "title": "RE: Artificial intelligence chatbots will revolutionize how cancer patients access information: ChatGPT represents a paradigm-shift",
                    "abstract": "A recent discussion initiated by the commentary compared ChatGPT responses to Google ’ s feature snippets for cancer-related inquiries (1). The commentary opened an important conversation about the potential and challenges of artificial intelligence (AI) in addressing health inquiries, and it highlighted crucial areas for further research. Both ChatGPT and Google"
                },
                {
                    "title": "Assessing AI-Powered Patient Education: A Case Study in Radiology.",
                    "abstract": null
                },
                {
                    "title": "Developing an AI-Assisted Educational Chatbot for Radiotherapy Using the IBM Watson Assistant Platform",
                    "abstract": "Objectives: This study aims to make radiotherapy knowledge regarding healthcare accessible to the general public by developing an AI-powered chatbot. The interactive nature of the chatbot is expected to facilitate better understanding of information on radiotherapy through communication with users. Methods: Using the IBM Watson Assistant platform on IBM Cloud, the chatbot was constructed following a pre-designed flowchart that outlines the conversation flow. This approach ensured the development of the chatbot with a clear mindset and allowed for effective tracking of the conversation. The chatbot is equipped to furnish users with information and quizzes on radiotherapy to assess their understanding of the subject. Results: By adopting a question-and-answer approach, the chatbot can engage in human-like communication with users seeking information about radiotherapy. As some users may feel anxious and struggle to articulate their queries, the chatbot is designed to be user-friendly and reassuring, providing a list of questions for the user to choose from. Feedback on the chatbot’s content was mostly positive, despite a few limitations. The chatbot performed well and successfully conveyed knowledge as intended. Conclusions: There is a need to enhance the chatbot’s conversation approach to improve user interaction. Including translation capabilities to cater to individuals with different first languages would also be advantageous. Lastly, the newly launched ChatGPT could potentially be developed into a medical chatbot to facilitate knowledge transfer."
                },
                {
                    "title": "Radiology-Llama2: Best-in-Class Large Language Model for Radiology",
                    "abstract": "This paper introduces Radiology-Llama2, a large language model specialized for radiology through a process known as instruction tuning. Radiology-Llama2 is based on the Llama2 architecture and further trained on a large dataset of radiology reports to generate coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other generative language models, with a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of localized language models designed and tuned for specialized domains like radiology. When properly evaluated and deployed, such models can transform fields like radiology by automating rote tasks and enhancing human expertise."
                },
                {
                    "title": "Use of Artificial Intelligence Chatbots for Cancer Treatment Information",
                    "abstract": "This survey study examines the performance of a large language model chatbot in providing cancer treatment recommendations that are concordant with National Comprehensive Cancer Network guidelines."
                },
                {
                    "title": "The imperative for regulatory oversight of large language models (or generative AI) in healthcare",
                    "abstract": null
                },
                {
                    "title": "Artificial Intelligence in Clinical Diagnosis: Opportunities, Challenges, and Hype.",
                    "abstract": "\n This Viewpoint examines various aspects of using generative artificial intelligence (AI) in health care, including assisting with making clinical diagnoses, and the challenges that come with using AI, such as ensuring the accuracy of the clinical data on which AI makes its diagnoses.\n"
                },
                {
                    "title": "A generalist vision-language foundation model for diverse biomedical tasks.",
                    "abstract": null
                },
                {
                    "title": "Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum.",
                    "abstract": "Importance\nThe rapid expansion of virtual health care has caused a surge in patient messages concomitant with more work and burnout among health care professionals. Artificial intelligence (AI) assistants could potentially aid in creating answers to patient questions by drafting responses that could be reviewed by clinicians.\n\n\nObjective\nTo evaluate the ability of an AI chatbot assistant (ChatGPT), released in November 2022, to provide quality and empathetic responses to patient questions.\n\n\nDesign, Setting, and Participants\nIn this cross-sectional study, a public and nonidentifiable database of questions from a public social media forum (Reddit's r/AskDocs) was used to randomly draw 195 exchanges from October 2022 where a verified physician responded to a public question. Chatbot responses were generated by entering the original question into a fresh session (without prior questions having been asked in the session) on December 22 and 23, 2022. The original question along with anonymized and randomly ordered physician and chatbot responses were evaluated in triplicate by a team of licensed health care professionals. Evaluators chose \"which response was better\" and judged both \"the quality of information provided\" (very poor, poor, acceptable, good, or very good) and \"the empathy or bedside manner provided\" (not empathetic, slightly empathetic, moderately empathetic, empathetic, and very empathetic). Mean outcomes were ordered on a 1 to 5 scale and compared between chatbot and physicians.\n\n\nResults\nOf the 195 questions and responses, evaluators preferred chatbot responses to physician responses in 78.6% (95% CI, 75.0%-81.8%) of the 585 evaluations. Mean (IQR) physician responses were significantly shorter than chatbot responses (52 [17-62] words vs 211 [168-245] words; t = 25.4; P < .001). Chatbot responses were rated of significantly higher quality than physician responses (t = 13.3; P < .001). The proportion of responses rated as good or very good quality (≥ 4), for instance, was higher for chatbot than physicians (chatbot: 78.5%, 95% CI, 72.3%-84.1%; physicians: 22.1%, 95% CI, 16.4%-28.2%;). This amounted to 3.6 times higher prevalence of good or very good quality responses for the chatbot. Chatbot responses were also rated significantly more empathetic than physician responses (t = 18.9; P < .001). The proportion of responses rated empathetic or very empathetic (≥4) was higher for chatbot than for physicians (physicians: 4.6%, 95% CI, 2.1%-7.7%; chatbot: 45.1%, 95% CI, 38.5%-51.8%; physicians: 4.6%, 95% CI, 2.1%-7.7%). This amounted to 9.8 times higher prevalence of empathetic or very empathetic responses for the chatbot.\n\n\nConclusions\nIn this cross-sectional study, a chatbot generated quality and empathetic responses to patient questions posed in an online forum. Further exploration of this technology is warranted in clinical settings, such as using chatbot to draft responses that physicians could then edit. Randomized trials could assess further if using AI assistants might improve responses, lower clinician burnout, and improve patient outcomes."
                },
                {
                    "title": "Evaluating large language models on medical evidence summarization",
                    "abstract": "Recent advances in large language models (LLMs) have demonstrated remarkable successes in zero- and few-shot performance on various downstream tasks, paving the way for applications in high-stakes domains. In this study, we systematically examine the capabilities and limitations of LLMs, specifically GPT-3.5 and ChatGPT, in performing zero-shot medical evidence summarization across six clinical domains. We conduct both automatic and human evaluations, covering several dimensions of summary quality. Our study demonstrates that automatic metrics often do not strongly correlate with the quality of summaries. Furthermore, informed by our human evaluations, we define a terminology of error types for medical evidence summarization. Our findings reveal that LLMs could be susceptible to generating factually inconsistent summaries and making overly convincing or uncertain statements, leading to potential harm due to misinformation. Moreover, we find that models struggle to identify the salient information and are more error-prone when summarizing over longer textual contexts."
                },
                {
                    "title": "Can the ChatGPT and other large language models with internet-connected database solve the questions and concerns of patient with prostate cancer and help democratize medical knowledge?",
                    "abstract": null
                },
                {
                    "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task",
                    "abstract": "Recently, ChatGPT and GPT-4 have emerged and gained immense global attention due to their unparalleled performance in language processing. Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested. Radiology presents unique linguistic phenomena distinct from open-domain data due to its specificity and complexity. Assessing the performance of large language models (LLMs) in such specific domains is crucial not only for a thorough evaluation of their overall performance but also for providing valuable insights into future model design directions: whether model design should be generic or domain-specific. To this end, in this study, we evaluate the performance of ChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned specifically on task-related data samples. We also conduct a comprehensive investigation on ChatGPT/GPT-4's reasoning ability by introducing varying levels of inference difficulty. Our results show that 1) GPT-4 outperforms ChatGPT in the radiology NLI task; 2) other specifically fine-tuned models require significant amounts of data samples to achieve comparable performance to ChatGPT/GPT-4. These findings demonstrate that constructing a generic model that is capable of solving various tasks across different domains is feasible."
                },
                {
                    "title": "CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models",
                    "abstract": "Large pre-trained language models (LLMs) have been shown to have significant potential in few-shot learning across various fields, even with minimal training data. However, their ability to generalize to unseen tasks in more complex fields, such as biology, has yet to be fully evaluated. LLMs can offer a promising alternative approach for biological inference, particularly in cases where structured data and sample size are limited, by extracting prior knowledge from text corpora. Our proposed few-shot learning approach uses LLMs to predict the synergy of drug pairs in rare tissues that lack structured data and features. Our experiments, which involved seven rare tissues from different cancer types, demonstrated that the LLM-based prediction model achieved significant accuracy with very few or zero samples. Our proposed model, the CancerGPT (with ~ 124M parameters), was even comparable to the larger fine-tuned GPT-3 model (with ~ 175B parameters). Our research is the first to tackle drug pair synergy prediction in rare tissues with limited data. We are also the first to utilize an LLM-based prediction model for biological reaction prediction tasks."
                },
                {
                    "title": "ChatGPT for good? On opportunities and challenges of large language models for education",
                    "abstract": null
                },
                {
                    "title": "Using ChatGPT to evaluate cancer myths and misconceptions: artificial intelligence and cancer information",
                    "abstract": "Abstract Data about the quality of cancer information that chatbots and other artificial intelligence systems provide are limited. Here, we evaluate the accuracy of cancer information on ChatGPT compared with the National Cancer Institute’s (NCI’s) answers by using the questions on the “Common Cancer Myths and Misconceptions” web page. The NCI’s answers and ChatGPT answers to each question were blinded, and then evaluated for accuracy (accurate: yes vs no). Ratings were evaluated independently for each question, and then compared between the blinded NCI and ChatGPT answers. Additionally, word count and Flesch-Kincaid readability grade level for each individual response were evaluated. Following expert review, the percentage of overall agreement for accuracy was 100% for NCI answers and 96.9% for ChatGPT outputs for questions 1 through 13 (ĸ = ‒0.03, standard error = 0.08). There were few noticeable differences in the number of words or the readability of the answers from NCI or ChatGPT. Overall, the results suggest that ChatGPT provides accurate information about common cancer myths and misconceptions."
                },
                {
                    "title": "The impending impacts of large language models on medical education",
                    "abstract": "Received: December 30, 2022 • Revised: February 2, 2023 • Accepted: February 7, 2023 Corresponding Author: Sangzin Ahn (https://orcid.org/0000-0003-2749-0014) Department of Pharmacology, College of Medicine, Inje University, 75 Bokji-ro, Busanjin-gu, Busan 47392, Korea Tel: +82.51.890.5909 Fax: +82.50.4043.1326 email: sangzinahn@inje.ac.kr Korean J Med Educ 2023 Mar; 35(1): 103-107 https://doi.org/10.3946/kjme.2023.253 eISSN: 2005-7288 C The Korean Society of Medical Education. All rights reserved. This is an open-access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http:// creativecommons.org/licenses/by-nc/3.0/), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited. The advent of large language models"
                },
                {
                    "title": "An artificial intelligence-based chatbot for prostate cancer education: Design and patient evaluation study",
                    "abstract": "Introduction Artificial intelligence (AI) is increasingly used in healthcare. AI-based chatbots can act as automated conversational agents, capable of promoting health and providing education at any time. The objective of this study was to develop and evaluate a user-friendly medical chatbot (prostate cancer communication assistant (PROSCA)) for provisioning patient information about early detection of prostate cancer (PC). Methods The chatbot was developed to provide information on prostate diseases, diagnostic tests for PC detection, stages, and treatment options. Ten men aged 49 to 81 years with suspicion of PC were enrolled in this study. Nine of ten patients used the chatbot during the evaluation period and filled out the questionnaires on usage and usability, perceived benefits, and potential for improvement. Results The chatbot was straightforward to use, with 78% of users not needing any assistance during usage. In total, 89% of the chatbot users in the study experienced a clear to moderate increase in knowledge about PC through the chatbot. All study participants who tested the chatbot would like to re-use a medical chatbot in the future and support the use of chatbots in the clinical routine. Conclusions Through the introduction of the chatbot PROSCA, we created and evaluated an innovative evidence-based health information tool in the field of PC, allowing targeted support for doctor–patient communication and offering great potential in raising awareness, patient education, and support. Our study revealed that a medical chatbot in the field of early PC detection is readily accepted and benefits patients as an additional informative tool."
                },
                {
                    "title": "Training language models to follow instructions with human feedback",
                    "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                },
                {
                    "title": "Empowerment in cancer patients: Does peer support make a difference? A systematic review",
                    "abstract": "Empowerment is critical for cancer patients to make informed choices, to manage medication, and to navigate through the oncological care system. Cancer peer support provides patients with information, emotional relief and may promote empowerment. This paper provides a systematic review of the literature examining the impact of cancer peer support interventions on psychological empowerment."
                },
                {
                    "title": "The Validity of Physiological Measures to Identify Differences in Intrinsic Cognitive Load",
                    "abstract": "A sample of 33 experiments was extracted from the Web-of-Science database over a 5-year period (2016–2020) that used physiological measures to measure intrinsic cognitive load. Only studies that required participants to solve tasks of varying complexities using a within-subjects design were included. The sample identified a number of different physiological measures obtained by recording signals from four main body categories (heart and lungs, eyes, skin, and brain), as well as subjective measures. The overall validity of the measures was assessed by examining construct validity and sensitivity. It was found that the vast majority of physiological measures had some level of validity, but varied considerably in sensitivity to detect subtle changes in intrinsic cognitive load. Validity was also influenced by the type of task. Eye-measures were found to be the most sensitive followed by the heart and lungs, skin, and brain. However, subjective measures had the highest levels of validity. It is concluded that a combination of physiological and subjective measures is most effective in detecting changes in intrinsic cognitive load."
                },
                {
                    "title": "Topic Extraction from A Cancer Health Forum",
                    "abstract": "This paper presents a methodology for topic extraction from a corpus of unstructured posts submitted to the online health forum r/Cancer. Topic extraction is important in many fields. It can provide an understanding of how patients and their caregivers manage the disease and related treatments. Reduced vector embeddings are generated for each post using a combination of a pre-trained language model and dimensionality reduction. These embeddings are then clustered with particle swarm optimization (PSO). An embedding size of 300 produced a topic model with a quality of 0.44. This quality level was obtained without biasing the words in the vocabulary towards a specific topic as currently practiced."
                },
                {
                    "title": "Treating the patient and not just the cancer: therapeutic burden in prostate cancer",
                    "abstract": null
                },
                {
                    "title": "The value of information and support: Experiences among patients with prostate cancer.",
                    "abstract": "AIMS AND OBJECTIVES\nTo explore and analyze prostate cancer survivors` experiences and critical reflections of information received during their cancer trajectory.\n\n\nBACKGROUND\nProstate cancer is one of the most prevalent cancer in men worldwide. Treatment causes side effects such as urinary incontinence, bowel changes, erection problems influencing sex life and manhood. Cancer pathways are designed to give patients and their relatives a predictable and as stress-free as possible treatment trajectory and minimize waiting time.\n\n\nDESIGN\nQualitative, explorative research design.\n\n\nMETHODS\nFocus groups with 16 prostate cancer survivors after having participated in an educational program. The COREQ checklist was followed to ensure rigor in the study.\n\n\nRESULTS\nThe main theme; \"Help me stay in control\" and three subthemes; To be met with interest and support, enough knowledge to understand what is happening, a plan to build the new life on, emerged from qualitative analysis and highlighted the participants' need for information and support, specially scheduled at critical times in the treatment trajectory: the diagnostic phase, the treatment phase and the life after treatment. They also highlighted the need for empathy and interest from health care professionals and highlighted the need for arenas to discuss vulnerable topics. Contact with peers was perceived as supportive and encouraging.\n\n\nCONCLUSION\nHealth care professionals must support prostate cancer survivors with empathy, interest and information tailored to their needs in three different phases. Continuity in information flow may increase trust and satisfaction among the prostate cancer survivors.\n\n\nRELEVANCE TO CLINICAL PRACTICE\nPCa patients need for information varied at critical times in their treatment trajectory. HCP should meet them with empathy and interest to be able to tailor their need for information and support."
                },
                {
                    "title": "Chatbot for Health Care and Oncology Applications Using Artificial Intelligence and Machine Learning: Systematic Review",
                    "abstract": "Background Chatbot is a timely topic applied in various fields, including medicine and health care, for human-like knowledge transfer and communication. Machine learning, a subset of artificial intelligence, has been proven particularly applicable in health care, with the ability for complex dialog management and conversational flexibility. Objective This review article aims to report on the recent advances and current trends in chatbot technology in medicine. A brief historical overview, along with the developmental progress and design characteristics, is first introduced. The focus will be on cancer therapy, with in-depth discussions and examples of diagnosis, treatment, monitoring, patient support, workflow efficiency, and health promotion. In addition, this paper will explore the limitations and areas of concern, highlighting ethical, moral, security, technical, and regulatory standards and evaluation issues to explain the hesitancy in implementation. Methods A search of the literature published in the past 20 years was conducted using the IEEE Xplore, PubMed, Web of Science, Scopus, and OVID databases. The screening of chatbots was guided by the open-access Botlist directory for health care components and further divided according to the following criteria: diagnosis, treatment, monitoring, support, workflow, and health promotion. Results Even after addressing these issues and establishing the safety or efficacy of chatbots, human elements in health care will not be replaceable. Therefore, chatbots have the potential to be integrated into clinical practice by working alongside health practitioners to reduce costs, refine workflow efficiencies, and improve patient outcomes. Other applications in pandemic support, global health, and education are yet to be fully explored. Conclusions Further research and interdisciplinary collaboration could advance this technology to dramatically improve the quality of care for patients, rebalance the workload for clinicians, and revolutionize the practice of medicine."
                },
                {
                    "title": "A scoping review of the characteristics and benefits of online prostate cancer communities",
                    "abstract": "Online prostate cancer communities (OPCaCs) have emerged as a new source of support, not bounded by geographic barriers, for men living with prostate cancer. This scoping review mapped the existing literature to explore the characteristics and benefits of OPCaCs, identify knowledge gaps, and direct future research."
                },
                {
                    "title": "The influence of online health information on health decisions: A systematic review.",
                    "abstract": null
                },
                {
                    "title": "Blended Learning Compared to Traditional Learning in Medical Education: Systematic Review and Meta-Analysis",
                    "abstract": "Background Blended learning, which combines face-to-face learning and e-learning, has grown rapidly to be commonly used in education. Nevertheless, the effectiveness of this learning approach has not been completely quantitatively synthesized and evaluated using knowledge outcomes in health education. Objective The aim of this study was to assess the effectiveness of blended learning compared to that of traditional learning in health education. Methods We performed a systematic review of blended learning in health education in MEDLINE from January 1990 to July 2019. We independently selected studies, extracted data, assessed risk of bias, and compared overall blended learning versus traditional learning, offline blended learning versus traditional learning, online blended learning versus traditional learning, digital blended learning versus traditional learning, computer-aided instruction blended learning versus traditional learning, and virtual patient blended learning versus traditional learning. All pooled analyses were based on random-effect models, and the I2 statistic was used to quantify heterogeneity across studies. Results A total of 56 studies (N=9943 participants) assessing several types of learning support in blended learning met our inclusion criteria; 3 studies investigated offline support, 7 studies investigated digital support, 34 studies investigated online support, 8 studies investigated computer-assisted instruction support, and 5 studies used virtual patient support for blended learning. The pooled analysis comparing all blended learning to traditional learning showed significantly better knowledge outcomes for blended learning (standardized mean difference 1.07, 95% CI 0.85 to 1.28, I2=94.3%). Similar results were observed for online (standardized mean difference 0.73, 95% CI 0.60 to 0.86, I2=94.9%), computer-assisted instruction (standardized mean difference 1.13, 95% CI 0.47 to 1.79, I2=78.0%), and virtual patient (standardized mean difference 0.62, 95% CI 0.18 to 1.06, I2=78.4%) learning support, but results for offline learning support (standardized mean difference 0.08, 95% CI –0.63 to 0.79, I2=87.9%) and digital learning support (standardized mean difference 0.04, 95% CI –0.45 to 0.52, I2=93.4%) were not significant. Conclusions From this review, blended learning demonstrated consistently better effects on knowledge outcomes when compared with traditional learning in health education. Further studies are needed to confirm these results and to explore the utility of different design variants of blended learning."
                },
                {
                    "title": "\"Help me figure this out\": Qualitative explorations of patient experiences with cancer pathology reports.",
                    "abstract": null
                },
                {
                    "title": "Digital Support for Patients Undergoing Bariatric Surgery: Narrative Review of the Roles and Challenges of Online Forums",
                    "abstract": "Background The internet has become an important medium within health care, giving patients the opportunity to search for information, guidance, and support to manage their health and well-being needs. Online forums and internet-based platforms appear to have changed the way many patients undergoing bariatric surgery view and engage with their health, before and after weight loss surgery. Given that significant health improvements result from sustained weight loss, ensuring patient adherence to recommended preoperative and postoperative guidance is critical for bariatric surgery success. In a patient cohort with high information needs preoperatively, and notoriously high attrition rates postoperatively, online forums may present an underutilized method of support. Objective The aim of this study was to conduct a narrative review focusing on the developing roles that online forums can play for patients with bariatric conditions preoperatively and postoperatively. Methods A literature search was conducted in October-November 2019 across 5 electronic databases: Scopus, EMBASE, PsycINFO, CINAHL, and MEDLINE. Qualitative or mixed methods studies were included if they evaluated patients undergoing bariatric surgery (or bariatric surgery health care professionals) engaging with, using, or analyzing online discussion forums or social media platforms. Using thematic analysis, themes were developed from coding patterns within the data to identify the roles and challenges of online forums for patients undergoing bariatric surgery. Results A total of 8 studies were included in this review, with 5 themes emerging around (1) managing expectations of a new life; (2) decision making and signposting; (3) supporting information seeking; (4) facilitating connectedness: peer-to-peer social and emotional support; and (5) enabling accessibility and connectivity with health care professionals. Conclusions Online forums could offer one solution to improving postoperative success by supporting and motivating patients. Future research should consider how best to design and moderate online forums for maximal effectiveness and the sharing of accurate information. The surgical multidisciplinary team may consider recommendations of online peer-support networks to complement care for patients throughout their surgical journey."
                },
                {
                    "title": "Person-centred and efficient care delivery for high-need, high-cost patients: primary care professionals’ experiences",
                    "abstract": null
                },
                {
                    "title": "Chatbot for Healthcare System Using Artificial Intelligence",
                    "abstract": "Healthcare is very important to lead a good life. However, it is very difficult to obtain the consultation with the doctor for every health problem. The idea is to create a medical chatbot using Artificial Intelligence that can diagnose the disease and provide basic details about the disease before consulting a doctor. This will help to reduce healthcare costs and improve accessibility to medical knowledge through medical chatbot. The chatbots are computer programs that use natural language to interact with users. The chatbot stores the data in the database to identify the sentence keywords and to make a query decision and answer the question. Ranking and sentence similarity calculation is performed using n-gram, TFIDF and cosine similarity. The score will be obtained for each sentence from the given input sentence and more similar sentences will be obtained for the query given. The third party, the expert program, handles the question presented to the bot that is not understood or is not present in the database."
                },
                {
                    "title": "Caring for patients with cancer in the COVID-19 era",
                    "abstract": null
                },
                {
                    "title": "Development and initial testing of a Health Confidence Score (HCS)",
                    "abstract": "Introduction Patients need to feel confident about looking after their own health. This is needed to improve patient outcomes and clinical support. With few suitable tools available to measure self-care health confidence, we developed and validated a short, generic survey instrument for use in evaluation and quality improvement. Methods The Health Confidence Score (HCS) was developed through literature review, patient and expert focus groups and discussions. This paper reports an initial survey (n = 1031, study 1) which identified some issues and a further face-to-face survey (n = 378, study 2) to test the construct and concurrent validity of the final version. Scores were correlated against the My Health Confidence (MHC) rating scale, howRu (health status measure) and relevant demographics. Results The HCS is short (50 words) with good readability (reading age 8). It has four items covering health knowledge, capability to self-manage, access to help and shared decision-making; each has four response options (strongly agree, agree, neutral disagree). Items are reported independently and as a summary score. The mean summary score was 76.7 (SD 20.4) on 0–100 scale. Cronbach’s alpha = 0.82. Exploratory factor analysis suggested that the four items relate to a single dimension. Correlation of the HCS summary score with MHC was high (Spearman r = 0.76). It was also associated with health status (Spearman r = 0.49), negatively with number of medications taken (r=–0.29) and age (r=–0.22) and not with ethnicity, having children or education level. Conclusions The HCS is short, easy to use, with good psychometric properties and construct validity. Each item is meaningful independently and the summary score gives an overall picture of health confidence."
                },
                {
                    "title": "Narrative medicine as a medical education tool: A systematic review",
                    "abstract": "Abstract Aim: Narrative medicine has been promoted as an innovative and effective means of stimulating medical students’ professional development by teaching them to approach their patients’ experiences of illness with more understanding and compassion. This systematic literature review aims to answer the following question: what evidence of effect is available in the literature about models for teaching narrative medicine? Methods: We conducted a narrative review of 36 articles and used the Best Evidence in Medical Education (BEME) Global Scale and Kirkpatrick Scale for strength and importance of evidence to categorize reported assessment strategies and to evaluate the effectiveness of their narrative medicine programs. Results: We found evidence that narrative medicine is an effective pedagogic tool with a clear and replicable structure and methodology. We also determined that a positive impact could be measured when pertaining to participation and modification of attitudes, knowledge, and skills. However, unequivocal evidence of the effect of narrative medicine on students’ behavior or ongoing interaction with colleagues and patients is still lacking. Conclusion: While many recent publications describe the goals and virtues of a narrative-based approach, more research is needed to determine whether or not there is an ideological consensus undergirding this approach. In addition, it is still unclear whether the long-term impact of narrative medicine classroom interventions are felt by patients, or whether such interventions positively impact patient care."
                },
                {
                    "title": "Peer support interventions for breast cancer patients: a systematic review",
                    "abstract": null
                },
                {
                    "title": "The role of internet resources in health decision-making: a qualitative study",
                    "abstract": "Objective Internet resources remain important for health information and advice but their specific role in decision-making is understudied, often assumed and remains unclear. In this article, we examine the different ways in which internet resources play a role in health decision-making within the context of distributed decision-making. Methods We conducted semi-structured interviews with 37 people in the United Kingdom who reported using the internet in relation to decision-making, and representing a range of long- and short-term health conditions. The interviews focused on decision-making activities across different settings and in relation to different stakeholders to understand how internet resources play a role in these activities. We carried out a thematic analysis of the interviews. Results We identified three main ways in which internet resources played a role in health decision-making. A supportive role (as a decision crutch), a stimulating role (as a decision initiator), and an interactional role (impacting on the doctor–patient relationship). These three roles spanned different resources and illustrated how the decision-making process can be impacted by the encounters people have with technology – specifically internet based health resources – in different ways and at different time points. Conclusions Examining health decisions with respect to internet resources highlights the complex and distributed nature of decision-making alongside the complexity of online health information sourcing. We discuss the role of internet resources in relation to the increasing importance of online personal experiences and their relevance within shared decision-making."
                },
                {
                    "title": "Machine learning to support social media empowered patients in cancer care and cancer treatment decisions",
                    "abstract": "Background A primary variant of social media, online support groups (OSG) extend beyond the standard definition to incorporate a dimension of advice, support and guidance for patients. OSG are complementary, yet significant adjunct to patient journeys. Machine learning and natural language processing techniques can be applied to these large volumes of unstructured text discussions accumulated in OSG for intelligent extraction of patient-reported demographics, behaviours, decisions, treatment, side effects and expressions of emotions. New insights from the fusion and synthesis of such diverse patient-reported information, as expressed throughout the patient journey from diagnosis to treatment and recovery, can contribute towards informed decision-making on personalized healthcare delivery and the development of healthcare policy guidelines. Methods and findings We have designed and developed an artificial intelligence based analytics framework using machine learning and natural language processing techniques for intelligent analysis and automated aggregation of patient information and interaction trajectories in online support groups. Alongside the social interactions aspect, patient behaviours, decisions, demographics, clinical factors, emotions, as subsequently expressed over time, are extracted and analysed. More specifically, we utilised this platform to investigate the impact of online social influences on the intimate decision scenario of selecting a treatment type, recovery after treatment, side effects and emotions expressed over time, using prostate cancer as a model. Results manifest the three major decision-making behaviours among patients, Paternalistic group, Autonomous group and Shared group. Furthermore, each group demonstrated diverse behaviours in post-decision discussions on clinical outcomes, advice and expressions of emotion during the twelve months following treatment. Over time, the transition of patients from information and emotional support seeking behaviours to providers of information and emotional support to other patients was also observed. Conclusions Findings from this study are a rigorous indication of the expectations of social media empowered patients, their potential for individualised decision-making, clinical and emotional needs. The increasing popularity of OSG further confirms that it is timely for clinicians to consider patient voices as expressed in OSG. We have successfully demonstrated that the proposed platform can be utilised to investigate, analyse and derive actionable insights from patient-reported information on prostate cancer, in support of patient focused healthcare delivery. The platform can be extended and applied just as effectively to any other medical condition."
                },
                {
                    "title": "Conceptualizing agent-human interactions during the conversational search process",
                    "abstract": "The conversational search task aims to enable a user to resolve \ninformation needs via natural language dialogue with \nan agent. In this paper, we aim to develop a conceptual framework \nof the actions and intents of users and agents explaining \nhow these actions enable the user to explore the search space \nand resolve their information need. We outline the different \nactions and intents, before discussing key decision points in \nthe conversation where the agent needs to decide how to \nsteer the conversational search process to a successful and/or \nsatisfactory conclusion. Essentially, this paper provides a \nconceptualization of the conversational search process between \nan agent and user, which provides a framework and \na starting point for research, development and evaluation of \nconversational search agents."
                },
                {
                    "title": "Empowerment from patient’s and caregiver’s perspective in cancer care",
                    "abstract": "The caregivers’ perceptions of the patients’ health condition may be biased and induce them to perceive higher needs than patients actually disclose. Our aim was to assess if the level of knowledge and awareness about cancer disease and treatment, and patient participation and assistance differs between caregivers and patients. A descriptive, cross-sectional study was conducted across five countries (Italy, United Kingdom, Spain, France and Germany) on a total of 510 participants who directly (patient) or indirectly (caregiver) faced a cancer diagnosis. Investigating this divergence could help to identify possible difficulties in patient–caregiver relationship, eventually improving patient empowerment."
                },
                {
                    "title": "Assessing Unmet Information Needs of Breast Cancer Survivors: Exploratory Study of Online Health Forums Using Text Classification and Retrieval",
                    "abstract": "Background Patient education materials given to breast cancer survivors may not be a good fit for their information needs. Needs may change over time, be forgotten, or be misreported, for a variety of reasons. An automated content analysis of survivors' postings to online health forums can identify expressed information needs over a span of time and be repeated regularly at low cost. Identifying these unmet needs can guide improvements to existing education materials and the creation of new resources. Objective The primary goals of this project are to assess the unmet information needs of breast cancer survivors from their own perspectives and to identify gaps between information needs and current education materials. Methods This approach employs computational methods for content modeling and supervised text classification to data from online health forums to identify explicit and implicit requests for health-related information. Potential gaps between needs and education materials are identified using techniques from information retrieval. Results We provide a new taxonomy for the classification of sentences in online health forum data. 260 postings from two online health forums were selected, yielding 4179 sentences for coding. After annotation of data and training alternative one-versus-others classifiers, a random forest-based approach achieved F1 scores from 66% (Other, dataset2) to 90% (Medical, dataset1) on the primary information types. 136 expressions of need were used to generate queries to indexed education materials. Upon examination of the best two pages retrieved for each query, 12% (17/136) of queries were found to have relevant content by all coders, and 33% (45/136) were judged to have relevant content by at least one. Conclusions Text from online health forums can be analyzed effectively using automated methods. Our analysis confirms that breast cancer survivors have many information needs that are not covered by the written documents they typically receive, as our results suggest that at most a third of breast cancer survivors’ questions would be addressed by the materials currently provided to them."
                },
                {
                    "title": "Cognitive Load and Its Implications for Health Care",
                    "abstract": null
                },
                {
                    "title": "Thematic Analysis",
                    "abstract": "As qualitative research becomes increasingly recognized and valued, it is imperative that it is conducted in a rigorous and methodical manner to yield meaningful and useful results. To be accepted as trustworthy, qualitative researchers must demonstrate that data analysis has been conducted in a precise, consistent, and exhaustive manner through recording, systematizing, and disclosing the methods of analysis with enough detail to enable the reader to determine whether the process is credible. Although there are numerous examples of how to conduct qualitative research, few sophisticated tools are available to researchers for conducting a rigorous and relevant thematic analysis. The purpose of this article is to guide researchers using thematic analysis as a research method. We offer personal insights and practical examples, while exploring issues of rigor and trustworthiness. The process of conducting a thematic analysis is illustrated through the presentation of an auditable decision trail, guiding interpreting and representing textual data. We detail our step-by-step approach to exploring the effectiveness of strategic clinical networks in Alberta, Canada, in our mixed methods case study. This article contributes a purposeful approach to thematic analysis in order to systematize and increase the traceability and verification of the analysis."
                },
                {
                    "title": "Health information seeking in the digital age: An analysis of health information seeking behavior among US adults",
                    "abstract": "Abstract We live in a digital age and this has changed the landscape of health information. With the changing US demographic, otherwise acute diseases morphing into chronic diseases as a result of treatment advancements, and evolving health needs of the population, there is need for increase in available and accessible health information. It is estimated that one in three US adults use the internet to diagnose or learn about a health concern. Nevertheless, a nagging question is whether the Web is reducing or creating disparities in health information availability and use for making health decisions. This study examined factors associated with heath information seeking from the internet, traditional media, and health care professionals among a diverse population of US adults. Data for the analysis was from four cycles (2011–2014) of the Health Information National Trends Survey (HINTS), a national survey of US adults. Controlling for age, race/ethnicity, gender, and socioeconomic status (SES), regression analyses were conducted. STATA 13 was used for analyses. Findings indicated that there is a possibility that while the Web is an easily available source of health information, it could also create inequalities in health information accessibility. The Web should not be considered a substitute for using alternative health information sources. Doing so, might create disproportionate access to health information essential for health decisions."
                },
                {
                    "title": "Development and validation of a short health confidence score",
                    "abstract": null
                },
                {
                    "title": "The effectiveness of Internet-based e-learning on clinician behaviour and patient outcomes: A systematic review.",
                    "abstract": null
                },
                {
                    "title": "A systematic review of unmet needs of newly diagnosed older cancer patients undergoing active cancer treatment",
                    "abstract": null
                },
                {
                    "title": "Educating patients: understanding barriers, learning styles, and teaching techniques.",
                    "abstract": null
                },
                {
                    "title": "Review of the literature on the effects of caring for a patient with cancer",
                    "abstract": "Objective: To adequately help family caregivers (FCs) of cancer patients, clinicians need to understand the complexity of the problems and responsibilities associated with cancer patients illness that FCs experience."
                },
                {
                    "title": "Order Matters: Using the 5E Model to Align Teaching with How People Learn",
                    "abstract": "“I have to teach someone to make a peanut butter and jelly sandwich. How am I supposed to do that? What should I start with? How can this be so hard?” \n \nI have found that teaching anything to another person is rife with far more decisions and dilemmas than I could have ever imagined at first. Years ago, I had a college roommate who wanted to participate in a summer teaching program. For her interview, she had to develop a lesson plan to teach someone else how to make a peanut butter and jelly sandwich. Have you ever thought about teaching someone else how to make a peanut butter and jelly sandwich? She had asked for my input, and once we started to really consider the possibilities, our minds reeled. How would you start? What would you do first? Next? After that? Who was the learner anyway? And had they made a sandwich before? Were they allergic to peanuts? How old were they? Should we let them have a knife? Should we show them how first? Talk them through it? Let them have a go at it on their own? Should we first teach them the names of all the tools and things we were going to use? Should we ask them why they needed to learn how to make a peanut butter and jelly sandwich in the first place? What were the critical issues in teaching someone how to make a peanut butter and jelly sandwich? \n \nMuch like in the “PBJ Dilemma” as we came to call it, there are many decisions to be made in designing effective learning experiences in undergraduate biology classes—and instructors are making these decisions constantly. It can seem overwhelming, yet the research literatures from cognitive science, psychology, and science education about how people learn suggest guidelines about constructing effective learning experiences (National Research Council [NRC], 1999 ). Much like the PBJ Dilemma, the order in which we decide to do things with students when we teach is critical, yet the order of things happening in a class session often goes undiscussed and unexamined. At first glance, the most pressing teaching dilemmas in our biology classrooms—student motivation, student retention of information, student understanding of difficult concepts—may seem unrelated to the order in which things are happening; however, what we do first, second, third, and so on can have many ramifications. For many instructors who have primarily learned from and used a lecture-based teaching approach, considerations of order have been primarily about the order of ideas. With the increasing use of active-learning strategies, class sessions are moving from having a single component—a lecture—to having many components over the course of even 50 minutes (e.g., a video clip, a pair discussion on a biology-based problem, a clicker question, a mini-lecture, and a final index card reflection). So, what is the optimal order for sequencing these elements to maximize student learning of biology?"
                },
                {
                    "title": "The Usability Metric for User Experience",
                    "abstract": null
                },
                {
                    "title": "So much to do, so little time: care for the socially disadvantaged and the 15-minute visit.",
                    "abstract": "There is so much to do in primary care, and so little time to do it. During 15-minute visits, physicians are expected to form partnerships with patients and their families, address complex acute and chronic biomedical and psychosocial problems, provide preventive care, coordinate care with specialists, and ensure informed decision making that respects patients' needs and preferences. This is a challenging task during straightforward visits, and it is nearly impossible when caring for socially disadvantaged patients with complex biomedical and psychosocial problems and multiple barriers to care. Consider the following scenario."
                },
                {
                    "title": "Fostering empowerment in online support groups",
                    "abstract": null
                },
                {
                    "title": "Consolidated criteria for reporting qualitative research (COREQ): a 32-item checklist for interviews and focus groups.",
                    "abstract": "BACKGROUND\nQualitative research explores complex phenomena encountered by clinicians, health care providers, policy makers and consumers. Although partial checklists are available, no consolidated reporting framework exists for any type of qualitative design.\n\n\nOBJECTIVE\nTo develop a checklist for explicit and comprehensive reporting of qualitative studies (in depth interviews and focus groups).\n\n\nMETHODS\nWe performed a comprehensive search in Cochrane and Campbell Protocols, Medline, CINAHL, systematic reviews of qualitative studies, author or reviewer guidelines of major medical journals and reference lists of relevant publications for existing checklists used to assess qualitative studies. Seventy-six items from 22 checklists were compiled into a comprehensive list. All items were grouped into three domains: (i) research team and reflexivity, (ii) study design and (iii) data analysis and reporting. Duplicate items and those that were ambiguous, too broadly defined and impractical to assess were removed.\n\n\nRESULTS\nItems most frequently included in the checklists related to sampling method, setting for data collection, method of data collection, respondent validation of findings, method of recording data, description of the derivation of themes and inclusion of supporting quotations. We grouped all items into three domains: (i) research team and reflexivity, (ii) study design and (iii) data analysis and reporting.\n\n\nCONCLUSIONS\nThe criteria included in COREQ, a 32-item checklist, can help researchers to report important aspects of the research team, study methods, context of the study, findings, analysis and interpretations."
                },
                {
                    "title": "Health Information—Seeking Behavior",
                    "abstract": "Seeking information about one's health is increasingly documented as a key coping strategy in health-promotive activities and psychosocial adjustment to illness. In this article, the authors critically examine the scientific literature from 1982 to 2006 on the concept of health information—seeking behavior (HISB) to determine its level of maturity and clarify the concept's essential characteristics. A principle-based method of concept analysis provides the framework for exploring the nature of HISB. The authors reviewed approximately 100 published articles and five books reporting on HISB. Although HISB is a popular concept used in various contexts, most HISB definitions provide little insight into the concept's specific meanings. The authors describe the concept's characteristics, contributing to a clearer understanding of HISB, and discuss operationalizations, antecedents, and outcomes of HISB. Such an analysis of HISB might guide further theorizing on this highly relevant concept and assist health care providers in designing optimal informational interventions."
                },
                {
                    "title": "Literacy and health outcomes",
                    "abstract": "Low literacy is associated with a variety of adverse health outcomes, including increased mortality, hospitalization, and in some cases poorer control of chronic health conditions.1–4 An increasing body of research, including several of the articles in this special issue of JGIM, has sought to understand the nature of the relationship between low literacy and adverse health outcomes. One important question is the degree to which low literacy skills directly or indirectly affect health outcomes (see Fig. 1). Direct relationships include those in which literacy is a necessary component of achieving good health outcomes; indirect relationships are those in which literacy is a marker for, but not a cause of, the actual factors that lead to poor health outcomes.\r\n\r\n\r\n\r\nFIGURE 1\r\n\r\nPotential pathways between literacy and health outcomes.\r\n\r\n\r\n\r\nUnderstanding the relative contributions of direct and indirect pathways is crucial for the development of effective interventions to improve health outcomes. To the extent that literacy directly affects health outcomes, interventions should focus on improving literacy skills or providing accommodations such that a high literacy level is no longer required for achieving good outcomes. If literacy is only a marker for other conditions, such as lack of health insurance or transportation, which are directly linked to adverse outcomes, then interventions should be focused on overcoming those specific barriers.\r\n\r\nOne of the most commonly postulated direct pathways that may explain the association between literacy and adverse health outcomes involves the potential limitations in self-care skills among patients with low literacy. Researchers have theorized that low literacy limits the ability of the patient to acquire optimal self-care knowledge and skills, because acquiring such skills may require processing a great deal of sometimes complex information. Such information is often not provided in easy-to-understand formats. In the absence of sufficient self-care support, patients with low literacy may be unable to meet the care requirements of their condition, leading to adverse outcomes.\r\n\r\nIn this special issue of JGIM, 4 articles examine the relationship between literacy and one important type of self-care skill, appropriate adherence to medication. Three studies examined the relationship between literacy and understanding of medication instructions.5–7 Understanding how to take one's medication is a necessary precondition to successful adherence. If a patient cannot obtain and understand basic information about how to take his or her medication, his or her ability to adhere to the prescribed treatment regimen is compromised; if he or she understands how to take the medication, then the issue of whether or not they can repeat that behavior on an ongoing basis becomes salient. Similarly, measuring self-reported adherence is only relevant if the patient understands the regimen they are being asked to adhere to.\r\n\r\nKripalani et al.5 found that patients with low literacy had trouble identifying their medication by name, but once the medication was pointed out to them, they could describe how to take the medication just as well as those with higher literacy. Davis et al. found that a sizable percentage of patients could not correctly interpret medication warning labels. Fang et al.6 found that patients with low literacy had less knowledge about their warfarin therapy, but this knowledge test was targeted toward reasons to use anticoagulation rather than how to take warfarin itself. These studies reinforce previous research showing that patients with low literacy consistently perform worse on knowledge tests than patients with higher literacy.1 Although these studies remind us of important limitations in our current information-giving system, they do not identify whether the specific knowledge deficits that were measured lead to worse health outcomes. In fact, the study by Fang demonstrates that the knowledge gap did not lead to a disparity in outcome (degree of proper anticoagulation control) according to literacy.6\r\n\r\nTwo studies examined the relationship among literacy, self-reported adherence to the medication regimen, and laboratory tests of medication effect.6,8 Paasche-Orlow et al.8 examined the relationship between literacy, self-reported medication adherence, and viral suppression among patients with HIV and alcohol abuse who were taking anti-retroviral therapy (ART).8 The aforementioned study by Fang et al.6 at San Francisco General Hospital examined the relationship between literacy, adherence with warfarin, and prevalence of time within therapeutic range for a population of vulnerable patients requiring chronic anticoagulation. In each study, the researchers did not find an association between low literacy and poor adherence or an association between literacy and the more distal health outcome (viral suppression or the % of time in therapeutic range with anticoagulation). In fact, Paasche-Orlow et al. found that patients with low literacy (<sixth grade level) were more likely to report perfect adherence with ART than patients with literacy levels above ninth grade on the Rapid Estimate of Adult Literacy in Medicine.\r\n\r\nPrevious studies have reached mixed results as to whether literacy is associated with poor adherence.9,10 Kalichman et al.9 found a relationship between literacy and self-reported adherence with ART among HIV-infected patients. This remains the only study that has demonstrated a relationship between literacy and medication adherence, but it had the limitations of a cross-sectional study with a one-time self-report assessment of adherence. Golin et al.10 found that literacy was not related to adherence to ART in a prospective study of patients with HIV that used several validated strategies to assess adherence in addition to self-report.\r\n\r\nAlthough adherence to taking pills is important, other aspects of self-care behavior require adherence and may be more susceptible to the barrier of low literacy. In an unpublished analysis of data from our randomized trial of patients with heart failure, we found that those patients with low literacy initially had more problems with correctly adhering to a diuretic self-adjustment plan, but after 4 months, there was no difference by literacy level in the number of incorrect actions.11 Williams et al.12 found that patients with asthma and low literacy were less likely to be able to demonstrate how to use an inhaler properly than patients with higher literacy skills. They did not examine other health outcomes such as self-reported adherence, symptom control, or hospitalizations.\r\n\r\nSo what can we conclude about the relationship between literacy and adherence? First, it is clear that low literacy is not universally associated with poor self-reported adherence to medications for the few conditions examined to date. Other research, however, suggests that patients with low literacy may have initial difficulty with learning self-care behaviors and enacting them, but that with ongoing support and additional training, they can overcome these barriers. Because few of the studies on adherence have been conducted within a study population that demonstrated a strong association between literacy and distal health outcomes, it remains unclear whether poor adherence mediates any of the relationship between low literacy and poor health outcomes seen in other studies. Future observational studies should measure self-care skills and adherence in the context of a documented association between low literacy and adverse health outcomes. Intervention trials should examine whether helping patients to learn and adhere to self-care plans can improve literacy-related health disparities.11"
                },
                {
                    "title": "Patient Education Materials about the Treatment of Early-Stage Prostate Cancer: A Critical Review",
                    "abstract": "Prostate cancer is the most prevalent noncutaneous malignant condition and, after lung cancer, the second leading cause of cancer deaths among U.S. men (1). Localized prostate cancer is typically treated with watchful waiting, radical prostatectomy, radiation therapy (external and brachytherapy), and, more rarely, hormone therapy. Men receiving a diagnosis of localized prostate cancer face a difficult treatment decision because most randomized, controlled trials are still in progress. However, a recent Swedish randomized, controlled trial found that while radical prostatectomy reduced mortality from prostate cancer more than did watchful waiting, men who had surgery lived no longer than those who were treated with watchful waiting (2). Because specialists disagree and the literature is inconclusive, patients are increasingly expected to participate in treatment decisions. Patients must learn the risks and benefits of each treatment and choose which risks they can tolerate and which benefits they prefer. Decision aids have become the evidence-based medicine information source for patients who face choices that 1) have clinically important differences in the balance between outcomes and complication rates, 2) require tradeoffs between near-term and long-term outcomes, 3) include treatments that may result in a grave outcome, or 4) have only marginal differences in outcomes. Decision aids are one form of patient education materials. A recent Cochrane review of randomized trials of decision aids showed that compared with usual care, decision aids improve patient knowledge and realistic expectations and reduce decisional conflict. Exposure to decision aids also seemed to increase patient participation in decisions and improve decision quality (3). We sought to determine the adequacy of publicly available patient education materials to offer these identified advantages to patients. More particularly, we first evaluated whether the needed information was presented (content review) and then evaluated how well the information was presented (quality review). Methods Development of Review Criteria We developed the criteria for evaluating the content of patient education materials by first examining the empirical and theoretical literature on informed consent and decision aids. For a primary-level analysis, we adopted the Cochrane definition of balanced presentation: 1) Are all options presented (including, if appropriate, watchful waiting)? and 2) Are potential harms as well as potential benefits presented (4)? We used the Cochrane criteria to eliminate all patient education materials that 1) did not discuss the 4 standard prostate cancer treatments (watchful waiting, radical prostatectomy, radiation therapy, and hormone therapy), 2) discussed only cancer in general, or 3) discussed only prostate cancer screening. In a second-level detailed analysis, we applied previously developed generic criteria specific to prostate cancer (5). Additional items to reflect patient needs were obtained through a literature review and through review of our criteria from a local prostate cancer support group and from prostate cancer experts (urologists, oncologists, nurses, researchers, and activists) from the Michigan Cancer Consortium Prostate Cancer Action Committee. Four of the authors independently developed common definitions and coded test documents. An initial list of 85 criteria was reduced to 54 criteria by consensus and was operationalized (Tables 1, 2, and 3). A weighting system was considered but was not implemented because no gold standard could be identified from the literature on decision aids or risk communication. Similarly, the Cochrane criteria provide an empirically validated set of criteria for the second-level detailed analysis. Table 1. Frequency of Clinical Condition Content Table 2. Frequency of Treatment Option Content Table 3. Frequency of Quantitative Elements and Other Factors In previous work, we have shown that decision aids can produce decisions that better reflect patient preferences (6, 7). The parallel criteria selected for prostate cancer are the categories of treatment outcomes and side effects. Traditional patient education elements of description of the anatomy and disease progression were also included. Identification of Patient Education Materials Print Materials, Videotapes, and CD-ROMs To identify these materials (identified between September and December 2001), we asked national organizations (including patient advocacy groups, government organizations, pharmaceutical companies, insurance companies, health maintenance organizations, universities, and comprehensive cancer centers) for their materials. To be included, all materials must be widely available to the public at no cost. Internet Search Strategy We first reviewed Web sites of prominent organizations (including all of those identified during the print material, videotape, and CD-ROM search). Second, we reviewed Web sites of pharmaceutical companies that had received approval from the U.S. Food and Drug Administration to produce prostate cancer drugs. Third, a naive patient with prostate cancer strategy used an open (broad-based) search strategy with Google and Yahoo, which located more than 300 000 Web sites. We reviewed at least the first 100 links provided and discontinued our search when relevance decreased. We did not evaluate Web sites composed of links to other Web sites or duplicated print materials previously reviewed (for example, the Web site of the National Cancer Institute). The initial review set of 546 materials was reduced by 502 materials after applying the Cochrane criteria for all standard treatment options to be presented (see the Appendix Table, for list of print materials not eligible for review). The remaining 44 materials (19 print materials, 19 Web sites, 4 videotapes, and 2 CD-ROMs) underwent a formal content and quality review (Table 4). Table 4. Full Review Documents Appendix Table. Print Materials Rejected for Formal Analysis Content Review Three coding teams composed of 2 coders were each responsible for reviewing one third of the materials. Thus, each material was independently scored by 2 coders. Criteria were rated as present if they were mentioned in the text, even if briefly (8). Paired ratings were compared, and differences were resolved by a second review and, in rare occasions, by a third reviewer. Percentage agreement achieved by the teams ranged from 91% to 94%. Quality Review The quality review evaluated 1) the accuracy of the information contained in the patient education material, 2) whether presentation of treatment options was balanced, and 3) whether the information was comprehensible to the average reader. An experienced clinician with previous prostate cancer research experience performed the accuracy and balance review. The clinician did not participate in the content review and was blind to content scores. Accuracy criteria were developed from the prostate cancer literature on the natural history of the disease and the efficacy and side effects of treatments. The estimates of balance were subjective, graded from minimal to marked bias toward any particular treatment method. To further evaluate the best materials identified through the simple content inclusion criteria, a health literacy expert performed an extensive plain-language review on the top 5 print materials and top 5 Web sites. To determine these top-rated materials, we developed a scoring system that identified how many of the 54 essential criteria each piece of patient education material contained but did not prioritize further. Those materials that contained inaccurate information or were strongly biased toward a particular treatment were excluded from consideration. Only 1 of the print materials was eliminated from the top rating because of presence of bias. The review assessed characteristics of text and design that affect reading ease and comprehension, incorporating the widely used Suitability Assessment of Materials system (9). Criteria include 1) readability; 2) amount and organization of content; 3) writing style as it affects literacy demands; 4) graphics, layout, and typography; 5) evidence of learning stimulation; and 6) cultural appropriateness. Each criterion was evaluated according to specific subcharacteristics rated on a 0- to 2-point scale (0 = unsuitable; 1 = adequate; 2 = superior; or not applicable). Final scores were calculated as percentages based on a denominator of 44 possible points. The 0- to 2-point scale was then translated into grade level. Web sites were evaluated by using similar criteria. Updating the Materials Several Web sites changed during the 4-month review period. The review described later in this paper reflects assessment of Web sites between 1 September 2001 and 14 December 2001. In April 2003, we rescored the top 5 and bottom 5 Web sites and brochures to determine whether they had changed substantially since our review. Role of the Funding Source The funding source was not involved in the design, conduct, or reporting of the study or in the decision to submit the manuscript for publication. Results Tables 1, 2, and 3 show the proportion of criteria (in consolidated form) scored as present in the 44 patient education materials. Although videotape and the CD-ROM materials are reported in Tables 1, 2, and 3, discussion in the paper is restricted to print materials and Web sites because these are the tools most available to public audiences. We found some differences while rescoring the top 5 and bottom 5 print materials and Web sites, but they were small and did not change the rankings of the materials. Content Review Disease Process Most patient education materials included basic information on prostate anatomy and physiology (95% and 80% of print materials and Web sites, respectively). Most print materials and Web sites also discussed prostate cancer staging (100% and 95%, respectively) "
                },
                {
                    "title": "Receiving social support online: implications for health education.",
                    "abstract": "Online support groups are expanding as the general public becomes more comfortable using computer-mediated communication technology. These support groups have certain benefits for users who may not be able to or do not have the desire to attend face-to-face sessions. Online support groups also present challenges when compared to traditional face-to-face group communication. Communication difficulties may arise resulting from lack of visual and aural cues found in traditional face-to-face communication. Online support groups have emerged within health care as a result of the need individuals have to know more about health conditions they are confronting. The proliferation of these online communities may provide an opportunity for health educators to reach target populations with specific messages. This paper reviews the development of health-related online support groups, examines research conducted within these communities, compares their utility with traditional support groups and discusses the implications of these groups for health education."
                },
                {
                    "title": "Cancer-related patient education: an overview of the last decade of evaluation and research.",
                    "abstract": "PURPOSE/OBJECTIVES\nTo provide an overview of cancer-related patient-education research to determine future research needs.\n\n\nDATA SOURCES\nA literature search of peer-reviewed articles from 1989-1999. Databases that were searched included Medline, CINAHL, HealthStar, ERIC, CancerLit, and PubMed.\n\n\nDATA SYNTHESIS\n176 articles were analyzed and synthesized into narrative form.\n\n\nCONCLUSIONS\nPatients with cancer want and benefit from information, especially when making treatment decisions. Education helps patients manage side effects and improves adherence. Literacy is an important factor in materials development. The efficacy of computer-assisted learning, audio and video programs, and telephone interventions is supported in a variety of patient groups. Pain education can improve pain control, but the impact on fatigue has not been well researched.\n\n\nIMPLICATIONS FOR NURSING PRACTICE\nPatient education is an important component of nursing care. Research has confirmed its impact in many areas but questions still remain."
                },
                {
                    "title": "Narrative Based Medicine.",
                    "abstract": "Narrative Based Medicine. Edited by Trisha Greenhalgh and Brian Hurwitz. (Pp xvi + 286; £19.95.) BMJ Books, 1998. ISBN 0-7279-1223-2.\\***|\n\nAfter two introductory contributions, the remaining 23 chapters (written by 29 contributors) are arranged under the following section headings: “Illness stories”, “Narrative in medicine”, “Learning and teaching narrative”, “Understanding narrative in health care”, and “Broader perspectives on narrative in health care”. There is also an appendix (by the editors) which contains valuable suggestions for further reading, followed by an adequate index.\n\nBut what …"
                },
                {
                    "title": "Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel",
                    "abstract": "Abstract : Three readability formulas were recalculated to be more suitable for Navy use. The three formulas are the Automated Readability Index (ARI), Fog Count, and Flesch Reading Ease Formula. They were derived from test results of 531 Navy enlisted personnel enrolled in four technical training schools. Personnel were tested for their reading comprehension level according to the comprehension section of the Gates-McGinitie reading test. At the same time, they were tested for their comprehension of 18 passages taken from Rate Training Manuals. Scores on the reading test and training material passages allowed the calculation of the grade level of the passages. This scaled reading grade level is based on Navy personnel reading Navy training material and comprehending it."
                },
                {
                    "title": "National",
                    "abstract": null
                },
                {
                    "title": ". Ethics and governance of artificial intelligence for health: guidance on large multi-modal models",
                    "abstract": null
                },
                {
                    "title": "2024. MedicalAIcouldbe‘dangerous’forpoorernations,WHOwarns",
                    "abstract": null
                },
                {
                    "title": "Tailoring Large Language Models to Radiology: A Preliminary Approach to LLM Adaptation for a Highly Specialized Domain",
                    "abstract": null
                },
                {
                    "title": "Deeplearningbasedanalysisofsentimentdynamicsinonlinecancercommunity forums: An experience",
                    "abstract": null
                },
                {
                    "title": "Acceptabilityofartificialintelligence(AI)-ledchatbotservicesinhealthcare: A mixed-methods study",
                    "abstract": null
                },
                {
                    "title": "Methodological exemplar of integrating quantitative and qualitative evidence – supportive care for men with prostate cancer: what are the most important components?",
                    "abstract": "AIMS\nTo present a methodological exemplar of integrating findings from a quantitative and qualitative review on the same topic to provide insight into components of care that contribute to supportive care that is acceptable to men with prostate cancer.\n\n\nBACKGROUND\nMen with prostate cancer are likely to live a long time with the disease, experience side effects from treatment and therefore have ongoing supportive care needs. Quantitative and qualitative reviews have been published but the findings have yet to be integrated.\n\n\nDESIGN\nIntegration of quantitative and qualitative synthesized evidence.\n\n\nDATA SOURCE\nTwo previously published systematic reviews.\n\n\nREVIEW METHODS\nSynthesized evidence on supportive care for men with prostate cancer was integrated from two previously published systematic reviews: a narrative quantitative review and a qualitative review with thematic synthesis. These two streams of synthesized evidence were synthesized using concurrent narrative summary. Data from both reviews were used to develop a set of propositions from which a summary of components of care that likely to contribute to supportive care acceptable to men with prostate cancer were identified.\n\n\nRESULTS\nNine propositions were developed which covered men's supportive care focusing on the role of health professionals. These propositions were used to compose nine components of care likely to lead to supportive care that is acceptable to men with prostate cancer. Some of these components are no/low cost such as developing a more empathic personalized approach, but more specific approaches need further investigation in randomized controlled trials, for example, online support.\n\n\nCONCLUSION\nThis methodological exemplar demonstrates the integration of quantitative and qualitative synthesized data to determine components of care likely to lead to provision of supportive care acceptable to men with prostate cancer."
                },
                {
                    "title": "The health information national trends survey (HINTS): A resource for consumer engagement and health communication research",
                    "abstract": "The contemporary healthcare system can help improve health literacy outcomes in two ways: first, by nurturing the skills and motivations needed for patients to be actively engaged in their own health and healthcare decisions; and, second, by creating a prepared and proactive healthcare system that adapts to patients' capacities and needs in efficacious ways. In 2001, the National Cancer Institute launched the Health Information National Trends Survey (HINTS) as a way for researchers and planners to understand how the public is interacting with a rapidly changing health information environment. Original iterations of the HINTS national probability sampling strategies took place on a biennial basis, but in subsequent years the protocol moved to a yearly administration. This yields a rich resource of cross-sectional, national surveillance data to evaluate for trends across and within vulnerable populations. Sixteen studies are presented from the published literature to illustrate how HINTS data were used to explore constructs of direct interest to health literacy researchers. Suggestions are given for how this ongoing public surveillance mechanism can be used: (a) to provide a sentinel view of how the public is interacting with information in the environment to address their health needs; (b) to generate research questions and hypotheses for further exploration using complementary methodologies; and"
                },
                {
                    "title": "Assessing reading levels of health information: uses and limitations of flesch formula",
                    "abstract": "Background: Written health information is commonly used by health-care professionals (HCPs) to inform and assess patients in clinical practice. With growing self-management of many health conditions and increased information seeking behavior among patients, there is a greater stress on HCPs and researchers to develop and implement readable and understandable health information. Readability formulas such as Flesch Reading Ease (FRE) and Flesch–Kincaid Reading Grade Level (FKRGL) are commonly used by researchers and HCPs to assess if health information is reading grade appropriate for patients. Purpose: In this article, we critically analyze the role and credibility of Flesch formula in assessing the reading level of written health information. Discussion: FRE and FKRGL assign a grade level by measuring semantic and syntactic difficulty. They serve as a simple tool that provides some information about the potential literacy difficulty of written health information. However, health information documents often involve complex medical words and may incorporate pictures and tables to improve the legibility. In their assessments, FRE and FKRGL do not take into account (1) document factors (layout, pictures and charts, color, font, spacing, legibility, and grammar), (2) person factors (education level, comprehension, health literacy, motivation, prior knowledge, information needs, anxiety levels), and (3) style of writing (cultural sensitivity, comprehensiveness, and appropriateness), and thus, inadequately assess reading level. New readability measures incorporate pictures and use complex algorithms to assess reading level but are only moderately used in health-care research and not in clinical practice. Future research needs to develop generic and disease-specific readability measures to evaluate comprehension of a written document based on individuals' literacy levels, cultural background, and knowledge of disease."
                },
                {
                    "title": "Positive Technology as a Driver for Health Engagement",
                    "abstract": "Despite the fact that older adults are healthier than in the past, the current trend of an ageing population implies an increased risk and severity of chronic diseases. Low-resource healthcare systems face increased organizational healthcare costs, which is likely to result in an allocation of limited health resources. Healthcare organizations themselves must deal with patients' increasing need for a more active role in all the steps of the care & cure process. Technological advances may play a crucial role in sustaining people's health management in daily life, but only if it is \"ecologically\" designed and well-attuned to people's health needs and expectations. Healthcare is more and more called to orient innovative research approaches that recognize the crucial role of a person's engagement in health and well-being. This will enable patients to reach a higher quality of life and achieve a general psychophysical well-being. Thus, positive technological innovation can sustain people's engagement in health and invoke community empowerment, as we shall discuss in this document."
                },
                {
                    "title": "Cancer care for the whole patient : meeting psychosocial health needs",
                    "abstract": "Cancer care today often provides state-of-the-science biomedical treatment, but fails to address the psychological and social (psychosocial) problems associated with the illness. This failure can compromise the effectiveness of health care and thereby adversely affect the health of cancer patients. Psychological and social problems created or exacerbated by cancer--including depression and other emotional problems; lack of information or skills needed to manage the illness; lack of transportation or other resources; and disruptions in work, school, and family life--cause additional suffering, weaken adherence to prescribed treatments, and threaten patients' return to health. Today, it is not possible to deliver high-quality cancer care without using existing approaches, tools, and resources to address patients' psychosocial health needs. All patients with cancer and their families should expect and receive cancer care that ensures the provision of appropriate psychosocial health services. Cancer Care for the Whole Patient recommends actions that oncology providers, health policy makers, educators, health insurers, health planners, researchers and research sponsors, and consumer advocates should undertake to ensure that this standard is met."
                },
                {
                    "title": "The BSCS 5E Instructional Model: Origins and Effectiveness",
                    "abstract": "s International 36(11): 7368A. Champagne, A. (1987). The psychological basis for a model of science instruction. Commissioned paper for IBM-supported design project. Colorado Springs, CO:"
                },
                {
                    "title": "The eMale: Prostate cancer, masculinity and online support as a challenge to medical expertise",
                    "abstract": null
                },
                {
                    "title": "Expanding the 5E Model.",
                    "abstract": null
                },
                {
                    "title": "2022. Survey on natural language processing in medical image analysis",
                    "abstract": null
                },
                {
                    "title": "2022. CancerBERT: a cancer domain-specific language model for extracting breast cancer phenotypes from electronic health records",
                    "abstract": null
                },
                {
                    "title": "2024. Outpatient reception via collaboration between nurses and a large language model: a randomized controlled trial",
                    "abstract": null
                },
                {
                    "title": "2023. Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building using Retrieval-Augmented Generation",
                    "abstract": null
                },
                {
                    "title": "Outlining the Borders for LLM Applications",
                    "abstract": null
                },
                {
                    "title": "2024. AI in education: A review of personalized learning and educational technology",
                    "abstract": null
                },
                {
                    "title": "2024. Comparison of ChatGPT and traditional patient education materials for men’s health",
                    "abstract": null
                },
                {
                    "title": "2024. ChatGPT can help guide and empower patients after prostate cancer diagnosis",
                    "abstract": null
                },
                {
                    "title": "Conference, Sep, 2024",
                    "abstract": null
                },
                {
                    "title": "2023. Designing Guiding Principles for NLP for Healthcare: A Case Study of Maternal Health",
                    "abstract": null
                },
                {
                    "title": "2023. The Relation Between eHealth Literacy and Health-Related Behaviors: Systematic Review and Meta-analysis",
                    "abstract": null
                },
                {
                    "title": "2024. Facts & Figures 2024",
                    "abstract": null
                },
                {
                    "title": "2022. Clinicalradiobert: Knowledge-infused few shot learning for clinical notes named entity recognition",
                    "abstract": null
                },
                {
                    "title": "2024. How Much Decision Power Should (A) I Have?: Investigating Patients’ Preferences Towards AI Autonomy in Healthcare Decision Making",
                    "abstract": null
                },
                {
                    "title": "2023. Efficacy of Alexa, Google Assistant, and Siri for Supporting Informed Prostate Cancer Screening Decisions for African-American Men",
                    "abstract": null
                },
                {
                    "title": "2023. Principles for Augmented Intelligence Development, Deployment, and Use",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "29765cda-7a71-4aaf-ba45-1fab2a9f17cd": {
                "pk": "29765cda-7a71-4aaf-ba45-1fab2a9f17cd",
                "project_name": null,
                "name": "Yuexing Hao",
                "bio": "I am a researcher dedicated to enhancing machine learning methodologies, particularly in the realms of neural network quantization, recommendation systems, and natural language processing. My recent work has focused on developing innovative techniques that bridge the gap between theoretical advancements and practical applications.\n\nOne of my notable contributions is the Attention Round quantization method, which optimizes post-training quantization by allowing parameters to map to a broader range of quantized values, achieving performance comparable to quantization aware training with minimal data. In the area of recommendation systems, I have proposed a dynamic user-item heterogeneous graph framework that captures user influence, significantly improving sequential recommendations by considering both user behaviors and their interactions.\n\nAdditionally, I have tackled the challenge of selection bias in observational data through my REST framework, which reconstructs latent exposure strategies to enhance social recommendations. My work in information extraction has led to a novel tagging scheme for joint entity and relation extraction, demonstrating superior performance over traditional methods.\n\nI am also passionate about applying advanced language models in healthcare, exemplified by my development of RadOnc-GPT, a specialized model designed to assist clinical teams in managing patient communications efficiently. This work not only improves response quality but also alleviates the workload on healthcare professionals.\n\nThrough my research, I aim to create impactful solutions that address real-world challenges, leveraging the power of machine learning to drive innovation across various domains.",
                "collaborators": [
                    "Zijian Li",
                    "Ruichu Cai",
                    "Fengzhu Wu",
                    "Hao Gu",
                    "Huabin Diao",
                    "Gongyan Li",
                    "Shaoyun Xu",
                    "Sili Zhang",
                    "Yuguang",
                    "Jie Qiao"
                ],
                "pub_titles": [
                    "Attention Round for Post-Training Quantization",
                    "TEA: A Sequential Recommendation Framework via Temporally Evolving Aggregations",
                    "REST: Debiased Social Recommendation via Reconstructing Exposure Strategies",
                    "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
                    "TAG : Type Auxiliary Guiding for Code Comment Generation",
                    "Retrospective Comparative Analysis of Prostate Cancer In-Basket Messages: Responses from Closed-Domain LLM vs. Clinical Teams"
                ],
                "pub_abstracts": [
                    "At present, the quantification methods of neural network models are mainly divided into post-training quantization (PTQ) and quantization aware training (QAT). Post-training quantization only need a small part of the data to complete the quantification process, but the performance of its quantitative model is not as good as the quantization aware training. This paper presents a novel quantification method called Attention Round. This method gives parameters w the opportunity to be mapped to all possible quantized values, rather than just the two quantized values nearby w in the process of quantization. The probability of being mapped to different quantified values is negatively correlated with the distance between the quantified values and w, and decay with a Gaussian function. In addition, this paper uses the lossy coding length as a measure to assign bit widths to the different layers of the model to solve the problem of mixed precision quantization, which effectively avoids to solve combinatorial optimization problem. This paper also performs quantitative experiments on different models, the results confirm the effectiveness of the proposed method. For ResNet18 and MobileNetV2, the post-training quantization proposed in this paper only require 1,024 training data and 10 minutes to complete the quantization process, which can achieve quantization performance on par with quantization aware training.",
                    "Sequential recommendation aims to choose the most suitable items for a user at a specific timestamp given historical behaviors. Existing methods usually model the user behavior sequence based on the transition-based methods like Markov Chain. However, these methods also implicitly assume that the users are independent of each other without considering the influence between users. In fact, this influence plays an important role in sequence recommendation since the behavior of a user is easily affected by others. Therefore, it is desirable to aggregate both user behaviors and the influence between users, which are evolved temporally and involved in the heterogeneous graph of users and items. In this paper, we incorporate dynamic user-item heterogeneous graphs to propose a novel sequential recommendation framework. As a result, the historical behaviors as well as the influence between users can be taken into consideration. To achieve this, we firstly formalize sequential recommendation as a problem to estimate conditional probability given temporal dynamic heterogeneous graphs and user behavior sequences. After that, we exploit the conditional random field to aggregate the heterogeneous graphs and user behaviors for probability estimation, and employ the pseudo-likelihood approach to derive a tractable objective function. Finally, we provide scalable and flexible implementations of the proposed framework. Experimental results on three real-world datasets not only demonstrate the effectiveness of our proposed method but also provide some insightful discoveries on sequential recommendation.",
                    "The recommendation system, relying on historical observational data to model the complex relationships among the users and items, has achieved great success in real-world applications. Selection bias is one of the most important issues of the existing observational data based approaches, which is actually caused by multiple types of unobserved exposure strategies (e.g. promotions and holiday effects). Though various methods have been proposed to address this problem, they are mainly relying on the implicit debiasing techniques but not explicitly modeling the unobserved exposure strategies. By explicitly Reconstructing Exposure STrategies (REST in short), we formalize the recommendation problem as the counterfactual reasoning and propose the debiased social recommendation method. In REST, we assume that the exposure of an item is controlled by the latent exposure strategies, the user, and the item. Based on the above generation process, we first provide the theoretical guarantee of our method via identification analysis. Second, we employ a variational auto-encoder to reconstruct the latent exposure strategies, with the help of the social networks and the items. Third, we devise a counterfactual reasoning based recommendation algorithm by leveraging the recovered exposure strategies. Experiments on four real-world datasets, including three published datasets and one private WeChat Official Account dataset, demonstrate significant improvements over several state-of-the-art methods.",
                    "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",
                    "Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.",
                    "In-basket message interactions play a crucial role in physician-patient communication, occurring during all phases (pre-, during, and post) of a patient's care journey. However, responding to these patients' inquiries has become a significant burden on healthcare workflows, consuming considerable time for clinical care teams. To address this, we introduce RadOnc-GPT, a specialized Large Language Model (LLM) powered by GPT-4 that has been designed with a focus on radiotherapeutic treatment of prostate cancer with advanced prompt engineering, and specifically designed to assist in generating responses. We integrated RadOnc-GPT with patient electronic health records (EHR) from both the hospital-wide EHR database and an internal, radiation-oncology-specific database. RadOnc-GPT was evaluated on 158 previously recorded in-basket message interactions. Quantitative natural language processing (NLP) analysis and two grading studies with clinicians and nurses were used to assess RadOnc-GPT's responses. Our findings indicate that RadOnc-GPT slightly outperformed the clinical care team in \"Clarity\" and \"Empathy,\" while achieving comparable scores in \"Completeness\" and \"Correctness.\" RadOnc-GPT is estimated to save 5.2 minutes per message for nurses and 2.4 minutes for clinicians, from reading the inquiry to sending the response. Employing RadOnc-GPT for in-basket message draft generation has the potential to alleviate the workload of clinical care teams and reduce healthcare costs by producing high-quality, timely responses."
                ],
                "domain": [
                    "Neural Network Quantization",
                    "Recommendation Systems",
                    "Information Extraction",
                    "Natural Language Processing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "3fa84be8-ba5b-45ad-8586-34e6cb2bf0d8": {
                "pk": "3fa84be8-ba5b-45ad-8586-34e6cb2bf0d8",
                "project_name": null,
                "name": "Jason Holmes",
                "bio": "As a researcher dedicated to the field of proton therapy, I focus on enhancing the precision and customization of treatment through advanced dose calculation techniques. My recent work emphasizes the critical importance of fast Monte Carlo dose calculations, which are essential for optimizing treatment plans in adaptive radiotherapy and proton-based stereotactic radiosurgery. I recognize that the speed of these calculations directly impacts the feasibility of implementing innovative treatment modalities, making it imperative to improve computational efficiency.\n\nIn my research, I explore the latest advancements in Monte Carlo methods, particularly those that leverage artificial intelligence to accelerate dose calculations. By achieving speeds of 10^6 to 10^7 protons per second, we can significantly enhance the optimality of treatment plans, ultimately leading to better patient outcomes. My goal is to bridge the gap between complex computational techniques and practical clinical applications, ensuring that the benefits of cutting-edge research translate into real-world improvements in cancer treatment. Through my work, I aim to contribute to the evolution of proton therapy, making it more effective and accessible for patients in need.",
                "collaborators": [
                    "Hongying Feng",
                    "Lian Zhang",
                    "Michael Fix",
                    "Steve B. Jiang",
                    "Wei Liu"
                ],
                "pub_titles": [
                    "Fast Monte Carlo Dose Calculation in Proton Therapy"
                ],
                "pub_abstracts": [
                    "This article examines the critical role of fast Monte Carlo dose calculations in advancing proton therapy techniques, particularly in the context of increasing treatment customization and precision. As adaptive radiotherapy and other patient-specific approaches evolve, the need for accurate and precise dose calculations, essential for techniques like proton-based stereotactic radiosurgery, becomes more prominent. These calculations, however, are time-intensive, with the treatment planning/optimization process constrained by the achievable speed of dose computations. Thus, enhancing the speed of Monte Carlo methods is vital, as it not only facilitates the implementation of novel treatment modalities but also improves the optimality of treatment plans. Today, the state-of-the-art in Monte Carlo dose calculation speeds is 106 - 107 protons per second. This review highlights the latest advancements in fast Monte Carlo dose calculations that have led to such speeds, including emerging artificial intelligence-based techniques, and discusses their application in both current and emerging proton therapy strategies."
                ],
                "domain": [
                    "Medical Physics",
                    "Proton Therapy",
                    "Monte Carlo Simulation",
                    "Artificial Intelligence"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f2c6dd75-ba5f-4db6-b861-902ae1d102a3": {
                "pk": "f2c6dd75-ba5f-4db6-b861-902ae1d102a3",
                "project_name": null,
                "name": "Mark Waddle",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge and enhancing search efficiency.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and machine learning, with the goal of making these technologies more accessible and effective for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "59bc00f2-5c12-4571-8789-e0fac87604e1": {
                "pk": "59bc00f2-5c12-4571-8789-e0fac87604e1",
                "project_name": null,
                "name": "Nathan Yu",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nIn addition to architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimizing performance across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for optimal model designs, making it more efficient and insightful.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and machine learning, with the goal of making these technologies more effective and accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "14d6bdb0-0377-440a-ae10-f48975a232e2": {
                "pk": "14d6bdb0-0377-440a-ae10-f48975a232e2",
                "project_name": null,
                "name": "Kirstin Vickers",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nIn addition to architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimizing performance across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for optimal model designs, making it more efficient and insightful.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and machine learning, with the goal of making these technologies more effective and accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "669b81c4-2d05-4897-8f4a-bcfc7860a4cb": {
                "pk": "669b81c4-2d05-4897-8f4a-bcfc7860a4cb",
                "project_name": null,
                "name": "Heather Preston",
                "bio": "As a researcher in astrophysics, I have dedicated my work to enhancing the capabilities of space-based photometry, particularly through the K2 mission, which repurposes the Kepler spacecraft for high-precision observations. My primary focus has been on developing a robust aperture photometry pipeline that automates the selection of dynamic aperture masks, estimates and subtracts background noise, and applies positional decorrelation techniques to mitigate the effects of spacecraft pointing jitter. \n\nThis pipeline not only improves the photometric precision of K2 data but also allows for the identification of secondary targets within the K2 \"postage stamps,\" enabling the production of light curves for these additional objects. I take pride in the fact that the results from our pipeline will be made accessible to the broader scientific community, fostering collaboration and further research. My work has also illustrated the utility of our methods through asteroseismic results derived from these serendipitous secondary targets, showcasing the potential of K2 data in advancing our understanding of stellar phenomena. I am passionate about contributing to the field of astrophysics and look forward to the continued exploration of the universe through innovative data analysis techniques.",
                "collaborators": [
                    "Derek L. Buzasi",
                    "Lindsey Carboneau",
                    "Carly Hessler",
                    "Andy Lezcano"
                ],
                "pub_titles": [
                    "Serendipitous Science from the K2 Mission"
                ],
                "pub_abstracts": [
                    "The K2 mission is a repurposed use of the Kepler spacecraft to perform high-precision photometry of selected fields in the ecliptic. We have developed an aperture photometry pipeline for K2 data which performs dynamic automated aperture mask selection, background estimation and subtraction, and positional decorrelation to minimize the effects of spacecraft pointing jitter. We also identify secondary targets in the K2 \"postage stamps\" and produce light curves for those targets as well. Pipeline results will be made available to the community. Here we describe our pipeline and the photometric precision we are capable of achieving with K2, and illustrate its utility with asteroseismic results from the serendipitous secondary targets."
                ],
                "domain": [
                    "Astronomy",
                    "Photometry",
                    "Asteroseismology",
                    "Space Missions"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "1cdb9d69-1297-45fb-b511-fde352213c7a": {
                "pk": "1cdb9d69-1297-45fb-b511-fde352213c7a",
                "project_name": null,
                "name": "Drew Margolin",
                "bio": "I am a researcher dedicated to exploring the intersection of media, technology, and social behavior, particularly in the context of political communication. My work has focused on the integrity of news reporting, especially during pivotal events like the 2016 U.S. Presidential Election. I developed a computational model to analyze partisan traits in news text attributions, achieving over 88% accuracy in identifying statements ascribed to candidates like Hillary Clinton and Donald Trump. \n\nAdditionally, I have investigated how \"media events,\" such as political debates, shape collective behavior on social media platforms. By analyzing data from approximately 200,000 politically-active Twitter users, I uncovered distinct patterns of engagement during these events, highlighting the concentrated attention on specific users and hashtags. \n\nMy research also delves into the lifecycle of hashtags during the 2012 U.S. presidential debates, where I introduced a \"conversational vibrancy\" framework to understand the dynamics of hashtag adoption. This work revealed critical insights into how retweets and replies influence the success and longevity of hashtags, contributing to our understanding of social influence and collective action in digital spaces. \n\nThrough my research, I aim to provide a deeper understanding of how media and social platforms interact, particularly in shaping public discourse and collective sensemaking during significant political moments.",
                "collaborators": [
                    "Yu-Ru Lin",
                    "Brian Keegan",
                    "David Lazer",
                    "Logan Martel",
                    "Edward Newell",
                    "Derek Ruths",
                    "Andrea Baronchelli"
                ],
                "pub_titles": [
                    "Assessing Partisan Traits of News Text Attributions",
                    "Rising tides or rising stars?: Dynamics of shared attention on Twitter during media events",
                    "#Bigbirds Never Die: Understanding Social Dynamics of Emergent Hashtag"
                ],
                "pub_abstracts": [
                    "On the topic of journalistic integrity, the current state of accurate, impartial news reporting has garnered much debate in context to the 2016 US Presidential Election. In pursuit of computational evaluation of news text, the statements (attributions) ascribed by media outlets to sources provide a common category of evidence on which to operate. In this paper, we develop an approach to compare partisan traits of news text attributions and apply it to characterize differences in statements ascribed to candidate, Hilary Clinton, and incumbent President, Donald Trump. In doing so, we present a model trained on over 600 in-house annotated attributions to identify each candidate with accuracy > 88%. Finally, we discuss insights from its performance for future research.",
                    "\"Media events\" such as political debates generate conditions of shared attention as many users simultaneously tune in with the dual screens of broadcast and social media to view and participate. Are collective patterns of user behavior under conditions of shared attention distinct from other \"bursts\" of activity like breaking news events? Using data from a population of approximately 200,000 politically-active Twitter users, we compare features of their behavior during eight major events during the 2012 U.S. presidential election to examine (1) the impact of \"media events\" have on patterns of social media use compared to \"typical\" time and (2) whether changes during media events are attributable to changes in behavior across the entire population or an artifact of changes in elite users' behavior. Our findings suggest that while this population became more active during media events, this additional activity reflects concentrated attention to a handful of users, hashtags, and tweets. Our work is the first study on distinguishing patterns of large-scale social behavior under condition of uncertainty and shared attention, suggesting new ways of mining information from social media to support collective sensemaking following major events.",
                    "We examine the growth, survival, and context of 256 novel hashtags during the 2012 U.S. presidential debates. Our analysis reveals the trajectories of hashtag use fall into two distinct classes: \"winners\" that emerge more quickly and are sustained for longer periods of time than other \"also-rans\" hashtags. We propose a \"conversational vibrancy\" framework to capture dynamics of hashtags based on their topicality, interactivity, diversity, and prominence. Statistical analyses of the growth and persistence of hashtags reveal novel relationships between features of this framework and the relative success of hashtags. Specifically, retweets always contribute to faster hashtag adoption, replies extend the life of \"winners\" while having no effect on \"also-rans.\" This is the first study on the lifecycle of hashtag adoption and use in response to purely exogenous shocks. We draw on theories of uses and gratification, organizational ecology, and language evolution to discuss these findings and their implications for understanding social influence and collective action in social media more generally."
                ],
                "domain": [
                    "Computational Social Science",
                    "Media Analysis",
                    "Social Media",
                    "Political Communication"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "061a7bb4-17fb-40a3-b1e6-fe4c398b0e73": {
                "pk": "061a7bb4-17fb-40a3-b1e6-fe4c398b0e73",
                "project_name": null,
                "name": "Corinna E. Löckenhoff",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to improve node embeddings by capturing their positions within the broader graph structure, achieving significant performance gains in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of message-passing frameworks by incorporating node identities during the aggregation process. This innovation has led to substantial accuracy improvements across various prediction tasks. Additionally, I proposed the ROLAND framework, which enables the adaptation of static GNNs to dynamic graphs, addressing scalability and evaluation challenges in real-world applications.\n\nMy research extends beyond GNNs; I have explored the architectural design space of GNNs, identifying optimal designs for diverse tasks through systematic evaluation. I also developed AutoTransfer, an AutoML solution that enhances search efficiency by leveraging prior architectural knowledge, significantly reducing computational costs.\n\nThrough my work, I aim to bridge the gap between theoretical advancements and practical applications, contributing to the evolving landscape of machine learning and graph-based data analysis.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "4b9e4519-d565-4788-9e12-aa7fbce502f8": {
                "pk": "4b9e4519-d565-4788-9e12-aa7fbce502f8",
                "project_name": null,
                "name": "Aditya Vashistha",
                "bio": "I am a researcher dedicated to understanding the complexities of social media, particularly WhatsApp, and its impact on communication, misinformation, and community support in the Global South. My work explores the dynamics of group moderation, revealing how WhatsApp group admins navigate challenges in managing content and fostering healthy discussions. Through qualitative interviews and content analysis, I have identified distinct moderation styles and provided design recommendations to enhance the effectiveness of group management.\n\nIn addition to moderation, I have developed interventions like Saharaline, a WhatsApp-based helpline that offers personalized support to teachers in low-income schools, demonstrating the potential of technology to address real-world challenges. My research also critically examines the implications of Western-centric AI models, revealing how they can homogenize cultural expression and diminish local nuances in writing.\n\nI am particularly passionate about advocating for marginalized voices, as seen in my studies on gender bias in Hindi and the experiences of people with disabilities online. By employing community-centric research designs, I aim to amplify underrepresented perspectives and promote inclusivity in technology design.\n\nMy recent work with ASHABot, a WhatsApp-based chatbot for community health workers, highlights the importance of integrating AI as a supportive resource rather than a replacement for human expertise. Through these diverse projects, I strive to contribute to a deeper understanding of how technology can be harnessed to foster community resilience and combat misinformation in a rapidly evolving digital landscape.",
                "collaborators": [
                    "Dhruv Agarwal",
                    "Farhana Shahid",
                    "Rama Adithya Varanasi",
                    "Nicola Dell",
                    "Mor Naaman",
                    "Mahika Phutane",
                    "Ananya Seelam",
                    "Kiran Garimella",
                    "Bharat Nayak",
                    "Simon Chauchard"
                ],
                "pub_titles": [
                    "'One Style Does Not Regulate All': Moderation Practices in Public and Private WhatsApp Groups",
                    "Saharaline: A Collective Social Support Intervention for Teachers in Low-Income Indian Schools",
                    "AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances",
                    "How Toxicity Classifiers and Large Language Models Respond to Ableism",
                    "Conversational Agents to Facilitate Deliberation on Harmful Content in WhatsApp Groups",
                    "Deciphering Viral Trends in WhatsApp: A Case Study From a Village in Rural India",
                    "Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology",
                    "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers"
                ],
                "pub_abstracts": [
                    "WhatsApp is the largest social media platform in the Global South and is a virulent force in global misinformation and political propaganda. Due to end-to-end encryption WhatsApp can barely review any content and mostly rely on volunteer moderation by group admins. Yet, little is known about how WhatsApp group admins manage their groups, what factors and values influence moderation decisions, and what challenges they face while managing their groups. To fill this gap, we interviewed admins of 32 diverse groups and reviewed content from 30 public groups in India and Bangladesh. We observed notable differences in the formation, members' behavior, and moderation of public versus private groups, as well as in how WhatsApp admins operate compared to those on other platforms. We used Baumrind's typology of 'parenting styles' as a lens to examine how admins enact care and control during volunteer moderation. We identified four styles based on how caring and controlling the admins are and discuss design recommendations to help them better manage problematic content in WhatsApp groups.",
                    "This paper presents Saharaline, an intervention designed to provide collective social support for teachers in low-income schools. Implemented as a WhatsApp-based helpline, Saharaline enables teachers to reach out for personalized, long-term assistance with a wide range of problems and stressors, including pedagogical, emotional, and technological challenges. Depending on the support needed, teachers' requests are routed to appropriate domain experts -- staff employed by educational non-profit organizations who understand teachers' on-the-ground realities -- who offer localized and contextualized assistance. Via a three-month exploratory deployment with 28 teachers in India, we show how Saharaline's design enabled a collective of diverse education experts to craft and deliver localized solutions that teachers could incorporate into their practice. We conclude by reflecting on the efficacy of our intervention in low-resource work contexts and provide recommendations to enhance collective social support interventions similar to Saharaline.",
                    "Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.",
                    "People with disabilities (PwD) regularly encounter ableist hate and microaggressions online. While online platforms use machine learning models to moderate online harm, there is little research investigating how these models interact with ableism. In this paper, we curated a dataset of 100 social media comments targeted towards PwD, and recruited 160 participants to rate and explain how toxic and ableist these comments were. We then prompted state-of-the art toxicity classifiers (TCs) and large language models (LLMs) to rate and explain the harm. Our analysis revealed that TCs and LLMs rated toxicity significantly lower than PwD, but LLMs rated ableism generally on par with PwD. However, ableism explanations by LLMs overlooked emotional harm, and lacked specificity and acknowledgement of context, important facets of PwD explanations. Going forward, we discuss challenges in designing disability-aware toxicity classifiers, and advocate for the shift from ableism detection to ableism interpretation and explanation.",
                    "WhatsApp groups have become a hotbed for the propagation of harmful content including misinformation, hate speech, polarizing content, and rumors, especially in Global South countries. Given the platform's end-to-end encryption, moderation responsibilities lie on group admins and members, who rarely contest such content. Another approach is fact-checking, which is unscalable, and can only contest factual content (e.g., misinformation) but not subjective content (e.g., hate speech). Drawing on recent literature, we explore deliberation -- open and inclusive discussion -- as an alternative. We investigate the role of a conversational agent in facilitating deliberation on harmful content in WhatsApp groups. We conducted semi-structured interviews with 21 Indian WhatsApp users, employing a design probe to showcase an example agent. Participants expressed the need for anonymity and recommended AI assistance to reduce the effort required in deliberation. They appreciated the agent's neutrality but pointed out the futility of deliberation in echo chamber groups. Our findings highlight design tensions for such an agent, including privacy versus group dynamics and freedom of speech in private spaces. We discuss the efficacy of deliberation using deliberative theory as a lens, compare deliberation with moderation and fact-checking, and provide design recommendations for future such systems. Ultimately, this work advances CSCW by offering insights into designing deliberative systems for combating harmful content in private group chats on social media.",
                    "This research studies the nature and spread of WhatsApp content among everyday users in a rural Indian village. Leveraging a dataset of hundreds of private WhatsApp groups collected with consent from participants, our study uncovers the kinds of WhatsApp groups users are part of, marking the first such categorization. The dataset comprises tens of thousands of messages, out of which we manually classified 604 pieces of content designated as 'forwarded many times'-indicating their viral status.   Our key findings indicate a high prevalence of content focused on national politics, with the viral messages overwhelmingly supporting a specific political party and disparaging the opposition. Significantly, these messages were fraught with misinformation, engendering hate against Muslims and promoting a narrative of Hindus being under threat. This trend was particularly noticeable within caste-based groups, which were dominated by misinformation, pro-BJP rhetoric, anti-Congress content, and Hindutva propaganda. Remarkably, much of the misinformation circulating had previously been discredited by established fact-checking organizations. This suggests not only a recurring cycle of debunked information reappearing but also that fact-checks are failing to penetrate these specific groups.   As the first quantitative analysis of everyday WhatsApp use in a rural context, this research has far-reaching implications for understanding the unique challenges posed by end-to-end encrypted platforms. It serves as a crucial baseline for designing more effective moderation policies aimed at combating misinformation and fostering a more responsible use of encrypted communication channels.",
                    "Existing research in measuring and mitigating gender bias predominantly centers on English, overlooking the intricate challenges posed by non-English languages and the Global South. This paper presents the first comprehensive study delving into the nuanced landscape of gender bias in Hindi, the third most spoken language globally. Our study employs diverse mining techniques, computational models, field studies and sheds light on the limitations of current methodologies. Given the challenges faced with mining gender biased statements in Hindi using existing methods, we conducted field studies to bootstrap the collection of such sentences. Through field studies involving rural and low-income community women, we uncover diverse perceptions of gender bias, underscoring the necessity for context-specific approaches. This paper advocates for a community-centric research design, amplifying voices often marginalized in previous studies. Our findings not only contribute to the understanding of gender bias in Hindi but also establish a foundation for further exploration of Indic languages. By exploring the intricacies of this understudied context, we call for thoughtful engagement with gender bias, promoting inclusivity and equity in linguistic and cultural contexts beyond the Global North.",
                    "Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training. This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India. Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot's role in addressing CHWs' informational needs. We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors. CHWs trusted the information they received on ASHABot and treated it as an authoritative resource. CHWs' supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability. We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support."
                ],
                "domain": [
                    "Social Media",
                    "Misinformation",
                    "Human-Computer Interaction",
                    "Gender Bias"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "7bc0b4b2-cb04-4e91-bff7-ecd529f42ab7": {
                "pk": "7bc0b4b2-cb04-4e91-bff7-ecd529f42ab7",
                "project_name": null,
                "name": "Marzyeh Ghassemi",
                "bio": "I am a researcher dedicated to leveraging machine learning to enhance healthcare outcomes through innovative modeling techniques. My work primarily focuses on understanding patient responses to interventions, risk prediction, and improving decision-making processes in clinical settings. I have explored the use of autoencoders to create low-dimensional embeddings of patient phenotypes, which I believe are crucial for predicting individual responses to treatments.\n\nIn my recent studies, I have demonstrated how machine learning classifiers can significantly outperform traditional logistic regression methods in predicting mortality among elderly patients, emphasizing the importance of using clinically relevant features. I have also tackled the challenges of domain shift in healthcare by developing methods for off-policy transfer, which allow for better policy learning in data-scarce environments.\n\nMy research extends to improving model robustness and generalization through novel data augmentation techniques and understanding the impact of spurious correlations in self-supervised learning. I have introduced frameworks for identifying worst-case decision points in clinical settings, ensuring that treatment policies are both safe and effective.\n\nAdditionally, I am passionate about addressing fairness in machine learning, particularly in clinical prediction tasks, and I have developed methods to ensure that personalized models provide equitable performance across diverse patient groups. My work aims to bridge the gap between advanced machine learning techniques and practical applications in healthcare, ultimately striving to improve patient outcomes and support clinical decision-making.",
                "collaborators": [
                    "Shalmali Joshi",
                    "Haoran Zhang",
                    "Taylor W. Killian",
                    "Nathan Ng",
                    "Aparna Balagopalan",
                    "Frank Rudzicz",
                    "Berk Ustun",
                    "Roger Grosse",
                    "Swami Sankaranarayanan",
                    "Harini Suresh"
                ],
                "pub_titles": [
                    "The Use of Autoencoders for Discovering Patient Phenotypes",
                    "Short-term Mortality Prediction for Elderly Patients Using Medicare Claims Data",
                    "Counterfactually Guided Off-policy Transfer in Clinical Settings",
                    "SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness",
                    "Quantifying the Task-Specific Information in Text-Based Classifications",
                    "Learning Optimal Predictive Checklists",
                    "When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction",
                    "Risk Sensitive Dead-end Identification in Safety-Critical Offline Reinforcement Learning",
                    "Measuring Stochastic Data Complexity with Boltzmann Influence Functions",
                    "Views Can Be Deceiving: Improved SSL Through Feature Space Augmentation",
                    "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts",
                    "Change is Hard: A Closer Look at Subpopulation Shift",
                    "ClinicalVis: Supporting Clinical Task-Focused Design Evaluation",
                    "Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals",
                    "Probabilistic Machine Learning for Healthcare",
                    "Confounding Feature Acquisition for Causal Effect Estimation",
                    "Medical Dead-ends and Learning to Identify High-risk States and Treatments",
                    "If Influence Functions are the Answer, Then What is the Question?",
                    "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors",
                    "The Effect of Heterogeneous Data for Alzheimer's Disease Detection from Speech"
                ],
                "pub_abstracts": [
                    "We use autoencoders to create low-dimensional embeddings of underlying patient phenotypes that we hypothesize are a governing factor in determining how different patients will react to different interventions. We compare the performance of autoencoders that take fixed length sequences of concatenated timesteps as input with a recurrent sequence-to-sequence autoencoder. We evaluate our methods on around 35,500 patients from the latest MIMIC III dataset from Beth Israel Deaconess Hospital.",
                    "Risk prediction is central to both clinical medicine and public health. While many machine learning models have been developed to predict mortality, they are rarely applied in the clinical literature, where classification tasks typically rely on logistic regression. One reason for this is that existing machine learning models often seek to optimize predictions by incorporating features that are not present in the databases readily available to providers and policy makers, limiting generalizability and implementation. Here we tested a number of machine learning classifiers for prediction of six-month mortality in a population of elderly Medicare beneficiaries, using an administrative claims database of the kind available to the majority of health care payers and providers. We show that machine learning classifiers substantially outperform current widely-used methods of risk prediction but only when used with an improved feature set incorporating insights from clinical medicine, developed for this study. Our work has applications to supporting patient and provider decision making at the end of life, as well as population health-oriented efforts to identify patients at high risk of poor outcomes.",
                    "Domain shift, encountered when using a trained model for a new patient population, creates significant challenges for sequential decision making in healthcare since the target domain may be both data-scarce and confounded. In this paper, we propose a method for off-policy transfer by modeling the underlying generative process with a causal mechanism. We use informative priors from the source domain to augment counterfactual trajectories in the target in a principled manner. We demonstrate how this addresses data-scarcity in the presence of unobserved confounding. The causal parametrization of our sampling procedure guarantees that counterfactual quantities can be estimated from scarce observational target data, maintaining intuitive stability properties. Policy learning in the target domain is further regularized via the source policy through KL-divergence. Through evaluation on a simulated sepsis treatment task, our counterfactual policy transfer procedure significantly improves the performance of a learned treatment policy when assumptions of \"no-unobserved confounding\" are relaxed.",
                    "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
                    "Recently, neural natural language models have attained state-of-the-art performance on a wide variety of tasks, but the high performance can result from superficial, surface-level cues (Bender and Koller, 2020; Niven and Kao, 2020). These surface cues, as the ``shortcuts'' inherent in the datasets, do not contribute to the *task-specific information* (TSI) of the classification tasks. While it is essential to look at the model performance, it is also important to understand the datasets. In this paper, we consider this question: Apart from the information introduced by the shortcut features, how much task-specific information is required to classify a dataset? We formulate this quantity in an information-theoretic framework. While this quantity is hard to compute, we approximate it with a fast and stable method. TSI quantifies the amount of linguistic knowledge modulo a set of predefined shortcuts -- that contributes to classifying a sample from each dataset. This framework allows us to compare across datasets, saying that, apart from a set of ``shortcut features'', classifying each sample in the Multi-NLI task involves around 0.4 nats more TSI than in the Quora Question Pair.",
                    "Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classification problems, and demonstrate its practical benefits by training a short-form checklist for PTSD screening. Our results show that our method can fit simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints.",
                    "Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the \"fair use\" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.",
                    "In safety-critical decision-making scenarios being able to identify worst-case outcomes, or dead-ends is crucial in order to develop safe and reliable policies in practice. These situations are typically rife with uncertainty due to unknown or stochastic characteristics of the environment as well as limited offline training data. As a result, the value of a decision at any time point should be based on the distribution of its anticipated effects. We propose a framework to identify worst-case decision points, by explicitly estimating distributions of the expected return of a decision. These estimates enable earlier indication of dead-ends in a manner that is tunable based on the risk tolerance of the designed task. We demonstrate the utility of Distributional Dead-end Discovery (DistDeD) in a toy domain as well as when assessing the risk of severely ill patients in the intensive care unit reaching a point where death is unavoidable. We find that DistDeD significantly improves over prior discovery approaches, providing indications of the risk 10 hours earlier on average as well as increasing detection by 20%.",
                    "Estimating the uncertainty of a model's prediction on a test point is a crucial part of ensuring reliability and calibration under distribution shifts. A minimum description length approach to this problem uses the predictive normalized maximum likelihood (pNML) distribution, which considers every possible label for a data point, and decreases confidence in a prediction if other labels are also consistent with the model and training data. In this work we propose IF-COMP, a scalable and efficient approximation of the pNML distribution that linearizes the model with a temperature-scaled Boltzmann influence function. IF-COMP can be used to produce well-calibrated predictions on test points as well as measure complexity in both labelled and unlabelled settings. We experimentally validate IF-COMP on uncertainty calibration, mislabel detection, and OOD detection tasks, where it consistently matches or beats strong baseline methods.",
                    "Supervised learning methods have been found to exhibit inductive biases favoring simpler features. When such features are spuriously correlated with the label, this can result in suboptimal performance on minority subgroups. Despite the growing popularity of methods which learn from unlabeled data, the extent to which these representations rely on spurious features for prediction is unclear. In this work, we explore the impact of spurious features on Self-Supervised Learning (SSL) for visual representation learning. We first empirically show that commonly used augmentations in SSL can cause undesired invariances in the image space, and illustrate this with a simple example. We further show that classical approaches in combating spurious correlations, such as dataset re-sampling during SSL, do not consistently lead to invariant representations. Motivated by these findings, we propose LateTVG to remove spurious information from these representations during pre-training, by regularizing later layers of the encoder via pruning. We find that our method produces representations which outperform the baselines on several benchmarks, without the need for group or label information during SSL.",
                    "Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The contribution of each distribution to the total performance change is then quantified as its Shapley value. We demonstrate the correctness and utility of our method on synthetic, semi-synthetic, and real-world case studies, showing its effectiveness in attributing performance changes to a wide range of distribution shifts.",
                    "Machine learning models often perform poorly on subgroups that are underrepresented in the training data. Yet, little is understood on the variation in mechanisms that cause subpopulation shifts, and how algorithms generalize across such diverse shifts at scale. In this work, we provide a fine-grained analysis of subpopulation shift. We first propose a unified framework that dissects and explains common shifts in subgroups. We then establish a comprehensive benchmark of 20 state-of-the-art algorithms evaluated on 12 real-world datasets in vision, language, and healthcare domains. With results obtained from training over 10,000 models, we reveal intriguing observations for future progress in this space. First, existing algorithms only improve subgroup robustness over certain types of shifts but not others. Moreover, while current algorithms rely on group-annotated validation data for model selection, we find that a simple selection criterion based on worst-class accuracy is surprisingly effective even without any group information. Finally, unlike existing works that solely aim to improve worst-group accuracy (WGA), we demonstrate the fundamental tradeoff between WGA and other important metrics, highlighting the need to carefully choose testing metrics. Code and data are available at: https://github.com/YyzHarry/SubpopBench.",
                    "Making decisions about what clinical tasks to prepare for is multi-factored, and especially challenging in intensive care environments where resources must be balanced with patient needs. Electronic health records (EHRs) are a rich data source, but are task-agnostic and can be difficult to use as summarizations of patient needs for a specific task, such as \"could this patient need a ventilator tomorrow?\" In this paper, we introduce ClinicalVis, an open-source EHR visualization-based prototype system for task-focused design evaluation of interactions between healthcare providers (HCPs) and EHRs. We situate ClinicalVis in a task-focused proof-of-concept design study targeting these interactions with real patient data. We conduct an empirical study of 14 HCPs, and discuss our findings on usability, accuracy, preference, and confidence in treatment decisions. We also present design implications that our findings suggest for future EHR interfaces, the presentation of clinical data for task-based planning, and evaluating task-focused HCP/EHR interactions in practice.",
                    "Heart failure is a debilitating condition that affects millions of people worldwide and has a significant impact on their quality of life and mortality rates. An objective assessment of cardiac pressures remains an important method for the diagnosis and treatment prognostication for patients with heart failure. Although cardiac catheterization is the gold standard for estimating central hemodynamic pressures, it is an invasive procedure that carries inherent risks, making it a potentially dangerous procedure for some patients. Approaches that leverage non-invasive signals - such as electrocardiogram (ECG) - have the promise to make the routine estimation of cardiac pressures feasible in both inpatient and outpatient settings. Prior models trained to estimate intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP)) in a supervised fashion have shown good discriminatory ability but have been limited to the labeled dataset from the heart failure cohort. To address this issue and build a robust representation, we apply deep metric learning (DML) and propose a novel self-supervised DML with distance-based mining that improves the performance of a model with limited labels. We use a dataset that contains over 5.4 million ECGs without concomitant central pressure labels to pre-train a self-supervised DML model which showed improved classification of elevated mPCWP compared to self-supervised contrastive baselines. Additionally, the supervised DML model that uses ECGs with access to 8,172 mPCWP labels demonstrated significantly better performance on the mPCWP regression task compared to the supervised baseline. Moreover, our data suggest that DML yields models that are performant across patient subgroups, even when some patient subgroups are under-represented in the dataset. Our code is available at https://github.com/mandiehyewon/ssldml",
                    "Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.",
                    "Reliable treatment effect estimation from observational data depends on the availability of all confounding information. While much work has targeted treatment effect estimation from observational data, there is relatively little work in the setting of confounding variable missingness, where collecting more information on confounders is often costly or time-consuming. In this work, we frame this challenge as a problem of feature acquisition of confounding features for causal inference. Our goal is to prioritize acquiring values for a fixed and known subset of missing confounders in samples that lead to efficient average treatment effect estimation. We propose two acquisition strategies based on i) covariate balancing (CB), and ii) reducing statistical estimation error on observed factual outcome error (OE). We compare CB and OE on five common causal effect estimation methods, and demonstrate improved sample efficiency of OE over baseline methods under various settings. We also provide visualizations for further analysis on the difference between our proposed methods.",
                    "Machine learning has successfully framed many sequential decision making problems as either supervised prediction, or optimal decision-making policy identification via reinforcement learning. In data-constrained offline settings, both approaches may fail as they assume fully optimal behavior or rely on exploring alternatives that may not exist. We introduce an inherently different approach that identifies possible \"dead-ends\" of a state space. We focus on the condition of patients in the intensive care unit, where a \"medical dead-end\" indicates that a patient will expire, regardless of all potential future treatment sequences. We postulate \"treatment security\" as avoiding treatments with probability proportional to their chance of leading to dead-ends, present a formal proof, and frame discovery as an RL problem. We then train three independent deep neural models for automated state construction, dead-end discovery and confirmation. Our empirical results discover that dead-ends exist in real clinical data among septic patients, and further reveal gaps between secure treatments and those that were administered.",
                    "Influence functions efficiently estimate the effect of removing a single training data point on a model's learned parameters. While influence estimates align well with leave-one-out retraining for linear models, recent works have shown this alignment is often poor in neural networks. In this work, we investigate the specific factors that cause this discrepancy by decomposing it into five separate terms. We study the contributions of each term on a variety of architectures and datasets and how they vary with factors such as network width and training time. While practical influence function estimates may be a poor match to leave-one-out retraining for nonlinear networks, we show they are often a good approximation to a different object we term the proximal Bregman response function (PBRF). Since the PBRF can still be used to answer many of the questions motivating influence functions, such as identifying influential or mislabeled examples, our results suggest that current algorithms for influence function estimation give more informative results than previous error analyses would suggest.",
                    "Deployed language models decay over time due to shifting inputs, changing user needs, or emergent world-knowledge gaps. When such problems are identified, we want to make targeted edits while avoiding expensive retraining. However, current model editors, which modify such behaviors of pre-trained models, degrade model performance quickly across multiple, sequential edits. We propose GRACE, a lifelong model editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}.",
                    "Speech datasets for identifying Alzheimer's disease (AD) are generally restricted to participants performing a single task, e.g. describing an image shown to them. As a result, models trained on linguistic features derived from such datasets may not be generalizable across tasks. Building on prior work demonstrating that same-task data of healthy participants helps improve AD detection on a single-task dataset of pathological speech, we augment an AD-specific dataset consisting of subjects describing a picture with multi-task healthy data. We demonstrate that normative data from multiple speech-based tasks helps improve AD detection by up to 9%. Visualization of decision boundaries reveals that models trained on a combination of structured picture descriptions and unstructured conversational speech have the least out-of-task error and show the most potential to generalize to multiple tasks. We analyze the impact of age of the added samples and if they affect fairness in classification. We also provide explanations for a possible inductive bias effect across tasks using model-agnostic feature anchors. This work highlights the need for heterogeneous datasets for encoding changes in multiple facets of cognition and for developing a task-independent AD detection model."
                ],
                "domain": [
                    "Machine Learning",
                    "Healthcare",
                    "Natural Language Processing",
                    "Causal Inference"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "58704a09-09ac-46eb-8834-1bf4fa6274ad": {
                "pk": "58704a09-09ac-46eb-8834-1bf4fa6274ad",
                "project_name": null,
                "name": "Saleh Kalantari",
                "bio": "I am a researcher dedicated to exploring the intersection of human behavior, design, and technology, particularly in the context of navigation, virtual reality (VR), and brain-computer interfaces (BCIs). My recent work has focused on understanding how individuals navigate complex environments and how technology can enhance this experience. For instance, I developed two methods for continuously measuring uncertainty during human navigation, which revealed valuable insights into wayfinding challenges in intricate buildings.\n\nI have also investigated the impact of lighting conditions on user preferences in virtual environments, contributing to the optimization of lighting design. My work with BCIs, such as the Neuron tool, empowers designers to create interactive prototypes using neurological data, bridging the gap between neuroscience and design. Additionally, I introduced MindSculpt, a BCI tool that allows users to generate geometries in real-time through thought, showcasing the potential of neurotechnology in creative processes.\n\nMy research extends to the application of augmented reality (AR) for wayfinding, particularly for older adults, where I developed NavMarkAR to enhance spatial navigation skills. I have conducted extensive studies comparing VR and real-world navigation tasks, revealing significant differences in user responses and emphasizing the need for cautious interpretation of VR findings.\n\nThrough my work, I aim to create innovative solutions that improve user experiences and foster social engagement, particularly among older adults. I am passionate about leveraging technology to enhance human interaction with the environment, ultimately contributing to better design practices and improved quality of life.",
                "collaborators": [
                    "Armin Mostafavi",
                    "Tong Bill Xu",
                    "Qi Yang",
                    "Sara Czaja",
                    "Walter Boot",
                    "Jesus G. Cruz-Garza",
                    "Zhiwen Qiu",
                    "Benjamin Kim",
                    "Angella Lee",
                    "Shuo Feng"
                ],
                "pub_titles": [
                    "Real-time Continuous Uncertainty Annotation (RCUA) for Spatial Navigation Studies",
                    "Assessing the Effects of Illuminance and Correlated Color Temperature on Emotional Responses and Lighting Preferences Using Virtual Reality",
                    "Visual Flow-based Programming Plugin for Brain Computer Interface in Computer-Aided Design",
                    "MindSculpt: Using a Brain-Computer Interface to Enable Designers to Create Diverse Geometries by Thinking",
                    "Co-Design with Myself: A Brain-Computer Interface Design Tool that Predicts Live Emotion to Enhance Metacognitive Monitoring of Designers",
                    "Use of Augmented Reality in Human Wayfinding: A Systematic Review",
                    "NavMarkAR: A Landmark-based Augmented Reality (AR) Wayfinding System for Enhancing Spatial Learning of Older Adults",
                    "EEG-based Investigation of the Impact of Classroom Design on Cognitive Performance of Students",
                    "Comparing Spatial Navigation and Human Environment Interaction in Virtual Reality vs. Identical Real Environments across the Adult Lifespan",
                    "Designing Virtual Environments for Social Engagement in Older Adults",
                    "Using Immersive Virtual Reality to Enhance Social Interaction among Older Adults: A Multi-site Study",
                    "Using a Nature-based Virtual Reality Environment for Improving Mood States and Cognitive Engagement in Older Adults: A Mixed-method Feasibility Study",
                    "Assessing the Feasibility, and Efficacy of Virtual Reality Navigational Training for Older Adults"
                ],
                "pub_abstracts": [
                    "This study introduces two methods for continuously measuring uncertainty during human navigation in complex buildings: one using a joystick (RCUA), and the other with annotations on videos of recent navigation activity (CUA). To evaluate the usability, reliability, and validity of both approaches, we conducted a study with 54 participants. We assessed the measures' reactivity during different sign-seeing events. We also evaluated the convergent validity of both measures by comparing their outcomes with a self-report questionnaire, and assessed their discriminative and predictive validity by comparing uncertain values between known groups and correlating those values with wayfinding performance. Our findings suggest that both approaches were valid at the task level, but RCUA was better at capturing fine-grained dynamics of human experience. These continuous uncertainty measures can provide valuable insights into the fleeting nature of human experience and help identify \"problem spots\" for wayfinding in complex buildings.",
                    "This paper presents a novel approach to assessing human lighting adjustment behavior and preference in diverse lighting conditions through the evaluation of emotional feedback and behavioral data using VR. Participants (n= 27) were exposed to different lighting (n=17) conditions with different levels of illuminance and correlated color temperature (CCT) with a randomized order in a virtual office environment. Results from this study significantly advanced our understanding of preferred lighting conditions in virtual reality environments, influenced by a variety of factors such as illuminance, color temperature, order of presentation, and participant demographics. Through a comprehensive analysis of user adjustment profiles, we obtained insightful data that can guide the optimization of lighting design across various settings.",
                    "Over the last half century, the main application of Brain Computer Interfaces, BCIs has been controlling wheelchairs and neural prostheses or generating text or commands for people with restricted mobility. There has been very limited attention in the field to applications for computer aided design, despite the potential of BCIs to provide a new form of environmental interaction. In this paper we introduce the development and application of Neuron, a novel BCI tool that enables designers with little experience in neuroscience or computer programming to gain access to neurological data, along with established metrics relevant to design, create BCI interaction prototypes, both with digital onscreen objects and physical devices, and evaluate designs based on neurological information and record measurements for further analysis. After discussing the BCI tool development, the article presents its capabilities through two case studies, along with a brief evaluation of the tool performance and a discussion of implications, limitations, and future improvement.",
                    "MindSculpt enables users to generate a wide range of hybrid geometries in Grasshopper in real time simply by thinking about those geometries. This design tool combines a brain-computer interface (BCI) with the parametric design platform Grasshopper, creating an intuitive design workflow that shortens the latency between ideation and implementation compared to traditional computer-aided design tools based on mouse-and-keyboard paradigms. The project arises from transdisciplinary research between neuroscience and architecture, with the goal of building a cyber-human collaborative tool that is capable of leveraging the complex and fluid nature of thinking in the design process. MindSculpt applies a supervised machine-learning approach, based on the support vector machine model (SVM), to identify patterns of brain waves that occur in EEG data when participants mentally rotate four different solid geometries. The researchers tested MindSculpt with participants who had no prior experience in design and found that the tool was enjoyable to use and could contribute to design ideation and artistic endeavors.",
                    "Intuition, metacognition, and subjective uncertainty interact in complex ways to shape the creative design process. Design intuition, a designer's innate ability to generate creative ideas and solutions based on implicit knowledge and experience, is often evaluated and refined through metacognitive monitoring. This self-awareness and management of cognitive processes can be triggered by subjective uncertainty, reflecting the designer's self-assessed confidence in their decisions. Despite their significance, few creativity support tools have targeted the enhancement of these intertwined components using biofeedback, particularly the affect associated with these processes. In this study, we introduce \"Multi-Self,\" a BCI-VR design tool designed to amplify metacognitive monitoring in architectural design. Multi-Self evaluates designers' affect (valence and arousal) to their work, providing real-time, visual biofeedback. A proof-of-concept pilot study with 24 participants assessed its feasibility. While feedback accuracy responses were mixed, most participants found the tool useful, reporting that it sparked metacognitive monitoring, encouraged exploration of the design space, and helped modulate subjective uncertainty.",
                    "Augmented reality technology has emerged as a promising solution to assist with wayfinding difficulties, bridging the gap between obtaining navigational assistance and maintaining an awareness of one's real-world surroundings. This article presents a systematic review of research literature related to AR navigation technologies. An in-depth analysis of 65 salient studies was conducted, addressing four main research topics: 1) current state-of-the-art of AR navigational assistance technologies, 2) user experiences with these technologies, 3) the effect of AR on human wayfinding performance, and 4) impacts of AR on human navigational cognition. Notably, studies demonstrate that AR can decrease cognitive load and improve cognitive map development, in contrast to traditional guidance modalities. However, findings regarding wayfinding performance and user experience were mixed. Some studies suggest little impact of AR on improving outdoor navigational performance, and certain information modalities may be distracting and ineffective. This article discusses these nuances in detail, supporting the conclusion that AR holds great potential in enhancing wayfinding by providing enriched navigational cues, interactive experiences, and improved situational awareness.",
                    "Wayfinding in complex indoor environments is often challenging for older adults due to declines in navigational and spatial-cognition abilities. This paper introduces NavMarkAR, an augmented reality navigation system designed for smart-glasses to provide landmark-based guidance, aiming to enhance older adults' spatial navigation skills. This work addresses a significant gap in design research, with limited prior studies evaluating cognitive impacts of AR navigation systems. An initial usability test involved 6 participants, leading to prototype refinements, followed by a comprehensive study with 32 participants in a university setting. Results indicate improved wayfinding efficiency and cognitive map accuracy when using NavMarkAR. Future research will explore long-term cognitive skill retention with such navigational aids.",
                    "This study investigated the neural dynamics associated with short-term exposure to different virtual classroom designs with different window placement and room dimension. Participants engaged in five brief cognitive tasks in each design condition including the Stroop Test, the Digit Span Test, the Benton Test, a Visual Memory Test, and an Arithmetic Test. Performance on the cognitive tests and Electroencephalogram (EEG) data were analyzed by contrasting various classroom design conditions. The cognitive-test-performance results showed no significant differences related to the architectural design features studied. We computed frequency band-power and connectivity EEG features to identify neural patterns associated to environmental conditions. A leave one out machine learning classification scheme was implemented to assess the robustness of the EEG features, with the classification accuracy evaluation of the trained model repeatedly performed against an unseen participant's data. The classification results located consistent differences in the EEG features across participants in the different classroom design conditions, with a predictive power that was significantly higher compared to a baseline classification learning outcome using scrambled data. These findings were most robust during the Visual Memory Test, and were not found during the Stroop Test and the Arithmetic Test. The most discriminative EEG features were observed in bilateral occipital, parietal, and frontal regions in the theta and alpha frequency bands. While the implications of these findings for student learning are yet to be determined, this study provides rigorous evidence that brain activity features during cognitive tasks are affected by the design elements of window placement and room dimensions.",
                    "Virtual reality (VR) is increasingly being used as a research platform for investigating human responses to environmental variables. While VR provides tremendous advantages in terms of variable isolation and manipulation, and ease of data-collection, some researchers have expressed concerns about the ecological validity of VR-based findings. In the current study we replicated a real-world, multi-level educational facility in VR, and compared data collected in the VR and real-world environments as participants (n=36) completed identical wayfinding tasks. We found significant differences in all of the measures used, including distance covered, number of mistakes made, time for task completion, spatial memory, extent of backtracking, observation of directional signs, perceived uncertainty levels, perceived cognitive workload, and perceived task difficulty. We also analyzed potential age-related effects to look for heightened VR/real response discrepancies among older adult participants (>55 years) compared to younger adults. This analysis yielded no significant effects of age. Finally, we examined the spatial distribution of self-reported wayfinding uncertainty across the building floorplan, finding that areas in which uncertainty was most pronounced were similar between the real-world and VR settings. Thus, participants appeared to be responding to the same environmental features in the real and VR conditions, but the extent of these responses was significantly different. Overall, the findings suggest that when VR is used to contrast varying environmental design conditions the resulting data should be interpreted cautiously and should not be generalized into real-world conclusions without further validation.",
                    "Virtual reality (VR) is increasingly used as a platform for social interaction, including as a means for older adults to maintain engagement. However, there has been limited research to examine the features of social VR that are most relevant to older adults experiences. The current study was conducted to qualitatively analyze the behavior of older adults in a collaborative VR environment and evaluate aspects of design that affected their engagement outcomes. We paired 36 participants over the age of 60, from three diverse geographic locations, and asked them to interact in collaborative VR modules. Video-based observation methods and thematic analyses were used to study the resulting interactions. The results indicated a strong link between perceived spatial presence in the VR and social engagement, while also highlighting the importance of individual personality and compatibility. The study provides new insights into design guidelines that could improve social VR programs for older adults.",
                    "Research examining older adults interactions with Virtual Reality (VR) and the impact of social VR experiences on outcomes such as social engagement has been limited, especially among older adults. This multi-site pilot study evaluated the feasibility and acceptability of a novel social virtual reality (VR) program that paired older adults from different geographic locations (New York City, Tallahassee, and Ithaca, N.Y) who engaged in virtual travel and productive engagement activities together. The sample included 36 individuals aged 60 and older, 25 percent of whom had cognitive impairment (CI). Older adults with and without CI reported high levels of engagement in the VR environment and perceived the social VR program to be enjoyable and usable. Perceived Spatial Presence was a central driver of the positive outcomes. Most also indicated a willingness to reconnect with their VR partner in the future. The data also identified important areas for improvement in the program, such as the use of more realistic and responsive avatars, controllers with larger controls, and more time for training. Overall, these findings suggest that VR social applications may foster social engagement among older adults.",
                    "Engaging with natural environments and representations of nature has been shown to improve mood states and reduce cognitive decline in older adults. The current study evaluated the use of virtual reality (VR) for presenting immersive 360 degree nature videos and a digitally designed interactive garden for this purpose. Fifty participants (age 60 plus), with varied cognitive and physical abilities, were recruited. Data were collected through pre/post-intervention surveys, standardized observations during the interventions, and post-intervention semi structured interviews. The results indicated significant improvements in attitudes toward VR and in some aspects of mood and engagement. The responses to the environment did not significantly differ among participants with different cognitive abilities; however, those with physical disabilities expressed stronger positive reactions on some metrics compared to participants without disabilities. Almost no negative impacts (cybersickness, task frustration) were found. In the interviews some participants expressed resistance to the technology, in particular the digital garden, indicating that it felt cartoonish or unappealing and that it could not substitute for real nature. However, the majority felt that the VR experiences could be a beneficial activity in situations when real-world contact with nature was not immediately feasible.",
                    "Objective. Evaluate the feasibility of Virtual Reality (VR) wayfinding training with aging adults, and examine the impact of the training on wayfinding performance. Design. Design involved wayfinding tasks in a study with three groups: active VR training, passive video training, and no training, assigned randomly. The training featured 5 tasks in a digital version of a real building. Post-training assessments had 10 tasks in this building, half familiar from training and half new. The study was double-blinded, with each intervention lasting 10 minutes. Participants. A convenience sample of 49 participants; inclusion criteria: age >58, unfamiliar with the building; exclusion criteria: mobility or vision impairments, history of motion sickness, or medical implants. Outcomes. Time spent and Distance traveled on each wayfinding task with a fixed 10-min limit. Results. Participants in VR group reported moderate usability (63.82, SD=14.55) with respect to the training intervention and high Self Location (3.71, SD=0.94). There were no differences in task performance among the groups in the similar tasks. In the new tasks, compared to the control condition, Time spent on tasks was marginally significantly reduced in the VR group; Distance traveled to finish tasks was also reduced in the VR group, and marginally significantly reduced in the Video training group. No differences were found between VR and Video conditions. No adverse effects were reported during or post intervention. Conclusions. This study provides preliminary evidence that VR training can effectively improve wayfinding performance in older adults with no reported adverse effect."
                ],
                "domain": [
                    "Virtual Reality",
                    "Human-Computer Interaction",
                    "Wayfinding",
                    "Brain-Computer Interface"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "43c05616-a573-4ce5-afb9-e2035c49d946": {
                "pk": "43c05616-a573-4ce5-afb9-e2035c49d946",
                "project_name": null,
                "name": "Wei Liu",
                "bio": "I am a researcher with a diverse background in applied mathematics, physics, and signal processing, focusing on the intersection of theoretical frameworks and practical applications. My recent work spans a variety of topics, including the study of primitive Hecke eigenforms and their associated L-functions, where I have developed asymptotic formulas that enhance our understanding of their properties.\n\nIn the realm of magnetohydrodynamics, I have conducted extensive simulations to explore magnetorotational instability (MRI) in liquid metals, revealing critical insights into angular momentum transport and flow dynamics under various conditions. My research also delves into nonlinear evolution equations, where I established existence and uniqueness results for solutions in Banach spaces, contributing to the understanding of complex partial differential equations.\n\nAdditionally, I have made significant strides in quaternion-valued signal processing, developing efficient algorithms for channel equalization and beamforming, which are crucial for advancing wireless communication systems. My work on light scattering and superscattering using radially anisotropic materials has opened new avenues for designing compact and efficient nanoantennas, with potential applications in solar energy and biosensing.\n\nThrough my interdisciplinary approach, I aim to bridge theoretical concepts with real-world applications, fostering innovation in fields ranging from photonics to autonomous systems. My ongoing research continues to explore the implications of cooperative perception in autonomous driving, enhancing decision-making processes in complex urban environments.",
                "collaborators": [],
                "pub_titles": [
                    "Weighted average values of automorphic $L$-functions",
                    "Numerical Study of the Magnetorotational Instability in Princeton MRI Experiment",
                    "Existence and Uniqueness of Solutions to Nonlinear Evolution Equations with Locally Monotone Operators",
                    "Antenna Array Signal Processing for Quaternion-Valued Wireless Communication Systems",
                    "Ultra-directional super-scattering of homogenous spherical particles with radial anisotropy",
                    "Superscattering pattern shaping for radially anisotropic nanowires",
                    "Generalized magnetic mirrors",
                    "Strong convergence rate of Euler-Maruyama method for stochastic differential equations with Hölder continuous drift coefficient driven by symmetric $α$-stable process",
                    "A minimax argument to a stronger version of the Jacobian conjecture",
                    "Situation-aware Autonomous Driving Decision Making with Cooperative Perception on Demand",
                    "Magnetized Ekman Layer and Stewartson Layer in a Magnetized Taylor-Couette Flow",
                    "Large Deviations for Stochastic Evolution Equations with Small Multiplicative Noise",
                    "Harnack Inequality and Applications for Stochastic Evolution Equations with Monotone Drifts",
                    "Noise-Sustained Convective Instability in a Magnetized Taylor-Couette Flow",
                    "Generalized Canonical Correlation Analysis and Its Application to Blind Source Separation Based on a Dual-Linear Predictor Structure",
                    "Channel Equalization and Beamforming for Quaternion-Valued Wireless Communication Systems"
                ],
                "pub_abstracts": [
                    "Let $S_2^*(q)$ be the set of primitive Hecke eigenforms of weight 2 and prime level $q$. For $p$ prime and $t\\in \\mathbb{R}$, we prove asymptotic formulas for the sums $$ \\mathcal {A}(p^j,q,t)=\\sum_{f\\in S_2^*(q)} L\\left(\\frac{1}{2}+it,f\\right)^2\\lambda_f(p^j),\\qquad j=1,2, $$ where $\\lambda_f(p^j)$ is the $p^j$-th normalized Fourier coefficient of $f$ and $L(s,f)$ is the $L$-function associated to $f$.",
                    "In preparation for an experimental study of magnetorotational instability (MRI) in liquid metal, we present non-ideal axisymmetric magnetohydrodynamic simulations of the nonlinear evolution of MRI in the experimental geometry. The simulations adopt fully insulating boundary conditions. No-slip conditions are imposed at the cylinders. A clear linear phase is observed with reduced linear growth rate. MRI results in an inflowing \"jet\" near the midplane and enhances the angular momentum transport at saturation.",
                    "In this paper we establish the existence and uniqueness of solutions for nonlinear evolution equations on Banach space with locally monotone operators, which is a generalization of the classical result by J.L. Lions for monotone operators. In particular, we show that local monotonicity implies the pseudo-monotonicity. The main result is applied to various types of PDE such as reaction-diffusion equations, generalized Burgers equation, Navier-Stokes equation, 3D Leray-$\\alpha$ model and $p$-Laplace equation with non-monotone perturbations.",
                    "Quaternion-valued wireless communication systems have been studied in the past. Although progress has been made in this promising area, a crucial missing link is lack of effective and efficient quaternion-valued signal processing algorithms for channel equalisation and beamforming. With most recent developments in quaternion-valued signal processing, in this work, we fill the gap to solve the problem and further derive the quaternion-valued Wiener solution for block-based calculation.",
                    "We study the light scattering of homogenous radially-anisotropic spherical particles. It is shown that radial anisotropy can be employed to tune effectively the electric resonances, and thus enable flexible overlapping of electric and magnetic dipoles of various numbers, which leads to unidirectional forward super-scattering at different spectral positions. We further reveal that through adjusting the radial anisotropy parameters, electric and magnetic resonances of higher orders can be also made overlapped, thus further collimating the forward scattering lobes. The ultra-directional super-scattering we have obtained with individual homogenous radially anisotropic spherical particles may shed new light to the design of compact and efficient nanoantennas, which may find various applications in solar cells, bio-sensing and many other antenna based researches.",
                    "We achieve efficient shaping of superscattering by radially anisotropic nanowires relying on resonant multipolar interferences. It is shown that the radial anisotropy of refractive index can be employed to resonantly overlap electric and magnetic multipoles of various orders, and as a result effective superscattering with different engineered angular patterns can be obtained. We further demonstrate that such superscattering shaping relying on unusual radial anisotropy parameters can be directly realised with isotropic multi-layered nanowires, which may shed new light to many fundamental researches and various applications related to scattering particles.",
                    "We propose generalized magnetic mirrors that can be achieved by excitations of sole electric resonances. Conventional approaches to obtain magnetic mirrors rely heavily on exciting the fundamental magnetic dipoles, whereas here we reveal that besides magnetic resonances, electric resonances of higher orders can be also employed to obtain highly efficient magnetic mirrors. Based on the electromagnetic duality, it is also shown that electric mirrors can be achieved by exciting magnetic resonances. We provide direct demonstrations of the generalized mirrors proposed in a simple system of one-dimensional periodic array of all-dielectric wires, which may shed new light to many advanced fields of photonics related to resonant multipolar excitations and interferences.",
                    "Euler-Maruyama method is studied to approximate stochastic differential equations driven by the symmetric $\\alpha$-stable additive noise with the $\\beta$ H\\\"older continuous drift coefficient. When $\\alpha \\in (1,2)$ and $\\beta \\in (0,\\alpha/2)$, for $p \\in (0,2]$ the $L^p$ strong convergence rate is proved to be $p\\beta/\\alpha$. The proofs in this paper are extensively based on H\\\"older's and Bihari's inequalities, which is significantly different from those in Huang and Liao (2018).",
                    "The main result of this paper is to prove the strong real Jacobian conjecture under the symmetric assumption and reveals the link between it and the Jacobian conjecture. Precisely, we assume that $F: \\mathbb{R}^n \\to \\mathbb{R}^n$ is of $C^1$ map, $n\\geqslant 2$, if for some $\\varepsilon >0$, $ 0\\notin Spec(F)~~\\mbox{and}~ Spec(F+F^T) \\subseteq (-\\infty,-\\varepsilon)~\\mbox{or} ~(\\varepsilon,+\\infty),$ where $Spec (F)$ denotes all eigenvalues of $JF$ and $Spec (F+F^T)$ denotes all eigenvalues of $JF+JF^T$, then we show that $F$ is injective. It is proved by using a minimax argument.",
                    "This paper investigates the impact of cooperative perception on autonomous driving decision making on urban roads. The extended perception range contributed by the cooperative perception can be properly leveraged to address the implicit dependencies within the vehicles, thereby the vehicle decision making performance can be improved. Meanwhile, we acknowledge the inherent limitation of wireless communication and propose a Cooperative Perception on Demand (CPoD) strategy, where the cooperative perception will only be activated when the extended perception range is necessary for proper situation-awareness. The situation-aware decision making with CPoD is modeled as a Partially Observable Markov Decision Process (POMDP) and solved in an online manner. The evaluation results demonstrate that the proposed approach can function safely and efficiently for autonomous driving on urban roads.",
                    "In this paper we present axisymmetric nonlinear simulations of magnetized Ekman and Stewartson layers in a magnetized Taylor-Couette flow with a centrifugally stable angular-momemtum profile and with a magnetic Reynolds number below the threshold of magnetorotational instability. The magnetic field is found to inhibit the Ekman suction. The width of the Ekman layer is reduced with increased magnetic field normal to the end plate. A uniformly-rotating region forms near the outer cylinder. A strong magnetic field leads to a steady Stewartson layer emanating from the junction between differentially rotating rings at the endcaps. The Stewartson layer becomes thinner with larger Reynolds number and penetrates deeper into the bulk flow with stronger magnetic field and larger Reynolds number. However, at Reynolds number larger than a critical value $\\sim 600$, axisymmetric, and perhaps also nonaxisymmetric, instabilities occur and result in a less prominent Stewartson layer that extends less far from the boundary.",
                    "The Freidlin-Wentzell large deviation principle is established for the distributions of stochastic evolution equations with general monotone drift and small multiplicative noise. As examples, the main results are applied to derive the large deviation principle for different types of SPDE such as stochastic reaction-diffusion equations, stochastic porous media equations and fast diffusion equations, and the stochastic p-Laplace equation in Hilbert space. The weak convergence approach is employed in the proof to establish the Laplace principle, which is equivalent to the large deviation principle in our framework.",
                    "In this paper, the dimension-free Harnack inequality is proved for the associated transition semigroups to a large class of stochastic evolution equations with monotone drifts. As applications, the ergodicity, hyper-(or ultra-)contractivity and compactness are established for the corresponding transition semigroups. Moreover, the exponential convergence of the transition semigroups to invariant measure and the existence of a spectral gap are also derived. The main results are applied to many concrete stochastic evolution equations such as stochastic reaction-diffusion equations, stochastic porous media equations and the stochastic p-Laplace equation in Hilbert space.",
                    "The helical magnetorotational instability of the magnetized Taylor-Couette flow is studied numerically in a finite cylinder. A distant upstream insulating boundary is shown to stabilize the convective instability entirely while reducing the growth rate of the absolute instability. The reduction is less severe with larger height. After modeling the boundary conditions properly, the wave patterns observed in the experiment turn out to be a noise-sustained convective instability. After the source of the noise resulted from unstable Ekman and Stewartson layers is switched off, a slowly-decaying inertial oscillation is observed in the simulation. We reach the conclusion that the experiments completed to date have not yet reached the regime of absolute instability.",
                    "Blind source separation (BSS) is one of the most important and established research topics in signal processing and many algorithms have been proposed based on different statistical properties of the source signals. For second-order statistics (SOS) based methods, canonical correlation analysis (CCA) has been proved to be an effective solution to the problem. In this work, the CCA approach is generalized to accommodate the case with added white noise and it is then applied to the BSS problem for noisy mixtures. In this approach, the noise component is assumed to be spatially and temporally white, but the variance information of noise is not required. An adaptive blind source extraction algorithm is derived based on this idea and a further extension is proposed by employing a dual-linear predictor structure for blind source extraction (BSE).",
                    "Quaternion-valued wireless communication systems have been studied in the past. Although progress has been made in this promising area, a crucial missing link is lack of effective and efficient quaternion-valued signal processing algorithms for channel equalization and beamforming. With most recent developments in quaternion-valued signal processing, in this work, we fill the gap to solve the problem by studying two quaternion-valued adaptive algorithms: one is the reference signal based quaternion-valued least mean square (QLMS) algorithm and the other one is the quaternion-valued constant modulus algorithm (QCMA). The quaternion-valued Wiener solution for possible block-based calculation is also derived. Simulation results are provided to show the working of the system."
                ],
                "domain": [
                    "Mathematics",
                    "Magnetohydrodynamics",
                    "Stochastic Processes",
                    "Signal Processing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively leverage large language models (LLMs) to enhance the educational experience of prostate cancer patients, addressing their specific informational and emotional support needs?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses a significant gap in patient education and support during cancer treatment. By developing an LLM-based chatbot tailored for prostate cancer patients, we can improve patient engagement, understanding, and emotional well-being. This research could lead to advancements in personalized healthcare technology, influencing future studies on AI applications in medical education and patient support. Ultimately, it has the potential to transform how patients interact with their treatment journey, leading to better health outcomes and enhanced patient autonomy.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem include the complexity of accurately understanding and responding to the diverse informational and emotional needs of cancer patients. Naive approaches may fail due to the nuanced nature of medical information and the need for empathetic communication. Technical obstacles include ensuring the LLM can provide accurate, contextually relevant information while maintaining a supportive tone. Theoretical challenges involve integrating patient feedback into the chatbot's design and ensuring it evolves with the patient's journey, which requires sophisticated co-design methodologies.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on either medical information delivery or emotional support separately, lacking a comprehensive approach that combines both. Barriers include limited collaboration between AI practitioners, medical experts, and patients, as well as insufficient understanding of patient needs in the context of cancer education. Our approach differs by emphasizing a co-design process that actively involves patients in the development of the LLM-based tool, ensuring that it is tailored to their specific needs and preferences, which has been overlooked in prior work.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology includes conducting a needs-assessment survey with prostate cancer patients to identify their educational challenges and preferences. We will utilize ChatGPT-4 as the foundation for the chatbot, focusing on its capabilities for real-time interaction and personalized responses. The evaluation metrics will include user satisfaction, engagement levels, and the effectiveness of information delivery. Expected outcomes include a validated LLM-based chatbot that significantly enhances the educational experience of prostate cancer patients, providing them with timely, relevant information and emotional support throughout their treatment journey."
    },
    "2409.20252": {
        "paper_data": {
            "title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
            "url": "http://arxiv.org/abs/2409.20252v2",
            "arxiv_id": "2409.20252",
            "authors": [
                "Morgan Fouesneau",
                "Ivelina G. Momcheva",
                "Urmila Chadayammuri",
                "Mariia Demianenko",
                "Antoine Dumont",
                "Raphael E. Hviding",
                "K. Angelique Kahle",
                "Nadiia Pulatova",
                "Bhavesh Rajpoot",
                "Marten B. Scheuck",
                "Rhys Seeburger",
                "Dmitry Semenov",
                "Jaime I. Villaseñor"
            ],
            "abstract": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly transforming multiple fields, offering powerful tools for a wide range of applications. These models, commonly trained on vast datasets, exhibit human-like text generation capabilities, making them useful for research tasks such as ideation, literature review, coding, drafting, and outreach. We conducted a study involving 13 astronomers at different career stages and research fields to explore LLM applications across diverse tasks over several months and to evaluate their performance in research-related activities. This work was accompanied by an anonymous survey assessing participants' experiences and attitudes towards LLMs. We provide a detailed analysis of the tasks attempted and the survey answers, along with specific output examples. Our findings highlight both the potential and limitations of LLMs in supporting research while also addressing general and research-specific ethical considerations. We conclude with a series of recommendations, emphasizing the need for researchers to complement LLMs with critical thinking and domain expertise, ensuring these tools serve as aids rather than substitutes for rigorous scientific inquiry.",
            "introduction": "   1 Introduction  Language models are probabilistic representations of the natural human language. The first such models appeared in the 1980s and were purely statistical, based on the frequencies of co-occurrence of phrases of different lengths, so-called n-grams. The power of language models has grown dramatically in the last 5 years with the development of transformers (Vaswani et al., 2017). A transformer is an advanced machine learning model that improves how data sequences are analyzed and generated by simultaneously considering the full context of the input and focusing on the most relevant parts, enhancing performance in language-related tasks. This new architecture, combined with neural networks and large datasets (frequently scraped from the internet), has led to the development of the current (as of mid-2024) generation of Large Language Models (LLMs) as advanced artificial intelligence (AI) systems capable of parsing and generating human-like text.   Current LLMs represent a tremendous technological leap. Unlike other leaps of technology, which were expensive and very few people had access to them at first, LLMs are (in many cases) free and available to (almost) anyone with an internet connection. As a result, their adoption has also been incredibly fast - ChatGPT reached 100 million users within only two months after its launch (Hu, 2023) while other services of similar popularity took between 9 months and 4.5 years to reach the same userbase111https://www.visualcapitalist.com/threads-100-million-users/. Unlike other technologies primarily automating repetitive work, LLMs excel at creative work like writing, coding, and generating ideas. Finally, the literature surrounding productivity boosts attributed to large language models (LLMs) suggests that there are significant improvements in various tasks, with estimates ranging from 20% to 80% in productivity boosts across different sectors (Peng et al., 2023; Noy & Zhang, 2023; Eloundou et al., 2023) compared to the 18-22% improvement brought on by steam power (Atack et al., 2008).   Also, unlike other technological advances, LLMs behave very differently from traditional technology. As discussed in (Mollick, 2024, p. 65-67), LLMs defy our expectations for software functionality: software should produce predictable, reliable, logical outcomes, and LLMs do not. On the contrary, LLMs (at the moment, at least) struggle with tasks that software is generally good at, such as repeating tasks, reproducing facts, and performing calculations. In contrast, they perform well on tasks we consider uniquely human: writing, chatting, analyzing, coding, brainstorming, and creating. In addition, LLMs have \"features\" that we do not encounter in other technologies: they confidently invent facts and make mistakes, aka hallucinate. These latter features make many researchers suspicious of using them in a research context.   LLMs are expected to have a large impact on a wide range of professions, especially those in highly paid and creative fields, including STEM. A 2023 McKinsey report (Ellingrud et al., 2023) predicts that 16% of the hours worked by STEM professionals today will be automated by generative AI by 2030. Undoubtedly, a growing number of astronomers have tried LLMs, and many frequently utilize them for a wide range of tasks, sometimes secretly and sometimes openly. Several universities have communicated recommendations to staff and students on the appropriate use of LLMs in teaching. Still, only a handful of professional astronomical entities",
            "references": [
                {
                    "title": "pathfinder: A Semantic Framework for Literature Review and Knowledge Discovery in Astronomy",
                    "abstract": "The exponential growth of astronomical literature poses significant challenges for researchers navigating and synthesizing general insights or even domain-specific knowledge. We present Pathfinder, a machine learning framework designed to enable literature review and knowledge discovery in astronomy, focusing on semantic searching with natural language instead of syntactic searches with keywords. Utilizing state-of-the-art large language models (LLMs) and a corpus of 350,000 peer-reviewed papers from the Astrophysics Data System (ADS), Pathfinder offers an innovative approach to scientific inquiry and literature exploration. Our framework couples advanced retrieval techniques with LLM-based synthesis to search astronomical literature by semantic context as a complement to currently existing methods that use keywords or citation graphs. It addresses complexities of jargon, named entities, and temporal aspects through time-based and citation-based weighting schemes. We demonstrate the tool's versatility through case studies, showcasing its application in various research scenarios. The system's performance is evaluated using custom benchmarks, including single-paper and multi-paper tasks. Beyond literature review, Pathfinder offers unique capabilities for reformatting answers in ways that are accessible to various audiences (e.g. in a different language or as simplified text), visualizing research landscapes, and tracking the impact of observatories and methodologies. This tool represents a significant advancement in applying AI to astronomical research, aiding researchers at all career stages in navigating modern astronomy literature."
                },
                {
                    "title": "Scientific text analysis with robots applied to observatory proposals",
                    "abstract": "To test the potential disruptive effect of Artificial Intelligence (AI) transformers (e.g., ChatGPT) and their associated Large Language Models on the time allocation process, both in proposal reviewing and grading, an experiment has been set-up at ESO for the P112 Call for Proposals. The experiment aims at raising awareness in the ESO community and build valuable knowledge by identifying what future steps ESO and other observatories might need to take to stay up to date with current technologies. We present here the results of the experiment, which may further be used to inform decision-makers regarding the use of AI in the proposal review process. We find that the ChatGPT-adjusted proposals tend to receive lower grades compared to the original proposals. Moreover, ChatGPT 3.5 can generally not be trusted in providing correct scientific references, while the most recent version makes a better, but far from perfect, job. We also studied how ChatGPT deals with assessing proposals. It does an apparent remarkable job at providing a summary of ESO proposals, although it doesn't do so good to identify weaknesses. When looking at how it evaluates proposals, however, it appears that ChatGPT systematically gives a higher mark than humans, and tends to prefer proposals written by itself."
                },
                {
                    "title": "Delving into the Utilisation of ChatGPT in Scientific Publications in Astronomy",
                    "abstract": "Rapid progress in the capabilities of machine learning approaches in natural language processing has culminated in the rise of large language models over the last two years. Recent works have shown unprecedented adoption of these for academic writing, especially in some fields, but their pervasiveness in astronomy has not been studied sufficiently. To remedy this, we extract words that ChatGPT uses more often than humans when generating academic text and search a total of 1 million articles for them. This way, we assess the frequency of word occurrence in published works in astronomy tracked by the NASA Astrophysics Data System since 2000. We then perform a statistical analysis of the occurrences. We identify a list of words favoured by ChatGPT and find a statistically significant increase for these words against a control group in 2024, which matches the trend in other disciplines. These results suggest a widespread adoption of these models in the writing of astronomy papers. We encourage organisations, publishers, and researchers to work together to identify ethical and pragmatic guidelines to maximise the benefits of these systems while maintaining scientific rigour."
                },
                {
                    "title": "Is ChatGPT Transforming Academics' Writing Style?",
                    "abstract": "Based on one million arXiv papers submitted from May 2018 to January 2024, we assess the textual density of ChatGPT's writing style in their abstracts by means of a statistical analysis of word frequency changes. Our model is calibrated and validated on a mixture of real abstracts and ChatGPT-modified abstracts (simulated data) after a careful noise analysis. We find that ChatGPT is having an increasing impact on arXiv abstracts, especially in the field of computer science, where the fraction of ChatGPT-revised abstracts is estimated to be approximately 35%, if we take the output of one of the simplest prompts,\"revise the following sentences\", as a baseline. We conclude with an analysis of both positive and negative aspects of the penetration of ChatGPT into academics' writing style."
                },
                {
                    "title": "Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases",
                    "abstract": "Prompt engineering is a challenging and important task due to the high sensitivity of Large Language Models (LLMs) to the given prompt and the inherent ambiguity of a textual task instruction. Automatic prompt engineering is essential to achieve optimized performance from LLMs. Recent studies have demonstrated the capabilities of LLMs to automatically conduct prompt engineering by employing a meta-prompt that incorporates the outcomes of the last trials and proposes an improved prompt. However, this requires a high-quality benchmark to compare different prompts, which is difficult and expensive to acquire in many real-world use cases. In this work, we introduce a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. During the optimization process, the system jointly generates synthetic data of boundary use cases and optimizes the prompt according to the generated dataset. We demonstrate the effectiveness of our method with respect to strong proprietary models on real-world tasks such as moderation and generation. Our method outperforms state-of-the-art methods with a limited number of annotated samples. Furthermore, we validate the advantages of each one of the system's key components. Our system is built in a modular way, facilitating easy adaptation to other tasks. The code is available $\\href{https://github.com/Eladlev/AutoPrompt}{here}$."
                },
                {
                    "title": "Does Writing with Language Models Reduce Content Diversity?",
                    "abstract": "Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content."
                },
                {
                    "title": "Accuracy of Chatbots in Citing Journal Articles",
                    "abstract": "This cross-sectional study quantifies the journal article citation error rate of an artificial intelligence chatbot."
                },
                {
                    "title": "Friend or Foe? Exploring the Implications of Large Language Models on the Science System",
                    "abstract": "The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 researchers specializing in AI and digitization. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and helps identify areas for future action."
                },
                {
                    "title": "Gender Representation Among Contributors to Open-Source Infrastructure : An Analysis of 20 Package Manager Ecosystems",
                    "abstract": "While the severe underrepresentation of women and non-binary people in open source is widely recognized, there is little empirical data on how the situation has changed over time and which subcommunities have been more effectively reducing the gender imbalance. To obtain a clearer image of gender representation in open source, we compiled and synthesized existing empirical data from the literature, and computed historical trends in the representation of women across 20 open source ecosystems. While inherently limited by the ability of automatic name-based gender inference to capture true gender identities at an individual level, our census still provides valuable population-level insights. Across all and in most ecosystems, we observed a promising upward trend in the percentage of women among code contributors over time, but also high variation in the percentage of women contributors across ecosystems. We also found that, in most ecosystems, women withdraw earlier from open-source participation than men."
                },
                {
                    "title": "Generative AI entails a credit–blame asymmetry",
                    "abstract": null
                },
                {
                    "title": "Galactic ChitChat: Using Large Language Models to Converse with Astronomy Literature",
                    "abstract": "We demonstrate the potential of the state-of-the-art OpenAI GPT-4 large language model to engage in meaningful interactions with Astronomy papers using in-context prompting. To optimize for efficiency, we employ a distillation technique that effectively reduces the size of the original input paper by 50%, while maintaining the paragraph structure and overall semantic integrity. We then explore the model’s responses using a multi-document context (ten distilled documents). Our findings indicate that GPT-4 excels in the multi-document domain, providing detailed answers contextualized within the framework of related research findings. Our results showcase the potential of large language models for the astronomical community, offering a promising avenue for further exploration, particularly the possibility of utilizing the models for hypothesis generation."
                },
                {
                    "title": "GPT detectors are biased against non-native English writers",
                    "abstract": null
                },
                {
                    "title": "Challenging the appearance of machine intelligence: Cognitive bias in LLMs",
                    "abstract": "Assessments of algorithmic bias in large language models (LLMs) are generally catered to uncovering systemic discrimination based on protected characteristics such as sex and ethnicity. However, there are over 180 documented cognitive biases that pervade human reasoning and decision making that are routinely ignored when discussing the ethical complexities of AI. We demonstrate the presence of these cognitive biases in LLMs and discuss the implications of using biased reasoning under the guise of expertise. We call for stronger education, risk management, and continued research as widespread adoption of this technology increases. Finally, we close with a set of best practices for when and how to employ this technology as widespread adoption continues to grow."
                },
                {
                    "title": "Editorial: On the Use of Chatbots in Writing Scientific Manuscripts",
                    "abstract": null
                },
                {
                    "title": "Academic integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyond",
                    "abstract": "This paper explores the academic integrity considerations of students’ use of Artificial Intelligence (AI) tools using Large Language Models (LLMs) such as ChatGPT in formal assessments. We examine the evolution of these tools, and highlight the potential ways that LLMs can support in the education of students in digital writing and beyond, including the teaching of writing and composition, the possibilities of co-creation between humans and AI, supporting EFL learners, and improving Automated Writing Evaluations (AWE). We describe and demonstrate the potential that these tools have in creating original, coherent text that can avoid detection by existing technological methods of detection and trained academic staff alike, demonstrating a major academic integrity concern related to the use of these tools by students. Analysing the various issues related to academic integrity that LLMs raise for both Higher Education Institutions (HEIs) and students, we conclude that it is not the student use of any AI tools that defines whether plagiarism or a breach of academic integrity has occurred, but whether any use is made clear by the student. Deciding whether any particular use of LLMs by students can be defined as academic misconduct is determined by the academic integrity policies of any given HEI, which must be updated to consider how these tools will be used in future educational environments."
                },
                {
                    "title": "The Impact of AI on Developer Productivity: Evidence from GitHub Copilot",
                    "abstract": "Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers."
                },
                {
                    "title": "Talking about Large Language Models",
                    "abstract": "Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us."
                },
                {
                    "title": "How to cheat on your final paper: Assigning AI for student writing",
                    "abstract": null
                },
                {
                    "title": "A study of gender in user reviews on the Google Play Store",
                    "abstract": null
                },
                {
                    "title": "Speeding up to keep up: exploring the use of AI in the research process",
                    "abstract": null
                },
                {
                    "title": "Using Artificial Intelligence to Support Science Prioritization by the Decadal Surveys",
                    "abstract": null
                },
                {
                    "title": "When eliminating bias isn’t fair: Algorithmic reductionism and procedural justice in human resource decisions",
                    "abstract": null
                },
                {
                    "title": "Binary stars: a cheat sheet",
                    "abstract": "I present a brief summary of three different types of binary star - astrometric, spectroscopic and eclipsing - and tabulate the properties of these systems that can be determined directly from observations. Eclipsing binary stars are the most valuable of these, as they are our main source of direct mass and radius measurements for normal stars. In good cases, masses and radii can be obtained to better than 1% precision and accuracy using only photometry, spectroscopy and geometry. These measurements constitute vital empirical data against which theoretical models of stars can be verified and improved. I give examples of the use of these systems for constraining stellar theory and the distance scale, and conclude with a presentation of preliminary results for the solar-type eclipsing binary 1SWASP J034114.25+201253.5."
                },
                {
                    "title": "Astro2020 APC White Paper: Astronomy should be in the clouds",
                    "abstract": "Commodity cloud computing, as provided by commercial vendors such as Amazon, Google, and Microsoft, has revolutionized computing in many sectors. With the advent of a new class of big data, public access astronomical facility such as LSST, DKIST, and WFIRST, there exists a real opportunity to combine these missions with cloud computing platforms and fundamentally change the way astronomical data is collected, processed, archived, and curated. Making these changes in a cross-mission, coordinated way can provide unprecedented economies of scale in personnel, data collection and management, archiving, algorithm and software development and, most importantly, science."
                },
                {
                    "title": "Energy and Policy Considerations for Deep Learning in NLP",
                    "abstract": "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice."
                },
                {
                    "title": "Gender differences in participation and reward on Stack Overflow",
                    "abstract": null
                },
                {
                    "title": "Weapons of math destruction",
                    "abstract": null
                },
                {
                    "title": "Attention is All you Need",
                    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                {
                    "title": "Women through the glass ceiling: gender asymmetries in Wikipedia",
                    "abstract": null
                },
                {
                    "title": "Software Use in Astronomy: an Informal Survey",
                    "abstract": "We report on an informal survey about the use of software in the worldwide astronomical community. The survey was carried out between December 2014 and February 2015, collecting responses from 1142 astronomers, spanning all career levels. We find that all participants use software in their research. The vast majority of participants, 90%, write at least some of their own software. Even though writing software is so wide-spread among the survey participants, only 8% of them report that they have received substantial training in software development. Another 49% of the participants have received \"little\" training. The remaining 43% have received no training. We also find that astronomers' software stack is fairly narrow. The 10 most popular tools among astronomers are (from most to least popular): Python, shell scripting, IDL, C/C++, Fortran, IRAF, spreadsheets, HTML/CSS, SQL and Supermongo. Across all participants the most common programing language is Python ($67\\pm 2\\%$), followed by IDL ($44\\pm 2\\%$), C/C++ ($37\\pm 2\\%$) and Fortran ($28\\pm 2\\%$). IRAF is used frequently by $24\\pm 1\\%$ of participants. We show that all trends are largely independent of career stage, area of research and geographic location."
                },
                {
                    "title": "Gender and the GeoWeb: divisions in the production of user-generated cartographic information",
                    "abstract": null
                },
                {
                    "title": "Learning to improve: using writing to increase critical thinking performance in general education biology.",
                    "abstract": "Increasingly, national stakeholders express concern that U.S. college graduates cannot adequately solve problems and think critically. As a set of cognitive abilities, critical thinking skills provide students with tangible academic, personal, and professional benefits that may ultimately address these concerns. As an instructional method, writing has long been perceived as a way to improve critical thinking. In the current study, the researchers compared critical thinking performance of students who experienced a laboratory writing treatment with those who experienced traditional quiz-based laboratory in a general education biology course. The effects of writing were determined within the context of multiple covariables. Results indicated that the writing group significantly improved critical thinking skills whereas the non-writing group did not. Specifically, analysis and inference skills increased significantly in the writing group but not the non-writing group. Writing students also showed greater gains in evaluation skills; however, these were not significant. In addition to writing, prior critical thinking skill and instructor significantly affected critical thinking performance, whereas other covariables such as gender, ethnicity, and age were not significant. With improved critical thinking skill, general education biology students will be better prepared to solve problems as engaged and productive citizens."
                },
                {
                    "title": "Steam Power, Establishment Size, and Labor Productivity Growth in Nineteenth Century American Manufacturing",
                    "abstract": null
                },
                {
                    "title": "Undiscovered Public Knowledge",
                    "abstract": "Knowledge can be public, yet undiscovered, if independently created fragments are logically related but never retrieved, brought together, and interpreted. Information retrieval, although essential for assembling such fragments, is always problematic. The search process, like a scientific theory, can be criticized and improved, but can never be verified as capable of retrieving all information relevant to a problem or theory. This essential incompleteness of search and retrieval therefore makes possible, and plausible, the existence of undiscovered public knowledge. Three examples intended to throw light on the logic of undiscovered knowledge are constructed and analyzed. The argument is developed within the framework of a Popperian or critical approach within science and on Popper's distinction between subjective and objective knowledge--the distinction between World 2 and World 3."
                },
                {
                    "title": "Large Language Model in Creative Work: The Role of Collaboration Modality and User Expertise",
                    "abstract": null
                },
                {
                    "title": ". Navi-gating the jagged technological frontier: Field experimental evidence of the effects of ai on knowledge worker productivity and quality",
                    "abstract": null
                },
                {
                    "title": "Legal issues in generative ai under japanese law - copyright",
                    "abstract": null
                },
                {
                    "title": "The pragmatic programmer",
                    "abstract": null
                },
                {
                    "title": "Making an image with generative ai uses as much energy as charging your phone",
                    "abstract": null
                },
                {
                    "title": "The impact of artificial intelligence on growth and employment",
                    "abstract": null
                },
                {
                    "title": "The Role of LLMs",
                    "abstract": null
                },
                {
                    "title": "2023. Opinion paper: “so what if chatgpt wrote it?”",
                    "abstract": null
                },
                {
                    "title": "2022. Machine learning in astronomy: Cautionary tales for the community",
                    "abstract": null
                },
                {
                    "title": "2023. Experimentalevidenceontheproductivityeffects of generative artificial intelligence",
                    "abstract": null
                },
                {
                    "title": "2024. Generativeartificialintelligenceenhances creativity but reduces the diversity of novel content",
                    "abstract": null
                },
                {
                    "title": "2024. Co-Intelligence: Living and Working with AI , Portfolio, New York, 1st",
                    "abstract": null
                },
                {
                    "title": "Chatgpt sets record for fastest-growing user base",
                    "abstract": null
                },
                {
                    "title": "2023. GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models",
                    "abstract": null
                },
                {
                    "title": "for generating coherent, context-aware text across a variety of applications. We tested both the 3",
                    "abstract": null
                },
                {
                    "title": "2022. Aligning artificial intelligence with climate change mitigation",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "4e0905c0-949b-428a-b240-e85f1239c57a": {
                "pk": "4e0905c0-949b-428a-b240-e85f1239c57a",
                "project_name": null,
                "name": "Morgan Fouesneau",
                "bio": "I am an astrophysicist with a keen interest in the study of star clusters and their role in understanding stellar evolution and galaxy formation. My research primarily focuses on the stochastic nature of star clusters, where I employ Bayesian methods to analyze their age and mass distributions using multi-wavelength photometric observations. I have developed innovative techniques to account for the inherent variability in cluster properties due to the random presence of bright stars, which has led to more accurate assessments of cluster characteristics.\n\nMy work extends to the analysis of the Milky Way's thick disk, where I investigate the radial age gradient of stars, revealing complex formation histories. I also contribute to the field of stellar classification, having created a supervised classifier based on Gaussian Mixture Models to categorize objects in the Gaia data release 2. This work has implications for understanding the distribution of quasars and galaxies in our universe.\n\nAdditionally, I am passionate about the intersection of astronomy and societal issues, as evidenced by my studies on gender balance in scientific conferences and the environmental impact of astronomical institutions. I strive to ensure that my research not only advances our understanding of the cosmos but also promotes inclusivity and sustainability within the scientific community. My goal is to leverage the wealth of data from missions like Gaia to refine our understanding of stellar populations and their evolution, ultimately contributing to a deeper comprehension of the universe we inhabit.",
                "collaborators": [
                    "Hans-Walter Rix",
                    "Ariane Lançon",
                    "Jan Rybizki",
                    "Rene Andrae",
                    "Coryn A. L. Bailer-Jones",
                    "Rupali Chandar",
                    "Bradley C. Whitmore",
                    "Markus Demleitner",
                    "Coryn Bailer-Jones",
                    "Marie Martig"
                ],
                "pub_titles": [
                    "Accounting for Stochastic Fluctuations when Analysing Integrated Light of Star Clusters. I: First Systematics",
                    "Constraining the evolution of red supergiants with the integrated light of star clusters",
                    "Analysing star cluster populations with stochastic models: the HST/WFC3 sample of clusters in M83",
                    "A Gaia DR2 Mock Stellar Catalog",
                    "A radial age gradient in the geometrically thick disk of the Milky Way",
                    "Autonomous Disentangling for Spectroscopic Surveys",
                    "Quasar and galaxy classification in Gaia Data Release 2",
                    "Estimating $\\left[ α/ \\text{Fe} \\right]$ from Gaia low-resolution BP/RP spectra using the ExtraTrees algorithm",
                    "An astronomical institute's perspective on meeting the challenges of the climate crisis",
                    "Studying Gender in Conference Talks -- data from the 223rd meeting of the American Astronomical Society",
                    "A Probabilistic Approach to Fitting Period-Luminosity Relations and Validating Gaia Parallaxes"
                ],
                "pub_abstracts": [
                    "Star clusters are studied widely both as benchmarks for stellar evolution models and in their own right. Cluster age and mass distributions within galaxies are probes of star formation histories, and of cluster formation and disruption processes. The vast majority of clusters in the Universe is small, and it is well known that the integrated fluxes and colors have broad probability distributions, due to small numbers of bright stars. This paper goes beyond the description of predicted probability distributions, and presents results of the analysis of cluster energy distributions in an explicitly stochastic context. The method developed is Bayesian. It provides posterior probability distributions in the age-mass-extinction space, using multi-wavelength photometric observations and a large collection of Monte-Carlo simulations of clusters of finite stellar masses. Both UBVI and UBVIK datasets are considered, and the study conducted in this paper is restricted to the solar metallicity. We first reassess and explain errors arising from the use of standard analysis methods, which are based on continuous population synthesis models: systematic errors on ages and random errors on masses are large, while systematic errors on masses tend to be smaller. The age-mass distributions obtained after analysis of a synthetic sample are very similar to those found for real galaxies in the literature. The Bayesian approach on the other hand, is very successful in recovering the input ages and masses. Taking stochastic effects into account is important, more important for instance than the choice of adding or removing near-IR data in many cases. We found no immediately obvious reason to reject priors inspired by previous (standard) analyses of cluster populations in galaxies, i.e. cluster distributions that scale with mass as M^-2 and are uniform on a logarithmic age scale.",
                    "The integrated properties of young star clusters are subject to large cluster-to-cluster variations because they depend directly on small numbers of bright stars. At ages at which red supergiants are expected to exist, these luminous but rare stars can be blamed for most of the variations. If unresolved clusters are to be used to constrain red supergiant models, methods must be developed that take these stochastic fluctuations into account. We discuss prospects and open issues in this field, based on recent work on high mass clusters in M82, and on first experiments towards a Bayesian study of cluster populations.",
                    "The majority of clusters in the Universe have masses well below 10^5 Msun. Hence their integrated fluxes and colors can be affected by the random presence of a few bright stars introduced by stochastic sampling of the stellar mass function. Specific methods are being developed to extend the analysis of cluster SEDs into the low-mass regime. In this paper, we apply such a method to observations of star clusters, in the nearby spiral galaxy M83. We reassess ages and masses of a sample of 1242 objects for which UBVIHalpha fluxes were obtained with the HST/WFC3 images. Synthetic clusters with known properties are used to characterize the limitations of the method. The ensemble of color predictions of the discrete cluster models are in good agreement with the distribution of observed colors. We emphasize the important role of the Halpha data in the assessment of the fraction of young objects, particularly in breaking the age-extinction degeneracy that hampers an analysis based on UBVI only. We find the mass distribution of the cluster sample to follow a power-law of index -2.1 +/-0.2, and the distribution of ages a power-law of index -1.0 +/-0.2 for M > 10^3.5 Msun and ages between 10^7 and 10^9 yr. An extension of our main method, that makes full use of the probability distributions of age and mass of the individual clusters, is explored. It produces similar power-law slopes and will deserve further investigation. Although the properties derived for individual clusters significantly differ from those obtained with traditional, non-stochastic models in ~30% of the objects, the first order aspect of the age and mass distributions are similar to those obtained previously for this M83 sample in the range of overlap of the studies. We extend the power-law description to lower masses with better mass and age resolution and without most of the artifacts produced by the classical method.",
                    "We present a mock catalog of Milky Way stars, matching in volume and depth the content of the Gaia data release 2 (GDR2). We generated our catalog using Galaxia, a tool to sample stars from a Besancon Galactic model, together with a realistic 3D dust extinction map. The catalog mimicks the complete GDR2 data model and contains most of the entries in the Gaia source catalog: 5-parameter astrometry, 3-band photometry, radial velocities, stellar parameters, and associated scaled nominal uncertainty estimates. In addition, we supplemented the catalog with extinctions and photometry for non-Gaia bands. This catalog can be used to prepare GDR2 queries in a realistic runtime environment, and it can serve as a Galactic model against which to compare the actual GDR2 data in the space of observables. The catalog is hosted through the virtual observatory GAVO's Heidelberg data center service and thus can be queried using ADQL as for GDR2 data.",
                    "In the Milky Way, the thick disk can be defined using individual stellar abundances, kinematics, or age; or geometrically, as stars high above the mid-plane. In nearby galaxies, where only a geometric definition can be used, thick disks appear to have large radial scale-lengths, and their red colors suggest that they are uniformly old. The Milky Way's geometrically thick disk is also radially extended, but it is far from chemically uniform: alpha-enhanced stars are confined within the inner Galaxy. In simulated galaxies, where old stars are centrally concentrated, geometrically thick disks are radially extended, too. Younger stellar populations flare in the simulated disks' outer regions, bringing those stars high above the mid-plane. The resulting geometrically thick disks therefore show a radial age gradient, from old in their central regions to younger in their outskirts. Based on our age estimates for a large sample of giant stars in the APOGEE survey, we can now test this scenario for the Milky Way. We find that the geometrically-defined thick disk in the Milky Way has indeed a strong radial age gradient: the median age for red clump stars goes from ~9 Gyr in the inner disk to 5 Gyr in the outer disk. We propose that at least some nearby galaxies could also have thick disks that are not uniformly old, and that geometrically thick disks might be complex structures resulting from different formation mechanisms in their inner and outer parts.",
                    "A suite of spectroscopic surveys is producing vast sets of stellar spectra with the goal of advancing stellar physics and Galactic evolution by determining their basic physical properties. A substantial fraction of these stars are in binary systems, but almost all large-survey modeling pipelines treat them as single stars. For sets of multi-epoch spectra, spectral disentangling is a powerful technique to recover or constrain the individual components' spectra of a multiple system. So far, this approach has focused on small samples or individual objects, usually with high resolution ($R \\gtrsim 10.000$) spectra and many epochs ($\\gtrsim 8$). Here, we present a disentangling implementation that accounts for several aspects of few-epoch spectra from large surveys: that vast sample sizes require automatic determination of starting guesses; that some of the most extensive spectroscopic surveys have a resolution of only $\\approx 2,000$; that few epochs preclude unique orbit fitting; that one needs effective regularisation of the disentangled solution to ensure resulting spectra are smooth. We describe the implementation of this code and show with simulated spectra how well spectral recovery can work for hot and cool stars at $R \\approx 2000$. Moreover, we verify the code on two established binary systems, the ``Unicorn'' and ``Giraffe''. This code can serve to explore new regimes in survey disentangling in search of massive stars with massive dark companions, e.g. the $\\gtrsim 200,000$ hot stars of the SDSS-V survey.",
                    "We construct a supervised classifier based on Gaussian Mixture Models to probabilistically classify objects in Gaia data release 2 (GDR2) using only photometric and astrometric data in that release. The model is trained empirically to classify objects into three classes -- star, quasar, galaxy -- for G<=14.5 mag down to the Gaia magnitude limit of G=21.0 mag. Galaxies and quasars are identified for the training set by a cross-match to objects with spectroscopic classifications from the Sloan Digital Sky Survey. Stars are defined directly from GDR2. When allowing for the expectation that quasars are 500 times rarer than stars, and galaxies 7500 times rarer than stars (the class imbalance problem), samples classified with a threshold probability of 0.5 are predicted to have purities of 0.43 for quasars and 0.28 for galaxies, and completenesses of 0.58 and 0.72 respectively. The purities can be increased up to 0.60 by adopting a higher threshold. Not accounting for this expected low frequency of extragalactic objects (the class prior) would give both erroneously optimistic performance predictions and severely impure samples. Applying our model to all 1.20 billion objects in GDR2 with the required features, we classify 2.3 million objects as quasars and 0.37 million objects as galaxies (with individual probabilities above 0.5). The small number of galaxies is due to the strong bias of the satellite detection algorithm and on-ground data selection against extended objects. We infer the true number of quasars and galaxies -- as these classes are defined by our training set -- to be 690,000 and 110,000 respectively (+/- 50%). The aim of this work is to see how well extragalactic objects can be classified using only GDR2 data. Better classifications should be possible with the low resolution spectroscopy (BP/RP) planned for GDR3.",
                    "Gaia Data Release 3 will contain more than a billion sources with positions, parallaxes, and proper motions. In addition, for hundreds of millions of stars, it will include low-resolution blue photometer (BP) and red photometer (RP) spectra. Obtained by dispersing light with prisms, these spectra have resolutions that are too low to allow us to measure individual spectral lines and bands. However, the combined BP/RP spectra can be used to estimate some stellar properties such as $T_{\\text{eff}}$, $\\log{g}$, and $\\left[ \\text{M} / \\text{H} \\right]$.   We investigate the feasibility of using the ExtraTrees algorithm to estimate the alpha element to iron abundance ratio $\\left[ \\alpha / \\text{H} \\right]$ from low-resolution BP/RP spectra.   To infer $\\left[ \\alpha / \\text{H} \\right]$ from the spectra, we created regression models trained on two samples: a set of synthetic spectra and a set of observed spectra from stars that have known $\\left[ \\alpha / \\text{H} \\right]$ since they have been observed using HERMES. We applied each model to the other sample and to a larger observed sample to assess the performance of the models. In addition, we used our models to analyse stars from the Gaia-Enceladus structure.   We find that models using the ExtraTrees algorithm can be used to estimate $\\left[ \\alpha / \\text{H} \\right]$ from low-resolution BP/RP spectra of cool stars. However, they do this by exploiting correlations between $\\left[ \\alpha / \\text{H} \\right]$ and other parameters, rather than the causal effect of $\\left[ \\alpha / \\text{H} \\right]$ on the spectrum. Hence, they are unlikely to be useful in studies that attempt to distinguish stars that only differ in $\\left[ \\alpha / \\text{H} \\right]$.",
                    "Analysing greenhouse gas emissions of an astronomical institute is a first step in reducing its environmental impact. Here, we break down the emissions of the Max Planck Institute for Astronomy in Heidelberg and propose measures for reductions.",
                    "We present a study on the gender balance, in speakers and attendees, at the recent major astronomical conference, the American Astronomical Society meeting 223, in Washington, DC. We conducted an informal survey, yielding over 300 responses by volunteers at the meeting. Each response included gender data about a single talk given at the meeting, recording the gender of the speaker and all question-askers. In total, 225 individual AAS talks were sampled. We analyze basic statistical properties of this sample. We find that the gender ratio of the speakers closely matched the gender ratio of the conference attendees. The audience asked an average of 2.8 questions per talk. Talks given by women had a slightly higher number of questions asked (3.2$\\pm$0.2) than talks given by men (2.6$\\pm$0.1). The most significant result from this study is that while the gender ratio of speakers very closely mirrors that of conference attendees, women are under-represented in the question-asker category. We interpret this to be an age-effect, as senior scientists may be more likely to ask questions, and are more commonly men. A strong dependence on the gender of session chairs is found, whereby women ask disproportionately fewer questions in sessions chaired by men. While our results point to laudable progress in gender-balanced speaker selection, we believe future surveys of this kind would help ensure that collaboration at such meetings is as inclusive as possible.",
                    "Pulsating stars, such as Cepheids, Miras, and RR Lyrae stars, are important distance indicators and calibrators of the \"cosmic distance ladder\", and yet their period-luminosity-metallicity (PLZ) relations are still constrained using simple statistical methods that cannot take full advantage of available data. To enable optimal usage of data provided by the Gaia mission, we present a probabilistic approach that simultaneously constrains parameters of PLZ relations and uncertainties in Gaia parallax measurements. We demonstrate this approach by constraining PLZ relations of type $ab$ RR Lyrae stars in near-infrared W1 and W2 bands, using Tycho-Gaia Astrometric Solution (TGAS) parallax measurements for a sample of $\\approx100$ type $ab$ RR Lyrae stars located within 2.5 kpc of the Sun. The fitted PLZ relations are consistent with previous studies, and in combination with other data, deliver distances precise to 6% (once various sources of uncertainty are taken into account). To a precision of 0.05 mas ($1\\sigma$), we do not find a statistically significant offset in TGAS parallaxes for this sample of distant RR Lyrae stars (median parallax of 0.8 mas and distance of 1.4 kpc). With only minor modifications, our probabilistic approach can be used to constrain PLZ relations of other pulsating stars, and we intend to apply it to Cepheid and Mira stars in the near future."
                ],
                "domain": [
                    "Stellar Evolution",
                    "Bayesian Analysis",
                    "Galactic Structure",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b98eb2b9-1ea9-4309-a371-c32bd4be1e4b": {
                "pk": "b98eb2b9-1ea9-4309-a371-c32bd4be1e4b",
                "project_name": null,
                "name": "Ivelina G. Momcheva",
                "bio": "I am an astrophysicist specializing in the analysis of galaxy formation and evolution, particularly through the lens of near-infrared observations from space-based telescopes like the James Webb Space Telescope (JWST) and the Hubble Space Telescope (HST). My recent work has focused on improving data quality for JWST's Near Infrared Imager and Slitless Spectrograph (NIRISS) by developing empirical backgrounds that mitigate spatially-dependent artifacts, enhancing the accuracy of astronomical analyses.\n\nI have also pioneered innovative observational techniques, such as the \"drift and shift\" (DASH) method, which allows for efficient wide-field near-infrared surveys with HST, significantly expanding the area covered by high-resolution imaging. My research has delved into the presence of non-stellar near-infrared emissions in galaxies, linking these emissions to increased star formation rates and providing insights into the star formation rate-stellar mass relation across different redshifts.\n\nThrough extensive spectroscopic surveys, I have identified and characterized galaxy groups in strong gravitational lens fields, revealing the complexities of their environments and their impact on lensing measurements. My work emphasizes the importance of understanding the interplay between galaxy structure, star formation, and environmental factors, contributing to a more nuanced view of galaxy evolution in the early universe.\n\nOverall, my research aims to bridge observational data with theoretical models, enhancing our understanding of the cosmos and the processes that shape it.",
                "collaborators": [
                    "Pieter G. van Dokkum",
                    "Marijn Franx",
                    "Katherine E. Whitaker",
                    "Erica J. Nelson",
                    "Gabriel B. Brammer",
                    "Joel Leja",
                    "Gabriel Brammer",
                    "Rosalind E. Skelton",
                    "Charles R. Keeton",
                    "Kurtis A. Williams"
                ],
                "pub_titles": [
                    "Improved Empirical Backgrounds for JWST NIRISS Image/WFSS Data Reduction",
                    "A New Method for Wide-Field Near-IR Imaging with the Hubble Space Telescope",
                    "Evidence for non-stellar rest-frame near-IR emission associated with increased star formation in galaxies at $z \\sim 1$",
                    "The Evolution of the Fractions of Quiescent and Star-forming Galaxies as a Function of Stellar Mass Since z=3: Increasing Importance of Massive, Dusty Star-forming Galaxies in the Early Universe",
                    "The Effect of Environment on Shear in Strong Gravitational Lenses",
                    "A Spectroscopic Survey of the Fields of 28 Strong Gravitational Lenses: Implications for $H_0$",
                    "A Spectroscopic Survey of the Fields of 28 Strong Gravitational Lenses: The Group Catalog",
                    "Quiescent Galaxies in the 3D-HST Survey: Spectroscopic Confirmation of a Large Number of Galaxies with Relatively Old Stellar Populations at z~2",
                    "Constraining the Low-Mass Slope of the Star Formation Sequence at 0.5<z<2.5",
                    "Spatially-resolved dust maps from Balmer decrements in galaxies at z~1.4",
                    "Predicting Quiescence: The Dependence of Specific Star Formation Rate on Galaxy Size and Central Density at 0.5<z<2.5"
                ],
                "pub_abstracts": [
                    "The Near Infrared Imager and Slitless Spectrograph (NIRISS) on the James Webb Space Telescope (JWST) is a versatile instrument for collecting imaging and wide-field slitless spectroscopy (WFSS) data for surveys of galaxy clusters, emission-line galaxies, stellar populations, and more. Dispersed zodiacal light imprints distinct structures on space-based near-infrared imaging and WFSS observations, necessitating careful subtraction during JWST NIRISS data reduction. As of 2024-09-24 NIRISS WFSS calibration backgrounds introduce significant spatially-dependent artifacts, up to 5% of the overall background level, which can severely affect data quality and following astronomical analysis. Notably, there are no existing backgrounds for NIRISS imaging data which also show systematic artifacts, such as the `light saber' effect. In this work, we present improved empirical JWST NIRISS imaging and WFSS backgrounds derived from all available public data in the F115W, F150W, and F200W filters. We demonstrate that our empirical backgrounds provide a more accurate representation of the background structure in NIRISS imaging and WFSS data than existing reference files, mitigating the impact of spatially-dependent artifacts. Our empirical backgrounds are publicly available and can be used to improve the quality of JWST NIRISS imaging and WFSS data reduction.",
                    "We present a new technique for wide and shallow observations using the near-infrared channel of Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST). Wide-field near-IR surveys with HST are generally inefficient, as guide star acquisitions make it impractical to observe more than one pointing per orbit. This limitation can be circumvented by guiding with gyros alone, which is possible as long as the telescope has three functional gyros. The method presented here allows us to observe mosaics of eight independent WFC3-IR pointings in a single orbit by utilizing the fact that HST drifts by only a very small amount in the 25 seconds between non-destructive reads of unguided exposures. By shifting the reads and treating them as independent exposures the full resolution of WFC3 can be restored. We use this \"drift and shift\" (DASH) method in the Cycle 23 COSMOS-DASH program, which will obtain 456 WFC3 $H_{160}$ pointings in 57 orbits, covering an area of 0.6 degree$^2$ in the COSMOS field down to $H_{160} = 25$. When completed, the program will more than triple the area of extra-galactic survey fields covered by near-IR imaging at HST resolution. We demonstrate the viability of the method with the first four orbits (32 pointings) of this program. We show that the resolution of the WFC3 camera is preserved, and that structural parameters of galaxies are consistent with those measured in guided observations.",
                    "We explore the presence of non-stellar rest-frame near-IR ($2-5 \\ \\mu \\mathrm{m}$) emission in galaxies at $z \\sim 1$. Previous studies identified this excess in relatively small samples and suggested that such non-stellar emission, which could be linked to the $3.3 \\ \\mu \\mathrm{m}$ polycyclic aromatic hydrocarbons feature or hot dust emission, is associated with an increased star formation rate (SFR). In this Letter, we confirm and quantify the presence of an IR excess in a significant fraction of galaxies in the 3D-HST GOODS catalogs. By constructing a matched sample of galaxies with and without strong non-stellar near-IR emission, we find that galaxies with such emission are predominantly star-forming galaxies. Moreover, star-forming galaxies with an excess show increased mid- and far-IR and H$\\alpha$ emission compared to other star-forming galaxies without. While galaxies with a near-IR excess show a larger fraction of individually detected X-ray active galactic nuclei (AGNs), an X-ray stacking analysis, together with the IR-colors and H$\\alpha$ profiles, shows that AGNs are unlikely to be the dominant source of the excess in the majority of galaxies. Our results suggest that non-stellar near-IR emission is linked to increased SFRs and is ubiquitous among star-forming galaxies. As such, the near-IR emission might be a powerful tool to measure SFRs in the era of the James Webb Space Telescope.",
                    "Using the UltraVISTA DR1 and 3D-HST catalogs, we construct a stellar-mass-complete sample, unique for its combination of surveyed volume and depth, to study the evolution of the fractions of quiescent galaxies, moderately unobscured star-forming galaxies, and dusty star-forming galaxies as a function of stellar mass over the redshift interval $0.2 \\le z \\le 3.0$. We show that the role of dusty star-forming galaxies within the overall galaxy population becomes more important with increasing stellar mass, and grows rapidly with increasing redshift. Specifically, dusty star-forming galaxies dominate the galaxy population with $\\log{(M_{\\rm star}/M_{\\odot})} \\gtrsim 10.3$ at $z\\gtrsim2$. The ratio of dusty and non-dusty star-forming galaxies as a function of stellar mass changes little with redshift. Dusty star-forming galaxies dominate the star-forming population at $\\log{(M_{\\rm star}/M_{\\odot})} \\gtrsim 10.0-10.5$, being a factor of $\\sim$3-5 more common, while unobscured star-forming galaxies dominate at $\\log{(M_{\\rm star}/M_{\\odot})} \\lesssim 10$. At $\\log{(M_{\\rm star}/M_{\\odot})} > 10.5$, red galaxies dominate the galaxy population at all redshift $z<3$, either because they are quiescent (at late times) or dusty star-forming (in the early universe).",
                    "Using new photometric and spectroscopic data in the fields of nine strong gravitational lenses that lie in galaxy groups, we analyze the effects of both the local group environment and line-of-sight galaxies on the lens potential. We use Monte Carlo simulations to derive the shear directly from measurements of the complex lens environment, providing the first detailed independent check of the shear obtained from lens modeling. We account for possible tidal stripping of the group galaxies by varying the fraction of total mass apportioned between the group dark matter halo and individual group galaxies. The environment produces an average shear of gamma = 0.08 (ranging from 0.02 to 0.17), significant enough to affect quantities derived from lens observables. However, the direction and magnitude of the shears do not match those obtained from lens modeling in three of the six 4-image systems in our sample (B1422, RXJ1131, and WFI2033). The source of this disagreement is not clear, implying that the assumptions inherent in both the environment and lens model approaches must be reconsidered. If only the local group environment of the lens is included, the average shear is gamma = 0.05 (ranging from 0.01 to 0.14), indicating that line-of-sight contributions to the lens potential are not negligible. We isolate the effects of various theoretical and observational uncertainties on our results. Of those uncertainties, the scatter in the Faber-Jackson relation and error in the group centroid position dominate. Future surveys of lens environments should prioritize spectroscopic sampling of both the local lens environment and objects along the line of sight, particularly those bright (I < 21.5) galaxies projected within 5' of the lens.",
                    "Strong gravitational lensing provides an independent measurement of the Hubble parameter ($H_0$). One remaining systematic is a bias from the additional mass due to a galaxy group at the lens redshift or along the sightline. We quantify this bias for more than 20 strong lenses that have well-sampled sightline mass distributions, focusing on the convergence $\\kappa$ and shear $\\gamma$. In 23% of these fields, a lens group contributes a $\\ge$1% convergence bias; in 57%, there is a similarly significant line-of-sight group. For the nine time delay lens systems, $H_0$ is overestimated by 11$^{+3}_{-2}$% on average when groups are ignored. In 67% of fields with total $\\kappa \\ge$ 0.01, line-of-sight groups contribute $\\gtrsim 2\\times$ more convergence than do lens groups, indicating that the lens group is not the only important mass. Lens environment affects the ratio of four (quad) to two (double) image systems; all seven quads have lens groups while only three of 10 doubles do, and the highest convergences due to lens groups are in quads. We calibrate the $\\gamma$-$\\kappa$ relation: $\\log(\\kappa_{\\rm{tot}}) = (1.94 \\pm 0.34) \\log(\\gamma_{\\rm{tot}}) + (1.31 \\pm 0.49)$ with a rms scatter of 0.34 dex. Shear, which, unlike convergence, can be measured directly from lensed images, can be a poor predictor of $\\kappa$; for 19% of our fields, $\\kappa$ is $\\gtrsim 2\\gamma$. Thus, accurate cosmology using strong gravitational lenses requires precise measurement and correction for all significant structures in each lens field.",
                    "With a large, unique spectroscopic survey in the fields of 28 galaxy-scale strong gravitational lenses, we identify groups of galaxies in the 26 adequately-sampled fields. Using a group finding algorithm, we find 210 groups with at least five member galaxies; the median number of members is eight. Our sample spans redshifts of 0.04 $\\le z_{grp} \\le$ 0.76 with a median of 0.31, including 174 groups with $0.1 < z_{grp} < 0.6$. Groups have radial velocity dispersions of 60 $\\le \\sigma_{grp} \\le$ 1200 km s$^{-1}$ with a median of 350 km s$^{-1}$. We also discover a supergroup in field B0712+472 at $z =$ 0.29 consisting of three main groups. We recover groups similar to $\\sim$ 85% of those previously reported in these fields within our redshift range of sensitivity and find 187 new groups with at least five members. The properties of our group catalog, specifically 1) the distribution of $\\sigma_{grp}$, 2) the fraction of all sample galaxies that are group members, and 3) the fraction of groups with significant substructure, are consistent with those for other catalogs. The distribution of group virial masses agrees well with theoretical expectations. Of the lens galaxies, 12 of 26 (46%) (B1422+231, B1600+434, B2114+022, FBQS J0951+2635, HE0435-1223, HST J14113+5211, MG0751+2716, MGJ1654+1346, PG 1115+080, Q ER 0047-2808, RXJ1131-1231, and WFI J2033-4723) are members of groups with at least five galaxies, and one more (B0712+472) belongs to an additional, visually identified group candidate. There are groups not associated with the lens that still are likely to affect the lens model; in six of 25 (24%) fields (excluding the supergroup), there is at least one massive ($\\sigma_{grp} \\ge$ 500 km s$^{-1}$) group or group candidate projected within 2$^{\\prime}$ of the lens.",
                    "Quiescent galaxies at z~2 have been identified in large numbers based on rest-frame colors, but only a small number of these galaxies have been spectroscopically confirmed to show that their rest-frame optical spectra show either strong Balmer or metal absorption lines. Here, we median stack the rest-frame optical spectra for 171 photometrically-quiescent galaxies at 1.4 < z < 2.2 from the 3D-HST grism survey. In addition to Hbeta (4861A), we unambiguously identify metal absorption lines in the stacked spectrum, including the G-band (4304A), Mg I (5175A), and Na I (5894A). This finding demonstrates that galaxies with relatively old stellar populations already existed when the universe was ~3 Gyr old, and that rest-frame color selection techniques can efficiently select them. We find an average age of 1.3^0.1_0.3 Gyr when fitting a simple stellar population to the entire stack. We confirm our previous result from medium-band photometry that the stellar age varies with the colors of quiescent galaxies: the reddest 80% of galaxies are dominated by metal lines and have a relatively old mean age of 1.6^0.5_0.4 Gyr, whereas the bluest (and brightest) galaxies have strong Balmer lines and a spectroscopic age of 0.9^0.2_0.1 Gyr. Although the spectrum is dominated by an evolved stellar population, we also find [OIII] and Hbeta emission. Interestingly, this emission is more centrally concentrated than the continuum with L_[OIII] = 1.7 +/- 0.3 x 10^40 erg s^-1, indicating residual central star formation or nuclear activity.",
                    "We constrain the slope of the star formation rate ($\\log\\Psi$) to stellar mass ($\\log\\mathrm{M_{\\star}}$) relation down to $\\log(\\mathrm{M_{\\star}/M_{\\odot}})=8.4$ ($\\log(\\mathrm{M_{\\star}/M_{\\odot}})=9.2$) at $z=0.5$ ($z=2.5$) with a mass-complete sample of 39,106 star-forming galaxies selected from the 3D-HST photometric catalogs, using deep photometry in the CANDELS fields. For the first time, we find that the slope is dependent on stellar mass, such that it is steeper at low masses ($\\log\\mathrm{\\Psi}\\propto\\log\\mathrm{M_{\\star}}$) than at high masses ($\\log\\mathrm{\\Psi}\\propto(0.3-0.6)\\log\\mathrm{M_{\\star}}$). These steeper low mass slopes are found for three different star formation indicators: the combination of the ultraviolet (UV) and infrared (IR), calibrated from a stacking analysis of Spitzer/MIPS 24$\\mu$m imaging; $\\beta$-corrected UV SFRs; and H$\\alpha$ SFRs. The normalization of the sequence evolves differently in distinct mass regimes as well: for galaxies less massive than $\\log(\\mathrm{M_{\\star}/M_{\\odot}})<10$ the specific SFR ($\\Psi/\\mathrm{M_{\\star}}$) is observed to be roughly self-similar with $\\Psi/\\mathrm{M_{\\star}}\\propto(1+z)^{1.9}$, whereas more massive galaxies show a stronger evolution with $\\Psi/\\mathrm{M_{\\star}}\\propto(1+z)^{2.2-3.5}$ for $\\log(\\mathrm{M_{\\star}/M_{\\odot}})=10.2-11.2$. The fact that we find a steep slope of the star formation sequence for the lower mass galaxies will help reconcile theoretical galaxy formation models with the observations. The results of this study support the analytical conclusions of Leja et al. (2014).",
                    "We derive average radial gradients in the dust attenuation towards HII regions in 609 galaxies at z~1.4, using measurements of the Balmer decrement out to r~3kpc. The Balmer decrements are derived from spatially resolved maps of Halpha and Hbeta emission from the 3D-HST survey. We find that with increasing stellar mass (M) both the normalization and strength of the gradient in dust attenuation increases. Galaxies with a mean mass of <log(M)> = 9.2Msun have little dust attenuation at all radii, whereas galaxies with <log(M)>= 10.2Msun have dust attenuation toward Halpha A(Halpha)~2mag in their central regions. We parameterize this as A(Halpha) = b + c log(r), with b = 0.9 + 1.0 log(M10), c = -1.9 - 2.2 log(M10), r in kpc, and M10 the stellar mass in units of 10^10Msun. This expression can be used to correct spatially resolved measurements of Halpha to radial distributions of star formation. When applied to our data, we find that the star formation rates in the central r<1kpc of galaxies in the highest mass bin are ~ 6 Msun/yr, six times higher than before correction and approximately half of the total star formation rate of these galaxies. If this high central star formation rate is maintained for several Gyr, a large fraction of the stars in present-day bulges likely formed in-situ.",
                    "In this paper, we investigate the relationship between star formation and structure, using a mass-complete sample of 27,893 galaxies at $0.5<z<2.5$ selected from 3D-HST. We confirm that star-forming galaxies are larger than quiescent galaxies at fixed stellar mass (M$_{\\star}$). However, in contrast with some simulations, there is only a weak relation between star formation rate (SFR) and size within the star-forming population: when dividing into quartiles based on residual offsets in SFR, we find that the sizes of star-forming galaxies in the lowest quartile are 0.27$\\pm$0.06 dex smaller than the highest quartile. We show that 50% of star formation in galaxies at fixed M$_{\\star}$ takes place within a narrow range of sizes (0.26 dex). Taken together, these results suggest that there is an abrupt cessation of star formation after galaxies attain particular structural properties. Confirming earlier results, we find that central stellar density within a 1 kpc fixed physical radius is the key parameter connecting galaxy morphology and star formation histories: galaxies with high central densities are red and have increasingly lower SFR/M$_{\\star}$, whereas galaxies with low central densities are blue and have a roughly constant (higher) SFR/M$_{\\star}$ at a given redshift. We find remarkably little scatter in the average trends and a strong evolution of $>$0.5 dex in the central density threshold correlated with quiescence from $z\\sim0.7-2.0$. Neither a compact size nor high-$n$ are sufficient to assess the likelihood of quiescence for the average galaxy; rather, the combination of these two parameters together with M$_{\\star}$ results in a unique quenching threshold in central density/velocity."
                ],
                "domain": [
                    "Astrophysics",
                    "Gravitational Lensing",
                    "Galaxy Formation",
                    "Near-Infrared Observations"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "5bd03bcd-7c3c-4c44-8018-9ac5777be866": {
                "pk": "5bd03bcd-7c3c-4c44-8018-9ac5777be866",
                "project_name": null,
                "name": "Urmila Chadayammuri",
                "bio": "I am an astrophysicist specializing in the study of black holes, galaxy clusters, and the intricate dynamics of the universe. My research primarily focuses on understanding the formation and growth of supermassive black holes (SMBHs) in dwarf galaxies and their implications for cosmological models. Through a combination of observational data from X-ray and multi-wavelength surveys, such as the eROSITA and Chandra missions, I have developed semi-analytical models to explore the black hole occupation fraction (BHOF) and the seeding mechanisms of black holes.\n\nMy recent work has involved analyzing the eROSITA all-sky survey to compile a catalog of accreting SMBHs in local dwarf galaxies, revealing insights into their growth history and the relationship between black holes and their host galaxies. I have also investigated the Kelvin-Helmholtz instabilities in galaxy clusters to constrain magnetic field strengths, and I have employed advanced hydrodynamic simulations to study the role of active galactic nuclei (AGN) in regulating star formation within galaxy clusters.\n\nAdditionally, I have contributed to the development of innovative machine learning techniques, such as U-net models, to enhance our understanding of galaxy cluster mass functions without direct mass measurements. My research aims to bridge the gap between theoretical models and observational data, providing a clearer picture of the cosmic evolution of black holes and their environments. As I continue to explore the complexities of the universe, I am committed to advancing our understanding of the fundamental processes that govern galaxy formation and evolution.",
                "collaborators": [
                    "John ZuHone",
                    "Daisuke Nagai",
                    "Akos Bogdan",
                    "Priyamvada Natarajan",
                    "Ralph Kraft",
                    "Angelo Ricarte",
                    "Christine Jones",
                    "Sharon Felix",
                    "Lindsay King",
                    "Marta Volonteri"
                ],
                "pub_titles": [
                    "Constraints from dwarf galaxies on black hole seeding and growth models with current and future surveys",
                    "X-ray bright AGN in local dwarf galaxies: insights from eROSITA",
                    "Turbulent magnetic fields in merging clusters: A case study of Abell 2146",
                    "Fountains and storms: The role of AGN and mergers in disrupting the cool-core in the RomulusC simulation",
                    "Painting baryons onto N-body simulations of galaxy clusters with image-to-image deep learning",
                    "Closing Pandora's Box -- The deepest X-ray observations of Abell 2744 and a multi-wavelength merger picture",
                    "Testing galaxy feedback models with the first resolved profiles of the circumgalactic medium",
                    "The X-ray Angular Power Spectrum of Extended Sources in the eROSITA Final Equatorial Depth Survey",
                    "Constraining Merging Galaxy Clusters with X-ray and Lensing Simulations and Observations: The case of Abell 2146",
                    "Decoding the Early Universe: Exploring a Merger Scenario for the High-Redshift Cluster JKCS041 using Numerical Models",
                    "Introducing RomulusC: A Cosmological Simulation of a Galaxy Cluster with Unprecedented Resolution",
                    "Weak-lensing mass bias in merging galaxy clusters",
                    "Evidence for heavy seed origin of early supermassive black holes from a z~10 X-ray quasar",
                    "Improved Constraints on Mergers with SZ, Hydrodynamical simulations, Optical, and X-ray (ICM-SHOX). Paper II: Galaxy cluster sample overview",
                    "ICM-SHOX. Paper I: Methodology overview and discovery of a gas--dark matter velocity decoupling in the MACS J0018.5+1626 merger",
                    "A candidate supermassive black hole in a gravitationally-lensed galaxy at $z\\approx10$"
                ],
                "pub_abstracts": [
                    "Dwarf galaxies are promising test beds for constraining models of supermassive and intermediate-mass black holes (MBH) via their black hole occupation fraction (BHOF). Disentangling seeding from the confounding effects of mass assembly over a Hubble time is a challenging problem, that we tackle in this study with a suite of semi-analytical models (SAMs). We show how measured BHOF depends on the lowest black hole mass or AGN luminosity achieved by a survey. To tell seeding models apart, we need to detect or model all AGN brighter than $10^{37}\\ \\rm{erg \\ s^{-1}}$ in galaxies of $M_* \\sim 10^{8-10} \\ \\rm{M_{\\odot}}$. Shallower surveys, like eRASS, cannot distinguish between seed models even with the compensation of a much larger survey volume. We show that the AMUSE survey, with its inference of the MBH population underlying the observed AGN, strongly favors heavy seed models, growing with either a power-law Eddington Ratio Distribution Function (ERDF) or one in which black hole accretion is tagged to the star-formation rate (AGN-MS). These two growth channels can then be distinguished by the AGN luminosity function at $> 10^{40}\\ \\rm{erg \\ s^{-1}}$, with the AGN-MS model requiring more accretion than observed at z $\\sim$ 0. Thus, current X-ray observations favour heavy seeds whose Eddington ratios follow a power-law distribution. The different models also predict different radio scaling relations, which we quantify using the fundamental plane of black hole activity. We close with recommendations for the design of upcoming multi-wavelength campaigns that can optimally detect MBHs in dwarf galaxies.",
                    "Although supermassive black holes (SMBHs) reside in the heart of virtually every massive galaxy, it remains debated whether dwarf galaxies also commonly host SMBHs. Because low-mass galaxies may retain a memory of the assembly history of their black holes, probing the black hole occupation fraction of local dwarf galaxies might offer insights into the growth and seeding mechanisms of the first black holes. In this work, we exploit the Western half of the eROSITA all-sky survey (covering $20,000~\\rm{deg^2}$) and compile a catalog of accreting SMBHs in local ($D<200$~Mpc) dwarf galaxies. After cleaning our sample from cosmic X-ray background sources, X-ray binaries, and ultraluminous X-ray sources, we identify 74 AGN-dwarf galaxy pairs. Using this large and uniform sample, we derive a luminosity function of dwarf galaxy AGN, fitting it with a power law function and obtaining ${\\rm d}N/{\\rm d}L_{\\rm X} = (15.9\\pm2.2)\\times L_{\\rm X}^{-1.63\\pm0.05}$. Measuring the offset between the centroid of dwarf galaxies and the X-ray sources, we find that about $50\\%$ of the AGN are likely off-nuclear, in agreement with theoretical predictions. We also compare the black hole-to-stellar mass relation of the AGN in our sample with the local and high-redshift relations, finding that our sources better adhere to the former. This suggests that local AGN across different mass scales underwent a similar growth history. Finally, we compare our sources with semi-analytical models: while our sample is too shallow to distinguish between different seeding models, it favors a growth mechanism linked to the star-formation rate of the host galaxy.",
                    "Kelvin-Helmholtz Instabilities (KHI) along contact discontinuities in galaxy clusters have been used to constrain the strength of magnetic fields in galaxy clusters, following the assumption that, as magnetic field lines drape around the interface between the cold and hot phases, their magnetic tension resists the growth of perturbations. This has been observed in simulations of rigid objects moving through magnetised media and sloshing galaxy clusters, and then applied in interpreting observations of merger cold fronts. Using a suite of MHD simulations of binary cluster mergers, we show that even magnetic field strengths stronger than yet observed ($\\beta = P_{\\rm th}/P_B = 50$) show visible KHI features. This is because our initial magnetic field is tangled, producing Alfven waves and associated velocity fluctuations in the ICM; stronger initial fields therefore seed larger fluctuations, so that even a reduced growth rate due to magnetic tension produces a significant KHI. The net result is that a stronger initial magnetic field produces more dramatic fluctuations in surface brightness and temperature, not the other way around. We show that this is hard to distinguish from the evolution of turbulent perturbations of the same initial magnitude. Therefore, in order to use observations of KHI in the ICM to infer magnetic field strengths by comparing to idealized simulations, the perturbations which seed the KHI must be well-understood and (if possible) carefully controlled.",
                    "The intracluster medium (ICM) is a multi-phase environment, dynamically regulated by Active Galactic Nuclei (AGN), the motions of galaxies through it, and mergers with other clusters. AGN as a central heating source are key to preventing runaway cooling flows, but their role in heating cores in a cosmological context is still poorly understood. The activity of the AGN is strongly linked to star formation, especially in the Brightest Cluster Galaxy (BCG), likely because both rely on cold phase gas. A self-consistent model for AGN and star formation in galaxy clusters thus requires cosmological context, higher resolution, and a careful modeling of cooling and heating balance. In this paper, we use the high-resolution hydrodynamical cosmological simulation of the RomulusC galaxy cluster to study in detail the role of AGN and a major, head-on merger in shaping the cluster core. The unprecedented resolution of the RomulusC simulation captures the multiphase structure of the ICM. The realistic large-scale outflows launched by very small-scale thermal injections, the improved modeling of turbulent diffusion and mixing, and the particle nature of the simulation allow us to carefully separate different heating channels. We show that AGN activity, while efficient at regulating star formation, is incapable of destroying a CC. Instead, that process is facilitated by a head-on, 1:8 mass ratio merger. The merger generates bulk and turbulent motions, which in turn mix high entropy gas generated by AGN and merger driven shocks, turbulent dissipation and sloshing of the ICM by infalling substructures. While central cooling times remain shorter than the Hubble time, restoring a CC is made more difficult by the reduced precipitation rates at larger radii, emphasizing that the AGN-ICM connection is truly a multi-scale problem.",
                    "Galaxy cluster mass functions are a function of cosmology, but mass is not a direct observable, and systematic errors abound in all its observable proxies. Mass-free inference can bypass this challenge, but it requires large suites of simulations spanning a range of cosmologies and models for directly observable quantities. In this work, we devise a U-net - an image-to-image machine learning algorithm - to ``paint'' the IllustrisTNG model of baryons onto dark-matter-only simulations of galaxy clusters. Using 761 galaxy clusters with $M_{200c} \\gtrsim 10^{14}M_\\odot$ from the TNG-300 simulation at $z<1$, we train the algorithm to read in maps of projected dark matter mass and output maps of projected gas density, temperature, and X-ray flux. The models train in under an hour on two GPUs, and then predict baryonic images for $\\sim2700$ dark matter maps drawn from the TNG-300 dark-matter-only (DMO) simulation in under two minutes. Despite being trained on individual images, the model reproduces the true scaling relation and scatter for the $M_{DM}-L_X$, as well as the distribution functions of the cluster X-ray luminosity and gas mass. For just one decade in cluster mass, the model reproduces three orders of magnitude in $L_X$. The model is biased slightly high when using dark matter maps from the DMO simulation. The model performs well on inputs from TNG-300-2, whose mass resolution is 8 times coarser; further degrading the resolution biases the predicted luminosity function high. We conclude that U-net-based baryon painting is a promising technique to build large simulated cluster catalogs which can be used to improve cluster cosmology by combining existing full-physics and large $N$-body simulations.",
                    "Abell 2744, also known as Pandora's Cluster, is a complex merging galaxy cluster. While a major merger is clear along the north-south axis, the dynamical state of the northwest subcluster has been highly uncertain. We present ultra-deep ($\\approx$2.1 Ms) X-ray observations of Abell 2744 obtained with the Chandra X-ray Observatory and reinterpret the multi-wavelength picture with a suite of idealised simulations of galaxy cluster mergers. The new data reveal in unprecedented detail the disruption of cool cores in the three X-ray luminous subclusters and confirm the presence of a shock to the NW. A position-velocity clustering of the cluster member galaxies shows a clearly separated S2 component, with a $\\Delta z$ implying a separation of 53 Mpc or a line-of-sight velocity of $4500\\ \\rm{km \\ s^{-1}}$, or likely some combination of the two. While binary simulations allow NW to have undergone a gravitational slingshot after the first pericenter passage, triple merger simulations rule out this scenario, because the two mergers would have had to occur $\\sim$0.5 Gyr apart, and the joint impact of the shocks from the two mergers would completely disrupt the SE and NW cool cores; they only reform after 1-2 Gyr, by which point the core separations greatly exceed observations. The scenario that best describes Abell 2744 is a head-on N-S merger $0.5-0.6$ Gyrs ago followed by a first infall of the NW subcluster. Furthermore, we note that a model with three cluster-size halos, with masses consistent with gravitational lensing constraints, nevertheless produces a lensing convergence and surface brightness lower than observed in most of the field of view, whereas the temperatures are consistent with observations. This suggests the presence of a large-scale overdensity, which contributes to the diffuse emission and total surface density without heating the densest gas.",
                    "The hot ($>10^6$ K) phase of the circumgalactic medium (CGM) contains a large fraction of baryons in galaxies. It also retains signatures of the processes that shaped the galaxies, such as feedback from active galactic nuclei (AGNs) and supernovae, and offers a uniquely powerful way to constrain theoretical models of feedback. It is, however, notoriously difficult to detect. By stacking 2643 optically selected galaxies in the eROSITA Final Equatorial Depth Survey (eFEDS), we present spatially resolved properties of the extended CGM in both star-forming and quiescent galaxies spanning an order of magnitude in stellar mass. We mask out resolved point sources and galaxy groups/clusters and model the contribution from X-ray binaries and the hot ISM, producing accurate radial profiles. We compare the profiles to mock X-ray observations of galaxy stacks in the IllustrisTNG100 (TNG) and EAGLE cosmological simulations. We detect extended emission from both the high-mass ($10.7<\\log(M_*/M_\\odot)<11.2$) and low-mass ($10.2<\\log(M_*/M_\\odot)<10.7$) galaxy stacks. Galaxies have somewhat more luminous CGM between $10-100$~kpc if they are more massive or star-forming. However, the luminosity increases slower with stellar mass than predicted in simulations. Simulated quenched galaxies are far dimmer than observed, suggesting that they rely too heavily on CGM ejection for quenching. Star-forming galaxies are observed to have flatter and more extended profiles than in simulations, suggesting under-efficient stellar feedback models. Our results highlight the need to modify future prescriptions of galaxy feedback models.",
                    "The eROSITA Final Equatorial Depth Survey (eFEDS), with a sky area of 140 square degrees with depth equivalent to the equatorial patch of the final eROSITA all-sky survey, represents the largest continuous non-full-sky X-ray fields to-date, making it the premier data set for measuring the angular power spectrum. In this work, we measure the X-ray angular power spectrum of galaxy clusters and groups in the eFEDS field. We show that the measured power spectrum is consistent with past observations, including the ROSAT All Sky Survey, and the Chandra COSMOS and Bootes fields. The predictions of cluster gas halo model that is calibrated from Chandra observations is also consistent with the eFEDS power spectrum. While the eFEDS does not have large enough sky coverage to provide meaningful cosmological constraints, we predict that the X-ray power spectrum from the cycle 4 of the eROSITA All-Sky Survey (eRASS4) will provide constraints on $\\Omega_M$ and $\\sigma_8$ at the 10% level.",
                    "Galaxy cluster mergers are a powerful laboratory for testing cosmological and astrophysical models. However, interpreting individual merging clusters depends crucially on their merger configuration, defined by the masses, velocities, impact parameters, and orientation of the merger axis with respect to the plane of the sky. In this work, we investigate the impact of merger parameters on the X-ray emitting intracluster medium and gravitational lensing maps using a suite of idealised simulations of binary cluster mergers performed using the GAMER-2 code. As a test case, we focus on modeling the Bullet Cluster-like merging system Abell 2146, in which deep \\textit{Chandra} X-ray and lensing observations revealed prominent merger shocks as well as the mass distribution and substructures associated with this merging cluster. We identify the most interesting parameter combinations, and evaluate the effects of various parameters on the properties of merger shocks observed by deep \\textit{Chandra} and lensing observations. We show that due gravitational compression of the cluster halos during the merger, previous mass estimates from weak lensing are too high. The plane of the merger is tilted further from the plane of the sky than estimated previously, up to $30^\\circ$ from the plane of the sky. We discuss the applicability of our results to multi-wavelength observations of merging galaxy clusters and their use as probes of cosmology and plasma physics.",
                    "JKCS041 ($z=1.8$) is one of the most distant galaxy cluster systems known, seen when the Universe was less than 4 billion years old. Recent Sunyaev-Zeldovich (SZ) observations show a temperature decrement that is less than expected based on mass estimates of the system from X-ray, weak gravitational lensing and galaxy richness measurements. In this paper we seek to explain the observables - in particular the low SZ decrement and single SZ peak, the projected offset between the X-ray and SZ peaks of $\\approx$220 kpc, the gas mass measurements and the lensing mass estimate. We use the GAMER-2 hydrodynamic code to carry out idealized numerical simulations of cluster mergers and compare resulting synthetic maps with the observational data. The observations are not well reproduced by an isolated cluster, while instead they are when considering cluster mergers viewed a few tenths of a Gyr after first core passage. A range of merger scenarios is consistent with the observations, but parts of parameter space can be ruled out, and generically some kind of merger process is necessary to reproduce the offset between the SZ and X-ray peaks. In particular, a total mass of $\\approx$2$\\times 10^{14} M_\\odot$, mass ratio of $\\approx$2:3, gas fraction of $0.05-0.1$ and Navarro, Frenk and White (NFW) mass density profile concentration $c$$\\approx$5 for both components are scenarios that are consistent with the observational data.",
                    "We present the first results from RomulusC, the highest resolution cosmological hydrodynamic simulation of a galaxy cluster run to date. RomulusC, a zoom-in simulation of a halo with $z=0$ mass $10^{14}$ M$_{\\odot}$, is run with the same sub-grid physics and resolution as Romulus25 (Tremmel et al. 2017). With unprecedented mass and spatial resolution, RomulusC represents a unique opportunity to study the evolution of galaxies in dense environments down to dwarf masses. We demonstrate that RomulusC results in an intracluster medium (ICM) consistent with observations. The star formation history and stellar mass of the brightest cluster galaxy (BCG) is consistent with observations and abundance matching results, indicating that our sub-grid models, optimized only to reproduce observations of field dwarf and Milky Way mass galaxies, are able to produce reasonable galaxy masses and star formation histories in much higher mass systems. Feedback from supermassive black holes (SMBHs) regulates star formation by driving large-scale, collimated outflows that coexist with a low entropy core. We find that non-BCG cluster member galaxies are substantially quenched compared to the field down to dwarf galaxy masses and, at low masses, quenching is seen to have no dependence on mass or distance from the cluster center. This enhanced quenched population extends beyond $R_{200}$ and is in place at high redshift. Similarly, we predict that SMBH activity is significantly suppressed within clusters outside of the BCG, but show how the effect could be lost when only focusing on the brightest AGN in the most massive galaxies.",
                    "Although weak lensing (WL) is a powerful method to estimate a galaxy cluster mass without any dynamical assumptions, a model bias can arise when the cluster density profile departs from the assumed model profile. In a merging system, the bias is expected to become most severe because the constituent halos undergo significant structural changes. In this study, we investigate WL mass bias in binary cluster mergers using a suite of idealized hydrodynamical simulations. Realistic WL shear catalogs are generated by matching the source galaxy properties, such as intrinsic shape dispersion, measurement noise, source densities, etc., to those from Subaru and {\\it Hubble Space Telescope} observations. We find that, with the typical mass-concentration ($M$-$c$) relation and the Navarro-Frenk-White (NFW) profile, the halo mass bias depends on the time since the first pericenter passage and increases with the mass of the companion cluster. The time evolution of the mass bias is similar to that of the concentration, indicating that, to first order, the mass bias is modulated by the concentration change. For a collision between two $\\sim10^{15}~M_{\\odot}$ clusters, the maximum bias amounts to $\\sim60\\%$. This suggests that previous WL studies may have significantly overestimated the mass of the clusters in some of the most massive mergers. Finally, we apply our results to three merger cases: Abell 2034, MACS J1752.0+4440, and ZwCl 1856.8+6616, and report their mass biases at the observed epoch, as well as their pre-merger masses, utilizing their merger shock locations as tracers of the merger phases.",
                    "Observations of quasars reveal that many supermassive black holes (BHs) were in place less than 700 million years after the Big Bang. However, the origin of the first BHs remains a mystery. Seeds of the first BHs are postulated to be either light (i.e., $10-100~\\rm{M_{\\odot}})$, remnants of the first stars or heavy (i.e., $10^4-10^5~\\rm{M_{\\odot}})$, originating from the direct collapse of gas clouds. Harnessing recent data from the Chandra X-ray Observatory, we report the detection of an X-ray-luminous massive BH in a gravitationally-lensed galaxy identified by JWST at $z\\approx10.3$ behind the cluster lens Abell 2744. This heavily-obscured quasar with a bolometric luminosity of $L_{\\rm bol}\\sim5\\times10^{45}~\\rm{erg\\ s^{-1}}$ harbors a $M_{\\rm BH}\\sim10^7-10^8~\\rm{M_{\\odot}}$ BH assuming accretion at the Eddington limit. This mass is comparable to the inferred stellar mass of its host galaxy, in contrast to what is found in the local Universe wherein the BH mass is $\\sim0.1\\%$ of the host galaxy's stellar mass. The combination of such a high BH mass and large BH-to-galaxy stellar mass ratio just $\\sim$500 Myrs after the Big Bang was theoretically predicted and is consistent with a picture wherein BHs originated from heavy seeds.",
                    "Galaxy cluster mergers are representative of a wide range of physics, making them an excellent probe of the properties of dark matter and the ionized plasma of the intracluster medium. To date, most studies have focused on mergers occurring in the plane of the sky, where morphological features can be readily identified. To allow study of mergers with arbitrary orientation, we have assembled multi-probe data for the eight-cluster ICM-SHOX sample sensitive to both morphology and line of sight velocity. The first ICM-SHOX paper (Silich+2023) provided an overview of our methodology applied to one member of the sample, MACS J0018.5+1626, in order to constrain its merger geometry. That work resulted in an exciting new discovery of a velocity space decoupling of its gas and dark matter distributions. In this work, we describe the availability and quality of multi-probe data for the full ICM-SHOX galaxy cluster sample. These datasets will form the observational basis of an upcoming full ICM-SHOX galaxy cluster sample analysis.",
                    "Galaxy cluster mergers are rich sources of information to test cluster astrophysics and cosmology. However, cluster mergers produce complex projected signals that are difficult to interpret physically from individual observational probes. Multi-probe constraints on the gas and dark matter cluster components are necessary to infer merger parameters that are otherwise degenerate. We present ICM-SHOX (Improved Constraints on Mergers with SZ, Hydrodynamical simulations, Optical, and X-ray), a systematic framework to jointly infer multiple merger parameters quantitatively via a pipeline that directly compares a novel combination of multi-probe observables to mock observables derived from hydrodynamical simulations. We report a first application of the ICM-SHOX pipeline to MACS J0018.5+1626, wherein we systematically examine simulated snapshots characterized by a wide range of initial parameters to constrain the MACS J0018.5+1626 merger geometry. We constrain the epoch of MACS J0018.5+1626 to the range $0$--$60$ Myr post-pericenter passage, and the viewing angle is inclined $\\approx 27$--$40$ degrees from the merger axis. We obtain constraints for the impact parameter ($\\lesssim 250$ kpc), mass ratio ($\\approx 1.5$--$3.0$), and initial relative velocity when the clusters are separated by 3 Mpc ($\\approx 1700$--3000 km s$^{-1}$). The primary and secondary clusters initially (at 3 Mpc) have gas distributions that are moderately and strongly disturbed, respectively. We discover a velocity space decoupling of the dark matter and gas distributions in MACS J0018.5+1626, traced by cluster-member galaxy velocities and the kinematic Sunyaev-Zel'dovich effect, respectively. Our simulations indicate this decoupling is dependent on the different collisional properties of the two distributions for particular merger epochs, geometries, and viewing angles.",
                    "While supermassive black holes (BHs) are widely observed in the nearby and distant universe, their origin remains debated with two viable formation scenarios with light and heavy seeds. In the light seeding model, the first BHs form from the collapse of massive stars with masses of $10-100 \\ \\rm{M_{\\odot}}$, while the heavy seeding model posits the formation of $10^{4-5} \\ \\rm{M_{\\odot}}$ seeds from direct collapse. The detection of BHs at redshifts $z\\gtrsim10$, edging closer to their formation epoch, provides critical observational discrimination between these scenarios. Here, we focus on the JWST-detected galaxy, GHZ 9, at $z\\approx10$ that is lensed by the foreground cluster, Abell 2744. Based on 2.1 Ms deep Chandra observations, we detect a candidate X-ray AGN, which is spatially coincident with the high-redshift galaxy, GHZ 9. The BH candidate is inferred to have a bolometric luminosity of $(1.0^{+0.5}_{-0.4})\\times10^{46} \\ \\rm{erg \\ s^{-1}}$, which corresponds to a BH mass of $(8.0^{+3.7}_{-3.2})\\times10^7 \\ \\rm{M_{\\odot}}$ assuming Eddington-limited accretion. This extreme mass at such an early cosmic epoch suggests the heavy seed origin for this BH candidate. Based on the Chandra and JWST discoveries of extremely high-redshift quasars, we have constructed the first simple AGN luminosity function extending to $z\\approx10$. Comparison of this luminosity function with theoretical models indicates an over-abundant $z\\approx10$ BH population, consistent with a higher-than-expected seed formation efficiency."
                ],
                "domain": [
                    "Astrophysics",
                    "Black Holes",
                    "Galaxy Clusters",
                    "Cosmology"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "16e14b4d-d435-4912-89e6-2ab68b548da1": {
                "pk": "16e14b4d-d435-4912-89e6-2ab68b548da1",
                "project_name": null,
                "name": "Mariia Demianenko",
                "bio": "I am an astronomer specializing in the study of black holes, particularly focusing on intermediate-mass black holes (IMBHs) and their role in the evolution of supermassive black holes (SMBHs). My recent work has involved developing innovative algorithms for point spread function (PSF) fitting and interpolation, which are crucial for high-precision astrometry in upcoming astronomical surveys, such as those conducted with the Extremely Large Telescope (ELT). \n\nI have also contributed to the analysis of light curves from the Zwicky Transient Facility (ZTF), where I designed methods to detect variability in active galactic nuclei (AGN) powered by IMBHs. My research has led to the identification of numerous IMBH candidates and the discovery of a new tidal disruption event, enhancing our understanding of these elusive objects.\n\nIn addition to my observational work, I am passionate about leveraging machine learning techniques to classify astronomical phenomena, such as supernovae, using their light curves. I have explored various neural network architectures to improve the accuracy and efficiency of these classifications, demonstrating that advanced models can significantly outperform traditional methods.\n\nThrough my research, I aim to bridge the gap between observational data and theoretical models, providing insights into the formation and growth of black holes in the universe. I am committed to making my findings accessible to the scientific community, as evidenced by my contributions to open-source tools and datasets that facilitate further research in this exciting field.",
                "collaborators": [
                    "Kirill Grishin",
                    "Victoria Toptun",
                    "Ivan Katkov",
                    "Vladimir Goradzhanov",
                    "Igor Chilingarian",
                    "Ivan Kuzmin",
                    "Ekaterina Samorodova",
                    "Mikhail Sysak",
                    "Aleksandr Shiriaev",
                    "Konstantin Malanchev"
                ],
                "pub_titles": [
                    "PSF quality metrics in the problem of revealing Intermediate-Mass Black Holes using MICADO@ELT",
                    "Optical Variability of \"Light-weight\" Supermassive Black Holes at a Few Percent Level from ZTF Forced-Photometry Light Curves",
                    "Optical light curves of light-weight supermassive black holes produced by the Zwicky Transient Facility Forced Photometry Service",
                    "Optical spectroscopic observations of intermediate-mass black holes and their host galaxies: the $M_{BH}-σ_*$ relation",
                    "Supernova Light Curves Approximation based on Neural Network Models",
                    "Understanding of the properties of neural network approaches for transient light curve approximations",
                    "Confirmation of intermediate-mass black holes candidates with X-ray observations"
                ],
                "pub_abstracts": [
                    "Nowadays, astronomers perform point spread function (PSF) fitting for most types of observational data. Interpolation of the PSF is often an intermediate step in such algorithms. In the case of the Multi-AO Imaging Camera for Deep Observations (MICADO) at the Extremely Large Telescope (ELT), PSF interpolation will play a crucial role in high-precision astrometry for stellar clusters and confirmation of the Intermediate-Mass Black Holes (IMBHs) presence. Significant PSF variations across the field of view invalidate the approach of deconvolution with a mean PSF or on-axis PSF. The ignoring of PSF variations can be especially unsatisfactory in the case of Single Conjugate Adaptive Optics (SCAO) observations, as these sophisticated and expensive systems are designed to achieve high resolution with ground-based telescopes by correcting for atmospheric turbulence in the direction of one reference star. In plenty of tasks, you face the question: How can I establish the quality of PSF fitting or interpolation? Our study aims to demonstrate the variety of PSF quality metrics, including the problem of revealing IMBHs in stellar clusters.",
                    "Large time-domain surveys provide a unique opportunity to detect and explore variability of millions of sources on timescales from days to years. Broadband photometric variability can be used as the key selection criteria for weak type-I active galactic nuclei (AGN), when other \"direct\" confirmation criteria like X-ray or radio emission are unavailable. However, to detect variability of rather weak AGN powered by intermediate-mass black holes, typical sensitivity provided by existing light curve databases is insufficient. Here we present an algorithm for post-processing of light curves for sources with stochastic variability, retrieved from the The Zwicky Transient Facility (ZTF) Forced Photometry service. Using our approach, we can filter out spurious data points related to data reduction artefacts and also eliminate long-term trends related to imperfect photometric calibration. We can now confidently detect the broad-band variability at the 1-3 $\\%$ level which can potentially be used as a substitute for expensive X-ray follow-up observations.",
                    "In this paper, we present an algorithm to correct optical light curves obtained using The Zwicky Transient Facility Forced Photometry Service and its application to the analysis of optical variability of 136 actvie galactic nuclei (AGN) powered by \"light-weight\" supermassive black holes (SMBH; $M_{BH}$<2*10^6 $\\odot$) including 24 intermediate-mass black holes (IMBH; $M_{BH}$<2*10^5 $\\odot$). We detected variability in nearly all sources and also analyzed its dependence on the X-ray luminosity for 101 objects. We also identified a previously unknown candidate tidal disruption event (TDE) in SDSS~J112637.74+513423.0.",
                    "Intermediate-mass black holes (IMBHs; $M_{BH} <2*10^{5} M_{\\odot}$) in galaxy centers are cruciel for painting a coherent picture of the formation and growth of supermassive black holes (SMBHs). Using Big Data analysis, we identified 305 IMBH candidates for IMBH and 1623 candidates of `light-weight' SMBHs ($2 * 10^{5} M_{odot} < M_{BH} <10^{6} M_{\\odot}$). For 35 host galaxies from this combined sample with the X-ray-confirmed active galactic nuclei (AGN) we collected and analyzed optical spectroscopic observations. These data show that bulge stellar velocity dispersions ($\\sigma_*$) lie in the range of 24$\\dots$118~km/s and do not follow the correlation with $M_{BH}$ established by larger SMBHs indicating that in the $10^{5}-10^{6} M_{\\odot}$ range the accretion is the prevailing BH growth channel.",
                    "Photometric data-driven classification of supernovae becomes a challenge due to the appearance of real-time processing of big data in astronomy. Recent studies have demonstrated the superior quality of solutions based on various machine learning models. These models learn to classify supernova types using their light curves as inputs. Preprocessing these curves is a crucial step that significantly affects the final quality. In this talk, we study the application of multilayer perceptron (MLP), bayesian neural network (BNN), and normalizing flows (NF) to approximate observations for a single light curve. We use these approximations as inputs for supernovae classification models and demonstrate that the proposed methods outperform the state-of-the-art based on Gaussian processes applying to the Zwicky Transient Facility Bright Transient Survey light curves. MLP demonstrates similar quality as Gaussian processes and speed increase. Normalizing Flows exceeds Gaussian processes in terms of approximation quality as well.",
                    "Modern-day time-domain photometric surveys collect a lot of observations of various astronomical objects and the coming era of large-scale surveys will provide even more information on their properties. Spectroscopic follow-ups are especially crucial for transients such as supernovae and most of these objects have not been subject to such studies. }{Flux time series are actively used as an affordable alternative for photometric classification and characterization, for instance, peak identifications and luminosity decline estimations. However, the collected time series are multidimensional and irregularly sampled, while also containing outliers and without any well-defined systematic uncertainties. This paper presents a search for the best-performing methods to approximate the observed light curves over time and wavelength for the purpose of generating time series with regular time steps in each passband.}{We examined several light curve approximation methods based on neural networks such as multilayer perceptrons, Bayesian neural networks, and normalizing flows to approximate observations of a single light curve. Test datasets include simulated PLAsTiCC and real Zwicky Transient Facility Bright Transient Survey light curves of transients.}{The tests demonstrate that even just a few observations are enough to fit the networks and improve the quality of approximation, compared to state-of-the-art models. The methods described in this work have a low computational complexity and are significantly faster than Gaussian processes. Additionally, we analyzed the performance of the approximation techniques from the perspective of further peak identification and transients classification. The study results have been released in an open and user-friendly Fulu Python library available on GitHub for the scientific community.",
                    "The origin of supermassive black holes (SMBH) in galaxy centers still remains uncertain. There are two possible ways of their formation - from massive ($10^5 - 10^6 M_{\\odot}$) and low-mass ($100 M_{\\odot}$) BH nuclei. The latter scenario should leave behind a large number of intermediate mass black holes (IMBH, $10^2 - 10^5 M_{\\odot}$). The largest published sample of bona-fide IMBH-powered AGN contains 10 objects confirmed in X-ray. Here we present a new sample of 15 bona-fide IMBHs, obtained by confirming the optically selected IMBH candidates by the presence of radiation from the galactic nucleus in the X-ray range, which increases the number of confirmed IMBHs at the centers of galaxies by 2.5 times. In the same way, 99 black holes with masses of $2\\cdot10^5 - 10^6 M_{\\odot}$ were confirmed. The sources of X-ray data were publicly available catalogs, archives of data, and our own observations on XMM-Newton, Chandra and Swift. The Eddington coefficients for 30% of the objects from both samples turned out to be close to critical, from 0.5 to 1, which is an unusually high fraction. Also for the first time for light-weight SMBH the correlations between the luminosity in the [OIII] emission line or the broad component of the $H\\alpha$ line and the luminosity in the X-ray range were plotted."
                ],
                "domain": [
                    "Astrophysics",
                    "Machine Learning",
                    "Time-domain Astronomy",
                    "Black Holes"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "85ed665c-7af4-4046-aa9f-0eacca6d4d1f": {
                "pk": "85ed665c-7af4-4046-aa9f-0eacca6d4d1f",
                "project_name": null,
                "name": "Antoine Dumont",
                "bio": "As a researcher in the field of astrophysics, my work primarily focuses on the near-infrared (NIR) emission from low-luminosity active galactic nuclei (LLAGNs). In my recent study, I analyzed a sample of 15 galaxies with detected X-ray emissions and dynamical black hole mass estimates, utilizing Gemini/NIFS integral field spectroscopy data. My findings reveal significant evidence of red continuum components, indicative of hot dust emission, which aligns with observations in higher luminosity AGNs.\n\nI have successfully decomposed spectral data into stellar and continuum components, detecting nuclear thermal emission in 14 out of 15 galaxies. This emission correlates strongly with X-ray flux, suggesting a robust AGN origin. Notably, my research highlights that the NIR emission can often exceed X-ray luminosities in LLAGNs, particularly in cases with low Eddington ratios. This intriguing result raises questions about the mechanisms behind NIR emissions in these faint AGNs, including the potential influence of jet emissions.\n\nI am excited about the implications of my work for future observations, particularly with the James Webb Space Telescope (JWST), which I believe will enhance our understanding of the lowest luminosity AGNs. My goal is to continue exploring the complexities of AGN emissions and their underlying physics, contributing to the broader understanding of galaxy evolution and black hole activity.",
                "collaborators": [
                    "Anil Seth",
                    "Jay Strader",
                    "Jenny E. Greene",
                    "Leonard Burtscher",
                    "Nadine Neumayer"
                ],
                "pub_titles": [
                    "Surprisingly Strong K-band Emission Found in Low Luminosity Active Galactic Nuclei"
                ],
                "pub_abstracts": [
                    "We examine the near-infrared (NIR) emission from low-luminosity AGNs (LLAGNs). Our galaxy sample includes 15 objects with detected 2-10 keV X-ray emission, dynamical black hole mass estimates from the literature, and available Gemini/NIFS integral field spectroscopy (IFU) data. We find evidence for red continuum components at the center of most galaxies, consistent with the hot dust emission seen in higher luminosity AGN. We decompose the spectral data cubes into a stellar and continuum component, assuming the continuum component comes from thermal emission from hot dust. We detect nuclear thermal emission in 14 out of 15 objects. This emission causes weaker CO absorption lines and redder continuum ($2.05-2.28\\:\\mu$m) in our $K$-band data, as expected from hot dust around an AGN. The NIR emission is clearly correlated with the 2-10 keV X-ray flux, with a Spearman coefficient of $r_{spearman}=0.69$ suggesting a $>99\\%$ significance of correlation, providing further evidence of an AGN origin. Our sample has typical X-ray and NIR fluxes $3-4$ orders of magnitude less luminous than previous work studying the NIR emission from AGN. We find that the ratio of NIR to X-ray emission increases towards lower Eddington ratios. The NIR emission in our sample is often brighter than the X-ray emission, with our $K$-band AGN luminosities comparable to or greater than the 2-10 keV X-ray luminosities in all objects with Eddington ratios below $0.01\\%$. The nature of this LLAGN NIR emission remains unclear, with one possibility being an increased contribution from jet emission at these low luminosities. These observations suggest JWST will be a useful tool for detecting the lowest luminosity AGN."
                ],
                "domain": [
                    "Astrophysics",
                    "Active Galactic Nuclei",
                    "Near-Infrared Emission"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "0d664b1c-0e97-42f1-97f7-2c06cada416a": {
                "pk": "0d664b1c-0e97-42f1-97f7-2c06cada416a",
                "project_name": null,
                "name": "Raphael E. Hviding",
                "bio": "I am an astrophysicist with a keen focus on the intersection of machine learning and observational astronomy, particularly in the study of Active Galactic Nuclei (AGNs) and the analysis of data from the James Webb Space Telescope (JWST). My recent work has involved developing improved empirical backgrounds for the NIRISS instrument on JWST, addressing significant spatial artifacts that can impact data quality. I have also pioneered a novel unsupervised machine-learning algorithm to identify AGN candidates, leading to the discovery of previously obscured populations of AGNs.\n\nMy research extends to the detailed spectroscopic analysis of nearby Seyfert galaxies, where I utilize Fabry-Pérot imaging to explore the influence of AGNs on ionized gas structures. I am particularly interested in the implications of my findings for understanding AGN photoionization processes on kiloparsec scales.\n\nIn a more controversial vein, I have challenged the prevailing consensus on exoplanets, proposing a new model involving cuboid stars, or \"squars,\" which can replicate observed stellar light curves without invoking planetary bodies. This work has sparked discussions about the assumptions underlying exoplanet research and the sociopolitical factors influencing astronomical inquiry.\n\nOverall, my research aims to refine our understanding of galaxy formation and evolution while advocating for critical scrutiny in the interpretation of astronomical phenomena. I am committed to leveraging innovative methodologies to enhance our exploration of the universe.",
                "collaborators": [
                    "Kevin N. Hainline",
                    "Ivelina G. Momcheva",
                    "Leonardo Clarke",
                    "Andy D. Goulding",
                    "Jenny E. Greene",
                    "Ryan Hickox",
                    "P. Väisänen",
                    "Rajin Ramphul",
                    "Kevin Hainline",
                    "Charity Woodrum"
                ],
                "pub_titles": [
                    "Improved Empirical Backgrounds for JWST NIRISS Image/WFSS Data Reduction",
                    "Spectroscopic Confirmation of Obscured AGN Populations from Unsupervised Machine Learning",
                    "The Kiloparsec Scale Influence of the AGN in NGC 1068 with SALT RSS Fabry-Pérot Spectroscopy",
                    "A Modest Proposal for the Non-existence of Exoplanets: The Expansion of Stellar Physics to Include Squars",
                    "A New Infrared Criterion for Selecting Active Galactic Nuclei to Lower Luminosities"
                ],
                "pub_abstracts": [
                    "The Near Infrared Imager and Slitless Spectrograph (NIRISS) on the James Webb Space Telescope (JWST) is a versatile instrument for collecting imaging and wide-field slitless spectroscopy (WFSS) data for surveys of galaxy clusters, emission-line galaxies, stellar populations, and more. Dispersed zodiacal light imprints distinct structures on space-based near-infrared imaging and WFSS observations, necessitating careful subtraction during JWST NIRISS data reduction. As of 2024-09-24 NIRISS WFSS calibration backgrounds introduce significant spatially-dependent artifacts, up to 5% of the overall background level, which can severely affect data quality and following astronomical analysis. Notably, there are no existing backgrounds for NIRISS imaging data which also show systematic artifacts, such as the `light saber' effect. In this work, we present improved empirical JWST NIRISS imaging and WFSS backgrounds derived from all available public data in the F115W, F150W, and F200W filters. We demonstrate that our empirical backgrounds provide a more accurate representation of the background structure in NIRISS imaging and WFSS data than existing reference files, mitigating the impact of spatially-dependent artifacts. Our empirical backgrounds are publicly available and can be used to improve the quality of JWST NIRISS imaging and WFSS data reduction.",
                    "We present the result of a spectroscopic campaign targeting Active Galactic Nucleus (AGN) candidates selected using a novel unsupervised machine-learning (ML) algorithm trained on optical and mid-infrared (mid-IR) photometry. AGN candidates are chosen without incorporating prior AGN selection criteria and are fainter, redder, and more numerous, $\\sim$340 AGN deg$^{-2}$, than comparable photometric and spectroscopic samples. In this work we obtain 178 rest-optical spectra from two candidate ML-identified AGN classes with the Hectospec spectrograph on the MMT Observatory. We find that our first ML-identified group, is dominated by Type I AGNs (85%) with a $<3$% contamination rate from non-AGNs. Our second ML-identified group is comprised mostly of Type II AGNs (65%) with a moderate contamination rate of 15% primarily from star-forming galaxies. Our spectroscopic analyses suggest that the classes recover more obscured AGNs, confirming that ML techniques are effective at recovering large populations of AGNs at high levels of extinction. We demonstrate the efficacy of pairing existing WISE data with large-area and deep optical/near-infrared photometric surveys to select large populations of AGNs and recover obscured SMBH growth. This approach is well suited to upcoming photometric surveys, such as Euclid, Rubin, and Roman.",
                    "We present Fabry-P\\'erot (FP) imaging and longslit spectroscopy of the nearby Seyfert II galaxy NGC 1068 using the Robert Stobie Spectrograph (RSS) on the Southern African Large Telescope (SALT) to observe the impact of the central Active Galactic Nucleus (AGN) on the ionized gas in the galaxy on kiloparsec scales. With SALT RSS FP we are able to observe the H$\\alpha$+[N II] emission line complex over a $\\sim$2.6 arcmin$^2$ field of view. Combined with the longslit observation, we demonstrate the efficacy of FP spectroscopy for studying nearby Type II Seyfert galaxies and investigate the kiloparsec-scale ionized gas in NGC 1068. We confirm the results of previous work from the TYPHOON/Progressive Integral Step Method (PrISM) survey that the kiloparsec-scale ionized features in NGC 1068 are driven by AGN photoionization. We analyze the spatial variation of the AGN intensity to put forward an explanation for the shape and structure of the kiloparsec-scale ionization features. Using a toy model, we suggest the ionization features may be understood as a light-echo from a burst of enhanced AGN activity $\\sim$2000 years in the past.",
                    "The search for exoplanets has become a focal point of astronomical research, captivating public attention and driving scientific inquiry; however, the rush to confirm exoplanet discoveries has often overlooked potential alternative explanations leading to a scientific consensus that is overly reliant on untested assumptions and limited data. We argue that the evidence in support of exoplanet observation is not necessarily definitive and that alternative interpretations are not only possible, but necessary. Our conclusion is therefore concise: exoplanets do not exist. Here, we present the framework for a novel type of cuboid star, or squar, which can precisely reproduce the full range of observed phenomena in stellar light curves, including the trapezoidal flux deviations (TFDs) often attributed to \"exoplanets.\" In this discovery paper, we illustrate the power of the squellar model, showing that the light curve of the well-studied \"exoplanet\" WASP-12b can be reconstructed simply from a rotating squar with proportions $1:1/8:1$, without invoking ad-hoc planetary bodies. Our findings cast serious doubt on the validity of current \"exoplanetary\" efforts, which have largely ignored the potential role of squars and have instead blindly accepted the exoplanet hypothesis without sufficient critical scrutiny. In addition, we discuss the sociopolitical role of climate change in spurring the current exoplanet fervor which has lead to the speculative state of \"exoplanetary science\" today. We strongly urge the astronomical community to take our model proposal seriously and treat its severe ramifications with the utmost urgency to restore rationality to the field of astronomy.",
                    "We present a spectroscopic and photometric analysis of a sample of 416,288 galaxies from the Sloan Digital Sky Survey (SDSS) matched to mid-infrared (mid-IR) data from the Wide-Field Infrared Survey Explorer (WISE). By using a new spectroscopic fitting package, GELATO (Galaxy/AGN Emission Line Analysis TOol), we are able to retrieve emission line fluxes and uncertainties for SDSS spectra and robustly determine the presence of broad lines and outflowing components, enabling us to investigate WISE color space as a function of optical spectroscopic properties. In addition, we pursue SED template fitting to assess the relative AGN contribution and nuclear obscuration to compare to existing mid-IR selection criteria with WISE. We present a selection criterion in mid-IR color space to select Active Galactic Nuclei (AGNs) with a $\\sim$80% accuracy and a completeness of $\\sim$16%. This is the first mid-IR color selection defined by solely using the distribution of Type I and Type II optical spectroscopic AGNs in WISE mid-IR color space. Our selection is an improvement of $\\sim$50% in the completeness of targeting spectroscopic AGNs with WISE down to an SDSS $r<17.77$ mag. In addition, our new criterion targets a less luminous population of AGNs, with on average lower [O III] luminosities by $\\sim$30% ($>0.1$ dex) compared to typical WISE color-color selections. With upcoming large photometric surveys without corresponding spectroscopy, our method presents a way to select larger populations of AGNs at lower AGN luminosities and higher nuclear obscuration levels than traditional mid-IR color selections."
                ],
                "domain": [
                    "Astrophysics",
                    "Active Galactic Nuclei",
                    "Machine Learning",
                    "Exoplanets"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b36f4f86-c428-4756-ae8f-6f66e466e7fe": {
                "pk": "b36f4f86-c428-4756-ae8f-6f66e466e7fe",
                "project_name": null,
                "name": "K. Angelique Kahle",
                "bio": "I am a researcher dedicated to unraveling the physical and chemical processes that govern the formation of low-mass stars, with a particular focus on the early stages of star formation. My work involves analyzing emission and absorption lines from various molecules to gain insights into the kinematics and chemistry surrounding protostars. Recently, I have conducted detailed studies on the well-known low-mass binary protostar IRAS 16293-2422 and the prestellar core 16293E, utilizing the LAsMA heterodyne array on the APEX telescope to map their molecular environments. \n\nThrough this research, I identified numerous molecular transitions and derived critical physical parameters, such as kinetic temperatures and volume densities, which have deepened our understanding of the interactions between outflows and surrounding material. Additionally, I have explored the Lagoon Nebula (M8), where I established an inventory of molecular species across various clumps affected by massive star formation. My findings highlight the impact of massive O- and B-type stars on their environments, revealing significant chemical and thermal variations among the clumps.\n\nOverall, my research aims to bridge the gap between observational data and theoretical models, contributing to our understanding of star formation processes and the evolution of molecular clouds. I am passionate about using advanced observational techniques to explore the complexities of the universe and share my findings with the broader astrophysical community.",
                "collaborators": [
                    "Friedrich Wyrowski",
                    "Karl M. Menten",
                    "Antonio Hernández-Gómez",
                    "Carsten König",
                    "Ivalu Barlach Christensen",
                    "Maitraiyee Tiwari"
                ],
                "pub_titles": [
                    "The molecular environment of the solar-type protostar IRAS 16293-2422",
                    "The effects of stellar feedback on molecular clumps in the Lagoon Nebula (M8)"
                ],
                "pub_abstracts": [
                    "Studying the physical and chemical processes leading to the formation of low-mass stars is crucial for understanding the origin of our Sun and the Solar System. In particular, analyzing the emission and absorption lines from molecules is a fundamental tool to obtain information on the kinematics and chemistry at the very early stages of star formation. In this work we aim to examine the spatial structures and molecular abundances of material surrounding the very well-known low-mass binary protostar IRAS 16293-2422 and the prestellar core 16293E, which are embedded in the Lynds 1689N dark cloud.   We have used the LAsMA heterodyne array installed on the Atacama Pathfinder EXperiment (APEX) 12 meter submillimeter telescope to image a region of about 0.12x0.12pc$^2$ around IRAS 16293-2422 and 16293E and to study their molecular environment covering 45.6GHz in a frequency range from 277GHz to 375GHz.   We have identified 144 transitions from 36 molecular species, including isotopologues. The maps reveal the envelope to have a complex morphology around the cloud cores and the emission peaks known as E1, E2, W1, W2, and HE2, including the outflow structure arising from IRAS 16293-2422. Using several transitions of para-H$_2$CO, we have derived new lower limits for the kinetic temperatures toward IRAS 16293-2422 and the surrounding emission peaks. Based on these temperatures, H$_2$ volume densities and column densities for all detected species were derived around the cloud cores and all emission peaks.   Our new observations further confirm the scenario of an outflow arising from IRAS 16293-2422 interacting with the prestellar core 16293E. We observe a large-scale velocity gradient across the molecular cloud. Furthermore, we see clear chemical differences at the examined positions. The data suggests that emission peak W2 may be related to a colder dust source.",
                    "The Lagoon Nebula (M8) is host to multiple regions with recent and ongoing massive star formation. With M8-Main and M8 East, two prominent regions of massive star formation have been studied in detail over the past years, while large parts of the nebula have received little attention. These largely unexplored regions comprise a large sample of molecular clumps that are affected by the presence of massive O- and B-type stars. We establish an inventory of species observed towards 37 known molecular clumps in M8 by conducting an unbiased line survey for each clump. For this, we used APEX and the IRAM 30m telescope for pointed on-off observations on the clumps. These observations cover bandwidths of 53GHz and 40GHz in frequency ranges from 210GHz to 280GHz and from 70GHz to 117GHz, respectively. Temperatures are derived from rotational transitions of CH3CN, CH3C2H and para-H2CO. Additional archival data from the Spitzer, Herschel, MSX, APEX, WISE, JCMT and AKARI telescopes are used to derive physical parameters of the dust emission by fitting spectral energy distributions to the observed flux densities. Across the observed M8 region, we identify 346 transitions from 70 different molecular species, including isotopologues. We detect tracers of photo-dissociation regions across all the clumps and 38% of these clumps show signs of star formation. We find that PDR tracers are most abundant in clumps with relatively lower H2 column densities. When comparing M8 clumps to ATLASGAL sources at similar distances, we find them to be slightly less massive and have compatible luminosities and radii. This possibly indicates a fragmentation of the gas caused by the O- and B-type stars. In contrast, dust temperatures of the clumps in M8 are found to be increased by approximately 5K (25%) indicating substantial external heating of the clumps by radiation of the present massive stars."
                ],
                "domain": [
                    "Astrophysics",
                    "Star Formation",
                    "Molecular Spectroscopy",
                    "Interstellar Medium"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "62409d47-96e1-438c-b6af-a96d6641641f": {
                "pk": "62409d47-96e1-438c-b6af-a96d6641641f",
                "project_name": null,
                "name": "Nadiia Pulatova",
                "bio": "I am an astrophysicist with a keen focus on the spectral analysis of quasars and the morphological properties of galaxies. My recent work involves a systematic examination of XMM-Newton spectra from nearby optically bright quasars, where I characterized their X-ray spectral properties across the 0.1 - 10 keV energy band. This research revealed intriguing insights, such as the prevalence of steep spectra among radio-loud quasars and the detection of Fe K-alpha lines, contributing to our understanding of quasar classifications.\n\nIn addition to my work on quasars, I have extensively analyzed morphological parameters of galaxies from the VIPERS survey, employing metrics like Gini, M20, and Sersic indices to differentiate between elliptical and spiral galaxies. My findings suggest significant cosmological evolution, highlighting a greater abundance of elliptical galaxies in the modern universe compared to earlier epochs.\n\nMoreover, I developed a novel approach to separate the spectral contributions of the AGN NGC 1275 from the surrounding Perseus cluster emissions without relying on complex spectral fitting models. This method leverages spatial resolution and background subtraction techniques to yield cleaner spectra and light curves, enhancing our understanding of AGN behavior in dense environments.\n\nThrough these diverse research endeavors, I aim to deepen our comprehension of the intricate relationships between active galactic nuclei, their host galaxies, and the broader cosmic environment.",
                "collaborators": [
                    "Lidiia Zadorozhna",
                    "Anatolii Tugay",
                    "Olexandr Gugnin",
                    "Oleh Maluy",
                    "Elena Fedorova",
                    "Alexander Ganz"
                ],
                "pub_titles": [
                    "X-ray spectral features and classification of selected QSOs",
                    "Advanced morphology of VIPERS galaxies",
                    "Separating the spectral counterparts in NGC 1275/Perseus cluster in X-rays"
                ],
                "pub_abstracts": [
                    "We present the results of a systematic analysis of the XMM-Newton spectra of nearby optically bright QSOs. The objects have been selected from X-ray Galaxy Catalog Xgal20. It is a catalog of 1172 manually identified and classified galaxies, obtained as a cross-correlation between the 4XMM-DR9 catalog and the Hyper-Linked Extragalactic Databases and Archives (HyperLeda) with an X-ray flux greater than 1E-13 erg/cm^2/s.   The goal of this work is to characterize the X-ray spectral properties of selected QSOs in the 0.1 - 10 keV energy band. The majority of the sources (6 out of 11), are classified as radio-quiet QSOs. We studied optical spectra, hardness ratios and performed X-ray spectral fits for the 10 brighter sources. In most cases, the power law model with absorption is good enough to simulate observed continua. Although the details of the spectrum in some sources significantly complicate the model for fitting. The majority of sources have steep spectra Gamma > 2.1. Extremely steep photon index 2.4 - 2.5 in our sample occurs for three radio-loud type I quasars. We detected Fe K-alpha line for two radio-loud type II quasars. We find no strong evidence for spectral hardening above 2 keV neither for quasars of type I nor for obscured type II. For each quasar its type was established both based on the features and details of observed X-ray spectrum and previous data.",
                    "We calculated morphological parameters for 70821 galaxies from VIPERS (spectroscopic galaxy survey performed on VIMOS spectroscope at VLT). These parameters includes Gini, M20, Concentration, Asymmetry and Smoothness. Results correlate with the distribution of these parameters for other simulated and observed samples. We also studied dependence of these parameters with Sersic power index of radial distribution of surface brightness of galaxy image. Our aim was to find a clear separation of VIPERS galaxies on elliptical and spiral. This is necessary for testing the method of Sersic index (ns) calculation in statmorph program. To find such bimodality we use B-V color index from VIPERS database.   To perform the error analysis of morphological parameters we simulated galaxy images with random background of different magnitude and estimated the errors as dispersion of the parameters. We also found asymptotic values of errors of morphological parameters by increasing the numbers of mock images.   To analyse the possible variation of each morphological parameter during the convolution of close galactic images, we have simulated them to research. In the result of this investigation we have analysed the dependence of the every morphological parameter from CAS and Gini/M20 statistics from the distance between galactic centers.   The differences between our results for VIPERS and Gini-M20 distribution for PanStarrs galaxies at z<0.5 could be explained it by cosmological evolution of galaxies. We found out that in modern Universe there are much more elliptical galaxies than at z>0.5 which corresponds to VIPERS sample. Also we concluded that galaxy mergers were more frequent in the early Universe.",
                    "We develop the recipe to separate the spectral counterparts of the AGN NGC 1275 from the emission of the Perseus cluster surrounding it in the spectra observed by Suzaku/XIS cameras with no usage of the spectral fitting models. The Perseus cluster emission reaches higher energies than is typical for the most AGN-situated dense surroundings (i.e. up to 9-10 keV). That is why the separation between the AGN and cluster spectra is especially important in this case. To avoid the degeneracy due to the huge quantity of the spectral fitting parameters such as abundances of elements the cluster consists of, thermal and Compton emission of the nucleus itself, and the jet SSC/IC emission spectral parameters as well we prefer to avoid the spectral fitting usage to perform this task. Instead, we use the spatial resolution of the components and double background subtracting. For this purpose we choose the following regions to collect all the photons from them: (1) circular or square-shaped region around the source (AGN); (2) ring-shaped (or non-overlapped square) region surrounding the AGN (for cluster); (3) remote empty circular region for the background. Having collected the photons from those regions we subtract the background (i.e. photons from the third region) from the source and cluster spectra. Next, we subtract the re-normalized cluster counts from the AGN spectrum; using the relation between the emission line amplitudes in the AGN and cluster spectra as the renormalization coefficient. We have performed this procedure on the whole set of the Suzaku/XIS observational data for NGC 1275 to obtain the cleaned spectra and light curve of the AGN emission in this system."
                ],
                "domain": [
                    "Astrophysics",
                    "X-ray Astronomy",
                    "Galaxy Morphology",
                    "Quasars"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f058bff9-d54d-45af-8b82-ebc957cd52e9": {
                "pk": "f058bff9-d54d-45af-8b82-ebc957cd52e9",
                "project_name": null,
                "name": "Bhavesh Rajpoot",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures and exploring innovative solutions.\n\nOne of my notable contributions is the development of Position-aware GNNs (P-GNNs), which effectively capture the positional context of nodes within a graph, significantly improving performance in tasks like link prediction and community detection. I also introduced Identity-aware GNNs (ID-GNNs), which enhance the expressive power of message passing by incorporating node identities, leading to substantial accuracy improvements across various prediction tasks.\n\nRecognizing the challenges posed by dynamic graphs, I proposed the ROLAND framework, which allows for the seamless adaptation of static GNNs to dynamic environments, ensuring scalability and efficiency. My research also delves into the architectural design space of GNNs, where I systematically explored over 315,000 designs to provide guidelines for optimizing GNN performance across different tasks.\n\nIn addition to my work on GNNs, I have ventured into automated machine learning (AutoML) with methods like FALCON and AutoTransfer, which aim to streamline the search for optimal model designs while leveraging prior knowledge to enhance efficiency.\n\nOverall, my research is driven by a passion for pushing the boundaries of what GNNs can achieve, fostering a deeper understanding of their structures, and making them more accessible and effective for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "2ec33277-0b55-47a3-bbad-240c07cf7de1": {
                "pk": "2ec33277-0b55-47a3-bbad-240c07cf7de1",
                "project_name": null,
                "name": "Marten B. Scheuck",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge and enhancing search efficiency.\n\nThrough these contributions, I strive to bridge the gap between theoretical advancements and practical applications, ultimately pushing the boundaries of what GNNs can achieve in real-world scenarios.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "849c5074-e85c-4de4-b6d7-51e734f1fd98": {
                "pk": "849c5074-e85c-4de4-b6d7-51e734f1fd98",
                "project_name": null,
                "name": "Rhys Seeburger",
                "bio": "I am an astrophysicist dedicated to unraveling the complexities of stellar systems, particularly focusing on binary stars and their interactions with dark companions, including black holes. My recent work has centered on developing innovative techniques for spectral disentangling, enabling the analysis of large spectroscopic survey data to recover individual stellar spectra from binary systems. This has allowed me to explore the properties of massive stars and their companions, revealing insights into their evolutionary paths.\n\nIn my research, I have employed advanced methods to estimate the ages of stellar populations, utilizing Gaia data to identify co-natal groups and their dispersal into the Galactic field. My investigations into binary systems, such as the \"Unicorn\" and \"Giraffe,\" have provided critical evidence for the existence of mass-gap black holes, showcasing the intricate dynamics of these systems and their implications for binary evolution.\n\nI have also contributed to the discovery of dormant black hole candidates, such as Gaia BH2, which has expanded our understanding of black hole demographics in the Milky Way. My work emphasizes the importance of combining spectroscopic and photometric data to constrain the properties of these elusive objects. Through my research, I aim to shed light on the formation and evolution of binary systems, ultimately enhancing our understanding of stellar physics and Galactic evolution.",
                "collaborators": [
                    "Hans-Walter Rix",
                    "Kareem El-Badry",
                    "Charlie Conroy",
                    "Antonio C. Rodriguez",
                    "Eliot Quataert",
                    "Keith Hawkins",
                    "Katelyn Breivik",
                    "Sahar Shahaf",
                    "Kevin B. Burdge",
                    "Dolev Bashi"
                ],
                "pub_titles": [
                    "Autonomous Disentangling for Spectroscopic Surveys",
                    "The Age Distribution of Stellar Orbit Space Clumps",
                    "Unicorns and Giraffes in the binary zoo: stripped giants with subgiant companions",
                    "A red giant orbiting a black hole",
                    "A Sun-like star orbiting a black hole"
                ],
                "pub_abstracts": [
                    "A suite of spectroscopic surveys is producing vast sets of stellar spectra with the goal of advancing stellar physics and Galactic evolution by determining their basic physical properties. A substantial fraction of these stars are in binary systems, but almost all large-survey modeling pipelines treat them as single stars. For sets of multi-epoch spectra, spectral disentangling is a powerful technique to recover or constrain the individual components' spectra of a multiple system. So far, this approach has focused on small samples or individual objects, usually with high resolution ($R \\gtrsim 10.000$) spectra and many epochs ($\\gtrsim 8$). Here, we present a disentangling implementation that accounts for several aspects of few-epoch spectra from large surveys: that vast sample sizes require automatic determination of starting guesses; that some of the most extensive spectroscopic surveys have a resolution of only $\\approx 2,000$; that few epochs preclude unique orbit fitting; that one needs effective regularisation of the disentangled solution to ensure resulting spectra are smooth. We describe the implementation of this code and show with simulated spectra how well spectral recovery can work for hot and cool stars at $R \\approx 2000$. Moreover, we verify the code on two established binary systems, the ``Unicorn'' and ``Giraffe''. This code can serve to explore new regimes in survey disentangling in search of massive stars with massive dark companions, e.g. the $\\gtrsim 200,000$ hot stars of the SDSS-V survey.",
                    "The orbit distribution of young stars in the Galactic disk is highly structured, from well-defined clusters to streams of stars that may be widely dispersed across the sky, but are compact in orbital action-angle space. The age distribution of such groups can constrain the timescales over which co-natal groups of stars disperse into the `field'. Gaia data have proven powerful to identify such groups in action-angle space, but the resulting member samples are often too small and have too narrow a CMD coverage to allow robust age determinations. Here, we develop and illustrate a new approach that can estimate robust stellar population ages for such groups of stars. This first entails projecting the predetermined action-angle distribution into the 5D space of positions, parallaxes and proper motions, where much larger samples of likely members can be identified over a much wider range of the CMD. It then entails isochrone fitting that accounts for a) widely varying distances and reddenings; b) outliers and binaries; c) sparsely populated main sequence turn-offs, by incorporating the age information of the low-mass main sequence; and d) the possible presence of an intrinsic age spread in the stellar population. When we apply this approach to 92 nearby stellar groups identified in 6D orbit space, we find that they are predominately young ($\\lesssim 1$ Gyr), mono-age populations. Many groups are established (known) localized clusters with possible tidal tails, others tend to be widely dispersed and manifestly unbound. This new age-dating tool offers a stringent approach to understanding on which orbits stars form in the solar neighborhood and how quickly they disperse into the field.",
                    "We analyze two binary systems containing giant stars, V723 Mon (\"the Unicorn\") and 2M04123153+6738486 (\"the Giraffe\"). Both giants orbit more massive but less luminous companions, previously proposed to be mass-gap black holes. Spectral disentangling reveals luminous companions with star-like spectra in both systems. Joint modeling of the spectra, light curves, and spectral energy distributions robustly constrains the masses, temperatures, and radii of both components: the primaries are luminous, cool giants ($T_{\\rm eff,\\,giant} = 3,800\\,\\rm K$ and $4,000\\,\\rm K$, $R_{\\rm giant}= 22.5\\,R_{\\odot}$ and $25\\,R_{\\odot}$) with exceptionally low masses ($M_{\\rm giant} \\approx 0.4\\,M_{\\odot}$) that likely fill their Roche lobes. The secondaries are only slightly warmer subgiants ($T_{\\rm eff,\\,2} = 5,800\\,\\rm K$ and $5,150\\,\\rm K$, $R_2= 8.3\\,R_{\\odot}$ and $9\\,R_{\\odot}$) and thus are consistent with observed UV limits that would rule out main-sequence stars with similar masses ($M_2 \\approx 2.8\\,M_{\\odot}$ and $\\approx 1.8\\,M_{\\odot}$). In the Unicorn, rapid rotation blurs the spectral lines of the subgiant, making it challenging to detect even at wavelengths where it dominates the total light. Both giants have surface abundances indicative of CNO processing and subsequent envelope stripping. The properties of both systems can be reproduced by binary evolution models in which a $1-2\\,M_{\\odot}$ primary is stripped by a companion as it ascends the giant branch. The fact that the companions are also evolved implies either that the initial mass ratio was very near unity, or that the companions are temporarily inflated due to rapid accretion. The Unicorn and Giraffe offer a window into into a rarely-observed phase of binary evolution preceding the formation of wide-orbit helium white dwarfs, and eventually, compact binaries containing two helium white dwarfs.",
                    "We report spectroscopic and photometric follow-up of a dormant black hole (BH) candidate from Gaia DR3. The system, which we call Gaia BH2, contains a $\\sim 1M_{\\odot}$ red giant and a dark companion with mass $M_2 = 8.9\\pm 0.3\\,M_{\\odot}$ that is very likely a BH. The orbital period, $P_{\\rm orb} = 1277$ days, is much longer than that of any previously studied BH binary. Our radial velocity (RV) follow-up over a 7-month period spans more than 90% of the orbit's dynamic range in RV and is in excellent agreement with predictions of the Gaia solution. UV imaging and high-resolution optical spectra rule out all plausible luminous companions that could explain the orbit. The star is a bright ($G=12.3$), slightly metal-poor ($\\rm [Fe/H]=-0.22$) low-luminosity giant ($T_{\\rm eff}=4600\\,\\rm K$; $R = 7.8\\,R_{\\odot}$; $\\log\\left[g/\\left({\\rm cm\\,s^{-2}}\\right)\\right] = 2.6$). The binary's orbit is moderately eccentric ($e=0.52$). The giant is strongly enhanced in $\\alpha-$elements, with $\\rm [\\alpha/Fe] = +0.26$, but the system's Galactocentric orbit is typical of the thin disk. We obtained X-ray and radio nondetections of the source near periastron, which support BH accretion models in which the net accretion rate at the horizon is much lower than the Bondi-Hoyle-Lyttleton rate. At a distance of 1.16 kpc, Gaia BH2 is the second-nearest known BH, after Gaia BH1. Its orbit -- like that of Gaia BH1 -- seems too wide to have formed through common envelope evolution. Gaia BH1 and BH2 have orbital periods at opposite edges of the Gaia DR3 sensitivity curve, perhaps hinting at a bimodal intrinsic period distribution for wide BH binaries. Dormant BH binaries like Gaia BH1 and Gaia BH2 likely significantly outnumber their close, X-ray bright cousins, but their formation pathways remain uncertain.",
                    "We report discovery of a bright, nearby ($G = 13.8;\\,\\,d = 480\\,\\rm pc$) Sun-like star orbiting a dark object. We identified the system as a black hole candidate via its astrometric orbital solution from the Gaia mission. Radial velocities validated and refined the Gaia solution, and spectroscopy ruled out significant light contributions from another star. Joint modeling of radial velocities and astrometry constrains the companion mass to $M_2 = 9.62\\pm 0.18\\,M_{\\odot}$. The spectroscopic orbit alone sets a minimum companion mass of $M_2>5\\,M_{\\odot}$; if the companion were a $5\\,M_{\\odot}$ star, it would be $500$ times more luminous than the entire system. These constraints are insensitive to the mass of the luminous star, which appears as a slowly-rotating G dwarf ($T_{\\rm eff}=5850\\,\\rm K$, $\\log g = 4.5$, $M=0.93\\,M_{\\odot}$), with near-solar metallicity ($\\rm [Fe/H] = -0.2$) and an unremarkable abundance pattern. We find no plausible astrophysical scenario that can explain the orbit and does not involve a black hole. The orbital period, $P_{\\rm orb}=185.6$ days, is longer than that of any known stellar-mass black hole binary. The system's modest eccentricity ($e=0.45$), high metallicity, and thin-disk Galactic orbit suggest that it was born in the Milky Way disk with at most a weak natal kick. How the system formed is uncertain. Common envelope evolution can only produce the system's wide orbit under extreme and likely unphysical assumptions. Formation models involving triples or dynamical assembly in an open cluster may be more promising. This is the nearest known black hole by a factor of 3, and its discovery suggests the existence of a sizable population of dormant black holes in binaries. Future Gaia releases will likely facilitate the discovery of dozens more."
                ],
                "domain": [
                    "Stellar Astrophysics",
                    "Binary Systems",
                    "Spectroscopy",
                    "Galactic Evolution"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "e818c1a3-3946-4788-a180-fbf78872ec51": {
                "pk": "e818c1a3-3946-4788-a180-fbf78872ec51",
                "project_name": null,
                "name": "Dmitry Semenov",
                "bio": "I am an astrophysicist specializing in the study of protoplanetary disks (PPDs) and their role in the formation of planetary systems. My research focuses on the intricate chemical and physical processes occurring within these disks, where gas and dust evolve into the building blocks of planets. I have explored various aspects of disk chemistry, including the formation of complex organic molecules and the impact of environmental conditions on chemical evolution.\n\nIn my recent work, I have investigated the chemical differentiation of complex organic molecules (COMs) in regions around Sgr B2, utilizing Monte Carlo simulations to understand the spatial distribution of these species under varying physical conditions. I have also contributed to the understanding of molecular deuteration as a tool for reconstructing the history of our Solar System, linking observations from pre-stellar cores to meteorites.\n\nMy studies emphasize the importance of accurate astrochemical models, particularly in understanding the role of CO as a gas mass tracer in protoplanetary disks. I have demonstrated that while CO is a reliable indicator of gas mass, alternative tracers like CO₂ and H₂O can also provide valuable insights when disk parameters are known.\n\nThrough my research, I aim to bridge observational data with theoretical models, paving the way for future investigations into the chemical processes that govern the birth of planetary systems. I am excited about the potential of upcoming observational facilities, such as the Atacama Large Millimeter Array (ALMA), to further enhance our understanding of these fascinating cosmic environments.",
                "collaborators": [
                    "Thomas Henning",
                    "Yaroslav Pavluchenkov",
                    "Eric Herbst",
                    "Ewine van Dishoeck",
                    "Matjaž Simončič",
                    "Serge Krasnokutski",
                    "Cornelia Jäger",
                    "Yao Wang",
                    "Fujun Du",
                    "Hongchi Wang"
                ],
                "pub_titles": [
                    "Chemistry in Protoplanetary Disks",
                    "Chemistry in Protoplanetary Disks",
                    "The birth and death of organic molecules in protoplanetary disks",
                    "On the Feasibility of Disk Chemical Modeling",
                    "Sensitivity of gas-grain chemical models to surface reaction barriers: Effect from a key carbon-insertion reaction, C + H$_2$ $\\rightarrow$ CH$_2$",
                    "Chemical modeling of the complex organic molecules in the extended region around Sagittarius B2",
                    "Deuterium Fractionation: the Ariadne's Thread from the Pre-collapse Phase to Meteorites and Comets today",
                    "Gas mass tracers in protoplanetary disks: CO is still the best"
                ],
                "pub_abstracts": [
                    "Protoplanetary disks (PPDs) surrounding young stars are short-lived (~0.3-10 Myr), compact (~10-1000 AU) rotating reservoirs of gas and dust. PPDs are believed to be birthplaces of planetary systems, where tiny grains are assembled into pebbles, then rocks, planetesimals, and eventually planets, asteroids, and comets. Strong variations of physical conditions (temperature, density, ionization rate, UV/X-rays intensities) make a variety of chemical processes active in disks, producing simple molecules in the gas phase and complex polyatomic (organic) species on the surfaces of dust particles. In this entry, we summarize the major modern observational methods and theoretical paradigms used to investigate disk chemical composition and evolution, and present the most important results. Future research directions that will become possible with the advent of the Atacama Large Millimeter Array (ALMA) and other forthcoming observational facilities are also discussed.",
                    "This comprehensive review summarizes our current understanding of the evolution of gas, solids and molecular ices in protoplanetary disks. Key findings related to disk physics and chemistry, both observationally and theoretically, are highlighted. We discuss which molecular probes are used to derive gas temperature, density, ionization state, kinematics, deuterium fractionation, and study organic matter in protoplanetary disks.",
                    "The most intriguing question related to the chemical evolution of protoplanetary disks is the genesis of pre-biotic organic molecules in the planet-forming zone. In this contribution we briefly review current observational knowledge of physical structure and chemical composition of disks and discuss whether organic molecules can be present in large amounts at the verge of planet formation. We predict that some molecules, including CO-bearing species such as H$_2$CO, can be underabundant in inner regions of accreting protoplanetary disks around low-mass stars due to the high-energy stellar radiation and chemical processing on dust grain surfaces. These theoretical predictions are further compared with high-resolution observational data and the limitations of current models are discussed.",
                    "In this paper, we compare the results of the modeling of a protoplanetary disk chemical evolution obtained with the UMIST95 and ``New Standard Model'' (NSM) chemical databases. Assuming the same initial conditions, it is found that the substitution of one chemical network by another causes almost no difference for the disk ionization degree. In contrast, the NSM and UMIST95 abundances of CO can differ by a factor of a hundred at some regions of the disk surface. However, relevant CO vertical column densities differ much less, at most by a factor of a few. In addition, we synthesize the single-dish CO(J=3-2) line by means of the 2D line radiative transfer for both considered chemical networks. It is shown that the intensity of this line in the case of the UMIST95 abundances is lower compared to the NSM case by about 15%.",
                    "The feasibility of contemporary gas-grain astrochemical models depends on the availability of accurate kinetics data, in particular, for surface processes. We study the sensitivity of gas-grain chemical models to the energy barrier Ea of the important surface reaction between some of the most abundant species: C and H2 (surface C + surface H2 = surface CH2). We used the gas-grain code ALCHEMIC to model the time-dependent chemical evolution over a 2D grid of densities (nH: 10^{3} - 10^{12}cm^{-3}) and temperatures (T: 10 - 300 K), assuming UV-dark (Av = 20 mag) and partly UV-irradiated (Av = 3 mag) conditions that are typical of the dense interstellar medium. We considered two values for the energy barrier of the surface reaction, Ea = 2500 K (as originally implemented in the networks) and Ea = 0 K (as measured in the laboratory and computed by quantum chemistry simulations). We find that if the C + H2 = CH2 surface reaction is barrierless, a more rapid conversion of the surface carbon atoms into methane ice occurs. Overproduction of the CHn hydrocarbon ices affects the surface formation of more complex hydrocarbons, cyanides and nitriles, and CS-bearing species at low temperatures < 10-15 K. The surface hydrogenation of CO and hence the synthesis of complex (organic) molecules become affected as well. As a result, important species whose abundances may change by more than a factor of two at 1 Myr include atomic carbon, small mono-carbonic (C1) and di-carbonic (C2) hydrocarbons, CO2, CN, HCN, HNC, HNCO, CS, H2CO, H2CS, CH2CO, and CH3OH (in either gas and/or ice). The abundances of key species, CO, H2O, and N2 as well as O, HCO+, N2H+, NH3, NO, and most of the S-bearing molecules, remain almost unaffected. Further accurate laboratory measurements and quantum chemical calculations of the surface reaction barriers will be crucial to improve the accuracy of astrochemical models.",
                    "The chemical differentiation of seven COMs in the extended region around Sgr B2 has been observed: CH$_2$OHCHO, CH$_3$OCHO, t-HCOOH, C$_2$H$_5$OH, and CH$_3$NH$_2$ were detected both in the extended region and near the hot cores Sgr B2(N) and Sgr B2(M), while CH$_3$OCH$_3$ and C$_2$H$_5$CN were only detected near the hot cores. The density and temperature in the extended region are relatively low. Different desorption mechanisms have been proposed to explain the observed COMs in cold regions but fail to explain the deficiency of CH$_3$OCH$_3$ and C$_2$H$_5$CN. We explored under what physical conditions the chemical simulations can fit the observations and explain the different spatial distribution of these species. We used the Monte Carlo method to perform a detailed parameter space study. We investigated how different mechanisms affect the results. All gas-grain chemical models based on static physics cannot fit the observations. The results based on evolving physical conditions can fit six COMs when $T\\sim30-60$ K, but the best-fit temperature is still higher than the observed dust temperature of 20 K. The best agreement at $T\\sim27$ K is achieved by considering a short-duration $\\sim 10^2$ yr X-ray burst with $\\zeta_{\\mathrm{CR}}=1.3\\times10^{-13}$ s$^{-1}$ when the temperature is 20 K. The reactive desorption is the key mechanism for producing these COMs and inducing the low abundances of CH$_3$OCH$_3$ and C$_2$H$_5$CN. The evolution of the extended region around Sgr~B2 may have begun with a cold, $T\\le10$ K phase followed by a warm-up phase. When its temperature reached $T\\sim20$ K, an X-ray flare from Sgr A* with a short duration of no more than 100 years was acquired, affecting strongly the Sgr B2 chemistry. The observed COMs retain their observed abundances only several hundred years after such a flare, which could imply that such short-term X-ray flares occur relatively often.",
                    "The Solar System formed about 4.6 billion years ago from a condensation of matter inside a molecular cloud. Trying to reconstruct what happened is the goal of this chapter. For that, we put together our understanding of Galactic objects that will eventually form new suns and planetary systems, with our knowledge on comets, meteorites and small bodies of the Solar System today. Our specific tool is the molecular deuteration, namely the amount of deuterium with respect to hydrogen in molecules. This is the Ariadne's thread that helps us to find the way out from a labyrinth of possible histories of our Solar System. The chapter reviews the observations and theories of the deuterium fractionation in pre-stellar cores, protostars, protoplanetary disks, comets, interplanetary dust particles and meteorites and links them together trying to build up a coherent picture of the history of the Solar System formation. We emphasise the interdisciplinary nature of the chapter, which gathers together researchers from different communities with the common goal of understanding the Solar System history.",
                    "Protoplanetary disk mass is a key parameter controlling the process of planetary system formation. CO molecular emission is often used as a tracer of gas mass in the disk. In this study we consider the ability of CO to trace the gas mass over a wide range of disk structural parameters and search for chemical species that could possibly be used as alternative mass tracers to CO. Specifically, we apply detailed astrochemical modeling to a large set of models of protoplanetary disks around low-mass stars, to select molecules with abundances correlated with the disk mass and being relatively insensitive to other disk properties. We do not consider sophisticated dust evolution models, restricting ourselves with the standard astrochemical assumption of $0.1~\\mu $m dust. We find that CO is indeed the best molecular tracer for total gas mass, despite the fact that it is not the main carbon carrier, provided reasonable assumptions about CO abundance in the disk are used. Typically, chemical reprocessing lowers the abundance of CO by a factor of 3, compared to the case of photo-dissociation and freeze-out as the only ways of CO depletion. On average only 13% C-atoms reside in gas-phase CO, albeit with variations from 2 to 30%. CO$_2$, H$_2$O and H$_2$CO can potentially serve as alternative mass tracers, the latter two being only applicable if disk structural parameters are known."
                ],
                "domain": [
                    "Astrochemistry",
                    "Protoplanetary Disks",
                    "Planet Formation",
                    "Molecular Evolution"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "36825720-4a5e-4d36-9881-787dde3b90ad": {
                "pk": "36825720-4a5e-4d36-9881-787dde3b90ad",
                "project_name": null,
                "name": "Jaime I. Villaseñor",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I am passionate about understanding the broader design space of GNNs. My work on AutoTransfer and FALCON aims to streamline the process of finding optimal model designs by leveraging prior knowledge and efficiently navigating the design space. I believe that by systematically studying these dimensions, we can unlock new potentials in machine learning applications.\n\nOverall, my research is driven by a desire to push the boundaries of GNNs, making them more effective and applicable to real-world challenges, while also contributing to the theoretical understanding of their underlying structures.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively integrate Large Language Models (LLMs) into the research practices of astronomers while addressing their limitations and ensuring reliable outcomes?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it could lead to a paradigm shift in how astronomical research is conducted, enhancing productivity and creativity in data analysis, writing, and idea generation. By effectively leveraging LLMs, researchers could unlock new insights and streamline workflows, ultimately advancing knowledge in the field. Furthermore, this integration could set a precedent for the use of AI in other scientific disciplines, fostering interdisciplinary collaboration and innovation.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in integrating LLMs into astronomical research stem from their unpredictable behavior, such as hallucination and inaccuracies in factual reproduction. Naive approaches that treat LLMs as traditional software may fail due to their inherent limitations in performing repetitive tasks and their tendency to generate unreliable outputs. Additionally, the lack of established frameworks for evaluating the reliability of LLM-generated content in a research context poses a significant obstacle, requiring careful consideration of both technical and theoretical aspects.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on the development of LLMs and their applications in general contexts, with limited attention given to their specific integration into specialized fields like astronomy. Barriers include a lack of understanding of LLMs' unique characteristics and the hesitance of researchers to adopt tools that may compromise the rigor of scientific inquiry. My approach differs by proposing a structured methodology for evaluating and utilizing LLMs in astronomical research, addressing both their strengths and weaknesses in a targeted manner.\n\n**[Question 5] - What are the key components of my approach and results?**  \nMy proposed methodology involves a multi-step process: first, conducting a comprehensive evaluation of LLM outputs against established astronomical datasets to assess accuracy and reliability. Second, developing a framework for integrating LLMs into research workflows, including guidelines for their use in data analysis and writing. Metrics for success will include the accuracy of LLM-generated content, user satisfaction among researchers, and the impact on productivity measured through comparative studies. Expected outcomes include a validated approach for LLM integration, enhanced research efficiency, and a set of best practices for future applications in astronomy and beyond."
    },
    "2409.20506": {
        "paper_data": {
            "title": "Nanosecond hardware regression trees in FPGA at the LHC",
            "url": "http://arxiv.org/abs/2409.20506v1",
            "arxiv_id": "2409.20506",
            "authors": [
                "Pavel Serhiayenka",
                "Stephen Roche",
                "Benjamin Carlson",
                "Tae Min Hong"
            ],
            "abstract": "We present a generic parallel implementation of the decision tree-based machine learning (ML) method in hardware description language (HDL) on field programmable gate arrays (FPGA). A regression problem in high energy physics at the Large Hadron Collider is considered: the estimation of the magnitude of missing transverse momentum using boosted decision trees (BDT). A forest of twenty decision trees each with a maximum depth of ten using eight input variables of 16-bit precision is executed with a latency of less than 10 ns using O(0.1%) resources on Xilinx UltraScale+ VU9P -- approximately ten times faster and five times smaller compared to similar designs using high level synthesis (HLS) -- without the use of digital signal processors (DSP) while eliminating the use of block RAM (BRAM). We also demonstrate a potential application in the estimation of muon momentum for ATLAS RPC at HL-LHC.",
            "introduction": "   1 Introduction  Deep architectures for machine learning (ML) methods continue to empower high energy physics experiments, such as the ATLAS [1] and CMS [2] experiments at the Large Hadron Collider (LHC) [3]. Various simplified approaches for ML designs aimed for trigger systems using field programmable gate arrays (FPGA) exist in the literature. Many use high level synthesis (HLS), where C-like syntax is converted to an RTL circuit using a translator provided by the vendor. The HLS version of neural networks by hls4ml [4] gave rise to a body of work in the implementation of artificial intelligence (AI), such as autoencoders and convolutional neural networks [5, 6], on firmware. Similarly, the HLS version of decision trees [7, 8] by fwXmachina (developed by us) also gave rise to an autoencoder [9]. There are also non-HLS approaches using hardware description language (HDL) for neural networks [10] and decision trees [11], as well as more dedicated applications, e.g., [12, 13].   In this paper, we present a more efficient implementation of decision trees with lower latency. We illustrate the physics potential using the problem in our previous paper [14], the estimation of missing transverse momentum (ETmisssuperscriptsubscript𝐸TmissE_{\\textrm{\\scriptsize T}}^{\\textrm{miss}}italic_E start_POSTSUBSCRIPT T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT miss end_POSTSUPERSCRIPT) at the LHC. ETmisssuperscriptsubscript𝐸TmissE_{\\textrm{\\scriptsize T}}^{\\textrm{miss}}italic_E start_POSTSUBSCRIPT T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT miss end_POSTSUPERSCRIPT is an important signature of physics with minimal interaction with the detector materials, including neutrinos and beyond the Standard Model (BSM) particles, including dark matter [15, 16] and supersymmetry (SUSY) [17, 18, 19]. The HL-LHC [20] upgrades of the trigger systems of ATLAS and CMS experiments [21, 22] provide an opportunity to implement AI/ML with deeper designs for a variety of applications.   This paper is organized as follows. Section 2 describes the ML training and firmware implementation on FPGA. Section 3 presents the results, with comparisons to previous publications. Appendix A gives the technical details of the subscore adder. Appendix B presents a study, following Ref. [23], of the estimation of muon momentum at the High Luminosity LHC (HL-LHC) using hits in the resistive plate chamber (RPC) subdetector in the ATLAS level-0 trigger system.     2 Method  The ML training is described followed by the firmware design. TMVA [24] is used to train a boosted decision tree (BDT) for regression with the “truth” ETmisssuperscriptsubscript𝐸TmissE_{\\textrm{\\scriptsize T}}^{\\textrm{miss}}italic_E start_POSTSUBSCRIPT T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT miss end_POSTSUPERSCRIPT as the target variable. The setup follows our previous publication [8] and uses the data samples described therein.   The firmware design is a VHDL adaptation of the Deep Decision Tree Engine (DDTE) originally written in HLS [8]. In python, fwXmachina program writes VHDL that reflects a given ML configuration from the training. In Figure 1, each tree is represented by a HDL Tree Engine (HTE) and the forest is managed by the HDL Tree Manager. The summing of the subscores is either clocked or combinational; see Appendix A. We note that operations are triggered by the rising edge of the clock, but it may be possible to to remove them for smaller designs and slower clock speeds.   Figure 1:  Block diagram of the VHDL version of the Deep Decision Tree Engine (DDTE). Each tree is represented by HDL Tree Engine (HTE), which are composed",
            "references": [
                {
                    "title": "Search for Nearly Mass-Degenerate Higgsinos Using Low-Momentum Mildly Displaced Tracks in pp Collisions at sqrt[s]=13  TeV with the ATLAS Detector.",
                    "abstract": "Higgsinos with masses near the electroweak scale can solve the hierarchy problem and provide a dark matter candidate, while detecting them at the LHC remains challenging if their mass splitting is O(1  GeV). This Letter presents a novel search for nearly mass-degenerate Higgsinos in events with an energetic jet, missing transverse momentum, and a low-momentum track with a significant transverse impact parameter using 140  fb^{-1} of proton-proton collision data at sqrt[s]=13  TeV collected by the ATLAS experiment. For the first time since LEP, a range of mass splittings between the lightest charged and neutral Higgsinos from 0.3 to 0.9 GeV is excluded at 95% confidence level, with a maximum reach of approximately 170 GeV in the Higgsino mass."
                },
                {
                    "title": "Testing a Neural Network for Anomaly Detection in the CMS Global Trigger Test Crate during Run 3",
                    "abstract": "We present the deployment and testing of an autoencoder trained for unbiased detection of new physics signatures in the CMS Level-1 Global Trigger (GT) test crate during LHC Run 3. The GT test crate is a copy of the main GT system, receiving the same input data, but whose output is not used to trigger the readout of CMS, providing a platform for thorough testing of new trigger algorithms on live data, but without interrupting data taking. We describe the integration of the Neural Network into the GT test crate, and the monitoring, testing, and validation of the algorithm during proton collisions."
                },
                {
                    "title": "Nanosecond anomaly detection with decision trees and real-time application to exotic Higgs decays",
                    "abstract": null
                },
                {
                    "title": "Development of the ATLAS Liquid Argon Calorimeter Readout Electronics and Machine Learning for the HL-LHC",
                    "abstract": "The High Luminosity era of the Large Hadron Collider (LHC) starting in 2029 promises exciting discovery potential, giving unprecedented sensitivity to key new physics models and precise characterization of the Higgs boson. In order to maintain current performance in this challenging environment, the ATLAS liquid argon electromagnetic calorimeter will get entirely new electronics that reads out the entire detector with full precision at the LHC frequency of 40 MHz, and provides high granularity trigger information, while withstanding high operational radiation doses. New results will be presented from both front-end and off-detector component development, along with highlights from machine learning applications. The future steps and outlook of the project will be discussed, with an eye towards installation in the ATLAS cavern beginning in 2026."
                },
                {
                    "title": "Nanosecond machine learning regression with deep boosted decision trees in FPGA for high energy physics",
                    "abstract": "We present a novel application of the machine learning / artificial intelligence method called boosted decision trees to estimate physical quantities on field programmable gate arrays (FPGA). The software package fwXmachina features a new architecture called parallel decision paths that allows for deep decision trees with arbitrary number of input variables. It also features a new optimization scheme to use different numbers of bits for each input variable, which produces optimal physics results and ultraefficient FPGA resource utilization. Problems in high energy physics of proton collisions at the Large Hadron Collider (LHC) are considered. Estimation of missing transverse momentum (ET miss) at the first level trigger system at the High Luminosity LHC (HL-LHC) experiments, with a simplified detector modeled by Delphes, is used to benchmark and characterize the firmware performance. The firmware implementation with a maximum depth of up to 10 using eight input variables of 16-bit precision gives a latency value of 𝒪(10) ns, independent of the clock speed, and 𝒪(0.1)% of the available FPGA resources without using digital signal processors."
                },
                {
                    "title": "Search for invisible Higgs-boson decays in events with vector-boson fusion signatures using 139 fb−1 of proton-proton data recorded by the ATLAS experiment",
                    "abstract": null
                },
                {
                    "title": "Fast muon tracking with machine learning implemented in FPGA",
                    "abstract": null
                },
                {
                    "title": "Development of a resource-efficient FPGA-based neural network regression model for the ATLAS muon trigger upgrades",
                    "abstract": null
                },
                {
                    "title": "Autoencoders on field-programmable gate arrays for real-time, unsupervised new physics detection at 40 MHz at the Large Hadron Collider",
                    "abstract": null
                },
                {
                    "title": "Metallographic analysis of 11 T dipole coils for High Luminosity-Large Hadron Collider (HL-LHC)",
                    "abstract": "For next-generation accelerator magnets for fields beyond those achievable using Nb–Ti, Nb3Sn is the most viable superconductor. The high luminosity upgrade for the Large Hadron Collider (HL-LHC) marks an important milestone as it will be the first project where Nb3Sn magnets will be installed in an accelerator. Nb3Sn is a brittle intermetallic, so magnet coils are typically wound from composite strands containing ductile precursors before heat treating the wire components to form Nb3Sn. However, some mechanical assembly is still required after the coils have been heat-treated. In this paper, we present direct evidence of cracking of the brittle Nb3Sn filaments in a prototype dipole that resulted in degraded magnet performance. The cracking of the Nb3Sn, in this case, can be attributed to an issue with the collaring process that is required in the assembly of dipole accelerator magnets. Metallographic procedures were developed to visualize cracks present in the cables, along with quantitative image analysis for location-based crack analysis. We show that the stresses experienced in the damaged coil are above the critical damage stress of Nb3Sn conductor, as evidenced by a measured Cu stabilizer hardness of 85 HV0.1, which is higher than the Cu stabilizer hardness in a reference Nb3Sn cable ten-stack that was subjected to a 210 MPa transverse compression. We also show that once the collaring procedure issue was rectified in a subsequent dipole, the Nb3Sn filaments were found to be undamaged, and the Cu stabilizer hardness values were reduced to the expected levels. This paper provides a post-mortem verification pathway to analyze the damage, provides strand level mechanical properties, which could be beneficial for improving model prediction capabilities. This method could be applied beyond Nb3Sn magnets to composite designs involving high work hardening materials."
                },
                {
                    "title": "The Phase-2 Upgrade of the CMS Level-1 Trigger",
                    "abstract": "This Technical Design Report describes the ongoing developments and plans towards the upgrade of the CMS Level-1 trigger for the High-Luminosity Large Hadron Collider."
                },
                {
                    "title": "Performance of the missing transverse momentum triggers for the ATLAS detector during Run-2 data taking",
                    "abstract": null
                },
                {
                    "title": "Performance of the ATLAS RPC detector and Level-1 muon barrel trigger at s = 13 TeV",
                    "abstract": "The Level-1 muon trigger system of the ATLAS experiment at the Large Hadron Collider selects muon candidates with six transverse momentum thresholds and associates them with the correct LHC bunch crossing. The barrel region of the ATLAS Muon Spectrometer is instrumented with Resistive Plate Chambers (RPCs), covering the pseudo-rapidity range ∣ η ∣ < 1.05 . The RPC detectors are arranged in three concentric double layers and consist of 3600 gas volumes, with a total surface of more than 4000 m2. This contribution will discuss the performance of the RPC detector and Level-1 Muon Barrel trigger system, using the data from proton-proton collisions recorded during the 2018 data-taking period at a centre-of-mass energy of 13 TeV."
                },
                {
                    "title": "Fast inference of Boosted Decision Trees in FPGAs for particle physics",
                    "abstract": "We describe the implementation of Boosted Decision Trees in the hls4ml library, which allows the translation of a trained model into FPGA firmware through an automated conversion process. Thanks to its fully on-chip implementation, hls4ml performs inference of Boosted Decision Tree models with extremely low latency. With a typical latency less than 100 ns, this solution is suitable for FPGA-based real-time processing, such as in the Level-1 Trigger system of a collider experiment. These developments open up prospects for physicists to deploy BDTs in FPGAs for identifying the origin of jets, better reconstructing the energies of muons, and enabling better selection of rare signal processes."
                },
                {
                    "title": "Searches for electroweak production of supersymmetric particles with compressed mass spectra in √s = 13 TeV pp collisions with the ATLAS detector",
                    "abstract": "This paper presents results of searches for the electroweak production of supersymmetric particles in models with compressed mass spectra. The searches use 139 fb(-1) of root s = 13 TeV proton-prot ..."
                },
                {
                    "title": "Search for invisible decays of a Higgs boson produced through vector boson fusion in proton-proton collisions at $\\sqrt{s} =$ 13 TeV",
                    "abstract": null
                },
                {
                    "title": "Fast inference of deep neural networks in FPGAs for particle physics",
                    "abstract": "Recent results at the Large Hadron Collider (LHC) have pointed to enhanced physics capabilities through the improvement of the real-time event processing techniques. Machine learning methods are ubiquitous and have proven to be very powerful in LHC physics, and particle physics as a whole. However, exploration of the use of such techniques in low-latency, low-power FPGA (Field Programmable Gate Array) hardware has only just begun. FPGA-based trigger and data acquisition systems have extremely low, sub-microsecond latency requirements that are unique to particle physics. We present a case study for neural network inference in FPGAs focusing on a classifier for jet substructure which would enable, among many other physics scenarios, searches for new dark sector particles and novel measurements of the Higgs boson. While we focus on a specific example, the lessons are far-reaching. A companion compiler package for this work is developed based on High-Level Synthesis (HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS increases accessibility across a broad user community and allows for a drastic decrease in firmware development time. We map out FPGA resource usage and latency versus neural network hyperparameters to identify the problems in particle physics that would benefit from performing neural network inference with FPGAs. For our example jet substructure model, we fit well within the available resources of modern FPGAs with a latency on the scale of 100 ns."
                },
                {
                    "title": "Software and firmware co-development using high-level synthesis",
                    "abstract": "Accelerating trigger applications on FPGAs (using VHDL/Verilog) at the CMS experiment at CERN's Large Hadron Collider warrants consistency between each trigger firmware and its corresponding C++ model. This tedious and time consuming process of convergence is exacerbated during each upgrade study. High-level synthesis, with its promise of increased productivity and C++ design entry bridges this gap exceptionally well. This paper explores the “single source code” approach using Vivado-HLS tool for redeveloping the upgraded CMS Endcap Muon Level-1 Track finder (EMTF). Guidelines for tight latency control, optimal resource usage and compatibility with CMS software framework are outlined in this paper."
                },
                {
                    "title": "Technical Design Report for the Phase-I Upgrade of the ATLAS TDAQ System",
                    "abstract": "The Phase-I upgrade of the ATLAS Trigger and Data Acquisition (TDAQ) system is to allow the ATLAS experiment to efficiently trigger and record data at instantaneous luminosities that are up to three times that of the original LHC design while maintaining trigger thresholds close to those used in the initial run of the LHC."
                },
                {
                    "title": "The ATLAS Experiment at the CERN Large Hadron Collider",
                    "abstract": "The ATLAS detector as installed in its experimental cavern at point 1 at CERN is described in this paper. A brief overview of the expected performance of the detector when the Large Hadron Collider begins operation is also presented."
                },
                {
                    "title": "The CMS experiment at the CERN LHC",
                    "abstract": "The Compact Muon Solenoid (CMS) detector is described. The detector operates at the Large Hadron Collider (LHC) at CERN. It was conceived to study proton-proton (and lead-lead) collisions at a centre-of-mass energy of 14 TeV (5.5 TeV nucleon-nucleon) and at luminosities up to 1034 cm-2 s-1 (1027 cm-2 s-1). At the core of the CMS detector sits a high-magnetic-field and large-bore superconducting solenoid surrounding an all-silicon pixel and strip tracker, a lead-tungstate scintillating-crystals electromagnetic calorimeter, and a brass-scintillator sampling hadron calorimeter. The iron yoke of the flux-return is instrumented with four stations of muon detectors covering most of the 4π solid angle. Forward sampling calorimeters extend the pseudorapidity coverage to high values (|η| ≤ 5) assuring very good hermeticity. The overall dimensions of the CMS detector are a length of 21.6 m, a diameter of 14.6 m and a total weight of 12500 t."
                },
                {
                    "title": "TMVA - Toolkit for Multivariate Data Analysis",
                    "abstract": "n high-energy physics, with the search for ever smaller signals in ever larger data sets, it has become essential to extract a maximum of the available information from the data. Multivariate classification methods based on machine learning techniques have become a fundamental ingredient to most analyses. Also the multivariate classifiers themselves have significantly evolved in recent years. Statisticians have found new ways to tune and to combine classifiers to further gain in performance. Integrated into the analysis framework ROOT, TMVA is a toolkit which hosts a large variety of multivariate classification algorithms. They range from rectangular cut optimization using a genetic algorithm and from one- and multidimensional likelihood estimators, over linear and nonlinear discriminants and neural networks, to sophisticated more recent classifiers such as a support vector machine, boosted decision trees and rule ensemble fitting. TMVA manages the simultaneous training, testing, and performance evaluation of all these classifiers with a user-friendly interface, and expedites the application of the trained classifiers to data."
                },
                {
                    "title": "Development of FPGA-based neural network regression models for the ATLAS Phase-II barrel muon trigger upgrade",
                    "abstract": "Effective selection of muon candidates is the cornerstone of the LHC physics programme. The ATLAS experiment uses a two-level trigger system for real-time selection of interesting collision events. The first-level hardware trigger system uses the Resistive Plate Chamber detector (RPC) for selecting muon candidates in the central (barrel) region of the detector. With the planned upgrades, the entirely new FPGA-based muon trigger system will be installed in 2025-2026. In this paper, neural network regression models are studied for potential applications in the new RPC trigger system. A simple simulation model of the current detector is developed for training and testing neural network regression models. Effects from additional cluster hits and noise hits are evaluated. Efficiency of selecting muon candidates is estimated as a function of the transverse muon momentum. Several models are evaluated and their performance is compared to that of the current detector, showing promising potential to improve on current algorithms for the ATLAS Phase-II barrel muon trigger upgrade."
                },
                {
                    "title": "LHC Machine",
                    "abstract": "The Large Hadron Collider (LHC) at CERN near Geneva is the world's newest and most powerful tool for Particle Physics research. It is designed to collide proton beams with a centre-of-mass energy of 14 TeV and an unprecedented luminosity of 1034 cm−2 s−1. It can also collide heavy (Pb) ions with an energy of 2.8 TeV per nucleon and a peak luminosity of 1027 cm−2 s−1. In this paper, the machine design is described."
                },
                {
                    "title": "Progress in resistive plate counters",
                    "abstract": null
                },
                {
                    "title": "Xilinx inputs for nanosecond hardware decision trees for missing transverse energy , 2024",
                    "abstract": null
                },
                {
                    "title": "MuonTriggerPhase2RPC: python code for toy simulation of ATLAS RPC trigger",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "17490a98-355d-4e36-85f7-3b022e9c509f": {
                "pk": "17490a98-355d-4e36-85f7-3b022e9c509f",
                "project_name": null,
                "name": "Pavel Serhiayenka",
                "bio": "I am a researcher specializing in the intersection of machine learning and hardware implementation, particularly focusing on anomaly detection using autoencoding algorithms. My recent work involves developing an interpretable autoencoder built on a forest of deep decision trees, specifically designed for deployment on field-programmable gate arrays (FPGAs). This innovative approach was applied to real-time scenarios at the Large Hadron Collider at CERN, where I trained the model using known physical processes from the Standard Model. \n\nMy design enables the detection of rare and exotic decays of the Higgs boson, achieving impressive inference latency of just 30 nanoseconds while maintaining efficient resource usage on the Xilinx Virtex UltraScale+ VU9P FPGA. I am passionate about creating solutions that cater to edge AI users with resource constraints, ensuring that advanced anomaly detection can be accessible and effective in high-stakes environments like particle physics research. My work not only enhances the capabilities of real-time trigger systems but also contributes to the broader field of interpretable machine learning in complex scientific applications.",
                "collaborators": [
                    "Stephen Roche",
                    "Quincy Bayer",
                    "Benjamin Carlson",
                    "William Ouligian",
                    "Joerg Stelzer",
                    "Tae Min Hong"
                ],
                "pub_titles": [
                    "Nanosecond anomaly detection with decision trees and real-time application to exotic Higgs decays"
                ],
                "pub_abstracts": [
                    "We present an interpretable implementation of the autoencoding algorithm, used as an anomaly detector, built with a forest of deep decision trees on FPGA, field programmable gate arrays. Scenarios at the Large Hadron Collider at CERN are considered, for which the autoencoder is trained using known physical processes of the Standard Model. The design is then deployed in real-time trigger systems for anomaly detection of unknown physical processes, such as the detection of rare exotic decays of the Higgs boson. The inference is made with a latency value of 30 ns at percent-level resource usage using the Xilinx Virtex UltraScale+ VU9P FPGA. Our method offers anomaly detection at low latency values for edge AI users with resource constraints."
                ],
                "domain": [
                    "Anomaly Detection",
                    "FPGA",
                    "Deep Learning",
                    "Edge AI"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "bd4934aa-fa91-4544-bb1a-873b8e7d1a65": {
                "pk": "bd4934aa-fa91-4544-bb1a-873b8e7d1a65",
                "project_name": null,
                "name": "Stephen Roche",
                "bio": "I am a researcher specializing in the intersection of machine learning and high-energy physics, with a particular focus on implementing boosted decision trees (BDT) on field programmable gate arrays (FPGAs). My work aims to achieve ultra-low latency classification for real-time event detection, which is crucial in high-energy physics experiments. Through my software package, fwXmachina, I have developed innovative architectures that allow for deep decision trees with a flexible number of input variables, optimizing both performance and resource utilization on FPGAs.\n\nIn my recent projects, I have successfully tackled complex problems such as distinguishing between electrons and photons, as well as selecting Higgs bosons produced via vector boson fusion while rejecting multijet processes. My implementations have demonstrated impressive latency values of around 10 ns, regardless of clock speed, and maintain low FPGA resource usage, making them suitable for high-performance trigger systems in experiments like those at the Large Hadron Collider (LHC).\n\nI am passionate about pushing the boundaries of real-time decision-making in physics, and I strive to provide tools that empower experts in custom electronics to enhance their experimental capabilities. My research not only contributes to the field of machine learning but also plays a vital role in advancing our understanding of fundamental physics through efficient data processing techniques.",
                "collaborators": [
                    "Tae Min Hong",
                    "Benjamin Carlson",
                    "Brandon Eubanks",
                    "Stephen Racz",
                    "Joerg Stelzer",
                    "Daniel Stumpp",
                    "Quincy Bayer"
                ],
                "pub_titles": [
                    "Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics",
                    "Nanosecond machine learning regression with deep boosted decision trees in FPGA for high energy physics"
                ],
                "pub_abstracts": [
                    "We present a novel implementation of classification using the machine learning / artificial intelligence method called boosted decision trees (BDT) on field programmable gate arrays (FPGA). The firmware implementation of binary classification requiring 100 training trees with a maximum depth of 4 using four input variables gives a latency value of about 10 ns, independent of the clock speed from 100 to 320 MHz in our setup. The low timing values are achieved by restructuring the BDT layout and reconfiguring its parameters. The FPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our setup. A software package called fwXmachina achieves this implementation. Our intended user is an expert of custom electronics-based trigger systems in high energy physics experiments or anyone that needs decisions at the lowest latency values for real-time event classification. Two problems from high energy physics are considered, in the separation of electrons vs. photons and in the selection of vector boson fusion-produced Higgs bosons vs. the rejection of the multijet processes.",
                    "We present a novel application of the machine learning / artificial intelligence method called boosted decision trees to estimate physical quantities on field programmable gate arrays (FPGA). The software package fwXmachina features a new architecture called parallel decision paths that allows for deep decision trees with arbitrary number of input variables. It also features a new optimization scheme to use different numbers of bits for each input variable, which produces optimal physics results and ultraefficient FPGA resource utilization. Problems in high energy physics of proton collisions at the Large Hadron Collider (LHC) are considered. Estimation of missing transverse momentum (ETmiss) at the first level trigger system at the High Luminosity LHC (HL-LHC) experiments, with a simplified detector modeled by Delphes, is used to benchmark and characterize the firmware performance. The firmware implementation with a maximum depth of up to 10 using eight input variables of 16-bit precision gives a latency value of O(10) ns, independent of the clock speed, and O(0.1)% of the available FPGA resources without using digital signal processors."
                ],
                "domain": [
                    "Machine Learning",
                    "FPGA",
                    "High Energy Physics",
                    "Decision Trees"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "9aa4c4f8-1616-420b-a398-c4384fdfc400": {
                "pk": "9aa4c4f8-1616-420b-a398-c4384fdfc400",
                "project_name": null,
                "name": "Benjamin Carlson",
                "bio": "I am a researcher specializing in the intersection of machine learning and high-energy physics, with a particular focus on implementing advanced algorithms on field programmable gate arrays (FPGAs). My work primarily revolves around the application of boosted decision trees (BDTs) to estimate physical quantities and enhance real-time decision-making in complex environments like the Large Hadron Collider (LHC).\n\nOne of my notable contributions is the development of the fwXmachina software package, which features a novel architecture called parallel decision paths. This allows for deep decision trees with an arbitrary number of input variables, optimizing both physics results and FPGA resource utilization. My research has successfully tackled challenges such as estimating missing transverse momentum (ETmiss) at the High Luminosity LHC, achieving impressive latency values and minimal resource usage.\n\nAdditionally, I have explored the use of autoencoding algorithms for anomaly detection, deploying interpretable models on FPGAs to identify rare events, such as exotic decays of the Higgs boson. My implementations prioritize low latency and efficiency, making them suitable for edge AI applications in resource-constrained environments.\n\nThrough my work, I aim to bridge the gap between cutting-edge machine learning techniques and practical applications in high-energy physics, providing tools that enable real-time event classification and anomaly detection with unprecedented speed and accuracy.",
                "collaborators": [
                    "Tae Min Hong",
                    "Stephen Roche",
                    "Quincy Bayer",
                    "Joerg Stelzer",
                    "William Ouligian",
                    "Pavel Serhiayenka",
                    "Brandon Eubanks",
                    "Stephen Racz",
                    "Daniel Stumpp"
                ],
                "pub_titles": [
                    "Nanosecond machine learning regression with deep boosted decision trees in FPGA for high energy physics",
                    "Nanosecond anomaly detection with decision trees and real-time application to exotic Higgs decays",
                    "Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics"
                ],
                "pub_abstracts": [
                    "We present a novel application of the machine learning / artificial intelligence method called boosted decision trees to estimate physical quantities on field programmable gate arrays (FPGA). The software package fwXmachina features a new architecture called parallel decision paths that allows for deep decision trees with arbitrary number of input variables. It also features a new optimization scheme to use different numbers of bits for each input variable, which produces optimal physics results and ultraefficient FPGA resource utilization. Problems in high energy physics of proton collisions at the Large Hadron Collider (LHC) are considered. Estimation of missing transverse momentum (ETmiss) at the first level trigger system at the High Luminosity LHC (HL-LHC) experiments, with a simplified detector modeled by Delphes, is used to benchmark and characterize the firmware performance. The firmware implementation with a maximum depth of up to 10 using eight input variables of 16-bit precision gives a latency value of O(10) ns, independent of the clock speed, and O(0.1)% of the available FPGA resources without using digital signal processors.",
                    "We present an interpretable implementation of the autoencoding algorithm, used as an anomaly detector, built with a forest of deep decision trees on FPGA, field programmable gate arrays. Scenarios at the Large Hadron Collider at CERN are considered, for which the autoencoder is trained using known physical processes of the Standard Model. The design is then deployed in real-time trigger systems for anomaly detection of unknown physical processes, such as the detection of rare exotic decays of the Higgs boson. The inference is made with a latency value of 30 ns at percent-level resource usage using the Xilinx Virtex UltraScale+ VU9P FPGA. Our method offers anomaly detection at low latency values for edge AI users with resource constraints.",
                    "We present a novel implementation of classification using the machine learning / artificial intelligence method called boosted decision trees (BDT) on field programmable gate arrays (FPGA). The firmware implementation of binary classification requiring 100 training trees with a maximum depth of 4 using four input variables gives a latency value of about 10 ns, independent of the clock speed from 100 to 320 MHz in our setup. The low timing values are achieved by restructuring the BDT layout and reconfiguring its parameters. The FPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our setup. A software package called fwXmachina achieves this implementation. Our intended user is an expert of custom electronics-based trigger systems in high energy physics experiments or anyone that needs decisions at the lowest latency values for real-time event classification. Two problems from high energy physics are considered, in the separation of electrons vs. photons and in the selection of vector boson fusion-produced Higgs bosons vs. the rejection of the multijet processes."
                ],
                "domain": [
                    "Machine Learning",
                    "FPGA",
                    "High Energy Physics",
                    "Anomaly Detection"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "195a4008-edd0-4562-bb10-f71d0a10e53f": {
                "pk": "195a4008-edd0-4562-bb10-f71d0a10e53f",
                "project_name": null,
                "name": "Tae Min Hong",
                "bio": "I am a researcher deeply engaged in the intersection of high-energy physics and machine learning, with a focus on advancing detection methods and treatment planning systems. My work spans a variety of applications, from searching for dark matter at the LHC using innovative analyses to developing machine learning algorithms for real-time anomaly detection on field programmable gate arrays (FPGAs). \n\nOne of my notable contributions is the implementation of boosted decision trees (BDTs) for estimating physical quantities, which has significantly improved the efficiency of FPGA resource utilization while maintaining low latency. I have also explored the potential of hypofractionation in HDR brachytherapy, demonstrating that biologically optimized treatment plans can achieve comparable target coverage with reduced implant frequency.\n\nMy research includes investigating Higgs boson decays to four bottom quarks, where I evaluated signal sensitivity and developed strategies for new trigger designs for the LHC's upcoming Run-3 period. I am passionate about leveraging machine learning techniques to enhance the capabilities of high-energy physics experiments, ensuring that we can detect rare processes and improve treatment methodologies in medical physics. Through my work, I aim to bridge the gap between theoretical predictions and practical applications, contributing to both fundamental physics and healthcare advancements.",
                "collaborators": [
                    "Benjamin Carlson",
                    "Stephen Roche",
                    "Quincy Bayer",
                    "Joerg Stelzer",
                    "Kaelyn Seeley",
                    "I-Chow J. Hsu",
                    "J. Adam Cunha",
                    "William Ouligian",
                    "Pavel Serhiayenka",
                    "Brandon Eubanks"
                ],
                "pub_titles": [
                    "Dark matter searches at the LHC",
                    "Nanosecond machine learning regression with deep boosted decision trees in FPGA for high energy physics",
                    "A new radiobiology-based HDR brachytherapy treatment planning algorithm used to investigate the potential for hypofractionation in cervical cancer",
                    "Nanosecond anomaly detection with decision trees and real-time application to exotic Higgs decays",
                    "Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics",
                    "Illuminating all-hadronic final states with a photon: Exotic decays of the Higgs boson to four bottom quarks in vector boson fusion plus gamma at hadron colliders"
                ],
                "pub_abstracts": [
                    "We present a summary of the current status of searches for dark matter at the LHC from the ATLAS and CMS experiments. For various assumptions in the simplified parameter space, the LHC exclusions is complementary to direct detection results. Mono-object analyses in search of dark matter and various analyses searching for dark matter mediators are presented.",
                    "We present a novel application of the machine learning / artificial intelligence method called boosted decision trees to estimate physical quantities on field programmable gate arrays (FPGA). The software package fwXmachina features a new architecture called parallel decision paths that allows for deep decision trees with arbitrary number of input variables. It also features a new optimization scheme to use different numbers of bits for each input variable, which produces optimal physics results and ultraefficient FPGA resource utilization. Problems in high energy physics of proton collisions at the Large Hadron Collider (LHC) are considered. Estimation of missing transverse momentum (ETmiss) at the first level trigger system at the High Luminosity LHC (HL-LHC) experiments, with a simplified detector modeled by Delphes, is used to benchmark and characterize the firmware performance. The firmware implementation with a maximum depth of up to 10 using eight input variables of 16-bit precision gives a latency value of O(10) ns, independent of the clock speed, and O(0.1)% of the available FPGA resources without using digital signal processors.",
                    "Most commercially available treatment planning systems for brachytherapy operate based on physical dose and do not incorporate fractionation or tissue-specific response. The purpose of this study is to investigate the potential for hypofractionation in HDR brachytherapy, thereby reducing the number of implants required. A new treatment planning algorithm was built in order to optimize based on tissue and fractionation specific parameters. Different fractionation schemes were considered for 6 patients, and plans were created using the new algorithm. A baseline fractionation scheme consisting of 5 fractions was compared to hypofractionated plans of 1 to 4 fractions. The effectiveness of each plan was evaluated using radiobiological criteria taken from GEC-ESTRO guidelines. The results of this study indicate that an optimization algorithm based on biological parameters has similar functionality to traditional planning methods with the additional ability to account for fractionation effects. Using this algorithm, it was shown that plans consisting of 3 and 4 fractions have comparable target coverage with equivalent normal tissue exposure. In some specific cases, further fractionation may present acceptable target coverage as well.",
                    "We present an interpretable implementation of the autoencoding algorithm, used as an anomaly detector, built with a forest of deep decision trees on FPGA, field programmable gate arrays. Scenarios at the Large Hadron Collider at CERN are considered, for which the autoencoder is trained using known physical processes of the Standard Model. The design is then deployed in real-time trigger systems for anomaly detection of unknown physical processes, such as the detection of rare exotic decays of the Higgs boson. The inference is made with a latency value of 30 ns at percent-level resource usage using the Xilinx Virtex UltraScale+ VU9P FPGA. Our method offers anomaly detection at low latency values for edge AI users with resource constraints.",
                    "We present a novel implementation of classification using the machine learning / artificial intelligence method called boosted decision trees (BDT) on field programmable gate arrays (FPGA). The firmware implementation of binary classification requiring 100 training trees with a maximum depth of 4 using four input variables gives a latency value of about 10 ns, independent of the clock speed from 100 to 320 MHz in our setup. The low timing values are achieved by restructuring the BDT layout and reconfiguring its parameters. The FPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our setup. A software package called fwXmachina achieves this implementation. Our intended user is an expert of custom electronics-based trigger systems in high energy physics experiments or anyone that needs decisions at the lowest latency values for real-time event classification. Two problems from high energy physics are considered, in the separation of electrons vs. photons and in the selection of vector boson fusion-produced Higgs bosons vs. the rejection of the multijet processes.",
                    "We investigate the potential to detect Higgs boson decays to four bottom quarks through a pair of pseudoscalars, a final state that is predicted by many theories beyond the Standard Model. For the first time, the signal sensitivity is evaluated for the final state using the vector boson fusion (VBF) production with and without an associated photon, for the Higgs at $m_H=125\\,\\textrm{GeV}$, at hadron colliders. The signal significance is $4$ to $6\\sigma$, depending on the pseudoscalar mass $m_a$, when setting the the Higgs decay branching ratio to unity, using an integrated luminosity of $150\\,\\textrm{fb}^{-1}$ at $\\sqrt{s}=13\\,\\textrm{TeV}$. This corresponds to an upper limit of $0.3$, on the Higgs branching ratio to four bottom quarks, with a non-observation of the decay. We also consider several variations of selection requirements - input variables for the VBF tagging and the kinematic variables for the photon - that could help guide the design of new triggers for the Run-3 period of the LHC and for the HL-LHC."
                ],
                "domain": [
                    "High Energy Physics",
                    "Machine Learning",
                    "FPGA",
                    "Dark Matter"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we implement a more efficient decision tree algorithm on FPGA to improve the estimation of missing transverse momentum (ETmiss) at the Large Hadron Collider?\n\n---\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for enhancing the performance of trigger systems in high-energy physics experiments, particularly at the LHC. Improved decision tree implementations can lead to more accurate estimations of ETmiss, which is vital for identifying elusive particles like neutrinos and potential dark matter candidates. This advancement could significantly impact future research in particle physics, enabling more sensitive searches for new physics beyond the Standard Model. Additionally, it could pave the way for practical applications of machine learning in real-time data processing in high-energy environments.\n\n---\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem include the complexity of designing efficient hardware implementations that can handle the high data rates and low latency requirements of LHC experiments. Naive approaches may fail due to the need for real-time processing and the intricacies of translating machine learning models into hardware description languages (HDL). Technical obstacles include optimizing the decision tree structure for FPGA architecture, managing memory constraints, and ensuring that the implementation can operate within the timing constraints of the LHC's trigger systems.\n\n---\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has focused on various machine learning implementations for FPGAs, but many have not achieved the necessary efficiency or speed for real-time applications in high-energy physics. Limitations in prior work include a lack of tailored designs for decision trees and insufficient exploration of hardware-software co-design strategies. Our approach differs by providing a VHDL adaptation of the Deep Decision Tree Engine (DDTE) that is specifically optimized for FPGA, addressing the shortcomings of earlier methods and enhancing performance metrics such as latency and accuracy.\n\n---\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves training a boosted decision tree (BDT) using TMVA with ETmiss as the target variable, followed by a firmware design that translates this model into VHDL using the fwXmachina program. The key components include the HDL Tree Engine (HTE) for each tree and the HDL Tree Manager for managing the forest. We will evaluate the performance using metrics such as latency and accuracy, comparing our results to previous implementations. The expected outcomes include a more efficient decision tree"
    },
    "2409.20044": {
        "paper_data": {
            "title": "Learning Parameterized Quantum Circuits with Quantum Gradient",
            "url": "http://arxiv.org/abs/2409.20044v1",
            "arxiv_id": "2409.20044",
            "authors": [
                "Keren Li",
                "Yuanfeng Wang",
                "Pan Gao",
                "Shenggen Zheng"
            ],
            "abstract": "Parameterized quantum circuits (PQCs) are crucial for quantum machine learning and circuit synthesis, enabling the practical implementation of complex quantum tasks. However, PQC learning has been largely confined to classical optimization methods, which suffer from issues like gradient vanishing. In this work, we introduce a nested optimization model that leverages quantum gradient to enhance PQC learning for polynomial-type cost functions. Our approach utilizes quantum algorithms to identify and overcome a type of gradient vanishing-a persistent challenge in PQC learning-by effectively navigating the optimization landscape. We also mitigate potential barren plateaus of our model and manage the learning cost via restricting the optimization region. Numerically, we demonstrate the feasibility of the approach on two tasks: the Max-Cut problem and polynomial optimization. The method excels in generating circuits without gradient vanishing and effectively optimizes the cost function. From the perspective of quantum algorithms, our model improves quantum optimization for polynomial-type cost functions, addressing the challenge of exponential sample complexity growth.",
            "introduction": "   I Introduction  Quantum circuit synthesis enables a wide range of applications, including unitary approximation, state preparation, ansatz design, and non-linear simulation furrutter2024quantum . Central to this process is the learning of parameterized quantum circuits (PQCs), which incorporate adjustable parameters into quantum gates. By fine-tuning these parameters, PQCs can represent complex quantum states and operations, making them essential tools in quantum computation and quantum machine learning benedetti2019parameterized . For the noisy intermediate-scale quantum devices, PQCs are significant due to their adaptability and potential for utilizing limited quantum resources. They are invaluable for mitigating noise and facilitating hybrid quantum-classical approaches cerezo2021variational ; bharti2022noisy . For the fault-tolerant quantum computers, PQCs are essential to provide a universal, scalable, and adaptable framework for constructing subroutines of quantum protocols. For example, PQCs are fundamental in quantum machine learning, enabling a streamlined representation of quantum neural networks architectures abbas2021power ; beer2020training .   Finding an appropriate PQC is crucial—a task commonly known as circuit synthesis—which involves exploring both the circuit architecture and its parameters. However, learning PQCs using only classical optimization methods faces significant challenges. A typical way is to pre-define an ansatz and train it using a classical optimizer, with well-established techniques such as the parameter shift rule for gradient-based optimization mitarai2018quantum . As the size of the problem increases, pre-definition and train within this high-dimensional Hilbert space becomes computationally challenging. Moreover, the persistent issue of gradient vanishing, caused not only by barren plateaus but just by the exponentially expanding Hilbert space, remains a major obstacle in fully leveraging the potential of PQCs. Various protocols have been proposed to mitigate barren plateaus wang2021noise ; mcclean2018barren , including the use of shallow or variable structure ansatz cerezo2021cost ; grimsley2023adaptive , specialized initialization strategies zhang2022escaping , and tailored architectures pesah2021absence . However, methods to address gradient vanishing caused just by the exponentially expanding Hilbert space are scarce.   For the limitations of classical methods in overcoming these obstacles, naturally, it raises a question: How about leveraging quantum algorithms to learn PQCs?   Given that research in this area is quite limited, in this work, we propose a novel approach that leverages quantum algorithms to enhance PQC learning. Specifically, we introduce a nested optimization model (NOM) for polynomial-type cost functions, utilizing quantum gradient algorithms to optimize directly within the geometry of the Hilbert space. By extending quantum gradient algorithms rebentrost2019quantum ; li2021optimizing ; li2020quantum ; gao2021quantum  from the real domain to the complex domain, our model effectively explores the optimization landscape, identifying and overcoming the persistent gradient vanishing issues encountered in Section II.1.   Our main contributions are as follows: First, we extend the quantum gradient algorithm from the real domain to the complex domain, creating a NOM for PQC learning with quantum resources. Second, we address gradient vanishing in PQC learning caused by inadequate or suboptimal parameter configurations, thereby enhancing training efficiency. Third, by managing the optimization region, we mitigate potential barren plateaus in the classical learning part of NOM. Additionally, by integrating a reinforcement learning (RL)-based method, we numerically demonstrate the effectiveness of the protocol. Finally, the NOM can address the key issue related to sample efficiency in quantum gradient algorithm, reducing the overall resource overhead.     II Results  Quantum circuit synthesis is the process",
            "references": [
                {
                    "title": "Polynomial optimization with linear combination of unitaries",
                    "abstract": null
                },
                {
                    "title": "Quantum circuit synthesis with diffusion models",
                    "abstract": null
                },
                {
                    "title": "Adaptive, problem-tailored variational quantum eigensolver mitigates rough parameter landscapes and barren plateaus",
                    "abstract": null
                },
                {
                    "title": "Escaping from the Barren Plateau via Gaussian Initializations in Deep Variational Quantum Circuits",
                    "abstract": "Variational quantum circuits have been widely employed in quantum simulation and quantum machine learning in recent years. However, quantum circuits with random structures have poor trainability due to the exponentially vanishing gradient with respect to the circuit depth and the qubit number. This result leads to a general standpoint that deep quantum circuits would not be feasible for practical tasks. In this work, we propose an initialization strategy with theoretical guarantees for the vanishing gradient problem in general deep quantum circuits. Specifically, we prove that under proper Gaussian initialized parameters, the norm of the gradient decays at most polynomially when the qubit number and the circuit depth increase. Our theoretical results hold for both the local and the global observable cases, where the latter was believed to have vanishing gradients even for very shallow circuits. Experimental results verify our theoretical findings in the quantum simulation and quantum chemistry."
                },
                {
                    "title": "Quantum second-order optimization algorithm for general polynomials",
                    "abstract": null
                },
                {
                    "title": "Cost function dependent barren plateaus in shallow parametrized quantum circuits",
                    "abstract": null
                },
                {
                    "title": "Optimizing a polynomial function on a quantum processor",
                    "abstract": null
                },
                {
                    "title": "Variational quantum algorithms",
                    "abstract": null
                },
                {
                    "title": "Qulacs: a fast and versatile quantum circuit simulator for research purpose",
                    "abstract": "To explore the possibilities of a near-term intermediate-scale quantum algorithm and long-term fault-tolerant quantum computing, a fast and versatile quantum circuit simulator is needed. Here, we introduce Qulacs, a fast simulator for quantum circuits intended for research purpose. We show the main concepts of Qulacs, explain how to use its features via examples, describe numerical techniques to speed-up simulation, and demonstrate its performance with numerical benchmarks."
                },
                {
                    "title": "Absence of Barren Plateaus in Quantum Convolutional Neural Networks",
                    "abstract": "Quantum neural networks (QNNs) have generated excitement around the possibility of efficiently analyzing quantum data. But this excitement has been tempered by the existence of exponentially vanishing gradients, known as barren plateau landscapes, for many QNN architectures. Recently, Quantum Convolutional Neural Networks (QCNNs) have been proposed, involving a sequence of convolutional and pooling layers that reduce the number of qubits while preserving information about relevant data features. In this work we rigorously analyze the gradient scaling for the parameters in the QCNN architecture. We find that the variance of the gradient vanishes no faster than polynomially, implying that QCNNs do not exhibit barren plateaus. This provides an analytical guarantee for the trainability of randomly initialized QCNNs, which singles out QCNNs as being trainable unlike many other QNN architectures. To derive our results we introduce a novel graph-based method to analyze expectation values over Haar-distributed unitaries, which will likely be useful in other contexts. Finally, we perform numerical simulations to verify our analytical results."
                },
                {
                    "title": "The power of quantum neural networks",
                    "abstract": null
                },
                {
                    "title": "Noise-induced barren plateaus in variational quantum algorithms",
                    "abstract": null
                },
                {
                    "title": "Quantum gradient algorithm for general polynomials",
                    "abstract": "Gradient-based algorithms, popular strategies to optimization problems, are essential for many modern machine-learning techniques. Theoretically, extreme points of certain cost functions can be found iteratively along the directions of the gradient. The time required to calculating the gradient of $d$-dimensional problems is at a level of $\\mathcal{O}(poly(d))$, which could be boosted by quantum techniques, benefiting the high-dimensional data processing, especially the modern machine-learning engineering with the number of optimized parameters being in billions. Here, we propose a quantum gradient algorithm for optimizing general polynomials with the dressed amplitude encoding, aiming at solving fast-convergence polynomials problems within both time and memory consumption in $\\mathcal{O}(poly (\\log{d}))$. Furthermore, numerical simulations are carried out to inspect the performance of this protocol by considering the noises or perturbations from initialization, operation and truncation. For the potential values in high-dimension optimizations, this quantum gradient algorithm is supposed to facilitate the polynomial-optimizations, being a subroutine for future practical quantum computer."
                },
                {
                    "title": "Parameterized quantum circuits as machine learning models",
                    "abstract": "Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications."
                },
                {
                    "title": "An initialization strategy for addressing barren plateaus in parametrized quantum circuits",
                    "abstract": "Parametrized quantum circuits initialized with random initial parameter values are characterized by barren plateaus where the gradient becomes exponentially small in the number of qubits. In this technical note we theoretically motivate and empirically validate an initialization strategy which can resolve the barren plateau problem for practical applications. The technique involves randomly selecting some of the initial parameter values, then choosing the remaining values so that the circuit is a sequence of shallow blocks that each evaluates to the identity. This initialization limits the effective depth of the circuits used to calculate the first parameter update so that they cannot be stuck in a barren plateau at the start of training. In turn, this makes some of the most compact ansätze usable in practice, which was not possible before even for rather basic problems. We show empirically that variational quantum eigensolvers and quantum neural networks initialized using this strategy can be trained using a gradient based method."
                },
                {
                    "title": "Training deep quantum neural networks",
                    "abstract": null
                },
                {
                    "title": "Random Compiler for Fast Hamiltonian Simulation.",
                    "abstract": "The dynamics of a quantum system can be simulated using a quantum computer by breaking down the unitary into a quantum circuit of one and two qubit gates. The most established methods are the Trotter-Suzuki decompositions, for which rigorous bounds on the circuit size depend on the number of terms L in the system Hamiltonian and the size of the largest term in the Hamiltonian Λ. Consequently, the Trotter-Suzuki method is only practical for sparse Hamiltonians. Trotter-Suzuki is a deterministic compiler but it was recently shown that randomized compiling offers lower overheads. Here we present and analyze a randomized compiler for Hamiltonian simulation where gate probabilities are proportional to the strength of a corresponding term in the Hamiltonian. This approach requires a circuit size independent of L and Λ, but instead depending on λ the absolute sum of Hamiltonian strengths (the ℓ_{1} norm). Therefore, it is especially suited to electronic structure Hamiltonians relevant to quantum chemistry. Considering propane, carbon dioxide, and ethane, we observe speed-ups compared to standard Trotter-Suzuki of between 306× and 1591× for physically significant simulation times at precision 10^{-3}. Performing phase estimation at chemical accuracy, we report that the savings are similar."
                },
                {
                    "title": "Barren plateaus in quantum neural network training landscapes",
                    "abstract": null
                },
                {
                    "title": "Quantum circuit learning",
                    "abstract": "We propose a classical-quantum hybrid algorithm for machine learning on near-term quantum processors, which we call quantum circuit learning. A quantum circuit driven by our framework learns a given task by tuning parameters implemented on it. The iterative optimization of the parameters allows us to circumvent the high-depth circuit. Theoretical investigation shows that a quantum circuit can approximate nonlinear functions, which is further confirmed by numerical simulations. Hybridizing a low-depth quantum circuit and a classical computer for machine learning, the proposed framework paves the way toward applications of near-term quantum devices for quantum machine learning."
                },
                {
                    "title": "Quantum gradient descent and Newton’s method for constrained polynomial optimization",
                    "abstract": "Optimization problems in disciplines such as machine learning are commonly solved with iterative methods. Gradient descent algorithms find local minima by moving along the direction of steepest descent while Newton’s method takes into account curvature information and thereby often improves convergence. Here, we develop quantum versions of these iterative optimization algorithms and apply them to polynomial optimization with a unit norm constraint. In each step, multiple copies of the current candidate are used to improve the candidate using quantum phase estimation, an adapted quantum state exponentiation scheme, as well as quantum matrix multiplications and inversions. The required operations perform polylogarithmically in the dimension of the solution vector and exponentially in the number of iterations. Therefore, the quantum algorithm can be useful for high-dimensional problems where a small number of iterations is sufficient."
                },
                {
                    "title": "Optimal Hamiltonian Simulation by Quantum Signal Processing.",
                    "abstract": "The physics of quantum mechanics is the inspiration for, and underlies, quantum computation. As such, one expects physical intuition to be highly influential in the understanding and design of many quantum algorithms, particularly simulation of physical systems. Surprisingly, this has been challenging, with current Hamiltonian simulation algorithms remaining abstract and often the result of sophisticated but unintuitive constructions. We contend that physical intuition can lead to optimal simulation methods by showing that a focus on simple single-qubit rotations elegantly furnishes an optimal algorithm for Hamiltonian simulation, a universal problem that encapsulates all the power of quantum computation. Specifically, we show that the query complexity of implementing time evolution by a d-sparse Hamiltonian H[over ^] for time-interval t with error ε is O[td∥H[over ^]∥_{max}+log(1/ε)/loglog(1/ε)], which matches lower bounds in all parameters. This connection is made through general three-step \"quantum signal processing\" methodology, comprised of (i) transducing eigenvalues of H[over ^] into a single ancilla qubit, (ii) transforming these eigenvalues through an optimal-length sequence of single-qubit rotations, and (iii) projecting this ancilla with near unity success probability."
                },
                {
                    "title": "A Quantum Approximate Optimization Algorithm",
                    "abstract": "We introduce a quantum algorithm that produces approximate solutions for combinatorial optimization problems. The algorithm depends on a positive integer p and the quality of the approximation improves as p is increased. The quantum circuit that implements the algorithm consists of unitary gates whose locality is at most the locality of the objective function whose optimum is sought. The depth of the circuit grows linearly with p times (at worst) the number of constraints. If p is fixed, that is, independent of the input size, the algorithm makes use of efficient classical preprocessing. If p grows with the input size a different strategy is proposed. We study the algorithm as applied to MaxCut on regular graphs and analyze its performance on 2-regular and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum algorithm always finds a cut that is at least 0.6924 times the size of the optimal cut."
                },
                {
                    "title": "A variational eigenvalue solver on a photonic quantum processor",
                    "abstract": null
                },
                {
                    "title": "Algorithms",
                    "abstract": "Most of the articles appearing in this column are oriented toward Common Lisp. However, a wider community of Lisp dialects still exists. One that is of particular interest is GNU Emacs Lisp---the Lisp that is the implementation language for GNU Emacs and its various user extensions."
                },
                {
                    "title": "states and thermal states on a quantum computer us-ing quantum imaginary time evolution",
                    "abstract": null
                },
                {
                    "title": "Supplemental Information for a detailed description",
                    "abstract": null
                },
                {
                    "title": "Determining eigen-8",
                    "abstract": null
                },
                {
                    "title": "Proximal policy optimization",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "0016092d-0bb1-4c48-9c06-2cedd3d715b5": {
                "pk": "0016092d-0bb1-4c48-9c06-2cedd3d715b5",
                "project_name": null,
                "name": "Keren Li",
                "bio": "I am a researcher deeply engaged in the intersection of quantum computing and statistical modeling. My work primarily focuses on harnessing the potential of noisy intermediate-scale quantum (NISQ) devices to tackle complex problems that traditional methods struggle with. I have developed heuristic methods for simulating hermitian matrix exponentiation using parameterized quantum circuits, which are designed to be efficient and practical for current quantum hardware. \n\nIn addition to quantum algorithms, I have explored innovative approaches in statistical design, such as the ForLion algorithm, which optimizes experimental plans with mixed factors under parametric models. This work not only enhances the efficiency of experimental designs but also significantly reduces the number of required experimental settings.\n\nMy research also delves into quantum state tomography, where I have proposed adaptive protocols for uniquely determining multi-qubit pure states with fewer measurements than existing methods. I am particularly interested in improving the efficiency of quantum state characterization through novel observables that allow for direct measurement of density matrix elements.\n\nWith a strong foundation in both quantum mechanics and statistical methodologies, I aim to contribute to the development of practical quantum algorithms and efficient experimental designs that can be readily implemented in real-world applications. My goal is to bridge the gap between theoretical advancements and practical implementations in quantum technology, paving the way for future breakthroughs in the field.",
                "collaborators": [
                    "Jie Yang",
                    "Yu Wang",
                    "Moira Chas",
                    "Bernard Maskit",
                    "Hanru Jiang",
                    "Yongxiang Liu",
                    "Pan Gao",
                    "Shijie Wei",
                    "Jiancun Gao",
                    "Guilu Long"
                ],
                "pub_titles": [
                    "Towards a NISQ Algorithm to Simulate Hermitian Matrix Exponentiation",
                    "Score-Matching Representative Approach for Big Data Analysis with Generalized Linear Models",
                    "Pure State Tomography with Fourier Transformation",
                    "Experiments suggesting that the distribution of the hyperbolic length of closed geodesics sampling by word length is Gaussian",
                    "Direct Measurement of Density Matrices via Dense Dual Bases",
                    "Quantum Gradient Algorithm for General Polynomials",
                    "ForLion: A New Algorithm for D-optimal Designs under General Parametric Statistical Models with Mixed Factors"
                ],
                "pub_abstracts": [
                    "A practical fault-tolerant quantum computer is worth looking forward to as it provides applications that outperform their known classical counterparts. However, millions of interacting qubits with stringent criteria are required, which is intractable with current quantum technologies. As it would take decades to make it happen, exploiting the power of noisy intermediate-scale quantum(NISQ) devices, which already exist, is becoming one of current goals. Matrix exponentiation, especially hermitian matrix exponentiation, is an essential element for quantum information processing. In this article, a heuristic method is reported as simulating a hermitian matrix exponentiation using parametrized quantum circuit(PQC). To generate PQCs for simulations, two strategies, each with its own advantages, are proposed, and can be deployed on near future quantum devices. Compared with the method such as product formula and density matrix exponentiation, the PQCs provided in our method require only low depth circuit and easily accessible gates, which benefit experimental realizations. Furthermore, in this paper, an ancilla-assisted parameterized quantum circuit is proposed to characterize and compress a unitary process, which is likely to be applicable to realizing applications on NISQ hardwares, such as phase estimation, principal component analyses, and matrix inversion. To support the feasibility of our method, numerical experiments were investigated via simulating evolutions by Bell state, GHZ state and Hamiltonian of Crotonic acid, which show an experimental friendly result when compared with their conventional methods. As pursuing a fault-tolerant quantum computer is still challenging and takes decades, our work, which gives a NISQ device friendly way, contributes to the field of NISQ algorithms and provides a possibility, exploiting the power with current quantum technology.",
                    "We propose a fast and efficient strategy, called the representative approach, for big data analysis with generalized linear models, especially for distributed data with localization requirements or limited network bandwidth. With a given partition of massive dataset, this approach constructs a representative data point for each data block and fits the target model using the representative dataset. In terms of time complexity, it is as fast as the subsampling approaches in the literature. As for efficiency, its accuracy in estimating parameters given a homogeneous partition is comparable with the divide-and-conquer method. Supported by comprehensive simulation studies and theoretical justifications, we conclude that mean representatives (MR) work fine for linear models or generalized linear models with a flat inverse link function and moderate coefficients of continuous predictors. For general cases, we recommend the proposed score-matching representatives (SMR), which may improve the accuracy of estimators significantly by matching the score function values. As an illustrative application to the Airline on-time performance data, we show that the MR and SMR estimates are as good as the full data estimate when available.",
                    "Extracting information from quantum devices has long been a crucial problem in the field of quantum mechanics. By performing elaborate measurements, quantum state tomography, an important and fundamental tool in quantum science and technology, can be used to determine unknown quantum states completely. In this study, we explore methods to determine multi-qubit pure quantum states uniquely and directly. Two adaptive protocols are proposed, with their respective quantum circuits. Herein, two or three observables are sufficient, while the number of measurement outcomes is either the same as or fewer than those in existing methods. Additionally, experiments on the IBM 5-qubit quantum computer, as well as numerical investigations, demonstrate the feasibility of the proposed protocols.",
                    "Each free homotopy class of directed closed curves on a surface with boundary can be described by a cyclic reduced word in the generators of the fundamental group and their inverses. The word length is the number of letters of the cyclic word.   If the surface has a hyperbolic metric with geodesic boundary, the geometric length of the class is the length of the unique geodesic.   By computer experiments, we investigate the distribution of the geometric length among all classes with a given word length in the pair of pants surface. Our experiments strongly suggest that the distribution is normal.",
                    "Efficient understanding of a quantum system fundamentally relies on the selection of observables. Pauli observables and mutually unbiased bases (MUBs) are widely used in practice and are often regarded as theoretically optimal for quantum state tomography (QST). However, Pauli observables require a large number of measurements for complete tomography and do not permit direct measurement of density matrix elements with a constant number of observables. For MUBs, the existence of complete sets of \\(d+1\\) bases in all dimensions remains unresolved, highlighting the need for alternative observables. In this work, we introduce a novel set of \\(2d\\) observables specifically designed to enable the complete characterization of any \\(d\\)-dimensional quantum state. To demonstrate the advantages of these observables, we explore two key applications. First, we show that direct measurement of density matrix elements is feasible without auxiliary systems, with any element extractable using only three selected observables. Second, we demonstrate that QST for unknown rank-\\(r\\) density matrices, excluding only a negligible subset, can be achieved with \\(O(r \\log d)\\) observables. This significantly reduces the number of unitary operations compared to compressed sensing with Pauli observables, which typically require \\(O(r d \\log^2 d)\\) operations. Each circuit is iteratively generated and can be efficiently decomposed into at most \\(O(n^4)\\) elementary gates for an \\(n\\)-qubit system. The proposed observables represent a substantial advancement in the characterization of quantum systems, enhancing both the efficiency and practicality of quantum state learning and offering a promising alternative to traditional methods.",
                    "Gradient-based algorithms, popular strategies to optimization problems, are essential for many modern machine-learning techniques. Theoretically, extreme points of certain cost functions can be found iteratively along the directions of the gradient. The time required to calculating the gradient of $d$-dimensional problems is at a level of $\\mathcal{O}(poly(d))$, which could be boosted by quantum techniques, benefiting the high-dimensional data processing, especially the modern machine-learning engineering with the number of optimized parameters being in billions. Here, we propose a quantum gradient algorithm for optimizing general polynomials with the dressed amplitude encoding, aiming at solving fast-convergence polynomials problems within both time and memory consumption in $\\mathcal{O}(poly (\\log{d}))$. Furthermore, numerical simulations are carried out to inspect the performance of this protocol by considering the noises or perturbations from initialization, operation and truncation. For the potential values in high-dimension optimizations, this quantum gradient algorithm is supposed to facilitate the polynomial-optimizations, being a subroutine for future practical quantum computer.",
                    "In this paper, we address the problem of designing an experimental plan with both discrete and continuous factors under fairly general parametric statistical models. We propose a new algorithm, named ForLion, to search for locally optimal approximate designs under the D-criterion. The algorithm performs an exhaustive search in a design space with mixed factors while keeping high efficiency and reducing the number of distinct experimental settings. Its optimality is guaranteed by the general equivalence theorem. We present the relevant theoretical results for multinomial logit models (MLM) and generalized linear models (GLM), and demonstrate the superiority of our algorithm over state-of-the-art design algorithms using real-life experiments under MLM and GLM. Our simulation studies show that the ForLion algorithm could reduce the number of experimental settings by 25% or improve the relative efficiency of the designs by 17.5% on average. Our algorithm can help the experimenters reduce the time cost, the usage of experimental devices, and thus the total cost of their experiments while preserving high efficiencies of the designs."
                ],
                "domain": [
                    "Quantum Computing",
                    "Machine Learning",
                    "Statistical Modeling",
                    "Quantum Algorithms"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "83dade69-e6d9-4874-842e-7d05463b733e": {
                "pk": "83dade69-e6d9-4874-842e-7d05463b733e",
                "project_name": null,
                "name": "Yuanfeng Wang",
                "bio": "I am a researcher specializing in statistical learning and covariance matrix estimation, particularly in contexts involving latent variables. My recent work focuses on developing efficient methods for estimating the marginal covariance matrix of observed variables through a tractable convex program. By decomposing the concentration matrix into a sparse component that captures the graphical structure of the observed variables and a low-rank component that accounts for the effects of latent variables, I have created a framework that enhances our understanding of complex data relationships.\n\nI have designed an efficient first-order algorithm based on the split Bregman method, which not only guarantees convergence under mild conditions but also significantly outperforms existing state-of-the-art algorithms in terms of speed. My research has practical implications, as demonstrated by applying this algorithm to gene expression data, where I found that a substantial portion of the correlations among observed variables can be attributed to just a few latent factors. This work not only advances theoretical understanding but also provides valuable insights for real-world applications in genomics and beyond.",
                "collaborators": [
                    "Gui-Bo Ye",
                    "Yifei Chen",
                    "Xiaohui Xie"
                ],
                "pub_titles": [
                    "Efficient Latent Variable Graphical Model Selection via Split Bregman Method"
                ],
                "pub_abstracts": [
                    "We consider the problem of covariance matrix estimation in the presence of latent variables. Under suitable conditions, it is possible to learn the marginal covariance matrix of the observed variables via a tractable convex program, where the concentration matrix of the observed variables is decomposed into a sparse matrix (representing the graphical structure of the observed variables) and a low rank matrix (representing the marginalization effect of latent variables). We present an efficient first-order method based on split Bregman to solve the convex problem. The algorithm is guaranteed to converge under mild conditions. We show that our algorithm is significantly faster than the state-of-the-art algorithm on both artificial and real-world data. Applying the algorithm to a gene expression data involving thousands of genes, we show that most of the correlation between observed variables can be explained by only a few dozen latent factors."
                ],
                "domain": [
                    "Covariance Matrix Estimation",
                    "Latent Variables",
                    "Convex Optimization",
                    "Statistical Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "48f8c1ec-52be-42a2-8bde-19d59f5f7e16": {
                "pk": "48f8c1ec-52be-42a2-8bde-19d59f5f7e16",
                "project_name": null,
                "name": "Pan Gao",
                "bio": "I am a researcher dedicated to advancing the fields of point cloud compression, video frame interpolation, and saliency prediction for omnidirectional images. My recent work has focused on developing innovative deep learning techniques to address the challenges posed by 3D data and high-resolution imagery. \n\nIn my research, I have proposed a patch-based compression method for point clouds that optimizes lossy geometry compression by processing patches independently, significantly improving rate-distortion performance, especially at low bitrates. I have also explored the vulnerabilities of neural networks to adversarial examples, enhancing traditional JPEG compression to better defend against these attacks while maintaining classification accuracy.\n\nMy work on depth map coding has introduced a dynamic programming solution to optimize occlusion-inducing depth pixels, improving coding efficiency without altering occlusion order. Additionally, I have developed a Flow Transformer Block for video frame interpolation, which leverages optical flow to enhance temporal self-attention, resulting in high-quality interpolated frames.\n\nI am particularly passionate about creating efficient models that balance performance and computational cost. My recent multi-stage recurrent generative adversarial network for saliency prediction in 360-degree images exemplifies this, as it predicts saliency maps in a lightweight manner while achieving superior accuracy. Through my research, I aim to contribute to the evolving landscape of 3D applications and immersive technologies, making them more accessible and effective for various industries.",
                "collaborators": [
                    "Kang You",
                    "Cheng Zhang",
                    "Cagri Ozcinar",
                    "Aljosa Smolic",
                    "Haoyue Tian",
                    "Jie Qin",
                    "Shengzhou Luo",
                    "Manoranjan Paul",
                    "Xinlang Chen",
                    "Rong Quan"
                ],
                "pub_titles": [
                    "Patch-Based Deep Autoencoder for Point Cloud Geometry Compression",
                    "Countering Adversarial Examples: Combining Input Transformation and Noisy Training",
                    "Optimization of Occlusion-Inducing Depth Pixels in 3-D Video Coding",
                    "Video Frame Interpolation with Flow Transformer",
                    "Rate-Distortion Modeling for Bit Rate Constrained Point Cloud Compression",
                    "MRGAN360: Multi-stage Recurrent Generative Adversarial Network for 360 Degree Image Saliency Prediction"
                ],
                "pub_abstracts": [
                    "The ever-increasing 3D application makes the point cloud compression unprecedentedly important and needed. In this paper, we propose a patch-based compression process using deep learning, focusing on the lossy point cloud geometry compression. Unlike existing point cloud compression networks, which apply feature extraction and reconstruction on the entire point cloud, we divide the point cloud into patches and compress each patch independently. In the decoding process, we finally assemble the decompressed patches into a complete point cloud. In addition, we train our network by a patch-to-patch criterion, i.e., use the local reconstruction loss for optimization, to approximate the global reconstruction optimality. Our method outperforms the state-of-the-art in terms of rate-distortion performance, especially at low bitrates. Moreover, the compression process we proposed can guarantee to generate the same number of points as the input. The network model of this method can be easily applied to other point cloud reconstruction problems, such as upsampling.",
                    "Recent studies have shown that neural network (NN) based image classifiers are highly vulnerable to adversarial examples, which poses a threat to security-sensitive image recognition task. Prior work has shown that JPEG compression can combat the drop in classification accuracy on adversarial examples to some extent. But, as the compression ratio increases, traditional JPEG compression is insufficient to defend those attacks but can cause an abrupt accuracy decline to the benign images. In this paper, with the aim of fully filtering the adversarial perturbations, we firstly make modifications to traditional JPEG compression algorithm which becomes more favorable for NN. Specifically, based on an analysis of the frequency coefficient, we design a NN-favored quantization table for compression. Considering compression as a data augmentation strategy, we then combine our model-agnostic preprocess with noisy training. We fine-tune the pre-trained model by training with images encoded at different compression levels, thus generating multiple classifiers. Finally, since lower (higher) compression ratio can remove both perturbations and original features slightly (aggressively), we use these trained multiple models for model ensemble. The majority vote of the ensemble of models is adopted as final predictions. Experiments results show our method can improve defense efficiency while maintaining original accuracy.",
                    "The optimization of occlusion-inducing depth pixels in depth map coding has received little attention in the literature, since their associated texture pixels are occluded in the synthesized view and their effect on the synthesized view is considered negligible. However, the occlusion-inducing depth pixels still need to consume the bits to be transmitted, and will induce geometry distortion that inherently exists in the synthesized view. In this paper, we propose an efficient depth map coding scheme specifically for the occlusion-inducing depth pixels by using allowable depth distortions. Firstly, we formulate a problem of minimizing the overall geometry distortion in the occlusion subject to the bit rate constraint, for which the depth distortion is properly adjusted within the set of allowable depth distortions that introduce the same disparity error as the initial depth distortion. Then, we propose a dynamic programming solution to find the optimal depth distortion vector for the occlusion. The proposed algorithm can improve the coding efficiency without alteration of the occlusion order. Simulation results confirm the performance improvement compared to other existing algorithms.",
                    "Video frame interpolation has been actively studied with the development of convolutional neural networks. However, due to the intrinsic limitations of kernel weight sharing in convolution, the interpolated frame generated by it may lose details. In contrast, the attention mechanism in Transformer can better distinguish the contribution of each pixel, and it can also capture long-range pixel dependencies, which provides great potential for video interpolation. Nevertheless, the original Transformer is commonly used for 2D images; how to develop a Transformer-based framework with consideration of temporal self-attention for video frame interpolation remains an open issue. In this paper, we propose Video Frame Interpolation Flow Transformer to incorporate motion dynamics from optical flows into the self-attention mechanism. Specifically, we design a Flow Transformer Block that calculates the temporal self-attention in a matched local area with the guidance of flow, making our framework suitable for interpolating frames with large motion while maintaining reasonably low complexity. In addition, we construct a multi-scale architecture to account for multi-scale motion, further improving the overall performance. Extensive experiments on three benchmarks demonstrate that the proposed method can generate interpolated frames with better visual quality than state-of-the-art methods.",
                    "As being one of the main representation formats of 3D real world and well-suited for virtual reality and augmented reality applications, point clouds have gained a lot of popularity. In order to reduce the huge amount of data, a considerable amount of research on point cloud compression has been done. However, given a target bit rate, how to properly choose the color and geometry quantization parameters for compressing point clouds is still an open issue. In this paper, we propose a rate-distortion model based quantization parameter selection scheme for bit rate constrained point cloud compression. Firstly, to overcome the measurement uncertainty in evaluating the distortion of the point clouds, we propose a unified model to combine the geometry distortion and color distortion. In this model, we take into account the correlation between geometry and color variables of point clouds and derive a dimensionless quantity to represent the overall quality degradation. Then, we derive the relationships of overall distortion and bit rate with the quantization parameters. Finally, we formulate the bit rate constrained point cloud compression as a constrained minimization problem using the derived polynomial models and deduce the solution via an iterative numerical method. Experimental results show that the proposed algorithm can achieve optimal decoded point cloud quality at various target bit rates, and substantially outperform the video-rate-distortion model based point cloud compression scheme.",
                    "Thanks to the ability of providing an immersive and interactive experience, the uptake of 360 degree image content has been rapidly growing in consumer and industrial applications. Compared to planar 2D images, saliency prediction for 360 degree images is more challenging due to their high resolutions and spherical viewing ranges. Currently, most high-performance saliency prediction models for omnidirectional images (ODIs) rely on deeper or broader convolutional neural networks (CNNs), which benefit from CNNs' superior feature representation capabilities while suffering from their high computational costs. In this paper, inspired by the human visual cognitive process, i.e., human being's perception of a visual scene is always accomplished by multiple stages of analysis, we propose a novel multi-stage recurrent generative adversarial networks for ODIs dubbed MRGAN360, to predict the saliency maps stage by stage. At each stage, the prediction model takes as input the original image and the output of the previous stage and outputs a more accurate saliency map. We employ a recurrent neural network among adjacent prediction stages to model their correlations, and exploit a discriminator at the end of each stage to supervise the output saliency map. In addition, we share the weights among all the stages to obtain a lightweight architecture that is computationally cheap. Extensive experiments are conducted to demonstrate that our proposed model outperforms the state-of-the-art model in terms of both prediction accuracy and model size."
                ],
                "domain": [
                    "Point Cloud Compression",
                    "Video Frame Interpolation",
                    "Adversarial Defense",
                    "Saliency Prediction"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "108408e1-4c4f-4571-8729-469b06fb9941": {
                "pk": "108408e1-4c4f-4571-8729-469b06fb9941",
                "project_name": null,
                "name": "Shenggen Zheng",
                "bio": "I am a researcher deeply engaged in the exploration of quantum computing, particularly focusing on quantum finite automata and their applications in various computational problems. My work spans a range of topics, including the state complexity of quantum finite automata, quantum query complexity, and the interplay between quantum and classical computational models. \n\nIn my recent publications, I have developed methods to derive state succinctness results from query complexity, and I have demonstrated optimal quantum query algorithms for symmetric and partial Boolean functions. I have also investigated the advantages of quantum automata over classical counterparts, revealing significant gaps in communication complexity for promise problems. My research extends to the application of machine learning techniques in quantum state tomography, where I proposed a novel method using Bidirectional Gated Recurrent Units to reconstruct complex quantum states efficiently.\n\nI am particularly interested in the role of coherence in quantum algorithms, such as Grover's search algorithm, and how manipulating coherence can enhance success probabilities. Additionally, I have explored the potential of quantum automata in modeling chemical reactions and biological systems, demonstrating their computational versatility.\n\nThrough my work, I aim to bridge the gap between theoretical quantum computing and practical applications, contributing to the understanding of quantum information processing and its implications for future technologies.",
                "collaborators": [
                    "Daowen Qiu",
                    "Jozef Gruska",
                    "Lvzhou Li",
                    "Haozhen Situ",
                    "Minghua Pan",
                    "Amandeep Singh Bhatia",
                    "Andris Ambainis",
                    "Huikang Huang",
                    "Taiping Xiong"
                ],
                "pub_titles": [
                    "From Quantum Query Complexity to State Complexity",
                    "Characterizations of symmetrically partial Boolean functions with exact quantum query complexity",
                    "Exact quantum algorithms have advantage for almost all Boolean functions",
                    "On the state complexity of semi-quantum finite automata",
                    "Bidirectional information flow quantum state tomography",
                    "Performance of Grover's search algorithm with diagonalizable collective noises",
                    "Complementarity between Success Probability and Coherence in Grover Search Algorithm",
                    "Potential of quantum finite automata with exact acceptance",
                    "Generalizations of the distributed Deutsch-Jozsa promise problem",
                    "Communication complexity of promise problems and their applications to finite automata",
                    "Promise problems solved by quantum and classical finite automata",
                    "Time-space tradeoffs for two-way finite automata",
                    "Two-tape finite automata with quantum and classical states",
                    "Some Languages Recognized by Two-Way Finite Automata with Quantum and Classical States",
                    "Power of the interactive proof systems with verifiers modeled by semi-quantum two-way finite automata",
                    "Global multipartite entanglement dynamics in Grover's search algorithm",
                    "Capability of local operations and classical communication for distinguishing bipartite unitary operations",
                    "A Quantum Finite Automata Approach to Modeling the Chemical Reactions",
                    "RNA-2QCFA: Evolving Two-way Quantum Finite Automata with Classical States for RNA Secondary Structures"
                ],
                "pub_abstracts": [
                    "State complexity of quantum finite automata is one of the interesting topics in studying the power of quantum finite automata. It is therefore of importance to develop general methods how to show state succinctness results for quantum finite automata. One such method is presented and demonstrated in this paper. In particular, we show that state succinctness results can be derived out of query complexity results.",
                    "We give and prove an optimal exact quantum query algorithm with complexity $k+1$ for computing the promise problem (i.e., symmetric and partial Boolean function) $DJ_n^k$ defined as: $DJ_n^k(x)=1$ for $|x|=n/2$, $DJ_n^k(x)=0$ for $|x|$ in the set $\\{0, 1,\\ldots, k, n-k, n-k+1,\\ldots,n\\}$, and it is undefined for the rest cases, where $n$ is even, $|x|$ is the Hamming weight of $x$. The case of $k=0$ is the well-known Deutsch-Jozsa problem. We outline all symmetric (and partial) Boolean functions with degrees 1 and 2, and prove their exact quantum query complexity. Then we prove that any symmetrical (and partial) Boolean function $f$ has exact quantum 1-query complexity if and only if $f$ can be computed by the Deutsch-Jozsa algorithm. We also discover the optimal exact quantum 2-query complexity for distinguishing between inputs of Hamming weight $\\{ \\lfloor n/2\\rfloor, \\lceil n/2\\rceil \\}$ and Hamming weight in the set $\\{ 0, n\\}$ for all odd $n$. In addition, a method is provided to determine the degree of any symmetrical (and partial) Boolean function.",
                    "It has been proved that almost all $n$-bit Boolean functions have exact classical query complexity $n$. However, the situation seemed to be very different when we deal with exact quantum query complexity. In this paper, we prove that almost all $n$-bit Boolean functions can be computed by an exact quantum algorithm with less than $n$ queries. More exactly, we prove that ${AND}_n$ is the only $n$-bit Boolean function, up to isomorphism, that requires $n$ queries.",
                    "Some of the most interesting and important results concerning quantum finite automata are those showing that they can recognize certain languages with (much) less resources than corresponding classical finite automata \\cite{Amb98,Amb09,AmYa11,Ber05,Fre09,Mer00,Mer01,Mer02,Yak10,ZhgQiu112,Zhg12}. This paper shows three results of such a type that are stronger in some sense than other ones because (a) they deal with models of quantum automata with very little quantumness (so-called semi-quantum one- and two-way automata with one qubit memory only); (b) differences, even comparing with probabilistic classical automata, are bigger than expected; (c) a trade-off between the number of classical and quantum basis states needed is demonstrated in one case and (d) languages (or the promise problem) used to show main results are very simple and often explored ones in automata theory or in communication complexity, with seemingly little structure that could be utilized.",
                    "The exact reconstruction of many-body quantum systems is one of the major challenges in modern physics, because it is impractical to overcome the exponential complexity problem brought by high-dimensional quantum many-body systems. Recently, machine learning techniques are well used to promote quantum information research and quantum state tomography has been also developed by neural network generative models. We propose a quantum state tomography method, which is based on Bidirectional Gated Recurrent Unit neural network (BiGRU), to learn and reconstruct both easy quantum states and hard quantum states in this paper. We are able to use fewer measurement samples in our method to reconstruct these quantum states and obtain high fidelity.",
                    "Grover's search algorithm (GSA) is known to experience a loss of its quadratic speedup when exposed to quantum noise. In this study, we partially agree with this result and present our findings. First, we examine different typical diagonalizable noises acting on the oracles in GSA and find that the success probability decreases and oscillates around $1/2$ as the number of iterations increases. Secondly, our results show that the performance of GSA can be improved by certain types of noise, such as bit flip and bit-phase flip noise. Finally, we determine the noise threshold for bit-phase flip noise to achieve a desired success probability and demonstrate that GSA with bit-phase flip noise still outperforms its classical counterpart. These results suggest new avenues for research in noisy intermediate-scale quantum (NISQ) computing, such as evaluating the feasibility of quantum algorithms with noise and exploring their applications in machine learning.",
                    "Coherence plays a very important role in Grover search algorithm (GSA). In this paper, we define the normalization coherence N(C), where C is a coherence measurement. In virtue of the constraint of large N and Shannon's maximum entropy principle, a surprising complementary relationship between the coherence and the success probability of GSA is obtained. Namely, P_s(t)+N(C(t))\\simeq 1, where C is in terms of the relative entropy of coherence and l_1 norm of coherence, t is the number of the search iterations in GSA. Moreover, the equation holds no matter in ideal or noisy environments. Considering the number of qubits is limited in the recent noisy intermediate-scale quantum (NISQ) era, some exact numerical calculation experiments are presented for different database sizes N with different types of noises. The results show that the complementary between the success probability and the coherence almost always hold. This work provides a new perspective to improve the success probability by manipulating its complementary coherence, and vice versa. It has an excellent potential for helping quantum algorithms design in the NISQ era.",
                    "The potential of the exact quantum information processing is an interesting, important and intriguing issue. For examples, it has been believed that quantum tools can provide significant, that is larger than polynomial, advantages in the case of exact quantum computation only, or mainly, for problems with very special structures. We will show that this is not the case.   In this paper the potential of quantum finite automata producing outcomes not only with a (high) probability, but with certainty (so called exactly) is explored in the context of their uses for solving promise problems and with respect to the size of automata. It is shown that for solving particular classes $\\{A^n\\}_{n=1}^{\\infty}$ of promise problems, even those without some very special structure, that succinctness of the exact quantum finite automata under consideration, with respect to the number of (basis) states, can be very small (and constant) though it grows proportional to $n$ in the case deterministic finite automata (DFAs) of the same power are used. This is here demonstrated also for the case that the component languages of the promise problems solvable by DFAs are non-regular. The method used can be applied in finding more exact quantum finite automata or quantum algorithms for other promise problems.",
                    "In the {\\em distributed Deutsch-Jozsa promise problem}, two parties are to determine whether their respective strings $x,y\\in\\{0,1\\}^n$ are at the {\\em Hamming distance} $H(x,y)=0$ or $H(x,y)=\\frac{n}{2}$. Buhrman et al. (STOC' 98) proved that the exact {\\em quantum communication complexity} of this problem is ${\\bf O}(\\log {n})$ while the {\\em deterministic communication complexity} is ${\\bf \\Omega}(n)$. This was the first impressive (exponential) gap between quantum and classical communication complexity.   In this paper, we generalize the above distributed Deutsch-Jozsa promise problem to determine, for any fixed $\\frac{n}{2}\\leq k\\leq n$, whether $H(x,y)=0$ or $H(x,y)= k$, and show that an exponential gap between exact quantum and deterministic communication complexity still holds if $k$ is an even such that $\\frac{1}{2}n\\leq k<(1-\\lambda) n$, where $0< \\lambda<\\frac{1}{2}$ is given. We also deal with a promise version of the well-known {\\em disjointness} problem and show also that for this promise problem there exists an exponential gap between quantum (and also probabilistic) communication complexity and deterministic communication complexity of the promise version of such a disjointness problem. Finally, some applications to quantum, probabilistic and deterministic finite automata of the results obtained are demonstrated.",
                    "Equality and disjointness are two of the most studied problems in communication complexity. They have been studied for both classical and also quantum communication and for various models and modes of communication. Buhrman et al. [Buh98] proved that the exact quantum communication complexity for a promise version of the equality problem is ${\\bf O}(\\log {n})$ while the classical deterministic communication complexity is $n+1$ for two-way communication, which was the first impressively large (exponential) gap between quantum and classical (deterministic and probabilistic) communication complexity. If an error is tolerated, both quantum and probabilistic communication complexities for equality are ${\\bf O}(\\log {n})$. However, even if an error is tolerated, the gaps between quantum (probabilistic) and deterministic complexity are not larger than quadratic for the disjointness problem. It is therefore interesting to ask whether there are some promise versions of the disjointness problem for which bigger gaps can be shown. We give a positive answer to such a question. Namely, we prove that there exists an exponential gap between quantum (even probabilistic) communication complexity and classical deterministic communication complexity of some specific versions of the disjointness problem.   Klauck [Kla00] proved, for any language, that the state complexity of exact quantum/classical finite automata, which is a general model of one-way quantum finite automata, is not less than the state complexity of an equivalent one-way deterministic finite automata (1DFA). In this paper we show, using a communication complexity result, that situation may be different for some promise problems. Namely, we show for certain promise problem that the gap between the state complexity of exact one-way quantum finite automata and 1DFA can be exponential.",
                    "The concept of promise problems was introduced and started to be systematically explored by Even, Selman, Yacobi, Goldreich, and other scholars. It has been argued that promise problems should be seen as partial decision problems and as such that they are more fundamental than decision problems and formal languages that used to be considered as the basic ones for complexity theory. The main purpose of this paper is to explore the promise problems accepted by classical, quantum and also semi-quantum finite automata. More specifically, we first introduce two acceptance modes of promise problems, recognizability and solvability, and explore their basic properties. Afterwards, we show several results concerning descriptional complexity on promise problems. In particular, we prove: (1) there is a promise problem that can be recognized exactly by measure-once one-way quantum finite automata (MO-1QFA), but no deterministic finite automata (DFA) can recognize it; (2) there is a promise problem that can be solved with error probability $\\epsilon\\leq 1/3$ by one-way finite automaton with quantum and classical states (1QCFA), but no one-way probability finite automaton (PFA) can solve it with error probability $\\epsilon\\leq 1/3$; and especially, (3) there are promise problems $A(p)$ with prime $p$ that can be solved {\\em with any error probability} by MO-1QFA with only two quantum basis states, but they can not be solved exactly by any MO-1QFA with two quantum basis states; in contrast, the minimal PFA solving $A(p)$ with any error probability (usually smaller than $1/2$) has $p$ states. Finally, we mention a number of problems related to promise for further study.",
                    "We explore bounds of {\\em time-space tradeoffs} in language recognition on {\\em two-way finite automata} for some special languages. We prove: (1) a time-space tradeoff upper bound for recognition of the languages $L_{EQ}(n)$ on {\\em two-way probabilistic finite automata} (2PFA): $TS={\\bf O}(n\\log n)$, whereas a time-space tradeoff lower bound on {\\em two-way deterministic finite automata} is ${\\bf \\Omega}(n^2)$, (2) a time-space tradeoff upper bound for recognition of the languages $L_{INT}(n)$ on {\\em two-way finite automata with quantum and classical states} (2QCFA): $TS={\\bf O}(n^{3/2}\\log n)$, whereas a lower bound on 2PFA is $TS={\\bf \\Omega}(n^2)$, (3) a time-space tradeoff upper bound for recognition of the languages $L_{NE}(n)$ on exact 2QCFA: $TS={\\bf O}(n^{1.87} \\log n)$, whereas a lower bound on 2PFA is $TS={\\bf \\Omega}(n^2)$.   It has been proved (Klauck, STOC'00) that the exact one-way quantum finite automata have no advantage comparing to classical finite automata in recognizing languages. However, the result (3) shows that the exact 2QCFA do have an advantage in comparison with their classical counterparts, which has been the first example showing that the exact quantum computing have advantage in time-space tradeoff comparing to classical computing.   Usually, two communicating parties, Alice and Bob, are supposed to have an access to arbitrary computational power in {\\em communication complexity} model that is used. Instead of that we will consider communication complexity in such a setting that two parties are using only finite automata and we prove in this setting that quantum automata are better than classical automata and also probabilistic automata are better than deterministic automata for some well known tasks.",
                    "{\\it Two-way finite automata with quantum and classical states} (2QCFA) were introduced by Ambainis and Watrous, and {\\it two-way two-tape deterministic finite automata} (2TFA) were introduced by Rabin and Scott. In this paper we study 2TFA and propose a new computing model called {\\it two-way two-tape finite automata with quantum and classical states} (2TQCFA). First, we give efficient 2TFA algorithms for recognizing languages which can be recognized by 2QCFA. Second, we give efficient 2TQCFA algorithms to recognize several languages whose status vis-a-vis 2QCFA have been posed as open questions, such as $L_{square}=\\{a^{n}b^{n^{2}}\\mid n\\in \\mathbf{N}\\}$. Third, we show that $\\{a^{n}b^{n^{k}}\\mid n\\in \\mathbf{N}\\}$ can be recognized by {\\it $(k+1)$-tape deterministic finite automata} ($(k+1)$TFA). Finally, we introduce {\\it $k$-tape automata with quantum and classical states} ($k$TQCFA) and prove that $\\{a^{n}b^{n^{k}}\\mid n\\in \\mathbf{N}\\}$ can be recognized by $k$TQCFA.",
                    "{\\it Two-way finite automata with quantum and classical states} (2QCFA) were introduced by Ambainis and Watrous, and it was shown that 2QCFA have superiority over {\\it two-way probabilistic finite automata} (2PFA) for recognizing some non-regular languages such as the language $L_{eq}=\\{a^{n}b^{n}\\mid n\\in \\mathbf{N}\\}$ and the palindrome language $L_{pal}=\\{\\omega\\in \\{a,b\\}^*\\mid\\omega=\\omega^R\\}$, where $x^R$ is $x$ in the reverse order. It is interesting to find more languages like these that witness the superiority of 2QCFA over 2PFA. In this paper, we consider the language $L_{m}=\\{xcy\\mid \\Sigma=\\{a, b, c\\}, x,y\\in\\{a,b\\}^{*},c\\in\\Sigma, |x|=|y|\\}$ that is similar to the middle language $L_{middle}=\\{xay\\mid x,y\\in\\Sigma^{*},a\\in\\Sigma, |x|=|y|\\}$. We prove that the language $L_{m}$ can be recognized by 2QCFA with one-sided error in polynomial expected time. Also, we show that $L_{m}$ can be recognized by 2PFA with bounded error, but only in exponential expected time. Thus $L_{m}$ is another witness of the fact that 2QCFA are more powerful than their classical counterparts.",
                    "In this paper we explore the power of AM for the case that verifiers are {\\em two-way finite automata with quantum and classical states} (2QCFA)--introduced by Ambainis and Watrous in 2002--and the communications are classical. It is of interest to consider AM with such \"semi-quantum\" verifiers because they use only limited quantum resources. Our main result is that such Quantum Arthur-Merlin proof systems (QAM(2QCFA)) with polynomial expected running time are more powerful than in the case verifiers are two-way probabilistic finite automata (AM(2PFA)) with polynomial expected running time. Moreover, we prove that there is a language which can be recognized by an exponential expected running time QAM(2QCFA), but can not be recognized by any AM(2PFA), and that the NP-complete language $L_{knapsack}$ can also be recognized by a QAM(2QCFA) working only on quantum pure states using unitary operators.",
                    "Entanglement is considered to be one of the primary reasons for why quantum algorithms are more efficient than their classical counterparts for certain computational tasks. The global multipartite entanglement of the multiqubit states in Grover's search algorithm can be quantified using the geometric measure of entanglement (GME). Rossi {\\em et al.} (Phys. Rev. A \\textbf{87}, 022331 (2013)) found that the entanglement dynamics is scale invariant for large $n$. Namely, the GME does not depend on the number $n$ of qubits; rather, it only depends on the ratio of iteration $k$ to the total iteration. In this paper, we discuss the optimization of the GME for large $n$. We prove that ``the GME is scale invariant'' does not always hold. We show that there is generally a turning point that can be computed in terms of the number of marked states and their Hamming weights during the curve of the GME. The GME is scale invariant prior to the turning point. However, the GME is not scale invariant after the turning point since it also depends on $n$ and the marked states.",
                    "The problem behind this paper is, if the number of queries to unitary operations is fixed, say $k$, then when do local operations and classical communication (LOCC) suffice for optimally distinguishing bipartite unitary operations? We consider the above problem for two-qubit unitary operations in the case of $k=1$, showing that for two two-qubit entangling unitary operations without local parties, LOCC achieves the same distinguishability as the global operations. Specifically, we obtain: (i) if such two unitary operations are perfectly distinguishable by global operations, then they are perfectly distinguishable by LOCC too, and (ii) if they are not perfectly distinguishable by global operations, then LOCC can achieve the same optimal discrimination probability as the global operations.",
                    "In recent years, the modeling interest has increased significantly from the molecular level to the atomic and quantum scale. The field of computational chemistry plays a significant role in designing computational models for the operation and simulation of systems ranging from atoms and molecules to industrial-scale processes. It is influenced by a tremendous increase in computing power and the efficiency of algorithms. The representation of chemical reactions using classical automata theory in thermodynamic terms had a great influence on computer science. The study of chemical information processing with quantum computational models is a natural goal. In this paper, we have modeled chemical reactions using two-way quantum finite automata, which are halted in linear time. Additionally, classical pushdown automata can be designed for such chemical reactions with multiple stacks. It has been proven that computational versatility can be increased by combining chemical accept/reject signatures and quantum automata models.",
                    "Recently, the use of mathematical methods and computer science applications have got significant response among biochemists and biologists to modeling the biological systems. The computational and mathematical methods have enormous potential for modeling the deoxyribonucleic acid (DNA) and ribonucleic acid (RNA) structures. The modeling of DNA and RNA secondary structures using automata theory had a significant impact in the fields of computer science. It is a natural goal to model the RNA secondary biomolecular structures using quantum computational models. Two-way quantum finite automata with classical states are more dominant than two-way probabilistic finite automata in language recognition. The main objective of this paper is on using two-way quantum finite automata with classical states to simulate, model and analyze the ribonucleic acid (RNA) sequences."
                ],
                "domain": [
                    "Quantum Computing",
                    "Finite Automata",
                    "Complexity Theory",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively learn parameterized quantum circuits (PQCs) while overcoming challenges such as gradient vanishing and barren plateaus in the context of quantum circuit synthesis?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for advancing quantum computation and quantum machine learning, as it enables more efficient and effective use of noisy intermediate-scale quantum devices. By improving PQC learning, we can enhance the performance of quantum algorithms, leading to practical applications in various fields such as optimization, simulation, and cryptography. This research could pave the way for future studies that explore more complex quantum systems and algorithms, ultimately contributing to the development of fault-tolerant quantum computers and broader quantum technologies.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the high-dimensional nature of the Hilbert space associated with PQCs, which complicates the pre-definition and training of ansatzes. Naive approaches may fail due to the persistent issue of gradient vanishing, exacerbated by the exponentially expanding Hilbert space. Additionally, existing methods to mitigate barren plateaus do not adequately address the specific gradient vanishing caused by suboptimal parameter configurations, making it difficult to achieve efficient training and optimization.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on classical optimization methods, which are limited in their ability to navigate the complexities of PQC learning. Existing solutions have not effectively tackled the gradient vanishing issue arising from the high-dimensional Hilbert space. Barriers include a lack of exploration into quantum algorithms for PQC learning and insufficient methodologies to manage the optimization landscape. Our approach differs by leveraging quantum gradient algorithms in a nested optimization model (NOM) that directly addresses these challenges, extending existing techniques into the complex domain.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves a nested optimization model (NOM) that utilizes quantum gradient algorithms to optimize polynomial-type cost functions directly within the geometry of the Hilbert space. We will employ a suitable dataset of PQCs and evaluate performance using metrics such as training efficiency and sample efficiency. The expected outcomes include enhanced training efficiency by addressing gradient vanishing, reduced resource overhead, and improved management of barren plateaus, ultimately demonstrating the effectiveness of our quantum-based approach in PQC learning."
    },
    "2409.19864": {
        "paper_data": {
            "title": "A Simple and Efficient Equivariant Message Passing Neural Network Model for Non-Local Potential Energy Surface",
            "url": "http://arxiv.org/abs/2409.19864v1",
            "arxiv_id": "2409.19864",
            "authors": [
                "Yibin Wu",
                "Junfan Xia",
                "Yaolong Zhang",
                "Bin Jiang"
            ],
            "abstract": "Machine learning potentials have become increasingly successful in atomistic simulations. Many of these potentials are based on an atomistic representation in a local environment, but an efficient description of non-local interactions that exceed a common local environment remains a challenge. Herein, we propose a simple and efficient equivariant model, EquiREANN, to effectively represent non-local potential energy surface. It relies on a physically inspired message passing framework, where the fundamental descriptors are linear combination of atomic orbitals, while both invariant orbital coefficients and the equivariant orbital functions are iteratively updated. We demonstrate that this EquiREANN model is able to describe the subtle potential energy variation due to the non-local structural change with high accuracy and little extra computational cost than an invariant message passing model. Our work offers a generalized approach to create equivariant message passing adaptations of other advanced local many-body descriptors.",
            "introduction": null,
            "references": [
                {
                    "title": "Machine Learning for Chemical Reactions.",
                    "abstract": "Machine learning (ML) techniques applied to chemical reactions have a long history. The present contribution discusses applications ranging from small molecule reaction dynamics to computational platforms for reaction planning. ML-based techniques can be particularly relevant for problems involving both computation and experiments. For one, Bayesian inference is a powerful approach to develop models consistent with knowledge from experiments. Second, ML-based methods can also be used to handle problems that are formally intractable using conventional approaches, such as exhaustive characterization of state-to-state information in reactive collisions. Finally, the explicit simulation of reactive networks as they occur in combustion has become possible using machine-learned neural network potentials. This review provides an overview of the questions that can and have been addressed using machine learning techniques, and an outlook discusses challenges in this diverse and stimulating field. It is concluded that ML applied to chemistry problems as practiced and conceived today has the potential to transform the way with which the field approaches problems involving chemical reactions, in both research and academic teaching."
                },
                {
                    "title": "The effective cutoff is proportionally extended as T increases, which is estimated and taking the left-terminal carbon atom as the center for reference",
                    "abstract": null
                },
                {
                    "title": "extra computational cost than an invariant message passing model",
                    "abstract": null
                },
                {
                    "title": "Heifei National Laboratory for Physical Science at the Microscale, Department",
                    "abstract": null
                },
                {
                    "title": "Department of Chemistry and Chemical Biology, Center for Computational Chemistry, University of New Mexico, Albuquerque, New Mexico 87131, USA",
                    "abstract": null
                },
                {
                    "title": "Key Laboratory of Precision and Intelligent Chemistry, Department of Chemical Physics, University of Science and Technology of China, Hefei, Anhui 230026, China",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "008050e5-010a-46c1-8322-0a63187aa2fd": {
                "pk": "008050e5-010a-46c1-8322-0a63187aa2fd",
                "project_name": null,
                "name": "Yibin Wu",
                "bio": "I am a researcher specializing in autonomous navigation systems, particularly focusing on the integration of inertial measurement units (IMUs) and visual-inertial technologies for mobile robots and vehicles. My work began with the development of VINS-Mono, a robust monocular visual-inertial state estimator that has gained significant attention for its accuracy and scalability across various platforms, including drones and smartphones.\n\nBuilding on this foundation, I introduced innovative dead reckoning (DR) solutions, such as Wheel-INS, which utilizes a wheel-mounted MEMS IMU to enhance navigation accuracy while mitigating drift errors. My research emphasizes the importance of sensor fusion and the use of multiple IMUs to improve positioning and heading accuracy in wheeled robots. I have also explored the integration of LiDAR with inertial systems, proposing the LIO-EKF framework, which simplifies parameter tuning while maintaining high performance.\n\nIn my recent studies, I have focused on leveraging environmental features for simultaneous localization and mapping (SLAM) using minimal sensor setups. By utilizing road bank angles as terrain features, I have demonstrated significant improvements in positioning accuracy, showcasing the potential of low-cost sensors in robust navigation systems.\n\nMy commitment to open science is reflected in my efforts to share source code and datasets with the research community, fostering collaboration and innovation in the field of autonomous navigation.",
                "collaborators": [
                    "Xiaoji Niu",
                    "Jian Kuang",
                    "Lasse Klingbeil",
                    "Heiner Kuhlmann",
                    "Tiziano Guadagnino",
                    "Louis Wiesmann",
                    "Cyrill Stachniss",
                    "Jens Behley"
                ],
                "pub_titles": [
                    "Formula Derivation and Analysis of the VINS-Mono",
                    "A Comparison of Three Measurement Models for the Wheel-mounted MEMS IMU-based Dead Reckoning System",
                    "Wheel-INS: A Wheel-mounted MEMS IMU-based Dead Reckoning System",
                    "Wheel-INS2: Multiple MEMS IMU-based Dead Reckoning System for Wheeled Robots with Evaluation of Different IMU Configurations",
                    "LIO-EKF: High Frequency LiDAR-Inertial Odometry using Extended Kalman Filters",
                    "Wheel-SLAM: Simultaneous Localization and Terrain Mapping Using One Wheel-mounted IMU"
                ],
                "pub_abstracts": [
                    "The VINS-Mono is a monocular visual-inertial 6 DOF state estimator proposed by Aerial Robotics Group of HKUST in 2017. It can be performed on MAVs, smartphones and many other intelligent platforms. Because of the excellent robustness, accuracy and scalability, it has gained extensive attention worldwide. In this manuscript, the main equations including IMU pre-integration, visual-inertial co-initialization and tightly-coupled nonlinear optimization are derived and analyzed.",
                    "A self-contained autonomous dead reckoning (DR) system is desired to complement the Global Navigation Satellite System (GNSS) for land vehicles, for which odometer-aided inertial navigation system (ODO/INS) is a classical solution. In this study, we use a wheel-mounted MEMS IMU (Wheel-IMU) to substitute the odometer, and further, investigate three types of measurement models, including the velocity measurement, displacement increment measurement, and contact point zero-velocity measurement, in the Wheel-IMU based DR system. The measurement produced by the Wheel-IMU along with the non-holonomic constraint (NHC) are fused with INS through an error-state extended Kalman filter (EKF). Theoretical discussion and field tests illustrate the feasibility and equivalence of the three measurements in terms of the overall DR performance. The maximum horizontal position drifts are all less than 2% of the total travelled distance. Additionally, the displacement increment measurement model is less sensitive to the lever arm error between the Wheel-IMU and the wheel center.",
                    "To improve the accuracy and robustness of the inertial navigation systems (INS) for wheeled robots without adding additional component cost, we propose Wheel-INS, a complete dead reckoning solution based on a wheel-mounted microelectromechanical system (MEMS) inertial measurement unit (IMU). There are two major advantages by mounting an IMU to the center of a non-steering wheel of the ground vehicle. Firstly, the gyroscope outputs can be used to calculate the wheel speed, so as to replace the traditional odometer to mitigate the error drift of INS. Secondly, with the rotation of the wheel, the constant bias error of the inertial sensor can be canceled to some extent. The installation scheme of the wheel-mounted IMU (Wheel-IMU), the system characteristics, and the dead reckoning error analysis are described. Experimental results show that the maximum position drift of Wheel-INS in the horizontal plane is less than 1.8% of the total traveled distance, reduced by 23% compared to the conventional odometer-aided INS (ODO/INS). In addition, Wheel-INS outperforms ODO/INS because of its inherent immunity to constant bias error of gyroscopes. The source code and experimental datasets used in this paper is made available to the community (https://github.com/i2Nav-WHU/Wheel-INS).",
                    "A reliable self-contained navigation system is essential for autonomous vehicles. Based on our previous study on Wheel-INS \\cite{niu2019}, a wheel-mounted inertial measurement unit (Wheel-IMU)-based dead reckoning (DR) system, in this paper, we propose a multiple IMUs-based DR solution for the wheeled robots. The IMUs are mounted at different places of the wheeled vehicles to acquire various dynamic information. In particular, at least one IMU has to be mounted at the wheel to measure the wheel velocity and take advantages of the rotation modulation. The system is implemented through a distributed extended Kalman filter structure where each subsystem (corresponding to each IMU) retains and updates its own states separately. The relative position constraints between the multiple IMUs are exploited to further limit the error drift and improve the system robustness. Particularly, we present the DR systems using dual Wheel-IMUs, one Wheel-IMU plus one vehicle body-mounted IMU (Body-IMU), and dual Wheel-IMUs plus one Body-IMU as examples for analysis and comparison. Field tests illustrate that the proposed multi-IMU DR system outperforms the single Wheel-INS in terms of both positioning and heading accuracy. By comparing with the centralized filter, the proposed distributed filter shows unimportant accuracy degradation while holds significant computation efficiency. Moreover, among the three multi-IMU configurations, the one Body-IMU plus one Wheel-IMU design obtains the minimum drift rate. The position drift rates of the three configurations are 0.82\\% (dual Wheel-IMUs), 0.69\\% (one Body-IMU plus one Wheel-IMU), and 0.73\\% (dual Wheel-IMUs plus one Body-IMU), respectively.",
                    "Odometry estimation is crucial for every autonomous system requiring navigation in an unknown environment. In modern mobile robots, 3D LiDAR-inertial systems are often used for this task. By fusing LiDAR scans and IMU measurements, these systems can reduce the accumulated drift caused by sequentially registering individual LiDAR scans and provide a robust pose estimate. Although effective, LiDAR-inertial odometry systems require proper parameter tuning to be deployed. In this paper, we propose LIO-EKF, a tightly-coupled LiDAR-inertial odometry system based on point-to-point registration and the classical extended Kalman filter scheme. We propose an adaptive data association that considers the relative pose uncertainty, the map discretization errors, and the LiDAR noise. In this way, we can substantially reduce the parameters to tune for a given type of environment. The experimental evaluation suggests that the proposed system performs on par with the state-of-the-art LiDAR-inertial odometry pipelines but is significantly faster in computing the odometry. The source code of our implementation is publicly available (https://github.com/YibinWu/LIO-EKF).",
                    "A reliable pose estimator robust to environmental disturbances is desirable for mobile robots. To this end, inertial measurement units (IMUs) play an important role because they can perceive the full motion state of the vehicle independently. However, it suffers from accumulative error due to inherent noise and bias instability, especially for low-cost sensors. In our previous studies on Wheel-INS \\cite{niu2021, wu2021}, we proposed to limit the error drift of the pure inertial navigation system (INS) by mounting an IMU to the wheel of the robot to take advantage of rotation modulation. However, Wheel-INS still drifted over a long period of time due to the lack of external correction signals. In this letter, we propose to exploit the environmental perception ability of Wheel-INS to achieve simultaneous localization and mapping (SLAM) with only one IMU. To be specific, we use the road bank angles (mirrored by the robot roll angles estimated by Wheel-INS) as terrain features to enable the loop closure with a Rao-Blackwellized particle filter. The road bank angle is sampled and stored according to the robot position in the grid maps maintained by the particles. The weights of the particles are updated according to the difference between the currently estimated roll sequence and the terrain map. Field experiments suggest the feasibility of the idea to perform SLAM in Wheel-INS using the robot roll angle estimates. In addition, the positioning accuracy is improved significantly (more than 30\\%) over Wheel-INS. The source code of our implementation is publicly available (https://github.com/i2Nav-WHU/Wheel-SLAM)."
                ],
                "domain": [
                    "Inertial Navigation",
                    "Autonomous Systems",
                    "SLAM",
                    "Robotics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "edbed369-c137-4c1d-80c6-b52ee002aec5": {
                "pk": "edbed369-c137-4c1d-80c6-b52ee002aec5",
                "project_name": null,
                "name": "Junfan Xia",
                "bio": "I am a researcher dedicated to advancing the field of machine-learned interatomic potentials, with a particular focus on developing robust and efficient models for atomistic systems. My recent work has led to the creation of the recursively embedded atom neural network model, which effectively incorporates complete many-body correlations while addressing challenges related to local completeness and nonlocality. This innovative approach allows for the seamless integration of high-order features without the computational burden typically associated with them.\n\nI have also developed a comprehensive deep neural network package that represents energies, forces, dipole moments, and polarizabilities of various atomistic systems. This package leverages the strengths of both physically inspired atomic descriptors and message-passing neural networks, ensuring state-of-the-art accuracy and scalability. Implemented in the PyTorch framework, it is designed for high efficiency and low memory usage, making it suitable for large-scale molecular dynamics simulations.\n\nMy goal is to provide an open-source toolbox that facilitates future research and applications in machine-learned potential energy surfaces and quantum-chemical properties. I am passionate about bridging the gap between theoretical advancements and practical applications, and I am excited to contribute to the ongoing evolution of computational materials science.",
                "collaborators": [
                    "Yaolong Zhang",
                    "Bin Jiang"
                ],
                "pub_titles": [
                    "Physically Motivated Recursively Embedded Atom Neural Networks: Incorporating Local Completeness and Nonlocality",
                    "REANN: A PyTorch-based End-to-End Multi-functional Deep Neural Network Package for Molecular, Reactive and Periodic Systems"
                ],
                "pub_abstracts": [
                    "Recent advances in machine-learned interatomic potentials largely benefit from the atomistic representation and locally invariant many-body descriptors. It was however recently argued that including three- (or even four-) body features is incomplete to distinguish specific local structures. Utilizing an embedded density descriptor made by linear combinations of neighboring atomic orbitals and realizing that each orbital coefficient physically depends on its own local environment, we propose a recursively embedded atom neural network model. We formally prove that this model can efficiently incorporate complete many-body correlations without explicitly computing high-order terms. This model not only successfully addresses challenges regarding local completeness and nonlocality in representative systems, but also provides an easy and general way to update local many-body descriptors to have a message-passing form without changing their basic structures.",
                    "In this work, we present a general purpose deep neural network package for representing energies, forces, dipole moments, and polarizabilities of atomistic systems. This so-called recursively embedded atom neural network model takes both advantages of the physically inspired atomic descriptor based neural networks and the message-passing based neural networks. Implemented in the PyTorch framework, the training process is parallelized on both CPU and GPU with high efficiency and low memory, in which all hyperparameters can be optimized automatically. We demonstrate the state-of-the-art accuracy, high efficiency, scalability, and universality of this package by learning not only energies (with or without forces), but also dipole moment vectors and polarizability tensors, in various molecular, reactive, and periodic systems. An interface between a trained model and LAMMPs is provided for large scale molecular dynamics simulations. We hope that this open-source toolbox will allow future method development and applications of machine learned potential energy surfaces and quantum-chemical properties of molecules, reactions, and materials."
                ],
                "domain": [
                    "Machine Learning",
                    "Neural Networks",
                    "Computational Chemistry",
                    "Interatomic Potentials"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "129bdedd-dee6-455d-83ea-d705e7484fe7": {
                "pk": "129bdedd-dee6-455d-83ea-d705e7484fe7",
                "project_name": null,
                "name": "Yaolong Zhang",
                "bio": "I am a researcher specializing in the intersection of machine learning and molecular simulations, with a focus on developing advanced interatomic potentials that accurately capture the complexities of molecular interactions in various environments. My recent work has led to the creation of innovative models such as the Field-Induced Recursively Embedded Atom Neural Network (FIREANN), which integrates external field effects into atomic descriptors, enabling precise simulations of molecular systems under electric fields. \n\nI have also pioneered the Embedded Atom Neural Network (EANN) approach, which simplifies the representation of high-dimensional potential energy surfaces while maintaining accuracy and efficiency. This model has proven to be particularly effective for both molecular and periodic systems, allowing for rapid molecular dynamics simulations. My research extends to tensorial properties, where I have developed machine learning models that preserve rotational symmetry, facilitating the study of molecular spectra and intermolecular forces.\n\nIn addition to these contributions, I have explored nonadiabatic effects in chemical reactions, particularly at metal surfaces, and have developed methods to accurately represent electronic friction tensors. My work aims to bridge the gap between classical and quantum mechanical approaches, providing robust tools for simulating complex chemical systems. I am passionate about advancing the field of computational chemistry through innovative machine learning techniques, and I strive to make my research accessible and applicable to a wide range of scientific inquiries.",
                "collaborators": [
                    "Bin Jiang",
                    "Ce Hu",
                    "Junfan Xia",
                    "Jun Jiang",
                    "Hua Guo",
                    "Reinhard J. Maurer",
                    "Rongrong Yin",
                    "Qidong Lin",
                    "Bin Zhao",
                    "Chaoqiang Feng"
                ],
                "pub_titles": [
                    "Universal Machine Learning for the Response of Atomistic Systems to External Fields",
                    "Embedded Atom Neural Network Potentials: Efficient and Accurate Machine Learning with a Physically Inspired Representation",
                    "Accelerating Atomistic Simulations with Piecewise Machine Learned Ab Initio Potentials at Classical Force Field-like Cost",
                    "Physically Motivated Recursively Embedded Atom Neural Networks: Incorporating Local Completeness and Nonlocality",
                    "Learning Dipole Moments and Polarizabilities",
                    "REANN: A PyTorch-based End-to-End Multi-functional Deep Neural Network Package for Molecular, Reactive and Periodic Systems",
                    "SchrödingerNet: A Universal Neural Network Solver for The Schrödinger Equation",
                    "Symmetry-Adapted High Dimensional Neural Network Representation of Electronic Friction Tensor of Adsorbates on Metals",
                    "Strong Vibrational Relaxation of NO Scattered from Au(111): Importance of an Accurate Adiabatic Potential Energy Surface",
                    "Automatically growing global reactive neural network potential energy surfaces: a trajectory free active learning strategy",
                    "Efficient Sampling for Machine Learning Electron Density and Its Response in Real Space",
                    "Efficient and Accurate Simulations of Vibrational and Electronic Spectra with Symmetry-Preserving Neural Network Models for Tensorial Properties",
                    "Determining the effect of hot electron dissipation on molecular scattering experiments at metal surfaces",
                    "Adiabatic and Nonadiabatic Energy Dissipation during Scattering of Vibrationally Excited CO from Au(111)"
                ],
                "pub_abstracts": [
                    "Machine learned interatomic interaction potentials have enabled efficient and accurate molecular simulations of closed systems. However, external fields, which can greatly change the chemical structure and/or reactivity, have been seldom included in current machine learning models. This work proposes a universal field-induced recursively embedded atom neural network (FIREANN) model, which integrates a pseudo field vector-dependent feature into atomic descriptors to represent system-field interactions with rigorous rotational equivariance. This \"all-in-one\" approach correlates various response properties like dipole moment and polarizability with the field-dependent potential energy in a single model, very suitable for spectroscopic and dynamics simulations in molecular and periodic systems in the presence of electric fields. Especially for periodic systems, we find that FIREANN can overcome the intrinsic multiple-value issue of the polarization by training atomic forces only. These results validate the universality and capability of the FIREANN method for efficient first-principles modeling of complicated systems in strong external fields.",
                    "We propose a simple, but efficient and accurate machine learning (ML) model for developing high-dimensional potential energy surface. This so-called embedded atom neural network (EANN) approach is inspired by the well-known empirical embedded atom method (EAM) model used in condensed phase. It simply replaces the scalar embedded atom density in EAM with a Gaussian-type orbital based density vector, and represents the complex relationship between the embedded density vector and atomic energy by neural networks. We demonstrate that the EANN approach is equally accurate as several established ML models in representing both big molecular and extended periodic systems, yet with much fewer parameters and configurations. It is highly efficient as it implicitly contains the three-body information without an explicit sum of the conventional costly angular descriptors. With high accuracy and efficiency, EANN potentials can vastly accelerate molecular dynamics and spectroscopic simulations in complex systems at ab initio level.",
                    "Machine learning methods have nowadays become easy-to-use tools for constructing high-dimensional interatomic potentials with ab initio accuracy. Although machine learned interatomic potentials are generally orders of magnitude faster than first-principles calculations, they remain much slower than classical force fields, at the price of using more complex structural descriptors. To bridge this efficiency gap, we propose an embedded atom neural network approach with simple piecewise switching function based descriptors, resulting in a favorable linear scaling with the number of neighbor atoms. Numerical examples validate that this piecewise machine learning model can be over an order of magnitude faster than various popular machine learned potentials with comparable accuracy for both metallic and covalent materials, approaching the speed of the fastest embedded atom method (i.e. several {\\mu}s/atom per CPU core). The extreme efficiency of this approach promises its potential in first-principles atomistic simulations of very large systems and/or in long timescale.",
                    "Recent advances in machine-learned interatomic potentials largely benefit from the atomistic representation and locally invariant many-body descriptors. It was however recently argued that including three- (or even four-) body features is incomplete to distinguish specific local structures. Utilizing an embedded density descriptor made by linear combinations of neighboring atomic orbitals and realizing that each orbital coefficient physically depends on its own local environment, we propose a recursively embedded atom neural network model. We formally prove that this model can efficiently incorporate complete many-body correlations without explicitly computing high-order terms. This model not only successfully addresses challenges regarding local completeness and nonlocality in representative systems, but also provides an easy and general way to update local many-body descriptors to have a message-passing form without changing their basic structures.",
                    "Machine learning of scalar molecular properties such as potential energy has enabled widespread applications. However, there are relatively few machine learning models targeting directional properties, including permanent and transition dipole (multipole) moments, as well as polarizability. These properties are essential to determine intermolecular forces and molecular spectra. In this chapter, we review machine learning models for these tensorial properties, with special focus on how to encode the rotational equivariance into these models by taking a similar form as the physical definition of these properties. You will then learn how to use an embedded atom neural network model to train dipole moments and polarizabilities of a representative molecule. The methodology discussed in this chapter can be extended to learn similar or higher-rank tensorial properties, such as magnetic dipole moments, non-adiabatic coupling vectors, and hyperpolarizabilities.",
                    "In this work, we present a general purpose deep neural network package for representing energies, forces, dipole moments, and polarizabilities of atomistic systems. This so-called recursively embedded atom neural network model takes both advantages of the physically inspired atomic descriptor based neural networks and the message-passing based neural networks. Implemented in the PyTorch framework, the training process is parallelized on both CPU and GPU with high efficiency and low memory, in which all hyperparameters can be optimized automatically. We demonstrate the state-of-the-art accuracy, high efficiency, scalability, and universality of this package by learning not only energies (with or without forces), but also dipole moment vectors and polarizability tensors, in various molecular, reactive, and periodic systems. An interface between a trained model and LAMMPs is provided for large scale molecular dynamics simulations. We hope that this open-source toolbox will allow future method development and applications of machine learned potential energy surfaces and quantum-chemical properties of molecules, reactions, and materials.",
                    "Recent advances in machine learning have facilitated numerically accurate solution of the electronic Schr\\\"{o}dingerNet equation (SE) by integrating various neural network (NN)-based wavefunction ansatzes with variational Monte Carlo methods. Nevertheless, such NN-based methods are all based on the Born-Oppenheimer approximation (BOA) and require a separate and computationally expensive training for each nuclear configuration. In this work, we propose a novel NN architecture, Schr\\\"{o}dingerNet, to solve the full electronic-nuclear SE by defining a loss function designed to equalize local energies across the system. This approach is based on a rotationally invariant total wavefunction ansatz that includes both nuclear and electronic coordinates. This strategy allows for an efficient and accurate generation of a continuous potential energy surface at any geometry within the well-sampled nuclear configuration space, and also incorporates non-BO corrections, through a single training process. Comparison with benchmarks of atomic and molecular systems demonstrates its accuracy and efficiency",
                    "Nonadiabatic effects in chemical reaction at metal surfaces, due to excitation of electron-hole pairs, stand at the frontier of the studies of gas-surface reaction dynamics. However, the first principles description of electronic excitation remains challenging. In an efficient molecular dynamics with electronic friction (MDEF) method, the nonadiabatic couplings are effectively included in a so-called electronic friction tensor (EFT), which can be computed from first-order time-dependent perturbation theory (TDPT) in terms of density functional theory (DFT) orbitals. This second-rank tensor depends on adsorbate position and features a complicated transformation with regard to the intrinsic symmetry operations of the system. In this work, we develop a new symmetry-adapted neural network representation of EFT, based on our recently proposed embedded atom neural network (EANN) framework. Inspired by the derivation of the nonadiabatic coupling matrix, we represent the tensorial friction by the first and second derivatives of multiple outputs of NNs with respect to atomic Cartesian coordinates. This rigorously preserves the positive semidefiniteness, directional property, and correct symmetry-equivariance of EFT. Unlike previous methods, our new approach can readily include both molecular and surface degrees of freedom, regardless of the type of surface. Tests on the H2+Ag(111) system show that this approach yields an accurate, efficient, and continuous representation of EFT, making it possible to perform large scale TDPT-based MDEF simulations to study both adiabatic and nonadiabatic energy dissipation in a unified framework.",
                    "Experimental observations of multi-quantum relaxation of highly vibrationally excited NO scattering from Au(111) are a benchmark for the breakdown of Born-Oppenheimer approximation in molecule-surface systems. This remarkable vibrational inelasticity was long thought to be almost exclusively mediated by electron transfer; but, no theories have quantitatively reproduced various experimental data. This was suggested to be due to errors in the adiabatic potential energy surface (PES) used in those studies. Here, we investigate electronically adiabatic molecular dynamics of this system with a globally accurate high dimensional PES, newly developed with neural networks from first principles. The NO vibrational energy loss is much larger than that on earlier adiabatic PES. Additionally, the translational inelasticity and translational energy dependence of vibrational inelasticity are also more accurately reproduced. There is reason to be optimistic that electronically nonadiabatic theories using this adiabatic PES as a starting point might accurately reproduce experimental results on this important system.",
                    "An efficient and trajectory-free active learning method is proposed to automatically sample data points for constructing globally accurate reactive potential energy surfaces (PESs) using neural networks (NNs). Although NNs do not provide the predictive variance as the Gaussian process regression does, we can alternatively minimize the negative of the squared difference surface (NSDS) given by two different NN models to actively locate the point where the PES is least confident. A batch of points in the minima of this NSDS can be iteratively added into the training set to improve the PES. The configuration space is gradually and globally covered with no need to run classical trajectory (or equivalently molecular dynamics) simulations. Through refitting the available analytical PESs of H3 and OH3 reactive systems, we demonstrate the efficiency and robustness of this new strategy, which enables fast convergence of the reactive PESs with respect to the number of points in terms of quantum scattering probabilities.",
                    "Electron density is a fundamental quantity, which can in principle determine all ground state electronic properties of a given system. Although machine learning (ML) models for electron density based on either an atom-centered basis or a real-space grid have been proposed, the demand for the number of high-order basis functions or grid points is enormous. In this work, we propose an efficient grid-point sampling strategy that combines a targeted sampling favoring large density and a screening of grid points associated with linearly independent atomic features. This new sampling strategy is integrated with a field-induced recursively embedded atom neural network model to develop a real-space grid-based ML model for electron density and its response to an electric field. This approach is applied to a QM9 molecular dataset, a H2O/Pt(111) interfacial system, and an Au(100) electrode under an electric field. The number of training points is found much smaller than previous models, when yielding comparably accurate predictions for the electron density of the entire grid. The resultant machine learned electron density model enables us to properly partition partial charge onto each atom and analyze the charge variation upon proton transfer in the H2O/Pt(111) system. The machined learned electronic response model allows us to predict charge transfer and the electrostatic potential change induced by an electric field in an Au(100) electrode.",
                    "Machine learning has revolutionized the high-dimensional representations for molecular properties such as potential energy. However, there are scarce machine learning models targeting tensorial properties, which are rotationally covariant. Here, we propose tensorial neural network (NN) models to learn both tensorial response and transition properties, in which atomic coordinate vectors are multiplied with scalar NN outputs or their derivatives to preserve the rotationally covariant symmetry. This strategy keeps structural descriptors symmetry invariant so that the resulting tensorial NN models are as efficient as their scalar counterparts. We validate the performance and universality of this approach by learning response properties of water oligomers and liquid water, and transition dipole moment of a model structural unit of proteins. Machine learned tensorial models have enabled efficient simulations of vibrational spectra of liquid water and ultraviolet spectra of realistic proteins, promising feasible and accurate spectroscopic simulations for biomolecules and materials.",
                    "Nonadiabatic effects that arise from the concerted motion of electrons and atoms at comparable energy and time scales are omnipresent in thermal and light-driven chemistry at metal surfaces. Excited (hot) electrons can measurably affect molecule-metal reactions by contributing to state-dependent reaction probabilities. Vibrational state-to-state scattering of NO on Au(111) has been one of the most studied examples in this regard, providing a testing ground for developing various nonadiabatic theories. This system is often cited as the prime example for the failure of electronic friction theory, a very efficient model accounting for dissipative forces on metal-adsorbed molecules due to the creation of hot electrons in the metal. However, the exact failings compared to experiment and their origin from theory are not established for any system, because dynamic properties are affected by many compounding simulation errors of which the quality of nonadiabatic treatment is just one. We use a high-dimensional machine learning representation of electronic structure theory to minimize errors that arise from quantum chemistry. This allows us to perform a comprehensive quantitative analysis of the performance of nonadiabatic molecular dynamics in describing vibrational state-to-state scattering of NO on Au(111) and compare directly to adiabatic results. We find that electronic friction theory accurately predicts elastic and single-quantum energy loss, but underestimates multi-quantum energy loss and overestimates molecular trapping at high vibrational excitation. Our analysis reveals that multi-quantum energy loss can potentially be remedied within friction theory, whereas the overestimation of trapping constitutes a genuine breakdown of electronic friction theory. Addressing this overestimation for dynamic processes in catalysis and surface chemistry will require more sophisticated theories.",
                    "A high-dimensional potential energy surface (PES) for CO interaction with the Au(111) surface is developed using a machine-learning algorithm. Including both molecular and surface coordinates, this PES enables the simulation of the recent experiment on scattering of vibrationally excited CO from Au(111). Trapping in a physisorption well is observed to increase with decreasing incidence energy. While energy dissipation of physisorbed CO is slow, due to weak coupling with both the phonons and electron-hole pairs, its access to the chemisorption well facilitates fast vibrational relaxation of CO through nonadiabatic coupling with surface electron-hole pairs."
                ],
                "domain": [
                    "Machine Learning",
                    "Molecular Simulations",
                    "Interatomic Potentials",
                    "Quantum Chemistry"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "3757ecaf-c312-4660-b9a0-6af775246e91": {
                "pk": "3757ecaf-c312-4660-b9a0-6af775246e91",
                "project_name": null,
                "name": "Bin Jiang",
                "bio": "I am a researcher deeply engaged in the intersection of urban studies, geography, and complex systems. My work primarily focuses on understanding the intricate structures of cities through the lens of scaling laws and fractal geometry. I have proposed a novel perspective on cognitive mapping, emphasizing the significance of scaling in urban artifacts and locations, which challenges traditional views in the field.\n\nMy recent publications explore the topological patterns of urban street networks, revealing their small-world and scale-free properties, and how these characteristics influence traffic flow and human movement. I advocate for a paradigm shift in geospatial analysis, moving from conventional Gaussian thinking to a Paretian approach that acknowledges the inherent complexities of urban environments. This shift is crucial for harnessing the potential of big data and volunteered geographic information (VGI) in understanding human mobility patterns.\n\nI also emphasize the importance of integrating data with scholarly communication, as seen in my editorial work for special issues on data-intensive geospatial computing. My research aims to bridge the gap between theory and practice, advocating for sustainable urban design grounded in the principles of wholeness and living structures, as articulated by Christopher Alexander. By employing innovative classification schemes like head/tail breaks, I strive to reveal the underlying hierarchies and dynamics of urban systems, ultimately contributing to the creation of more livable and harmonious cities.",
                "collaborators": [],
                "pub_titles": [
                    "Why Can the Image of the City be Formed?",
                    "Editorial: Making GIScience Research More Open Access",
                    "Big Data Is not just a New Type, but a New Paradigm",
                    "Spatial Heterogeneity, Scale, Data Character, and Sustainable Transport in the Big Data Era",
                    "A Topological Pattern of Urban Street Networks: Universality and Peculiarity",
                    "Street Hierarchies: A Minority of Streets Account for a Majority of Traffic Flow",
                    "Volunteered Geographic Information and Computational Geography: New Perspectives",
                    "Line Simplification",
                    "The Status Quo of Architecture and Its Impact on Urban Management: Christopher Alexander's Insights",
                    "The Flow Dimension and Capacity for Structuring Urban Street Networks",
                    "Ranking Spaces for Predicting Human Movement in an Urban Environment",
                    "Head/tail Breaks: A New Classification Scheme for Data with a Heavy-tailed Distribution",
                    "Computing the Image of the City",
                    "Geospatial Analysis Requires a Different Way of Thinking: The Problem of Spatial Heterogeneity",
                    "A Complex-Network Perspective on Alexander's Wholeness",
                    "Head/tail Breaks for Visualization of City Structure and Dynamics",
                    "The Fractal Nature of Maps and Mapping",
                    "A City Is a Complex Network",
                    "A Recursive Definition of Goodness of Space for Bridging the Concepts of Space and Place for Sustainability",
                    "Alexander's Wholeness as the Scientific Foundation of Sustainable Urban Design and Planning"
                ],
                "pub_abstracts": [
                    "The aim of this short paper is to summarize my recent preprint (arXiv:1209.1112), in which I proposed a novel and probably controversial view about cognitive mapping; that is, the image of the city out of the underlying scaling of city artifacts or locations. The scaling refers to a recurring structure of far more small things than large ones. In this paper, I attempt to further clarify the central argument, and identify some possible areas of misunderstanding for readers.",
                    "This is the editorial for the special issue on \"data-intensive geospatial computing\", which I guest edited with the International Journal of Geographical Information Science (Taylor & Francis). As remarked in the editorial, the special issue is particularly special in the sense that all source and data are published together with the published papers. This editorial elaborates on scholarly communication, with particular attention to publishing data alongside papers and the emergence of open access journals, in order to make our research more open access.",
                    "This paper is a first draft of the introduction to the special issue on volunteered geographic information published in Computers, Environment and Urban Systems (2015, 53, 1-122). In this short paper, I put georeferenced big data (hereafter, big data) such as tweets locations in comparison with small data such as census data in terms of data characteristics, and further argued that big data differs fundamentally from small data in terms of data analytics, both geometrially and statistically. I would like to thank my colleague Dr. Jean-Claude Thill, who expanded the draft towards a broader scope.",
                    "In light of the emergence of big data, I have advocated and argued for a paradigm shift from Tobler's law to scaling law, from Euclidean geometry to fractal geometry, from Gaussian statistics to Paretian statistics, and - more importantly - from Descartes' mechanistic thinking to Alexander's organic thinking. Fractal geometry falls under the third definition of fractal - that is, a set or pattern is fractal if the scaling of far more small things than large ones recurs multiple times (Jiang and Yin 2014) - rather than under the second definition of fractal, which requires a power law between scales and details (Mandelbrot 1982). The new fractal geometry is more towards living geometry that \"follows the rules, constraints, and contingent conditions that are, inevitably, encountered in the real world\" (Alexander et al. 2012, p. 395), not only for understanding complexity, but also for creating complex or living structure (Alexander 2002-2005). This editorial attempts to clarify why the paradigm shift is essential and to elaborate on several concepts, including spatial heterogeneity (scaling law), scale (or the fourth meaning of scale), data character (in contrast to data quality), and sustainable transport in the big data era.",
                    "In this paper, we derive a topological pattern of urban street networks using a large sample (the largest so far to the best of our knowledge) of 40 U.S. cities and a few more from elsewhere of different sizes. It is found that all the topologies of urban street networks based on street-street intersection demonstrate a small world structure, and a scale-free property for both street length and connectivity degree. More specifically, for any street network, about 80% of its streets have length or degrees less than its average value, while 20% of streets have length or degrees greater than the average. Out of the 20%, there are less than 1% of streets which can form a backbone of the street network. Based on the finding, we conjecture that the 20% streets account for 80% of traffic flow, and the 1% streets constitute a cognitive map of the urban street network. We illustrate further a peculiarity about the scale-free property.",
                    "Urban streets are hierarchically organized in the sense that a majority of streets are trivial, while a minority of streets is vital. This hierarchy can be simply, but elegantly, characterized by the 80/20 principle, i.e. 80 percent of streets are less connected (below the average), while 20 percent of streets are well connected (above the average); out of the 20 percent, there is 1 percent of streets that are extremely well connected. This paper, using a European city as an example, examined, at a much more detailed level, such street hierarchies from the perspective of geometric and topological properties. Based on an empirical study, we further proved a previous conjecture that a minority of streets accounts for a majority of traffic flow; more accurately, the 20 percent of top streets accommodate 80 percent of traffic flow (20/80), and the 1 percent of top streets account for more than 20 percent of traffic flow (1/20). Our study provides new evidence as to how a city is (self-)organized, contributing to the understanding of cities and their evolution using increasingly available mobility geographic information.",
                    "Volunteered geographic information (VGI), one of the most important types of user-generated web content, has been emerging as a new phenomenon. VGI is contributed by numerous volunteers and supported by web 2.0 technologies. This chapter discusses how VGI provides new perspectives for computational geography, a transformed geography based on the use of data-intensive computing and simulations to uncover the underlying mechanisms behind geographic forms and processes. We provide several exemplars of computational geography using OpenStreetMap data and GPS traces to investigate the scaling of geographic space and its implications for human mobility patterns. We illustrate that the field of geography is experiencing a dramatic change and that geoinformatics and computational geography deserve to be clearly distinguished, with the former being a study of engineering and the latter being a science.   Keywords geoinformatics, openstreetmap, scaling of geographic space, spatial heterogeneity",
                    "As an important practice of map generalization, the aim of line simplification is to reduce the number of points without destroying the essential shape or the salient character of a cartographic curve. This subject has been well-studied in the literature. This entry attempts to introduce how line simplification can be guided by fractal geometry, or the recurring scaling pattern of far more small things than large ones. The line simplification process involves nothing more than removing small things while retaining large ones based on head/tail breaks.",
                    "Christopher Alexander offers a critical perspective on the modernist approach to architecture, which he argues has prioritized innovation, abstraction, and mechanistic efficiency at the expense of human-centered and organic values. This shift has led to the proliferation of buildings that, while visually striking, often feel cold, impersonal, and disconnected from the deeper needs of the people who inhabit them. Alexander advocates for a return to timeless architectural principles such as harmony, balance, and a deep connection to the natural and cultural context. He introduces the concept of living structure, which emphasizes creating spaces that resonate with the intrinsic order found in nature and human life, fostering environments that are not only functional and beautiful but also profoundly life-affirming. By challenging the dominance of \"iconic\" but alienating designs, Alexander calls for a holistic, human-centered approach to architecture-one that prioritizes the well-being of individuals and communities, creating spaces that nurture a sense of place, belonging, and harmony with the world around them.",
                    "This paper aims to measure the efficiency of urban street networks (a kind of complex networks) from the perspective of the multidimensional chain of connectivity (or flow). More specifically, we define two quantities: flow dimension and flow capacity, to characterize structures of urban street networks. To our surprise for the topologies of urban street networks, previously confirmed as a form of small world and scale-free networks, we find that (1) the range of their flow dimension is rather wider than their random and regular counterparts, (2) their flow dimension shows a power-law distribution, and (3) they have a higher flow capacity than their random and regular counterparts. The findings confirm that (1) both the wider range of flow dimension and the higher flow capacity can be a signature of small world networks, and (2) the flow capacity can be an alternative quantity for measuring the efficiency of networks or that of the individual nodes. The findings are illustrated using three urban street networks (two in the Europe and one in the USA).",
                    "A city can be topologically represented as a connectivity graph, consisting of nodes representing individual spaces and links if the corresponding spaces are intersected. It turns out in the space syntax literature that some defined topological metrics can capture human movement rates in individual spaces. In other words, the topological metrics are significantly correlated to human movement rates, and individual spaces can be ranked by the metrics for predicting human movement. However, this correlation has never been well justified. In this paper, we study the same issue by applying the weighted PageRank algorithm to the connectivity graph or space-space topology for ranking the individual spaces, and find surprisingly that (1) the PageRank scores are better correlated to human movement rates than the space syntax metrics, and (2) the underlying space-space topology demonstrates small world and scale free properties. The findings provide a novel justification as to why space syntax, or topological analysis in general, can be used to predict human movement. We further conjecture that this kind of analysis is no more than predicting a drunkard's walking on a small world and scale free network.   Keywords: Space syntax, topological analysis of networks, small world, scale free, human movement, and PageRank",
                    "This paper introduces a new classification scheme - head/tail breaks - in order to find groupings or hierarchy for data with a heavy-tailed distribution. The heavy-tailed distributions are heavily right skewed, with a minority of large values in the head and a majority of small values in the tail, commonly characterized by a power law, a lognormal or an exponential function. For example, a country's population is often distributed in such a heavy-tailed manner, with a minority of people (e.g., 20 percent) in the countryside and the vast majority (e.g., 80 percent) in urban areas. This heavy-tailed distribution is also called scaling, hierarchy or scaling hierarchy. This new classification scheme partitions all of the data values around the mean into two parts and continues the process iteratively for the values (above the mean) in the head until the head part values are no longer heavy-tailed distributed. Thus, the number of classes and the class intervals are both naturally determined. We therefore claim that the new classification scheme is more natural than the natural breaks in finding the groupings or hierarchy for data with a heavy-tailed distribution. We demonstrate the advantages of the head/tail breaks method over Jenks' natural breaks in capturing the underlying hierarchy of the data.   Keywords: data classification, head/tail division rule, natural breaks, scaling, and hierarchy",
                    "Kevin Lynch proposed a theory of the image of the city identifying five elements that make the city legible or imageable. The resulting mental map of the city was conventionally derived through some qualitative processes, relying on interactions with city residents to ask them to recall city elements from their minds. This paper proposes a process by which the image of the city can be quantitatively derived automatically using computer technology and geospatial databases of the city. This method is substantially based on and inspired by Christopher Alexander's living structure and Nikos Salingaros' structural order, as a city with the living structure or structural order tends to be legible and imageable. With the increasing availability of geographic information of urban environments at very fine scales or resolutions (for example, trajectories data about human activities), the proposal or solution described in this paper is particularly timely and relevant for urban studies and architectural design.   Keywords: Mental maps, head/tail division rule, legibility, imageability, power law, scaling, and hierarchy.",
                    "Geospatial analysis is very much dominated by a Gaussian way of thinking, which assumes that things in the world can be characterized by a well-defined mean, i.e., things are more or less similar in size. However, this assumption is not always valid. In fact, many things in the world lack a well-defined mean, and therefore there are far more small things than large ones. This paper attempts to argue that geospatial analysis requires a different way of thinking - a Paretian way of thinking that underlies skewed distribution such as power laws, Pareto and lognormal distributions. I review two properties of spatial dependence and spatial heterogeneity, and point out that the notion of spatial heterogeneity in current spatial statistics is only used to characterize local variance of spatial dependence. I subsequently argue for a broad perspective on spatial heterogeneity, and suggest it be formulated as a scaling law. I further discuss the implications of Paretian thinking and the scaling law for better understanding of geographic forms and processes, in particular while facing massive amounts of social media data. In the spirit of Paretian thinking, geospatial analysis should seek to simulate geographic events and phenomena from the bottom up rather than correlations as guided by Gaussian thinking.",
                    "The wholeness, conceived and developed by Christopher Alexander, is what exists to some degree or other in space and matter, and can be described by precise mathematical language. However, it remains somehow mysterious and elusive, and therefore hard to grasp. This paper develops a complex network perspective on the wholeness to better understand the nature of order or beauty for sustainable design. I bring together a set of complexity-science subjects such as complex networks, fractal geometry, and in particular underlying scaling hierarchy derived by head/tail breaks - a classification scheme and a visualization tool for data with a heavy-tailed distribution, in order to make Alexander's profound thoughts more accessible to design practitioners and complexity-science researchers. Through several case studies (some of which Alexander studied), I demonstrate that the complex-network perspective helps reduce the mystery of wholeness and brings new insights to Alexander's thoughts on the concept of wholeness or objective beauty that exists in fine and deep structure. The complex-network perspective enables us to see things in their wholeness, and to better understand how the kind of structural beauty emerges from local actions guided by the 15 fundamental properties, and by differentiation and adaptation processes. The wholeness goes beyond current complex network theory towards design or creation of living structures.   Keywords: Theory of centers, living geometry, Christopher Alexander, head/tail breaks, and beauty",
                    "The things surrounding us vary dramatically, which implies that there are far more small things than large ones, e.g., far more small cities than large ones in the world. This dramatic variation is often referred to as fractal or scaling. To better reveal the fractal or scaling structure, a new classification scheme, namely head/tail breaks, has been developed to recursively derive different classes or hierarchical levels. The head/tail breaks works as such: divide things into a few large ones in the head (those above the average) and many small ones (those below the average) in the tail, and recursively continue the dividing process for the large ones (or the head) until the notion of far more small things than large ones has been violated. This paper attempts to argue that head/tail breaks can be a powerful visualization tool for illustrating structure and dynamics of natural cities. Natural cities refer to naturally or objectively defined human settlements based on a meaningful cutoff averaged from a massive amount of units extracted from geographic information. To illustrate the effectiveness of head/tail breaks in visualization, I have developed several case studies applied to natural cities derived from the points of interest, social media location data, and time series nighttime images. I further elaborate on head/tail breaks related to fractals, beauty, and big data.   Keywords: Big data, social media, nighttime images, natural cities, fractals, head/tail breaks, ht-index",
                    "A fractal can be simply understood as a set or pattern in which there are far more small things than large ones, e.g., far more small geographic features than large ones on the earth surface, or far more large-scale maps than small-scale maps for a geographic region. This paper attempts to argue and provide evidence for the fractal nature of maps and mapping. It is the underlying fractal structure of geographic features, either natural or human-made, that make reality mappable, large-scale maps generalizable, and cities imageable. The fractal nature is also what underlies the beauty of maps. After introducing some key fractal concepts such as recursion, self-similarity, scaling ratio, and scaling exponent, this paper demonstrates that fractal thought is rooted in long-standing map-making practices such as series maps subdivision, visual hierarchy, and T\\\"opfer's radical law. Drawing on previous studies on head/tail breaks, mapping can be considered a head/tail breaks process; that is to divide things around an average, according to their geometric, topological and/or semantic properties, into the head (for those above the average) and the tail (for those below the average), and recursively continue the dividing process for the head for map generalization, statistical mapping, and cognitive mapping. Given the fractal nature of maps and mapping, cartography should be considered a perfect combination of science and art, and scaling must be formulated as a law of cartography or that of geography in general.   Keywords: Scaling of geographic features, map generalization, statistical mapping, cognitive mapping, head/tail breaks",
                    "A city is not a tree but a semi-lattice. To use a perhaps more familiar term, a city is a complex network. The complex network constitutes a unique topological perspective on cities and enables us to better understand the kind of problem a city is. The topological perspective differentiates it from the perspectives of Euclidean geometry and Gaussian statistics that deal with essentially regular shapes and more or less similar things. Many urban theories, such as the Central Place Theory, Zipf's Law, the Image of the City, and the Theory of Centers can be interpreted from the point of view of complex networks. A livable city consists of far more small things than large ones, and their shapes tend to be irregular and rough. This chapter illustrates the complex network view and argues that we must abandon the kind of thinking (mis-)guided by Euclidean geometry and Gaussian statistics, and instead adopt fractal geometry, power-law statistics, and Alexander's living geometry to develop sustainable cities.   Keywords: Scaling, living structure, theory of centers, objective beauty, head/tail breaks",
                    "Conceived and developed by Christopher Alexander through his life's work: The Nature of Order, wholeness is defined as a mathematical structure of physical space in our surroundings. Yet, there was no mathematics, as Alexander admitted then, that was powerful enough to capture his notion of wholeness. Recently, a mathematical model of wholeness, together with its topological representation, has been developed that is capable of addressing not only why a space is good, but also how much goodness the space has. This paper develops a structural perspective on goodness of space - both large- and small-scale - in order to bridge two basic concepts of space and place through the very concept of wholeness. The wholeness provides a de facto recursive definition of goodness of space from a holistic and organic point of view. A space is good, genuinely and objectively, if its adjacent spaces are good, the larger space to which it belongs is good, and what is contained in the space is also good. Eventually, goodness of space - sustainability of space - is considered a matter of fact rather than of opinion under the new view of space: space is neither lifeless nor neutral, but a living structure capable of being more living or less living, or more sustainable or less sustainable. Under the new view of space, geography or architecture will become part of complexity science, not only for understanding complexity, but also for making and remaking complex or living structures. Keywords: Scaling law, head/tail breaks, living structure, beauty, streets, cities",
                    "As Christopher Alexander conceived and defined through his life's work - The Nature of Order - wholeness is a recursive structure that recurs in space and matter and is reflected in human minds and cognition. Based on the definition of wholeness, a mathematical model of wholeness, together with its topological representation, has been developed, and it is able to address not only why a structure is beautiful, but also how much beauty the structure has. Given the circumstance, this paper is attempted to argue for the wholeness as the scientific foundation of sustainable urban design and planning, with the help of the mathematical model and topological representation. We start by introducing the wholeness as a mathematical structure of physical space that pervasively exists in our surroundings, along with two fundamental laws - scaling law and Tobler's law - that underlie the 15 properties for characterizing and making living structures. We argue that urban design and planning can be considered to be wholeness-extending processes, guided by two design principles of differentiation and adaptation, to transform a space - in a piecemeal fashion - into a living or more living structure. We further discuss several other urban design theories and how they can be justified by and placed within the theory of wholeness. With the wholeness as the scientific foundation, urban design can turn into a rigorous science with creation of living structures as the primary aim.   Keywords: Beauty, life, scaling law, adaptation, differentiation, and organic world view"
                ],
                "domain": [
                    "Geospatial Analysis",
                    "Urban Studies",
                    "Complex Networks",
                    "Fractal Geometry"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "[Question 1] - What is the problem?  \nHow can we improve the interpretability of deep learning models in high-stakes applications such as healthcare and finance?\n\n[Question 2] - Why is it interesting and important?  \nImproving the interpretability of deep learning models is crucial for building trust and accountability in AI systems, especially in high-stakes domains where decisions can significantly impact human lives. By addressing this problem, we can enhance the transparency of AI systems, enabling practitioners to understand model behavior, validate decisions, and comply with regulatory requirements. This research could lead to the development of more robust and reliable AI applications, fostering greater adoption and integration of machine learning technologies in critical sectors.\n\n[Question 3] - Why is it hard?  \nThe challenge of improving interpretability lies in the inherent complexity of deep learning models, which often operate as \"black boxes.\" Naive approaches, such as simply visualizing model weights or outputs, may fail to capture the intricate relationships and interactions within the data. Additionally, there are theoretical obstacles, such as the trade-off between model accuracy and interpretability, and practical challenges, including the need for domain-specific knowledge to contextualize model explanations. Overcoming these complexities requires innovative methodologies that can balance performance with clarity.\n\n[Question 4] - Why hasn't it been solved before?  \nPrevious research has often focused on enhancing model accuracy rather than interpretability, leading to a lack of effective frameworks for understanding complex models. Existing solutions may be limited in scope, often addressing only specific types of models or datasets. Barriers such as the absence of standardized metrics for interpretability and the difficulty in generalizing findings across different applications have hindered progress. My approach aims to integrate interpretability techniques with state-of-the-art deep learning architectures, providing a more comprehensive solution that addresses these limitations.\n\n[Question 5] - What are the key components of my approach and results?  \nMy proposed methodology involves developing a hybrid framework that combines attention mechanisms with interpretable model architectures, utilizing a diverse dataset from healthcare and finance. I will employ metrics such as SHAP (SHapley Additive exPlanations) values and user studies to evaluate interpretability and usability. The expected outcomes include a set of interpretable models that maintain competitive performance while providing clear, actionable insights into their decision-making processes, ultimately contributing to safer and more trustworthy AI applications."
    },
    "2409.18815": {
        "paper_data": {
            "title": "Seeing the Invisible through Speckle Images",
            "url": "http://arxiv.org/abs/2409.18815v1",
            "arxiv_id": "2409.18815",
            "authors": [
                "Weiru Fan",
                "Xiaobin Tang",
                "Xingqi Xu",
                "Huizhu Hu",
                "Vladislav V. Yakovlev",
                "Shi-Yao Zhu",
                "Da-Wei Wang",
                "Delong Zhang"
            ],
            "abstract": "Scattering obscures information carried by wave by producing a speckle pattern, posing a common challenge across various fields, including microscopy and astronomy. Traditional methods for extracting information from speckles often rely on significant physical assumptions, complex devices, or intricate algorithms. Recently, machine learning has emerged as a scalable and widely adopted tool for interpreting speckle patterns. However, most current machine learning techniques depend heavily on supervised training with extensive labeled datasets, which is problematic when labels are unavailable. To address this, we propose a strategy based on unsupervised learning for speckle recognition and evaluation, enabling to capture high-level information, such as object classes, directly from speckles without labeled data. By deriving invariant features from speckles, this method allows for the classification of speckles and facilitates diverse applications in image sensing. We experimentally validated our strategy through two significant applications: a noninvasive glucose monitoring system capable of differentiating time-lapse glucose concentrations, and a high-throughput communication system utilizing multimode fibers in dynamic environments. The versatility of this method holds promise for a broad range of far-reaching applications, including biomedical diagnostics, quantum network decoupling, and remote sensing.",
            "introduction": " Introduction When coherent waves pass through inhomogeneous media, elastic light scattering leads to random wave interference producing speckle patterns characterized by a variable distribution of intensities [1, 2]. This effect often disrupts the information carried by the incident wave, hindering the imaging and sensing capabilities of optical systems. For many applications involving coherent waves, such as optical and ultrasound imaging through turbid media, speckle is often considered noise, and sophisticated signal/image processing results for the data transport in dynamic MMF. The image in Fig. 4d is repeatedly transmitted 5000 times to obtain statistics of transport fidelity. aandbemploy the Pearson correlation coefficient (PCC), while candduse the structural similarity index measure (SSIM) to evaluate image fidelity. Ave. stands for the average value, and S.D. stands for the standard deviation. 27 Results of predicted classes. e,Quantita- tive Methods Experimental Details Experimental Setup : As shown in Extended Data Fig. 2, a continuous wave with a 532 nm wavelength was generated by laser (YFA-SF-1064-50-CW, Precilaser), and injected into a single-mode optical fiber with a length of 3m to shape the beam pro- file. To match the polarization state of the spatial light modulator (SLM; X13139-09, Hamamatsu), a zero-order half-wave plate (HWP) and a polarizing beam splitter (PBS) were inserted to produce the polarization along the SLM and control the power of the light. In the case of scattering media (ZnO with 400 µm thickness or ground glass with 600 grit), the beam was focused by an objective lens (20 ×/0.4NA, Olympus). For MMF (SR-opt-8039, 2 m length, ∅105µm, 0.22NA, Andor) and glucose aqueous solu- tion, the beam was focused through an objective lens with a larger numerical aperture (50×/0.6NA, Nikon). Then, speckles were collected by an objective lens (10 ×/0.3NA, Olympus) and recorded by a charge-coupled device (CCD; Prosilica GT1910, AVT) for further subsequent information extraction. Dynamic environment for MMF : A MMF with a length of 2 m was wound into coils with a 10 cm diameter. To mimic environmental disturbances, an inhomogeneous stress was imposed on such coils by a motorized rotator with a constant velocity of 20 degrees/s and two loose clamps, where friction was randomly applied to the MMF. The collected speckles were randomly attenuated in time to verify the compatibility of SURE with intensity modulation for optical communications. When the nonlin- ear effect of the fiber can be ignored, the time-sequence random attenuation of the collected data is equivalent to directly attenuating the power of light. 12Glucose solution preparation : The glucose stock solution was prepared with a concentration of 0.305 mol/L (D-glucose: 549 mg, deionized water: 10 mL). In the experiment, 3 mL of water and 0.08 mL of the stock solution were added into the cuvette to obtain an 8 mmol/L glucose aqueous solution corresponding to the average level of a healthy people. Subsequently, 0.04 mL of the stock solution was continued to be added to obtain a 12 mmol/L corresponding to the average level of patient with hyperglycemia. Noted that the cuvette was wrapped with two layers of scotch tape to simulate thin scattering media. The rear surface of the cuvette had an extra layer of egg membrane as a thick backscattering layer. Data acquisition In the task of speckle image clustering, the MNIST dataset was used as the modulation phase map, where the size of each image was interpolated from 28 ×28 to",
            "references": [
                {
                    "title": "Noninvasive blood glucose sensing by secondary speckle pattern artificial intelligence analyses",
                    "abstract": "Abstract. Significance Diabetes is a prevalent disease worldwide that can cause severe health problems. Accurate blood glucose detection is crucial for diabetes management, and noninvasive methods can be more convenient and less painful than traditional finger-prick methods. Aim We aim to report a noncontact speckle-based blood glucose measurement system that utilizes artificial intelligence (AI) data processing to improve glucose detection accuracy. The study also explores the influence of an alternating current (AC) induced magnetic field on the sensitivity and selectivity of blood glucose detection. Approach The proposed blood glucose sensor consists of a digital camera, an AC-generated magnetic field source, a laser illuminating the subject’s finger, and a computer. A magnetic field is applied to the finger, and a camera records the speckle patterns generated by the laser light reflected from the finger. The acquired video data are preprocessed for machine learning (ML) and deep neural networks (DNNs) to classify blood plasma glucose levels. The standard finger-prick method is used as a reference for blood glucose level classification. Results The study found that the noncontact speckle-based blood glucose measurement system with AI data processing allows for the detection of blood plasma glucose levels with high accuracy. The ML approach gives better results than the tested DNNs as the proposed data preprocessing is highly selective and efficient. Conclusions The proposed noncontact blood glucose sensing mechanism utilizing AI data processing and a magnetic field can potentially improve glucose detection accuracy, making it more convenient and less painful for patients. The system also allows for inexpensive blood glucose sensing mechanisms and fast blood glucose screening. The results suggest that noninvasive methods can improve blood glucose detection accuracy, which can have significant implications for diabetes management. Investigations involving representative sampling data, including subjects of different ages, gender, race, and health status, could allow for further improvement."
                },
                {
                    "title": "Backpropagation-free training of deep physical neural networks",
                    "abstract": "Recent successes in deep learning for vision and natural language processing are attributed to larger models but come with energy consumption and scalability issues. Current training of digital deep-learning models primarily relies on backpropagation that is unsuitable for physical implementation. In this work, we propose a simple deep neural network architecture augmented by a physical local learning (PhyLL) algorithm, which enables supervised and unsupervised training of deep physical neural networks without detailed knowledge of the nonlinear physical layer’s properties. We trained diverse wave-based physical neural networks in vowel and image classification experiments, showcasing the universality of our approach. Our method shows advantages over other hardware-aware training schemes by improving training speed, enhancing robustness, and reducing power consumption by eliminating the need for system modeling and thus decreasing digital computation. Editor’s summary The recent development of large-scale deep neural networks (NNs) and other artificial intelligence (AI) applications is accompanied by growing concerns about the energy consumption needed to train and operate them. Physical NNs could become a solution to this problem, but the direct hardware implementation of conventional algorithms faces multiple difficulties. For instance, training NNs using conventional backpropagation algorithms is associated with challenges such as lack of scalability, complexity of operation during training, and dependency on digitally trained models. Inspired by the forward-forward algorithm, Momeni et al. report the practical demonstration of backpropagation-free training of wave-based physical NNs. Their work is an important step in optimizing the energy-intensive training step in NNs for more efficient solutions for modern AI systems. —Yury Suleymanov Wave-based physical neural networks were trained without backpropagation using a forward-forward algorithm."
                },
                {
                    "title": "Non-invasive blood glucose sensing by machine learning of optic fiber-based speckle pattern variation",
                    "abstract": "Abstract. Significance: The ability to perform frequent non-invasive monitoring of glucose in the bloodstream is very applicable for diabetic patients. Aim: We experimentally verified a non-invasive multimode fiber-based technique for sensing glucose concentration in the bloodstream by extracting and analyzing the collected speckle patterns. Approach: The proposed sensor consists of a laser source, digital camera, computer, multimode fiber, and alternating current (AC) generated magnetic field source. The experiments were performed using a covered (with cladding and jacket) and uncovered (without cladding and jacket) multimode fiber touching the skin under a magnetic field and without it. The subject’s finger was placed on a fiber to detect the glucose concentration. The method tracks variations in the speckle patterns due to light interaction with the bloodstream affected by blood glucose. Results: The uncovered fiber placed above the finger under the AC magnetic field (150 G) at 140 Hz was found to have a lock-in amplification role, improving the glucose detection precision. The application of the machine learning algorithms in preprocessed speckle pattern data increase glucose measurement accuracy. Classification of the speckle patterns for uncovered fiber under the AC magnetic field allowed for detection of the blood glucose with high accuracy for all tested subjects compared with other tested configurations. Conclusions: The proposed technique was theoretically analyzed and experimentally validated in this work. The results were verified by the traditional finger-prick method, which was also used for classification as a conventional reference marker of blood glucose levels. The main goal of the proposed technique was to develop a non-invasive, low-cost blood glucose sensor for easy use by humans."
                },
                {
                    "title": "Image sensing with multilayer nonlinear optical neural networks",
                    "abstract": null
                },
                {
                    "title": "Wide-Field Super-Resolution Optical Fluctuation Imaging through Dynamic Near-Field Speckle Illumination",
                    "abstract": "Stochastic optical fluctuation imaging (SOFI) generates super-resolution fluorescence images by emphasizing the positions of fluorescent emitters via statistical analysis of their on-and-off blinking dynamics. In SOFI with speckle illumination (S-SOFI), the diffraction-limited grain size of the far-field speckles prevents independent blinking of closely located emitters, becoming a hurdle to realize the full super-resolution granted by SOFI processing. Here, we present a surface-sensitive super-resolution technique exploiting dynamic near-field speckle illumination to bring forth the full super-resolving power of SOFI without blinking fluorophores. With our near-field S-SOFI technique, up to 2.8- and 2.3-fold enhancements in lateral spatial resolution are demonstrated with computational and experimental fluorescent test targets labeled with conventional fluorophores, respectively. Fluorescent beads separated by 175 nm are also super-resolved by near-field speckles of 150 nm grain size, promising sub-100 nm resolution with speckle patterns of much smaller grain size."
                },
                {
                    "title": "Far-field super-resolution ghost imaging with a deep neural network constraint",
                    "abstract": null
                },
                {
                    "title": "Signal-carrying speckle in optical coherence tomography: a methodological review on biomedical applications",
                    "abstract": "Abstract. Significance: Speckle has historically been considered a source of noise in coherent light imaging. However, a number of works in optical coherence tomography (OCT) imaging have shown that speckle patterns may contain relevant information regarding subresolution and structural properties of the tissues from which it is originated. Aim: The objective of this work is to provide a comprehensive overview of the methods developed for retrieving speckle information in biomedical OCT applications. Approach: PubMed and Scopus databases were used to perform a systematic review on studies published until December 9, 2021. From 146 screened studies, 40 were eligible for this review. Results: The studies were clustered according to the nature of their analysis, namely static or dynamic, and all features were described and analyzed. The results show that features retrieved from speckle can be used successfully in different applications, such as classification and segmentation. However, the results also show that speckle analysis is highly application-dependant, and the best approach varies between applications. Conclusions: Several of the reviewed analyses were only performed in a theoretical context or using phantoms, showing that signal-carrying speckle analysis in OCT imaging is still in its early stage, and further work is needed to validate its applicability and reproducibility in a clinical context."
                },
                {
                    "title": "Deep learning enhances polarization speckle for in vivo skin cancer detection",
                    "abstract": null
                },
                {
                    "title": "Computational imaging without a computer: seeing through random diffusers at the speed of light",
                    "abstract": null
                },
                {
                    "title": "Compensation-free high-dimensional free-space optical communication using turbulence-resilient vector beams",
                    "abstract": null
                },
                {
                    "title": "High‐Fidelity Image Reconstruction through Multimode Fiber via Polarization‐Enhanced Parametric Speckle Imaging",
                    "abstract": "High‐quality signal transmission and imaging through a multimode fiber is essential for optical communications and medical endoscopic imaging, as it can provide high fidelity of the transferred information. Multiple mode superposition and mode coupling distort the incident wave, and a speckle pattern formed at the exit end of a multimode fiber makes the reconstruction of the original object challenging. The transmission matrix method is proposed to characterize the complex mesoscopic optical transmission channels, allowing light to traverse through the random medium. Here, a novel approach, namely polarization‐enhanced parametric speckle imaging, is proposed to improve imaging quality through a multimode fiber by utilizing the properties of polarization evolution through a multimode fiber. The demonstrated superior performance of polarization‐enhanced parametric speckle imaging opens novel avenues for optical communication system and photonics‐based endoscopic diagnosis."
                },
                {
                    "title": "Non-Invasive Blood Glucose Monitoring Technology: A Review",
                    "abstract": "In recent years, with the rise of global diabetes, a growing number of subjects are suffering from pain and infections caused by the invasive nature of mainstream commercial glucose meters. Non-invasive blood glucose monitoring technology has become an international research topic and a new method which could bring relief to a vast number of patients. This paper reviews the research progress and major challenges of non-invasive blood glucose detection technology in recent years, and divides it into three categories: optics, microwave and electrochemistry, based on the detection principle. The technology covers medical, materials, optics, electromagnetic wave, chemistry, biology, computational science and other related fields. The advantages and limitations of non-invasive and invasive technologies as well as electrochemistry and optics in non-invasives are compared horizontally in this paper. In addition, the current research achievements and limitations of non-invasive electrochemical glucose sensing systems in continuous monitoring, point-of-care and clinical settings are highlighted, so as to discuss the development tendency in future research. With the rapid development of wearable technology and transdermal biosensors, non-invasive blood glucose monitoring will become more efficient, affordable, robust, and more competitive on the market."
                },
                {
                    "title": "Circumventing the optical diffraction limit with customized speckles",
                    "abstract": "Speckle patterns have been widely used in imaging techniques such as ghost imaging, dynamic speckle illumination microscopy, structured illumination microscopy, and photoacoustic fluctuation imaging. Recent advances in the ability to control the statistical properties of speckles has enabled the customization of speckle patterns for specific imaging applications. In this work, we design and create special speckle patterns for parallelized nonlinear pattern-illumination microscopy based on fluorescence photoswitching. We present a proof-of-principle experimental demonstration where we obtain a spatial resolution three times higher than the diffraction limit of the illumination optics in our setup. Furthermore, we show that tailored speckles vastly outperform standard speckles. Our work establishes that customized speckles are a potent tool in parallelized super-resolution microscopy."
                },
                {
                    "title": "Actor neural networks for the robust control of partially measured nonlinear systems showcased for image propagation through diffuse media",
                    "abstract": null
                },
                {
                    "title": "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere",
                    "abstract": "Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard vision and language datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning. \nProject Page: this https URL \nCode: this https URL , this https URL"
                },
                {
                    "title": "Noninvasive Glucose Measurement Using Machine Learning and Neural Network Methods and Correlation with Heart Rate Variability",
                    "abstract": "Diabetes is one of today’s greatest global problems, and it is only becoming bigger. Constant measuring of blood glucose level is a prerequisite for monitoring glucose blood level and establishing diabetes treatment procedures. The usual way of glucose level measuring is by an invasive procedure that requires finger pricking with the lancet and might become painful and obeying, especially if this becomes a daily routine. In this study, we analyze noninvasive glucose measurement approaches and present several classification dimensions according to different criteria: size, invasiveness, analyzed media, sensing properties, applied method, activation type, response delay, measurement duration, and access to results. We set the focus on using machine learning and neural network methods and correlation with heart rate variability and electrocardiogram, as a new research and development trend."
                },
                {
                    "title": "Deep Convolutional Neural Network-Based Automatic Classification of Neonatal Hip Ultrasound Images: A Novel Data Augmentation Approach with Speckle Noise Reduction.",
                    "abstract": null
                },
                {
                    "title": "A survey on semi-supervised learning",
                    "abstract": null
                },
                {
                    "title": "A Comprehensive Survey on Transfer Learning",
                    "abstract": "Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice."
                },
                {
                    "title": "Theoretical issues in deep networks",
                    "abstract": "While deep learning is successful in a number of applications, it is not yet well understood theoretically. A theoretical characterization of deep learning should answer questions about their approximation power, the dynamics of optimization, and good out-of-sample performance, despite overparameterization and the absence of explicit regularization. We review our recent results toward this goal. In approximation theory both shallow and deep networks are known to approximate any continuous functions at an exponential cost. However, we proved that for certain types of compositional functions, deep networks of the convolutional type (even without weight sharing) can avoid the curse of dimensionality. In characterizing minimization of the empirical exponential loss we consider the gradient flow of the weight directions rather than the weights themselves, since the relevant function underlying classification corresponds to normalized networks. The dynamics of normalized weights turn out to be equivalent to those of the constrained problem of minimizing the loss subject to a unit norm constraint. In particular, the dynamics of typical gradient descent have the same critical points as the constrained problem. Thus there is implicit regularization in training deep networks under exponential-type loss functions during gradient flow. As a consequence, the critical points correspond to minimum norm infima of the loss. This result is especially relevant because it has been recently shown that, for overparameterized models, selection of a minimum norm solution optimizes cross-validation leave-one-out stability and thereby the expected error. Thus our results imply that gradient descent in deep networks minimize the expected error."
                },
                {
                    "title": "Optical orbital-angular-momentum-multiplexed data transmission under high scattering",
                    "abstract": null
                },
                {
                    "title": "High-resolution limited-angle phase tomography of dense layered objects using deep neural networks",
                    "abstract": "Significance We demonstrate that it is possible to use deep neural networks to produce tomographic reconstructions of dense layered objects with small illumination angle as low as 10 °. It is also shown that a DNN trained on synthetic data can generalize well to and produce reconstructions from experimental measurements. This work has application in the field of X-ray tomography for the inspection of integrated circuits and other materials studies. We present a machine learning-based method for tomographic reconstruction of dense layered objects, with range of projection angles limited to ±10○. Whereas previous approaches to phase tomography generally require 2 steps, first to retrieve phase projections from intensity projections and then to perform tomographic reconstruction on the retrieved phase projections, in our work a physics-informed preprocessor followed by a deep neural network (DNN) conduct the 3-dimensional reconstruction directly from the intensity projections. We demonstrate this single-step method experimentally in the visible optical domain on a scaled-up integrated circuit phantom. We show that even under conditions of highly attenuated photon fluxes a DNN trained only on synthetic data can be used to successfully reconstruct physical samples disjoint from the synthetic training set. Thus, the need for producing a large number of physical examples for training is ameliorated. The method is generally applicable to tomography with electromagnetic or other types of radiation at all bands."
                },
                {
                    "title": "Strategies for reducing speckle noise in digital holography",
                    "abstract": null
                },
                {
                    "title": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation",
                    "abstract": "We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC"
                },
                {
                    "title": "Ensemble learning: A survey",
                    "abstract": "Ensemble methods are considered the state‐of‐the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state‐of‐the‐art ensemble methods and discusses current challenges and trends in the field."
                },
                {
                    "title": "Remote photonic sensing of glucose concentration via analysis of time varied speckle patterns",
                    "abstract": "The ability to perform a remote sensing of glucose in the blood stream can be very applicable. The novel method presented in this paper is based on two optical approaches both based on the extraction and analysis of the changes in the collected speckle field. The first physical effect used for the detection is the temporal changes of the back scattered secondary speckles produced in the skin due to the changes of the blood stream parameters as a function of the glucose concentration in the blood. These cardio related changes can be analyzed with different machine learning algorithms to enhance the sensitivity of the measurements. The second physical effect assisting in performing the remote glucose sensing is the Faraday rotation effect in which the polarization of linearly polarized light is rotated when scattered from materials exhibiting this effect while being exposed to a magnetic field. Copyright © 2018 VBRI Press."
                },
                {
                    "title": "Deep speckle correlation: a deep learning approach toward scalable imaging through scattering media",
                    "abstract": "Imaging through scattering is an important yet challenging problem. Tremendous progress has been made by exploiting the deterministic input–output “transmission matrix” for a fixed medium. However, this “one-to-one” mapping is highly susceptible to speckle decorrelations – small perturbations to the scattering medium lead to model errors and severe degradation of the imaging performance. Our goal here is to develop a new framework that is highly scalable to both medium perturbations and measurement requirement. To do so, we propose a statistical “one-to-all” deep learning (DL) technique that encapsulates a wide range of statistical variations for the model to be resilient to speckle decorrelations. Specifically, we develop a convolutional neural network (CNN) that is able to learn the statistical information contained in the speckle intensity patterns captured on a set of diffusers having the same macroscopic parameter. We then show for the first time, to the best of our knowledge, that the trained CNN is able to generalize and make high-quality object predictions through an entirely different set of diffusers of the same class. Our work paves the way to a highly scalable DL approach for imaging through scattering media."
                },
                {
                    "title": "Learning to see through multimode fibers",
                    "abstract": "We use Deep Neural Networks (DNNs) to classify and reconstruct a large database of handwritten digits from the intensity of the speckle patterns that result after the images propagated through multimode fibers (MMF). Images transmitted through fibers with up to 1km length were recovered. The ability of the network to recognize the input degraded with fiber length but the performance could be enhanced if the neural networks were trained to first reconstruct the image and then classify it rather than classify it directly from the speckle intensity."
                },
                {
                    "title": "Imaging through glass diffusers using densely connected convolutional networks",
                    "abstract": "Computational imaging through scatter generally is accomplished by first characterizing the scattering medium so that its forward operator is obtained; and then imposing additional priors in the form of regularizers on the reconstruction functional so as to improve the condition of the originally ill-posed inverse problem. In the functional, the forward operator and regularizer must be entered explicitly or parametrically (e.g. scattering matrices and dictionaries, respectively.) However, the process of determining these representations is often incomplete, prone to errors, or infeasible. Recently, deep learning architectures have been proposed to instead learn both the forward operator and regularizer through examples. Here, we propose for the first time, to our knowledge, a convolutional neural network architecture called \"IDiffNet\" for the problem of imaging through diffuse media and demonstrate that IDiffNet has superior generalization capability through extensive tests with well-calibrated diffusers. We found that the Negative Pearson Correlation Coefficient loss function for training is more appropriate for spatially sparse objects and strong scattering conditions. Our results show that the convolutional architecture is robust to the choice of prior, as demonstrated by the use of multiple training and testing object databases, and capable of achieving higher space-bandwidth product reconstructions than previously reported."
                },
                {
                    "title": "DiffuserCam: Lensless Single-exposure 3D Imaging",
                    "abstract": "We demonstrate a compact and easy-to-build computational camera for single-shot 3D imaging. Our lensless system consists solely of a diffuser placed in front of a standard image sensor. Every point within the volumetric field-of-view projects a unique pseudorandom pattern of caustics on the sensor. By using a physical approximation and simple calibration scheme, we solve the large-scale inverse problem in a computationally efficient way. The caustic patterns enable compressed sensing, which exploits sparsity in the sample to solve for more 3D voxels than pixels on the 2D sensor. Our 3D voxel grid is chosen to match the experimentally measured two-point optical resolution across the field-of-view, resulting in 100 million voxels being reconstructed from a single 1.3 megapixel image. However, the effective resolution varies significantly with scene content. Because this effect is common to a wide range of computational cameras, we provide new theory for analyzing resolution in such systems."
                },
                {
                    "title": "Observation of mean path length invariance in light-scattering media",
                    "abstract": "Scattered light, it is all the same Materials can vary from transparent to opaque depending on the density of scatters within the medium. As light propagates through a material, intuition might suggest that the more scatters there are, the shorter the path along which the light can propagate. Savo et al. confirm a recent theoretical proposal that predicts that this is not the case. They shone light through a series of samples of varying scatterer density and found that the average path length that the light traveled was independent of the sample microstructure. This finding should also be applicable to acoustics and matter waves. Science, this issue p. 765 The average path length of light through a disordered medium is independent of the medium’s microstructure. The microstructure of a medium strongly influences how light propagates through it. The amount of disorder it contains determines whether the medium is transparent or opaque. Theory predicts that exciting such a medium homogeneously and isotropically makes some of its optical properties depend only on the medium’s outer geometry. Here, we report an optical experiment demonstrating that the mean path length of light is invariant with respect to the microstructure of the medium it scatters through. Using colloidal solutions with varying concentration and particle size, the invariance of the mean path length is observed over nearly two orders of magnitude in scattering strength. Our results can be extended to a wide range of systems—however ordered, correlated, or disordered—and apply to all wave-scattering problems."
                },
                {
                    "title": "Light fields in complex media: Mesoscopic scattering meets wave control",
                    "abstract": "Wave front shaping, the ability to manipulate light fields both spatially and temporally, in complex media is an emerging field with many applications. This review summarizes how insights from mesoscopic scattering theory have direct relevance for optical wave control experiments and vice versa. The results are expected to have an impact on a number of fields ranging from biomedical imaging to nanophotonics, quantum information, and communication technology."
                },
                {
                    "title": "Twisted light transmission over 143 km",
                    "abstract": "Significance Light is the main carrier of information. Its spatial mode allows the encoding of more than 1 bit per photon, and thus can increase the information capacity. For communication purposes, these modes need to be transmitted over large distances. Nowadays, fiber-based solutions are in their infancy, which renders free-space transmission the only possibility. We present an experiment where we investigate the behavior of the spatial modes after a distance of 143 km. With the help of an artificial neural network, we distinguished different mode superpositions up to the third order with more than 80% accuracy. Our results indicate that with state-of-the-art adaptive optics systems, both classical communication and entanglement transmission is feasible over distances of more than 100 km. Spatial modes of light can potentially carry a vast amount of information, making them promising candidates for both classical and quantum communication. However, the distribution of such modes over large distances remains difficult. Intermodal coupling complicates their use with common fibers, whereas free-space transmission is thought to be strongly influenced by atmospheric turbulence. Here, we show the transmission of orbital angular momentum modes of light over a distance of 143 km between two Canary Islands, which is 50× greater than the maximum distance achieved previously. As a demonstration of the transmission quality, we use superpositions of these modes to encode a short message. At the receiver, an artificial neural network is used for distinguishing between the different twisted light superpositions. The algorithm is able to identify different mode superpositions with an accuracy of more than 80% up to the third mode order and decode the transmitted message with an error rate of 8.33%. Using our data, we estimate that the distribution of orbital angular momentum entanglement over more than 100 km of free space is feasible. Moreover, the quality of our free-space link can be further improved by the use of state-of-the-art adaptive optics systems."
                },
                {
                    "title": "Improved noncontact optical sensor for detection of glucose concentration and indication of dehydration level.",
                    "abstract": "The ability to extract different bio-medical parameters from one single wristwatch device can be very applicable. The wearable device that is presented in this paper is based on two optical approaches. The first is the extraction and separation of remote vibration sources and the second is the rotation of linearly polarized light by certain materials exposed to magnetic fields. The technique is based on tracking of temporal changes of reflected secondary speckles produced in the wrist when being illuminated by a laser beam. Change in skin's temporal vibration profile together with change in the magnetic medium that is generated by time varied glucose concentration caused these temporal changes. In this paper we present experimental tests which are the first step towards an in vivo noncontact device for detection of glucose concentration in blood. The paper also shows very preliminary results for qualitative capability for indication of dehydration."
                },
                {
                    "title": "Non-invasive single-shot imaging through scattering layers and around corners via speckle correlations",
                    "abstract": null
                },
                {
                    "title": "Compact spectrometer based on a disordered photonic chip",
                    "abstract": null
                },
                {
                    "title": "Terabit-Scale Orbital Angular Momentum Mode Division Multiplexing in Fibers",
                    "abstract": "A Twist on the Capacity Crunch The rate at which data can be transmitted down optic fibers is approaching a limit because of nonlinear optical effects. Multiplexing allows data to be encoded in different modes of light such as polarization, wavelength, amplitude, and phase and to be sent down the fibers in parallel. Optical angular momentum (OAM) can provide another degree of freedom whereby the photons are given a well-defined twist or helicity. Bozinovic et al. (p. 1545) were able to transmit high-bandwidth data using OAM modes in long lengths of optical fibers, thus providing a possible route to get yet more capacity through optic fiber networks. Encoding data in the twist, or helicity, of photons provides a route to increase optical communication rates in fibers. Internet data traffic capacity is rapidly reaching limits imposed by optical fiber nonlinear effects. Having almost exhausted available degrees of freedom to orthogonally multiplex data, the possibility is now being explored of using spatial modes of fibers to enhance data capacity. We demonstrate the viability of using the orbital angular momentum (OAM) of light to create orthogonal, spatially distinct streams of data-transmitting channels that are multiplexed in a single fiber. Over 1.1 kilometers of a specially designed optical fiber that minimizes mode coupling, we achieved 400-gigabits-per-second data transmission using four angular momentum modes at a single wavelength, and 1.6 terabits per second using two OAM modes over 10 wavelengths. These demonstrations suggest that OAM could provide an additional degree of freedom for data multiplexing in future fiber networks."
                },
                {
                    "title": "Non-invasive imaging through opaque scattering layers",
                    "abstract": "Non-invasive optical imaging techniques, such as optical coherence tomography, are essential diagnostic tools in many disciplines, from the life sciences to nanotechnology. However, present methods are not able to image through opaque layers that scatter all the incident light. Even a very thin layer of a scattering material can appear opaque and hide any objects behind it. Although great progress has been made recently with methods such as ghost imaging and wavefront shaping, present procedures are still invasive because they require either a detector or a nonlinear material to be placed behind the scattering layer. Here we report an optical method that allows non-invasive imaging of a fluorescent object that is completely hidden behind an opaque scattering layer. We illuminate the object with laser light that has passed through the scattering layer. We scan the angle of incidence of the laser beam and detect the total fluorescence of the object from the front. From the detected signal, we obtain the image of the hidden object using an iterative algorithm. As a proof of concept, we retrieve a detailed image of a fluorescent object, comparable in size (50 micrometres) to a typical human cell, hidden 6 millimetres behind an opaque optical diffuser, and an image of a complex biological sample enclosed between two opaque screens. This approach to non-invasive imaging through strongly scattering media can be generalized to other contrast mechanisms and geometries."
                },
                {
                    "title": "Structured illumination microscopy using unknown speckle patterns",
                    "abstract": null
                },
                {
                    "title": "Speckle-free laser imaging using random laser illumination",
                    "abstract": null
                },
                {
                    "title": "Optical methods for sensing glucose.",
                    "abstract": "This critical review covers the present state of the art in optical sensing of glucose. Following an introduction into the significance of (continuous) sensing of glucose and a brief look back, we discuss methods based on (a) monitoring the optical properties of intrinsically fluorescent or labeled enzymes, their co-enzymes and co-substrates; (b) the measurement of the products of enzymatic oxidation of glucose by glucose oxidase; (c) the use of synthetic boronic acids; (d) the use of Concanavalin A; and (e) the application of other glucose-binding proteins. We finally present an assessment in terms of the advantages and disadvantages of the various methods (237 references)."
                },
                {
                    "title": "Measuring the transmission matrix in optics: an approach to the study and control of light propagation in disordered media.",
                    "abstract": "We introduce a method to experimentally measure the monochromatic transmission matrix of a complex medium in optics. This method is based on a spatial phase modulator together with a full-field interferometric measurement on a camera. We determine the transmission matrix of a thick random scattering sample. We show that this matrix exhibits statistical properties in good agreement with random matrix theory and allows light focusing and imaging through the random medium. This method might give important insight into the mesoscopic properties of a complex medium."
                },
                {
                    "title": "Handbook of Optical Sensing of Glucose in Biological Fluids and Tissues",
                    "abstract": "Preface Glucose: Physiological Norm and Pathology Lidia I. Malinova and Tatyana P. Denisova Commercial Biosensors for Diabetes Vasiliki Fragkou and Anthony P.F. Turner Monte Carlo Simulation of Light Propagation in Human Tissues and Noninvasive Glucose Sensing Andrey V. Bykov, Mikhail Yu. Kirillin, and Alexander V. Priezzhev Statistical Analysis for Glucose Prediction in Blood Samples by Infrared Spectroscopy Gilwon Yoon Near-Infrared Reflection Spectroscopy for Noninvasive Monitoring of Glucose-Established and Novel Strategies for Multivariate Calibration H. Michael Heise, Peter Lampen, and Ralf Marbach Characterizing the Influence of Acute Hyperglycaemia on Cerebral Hemodynamics by Optical Imaging Qingming Luo, Zhen Wang, Weihua Luo, and Pengcheng Li Near-Infrared Thermo-Optical Response of the Localized Reflectance of Diabetic and Non-Diabetic Human Skin Omar S. Khalil In Vivo Nondestructive Measurement of Blood Glucose by Near-Infrared Diffuse-Reflectance Spectroscopy Yukihiro Ozaki, Hideyuki Shinzawa, Katsuhiko Maruo, Yi Ping Du, and Sumaporn Kasemsumran Glucose Correlation with Light Scattering Patterns Ilya Fine Challenges and Countermeasures in NIR Noninvasive Blood Glucose Monitoring Kexin Xu and Ruikang K. Wang Fluorescence-Based Glucose Biosensors Gerard L. Cote, M. McShane, and M.V. Pishko Quantitative Biological Raman Spectroscopy Wei-Chuan Shih, Kate L. Bechtel, and Michael S. Feld Tear Fluid Photonic Crystal Contact Lens Noninvasive Glucose Sensors Sanford A. Asher and Justin T. Baca Pulsed Photoacoustic Techniques and Glucose Determination in Human Blood and Tissue Risto Myllyla, Zuomin Zhao, and Matti Kinnunen A Noninvasive Glucose Sensor Based on Polarimetric Measurements through the Aqueous Humor of the Eye Gerard L. Cote and Brent D. Cameron Noninvasive Measurements of Glucose in the Human Body Using Polarimetry and Brewster Reflection off of the Eye Lens Luigi Rovati and Rafat R. Ansari Toward Noninvasive Glucose Sensing Using Polarization Analysis of Multiply Scattered Light Michael F.G. Wood, Nirmalya Ghosh, Xinxin Guo, and I. Alex Vitkin Noninvasive Monitoring of Glucose Concentration with Optical Coherence Tomography Rinat O. Esenaliev and Donald S. Prough Measurement of Glucose Diffusion Coefficients in Human Tissues Alexey N. Bashkatov, Elina A. Genina, and Valery V. Tuchin Monitoring of Glucose Diffusion in Epithelial Tissues with Optical Coherence Tomography Kirill V. Larin and Valery V. Tuchin Glucose-Induced Optical Clearing Effects in Tissues and Blood Elina A. Genina, Alexey N. Bashkatov, and Valery V. Tuchin Index"
                },
                {
                    "title": "Despeckling of medical ultrasound images",
                    "abstract": "Speckle noise is an inherent property of medical ultrasound imaging, and it generally tends to reduce the image resolution and contrast, thereby reducing the diagnostic value of this imaging modality. As a result, speckle noise reduction is an important prerequisite, whenever ultrasound imaging is used for tissue characterization. Among the many methods that have been proposed to perform this task, there exists a class of approaches that use a multiplicative model of speckled image formation and take advantage of the logarithmical transformation in order to convert multiplicative speckle noise into additive noise. The common assumption made in a dominant number of such studies is that the samples of the additive noise are mutually uncorrelated and obey a Gaussian distribution. The present study shows conceptually and experimentally that this assumption is oversimplified and unnatural. Moreover, it may lead to inadequate performance of the speckle reduction methods. The study introduces a simple preprocessing procedure, which modifies the acquired radio-frequency images (without affecting the anatomical information they contain), so that the noise in the log-transformation domain becomes very close in its behavior to a white Gaussian noise. As a result, the preprocessing allows filtering methods based on assuming the noise to be white and Gaussian, to perform in nearly optimal conditions. The study evaluates performances of three different, nonlinear filters - wavelet denoising, total variation filtering, and anisotropic diffusion - and demonstrates that, in all these cases, the proposed preprocessing significantly improves the quality of resultant images. Our numerical tests include a series of computer-simulated and in vivo experiments."
                },
                {
                    "title": "Texture analysis of optical coherence tomography images: feasibility for tissue classification.",
                    "abstract": "Optical coherence tomography (OCT) acquires cross-sectional images of tissue by measuring back-reflected light. Images from in vivo OCT systems typically have a resolution of 10 to 15 mm, and are thus best suited for visualizing structures in the range of tens to hundreds of microns, such as tissue layers or glands. Many normal and abnormal tissues lack visible structures in this size range, so it may appear that OCT is unsuitable for identification of these tissues. However, examination of structure-poor OCT images reveals that they frequently display a characteristic texture that is due to speckle. We evaluated the application of statistical and spectral texture analysis techniques for differentiating tissue types based on the structural and speckle content in OCT images. Excellent correct classification rates were obtained when images had slight visual differences (mouse skin and fat, correct classification rates of 98.5 and 97.3%, respectively), and reasonable rates were obtained with nearly identical-appearing images (normal versus abnormal mouse lung, correct classification rates of 64.0 and 88.6%, respectively). This study shows that texture analysis of OCT images may be capable of differentiating tissue types without reliance on visible structures."
                },
                {
                    "title": "Dielectric Study of Concentration Fluctuation in Solutions of Polystyrene",
                    "abstract": "The effect of the solvent quality on local concentration fluctuation was studied for concentrated solutions of polystyrene (PS) in toluene (Tol), ethylbenzene (EtBz), n-propylbenzene (PrBz), and n-butylbenzene (BuBz) by using dielectric spectroscopy. The exponents of the Mark−Houwink−Sakurada equation of those solutions indicate that the solvent quality deteriorates in the order of Tol, EtBz, PrBz, and BuBz. Three dielectric relaxations designated as α, β, and γ are observed in the temperature dependence curves of the dielectric loss in the order of decreasing temperature. According to our previous study on PS/Tol solution, the α, β, and γ relaxations have been assigned to local segmental motions of PS, rotation of the solvent molecules, and the secondary relaxation in the glassy state, respectively. The half-width of the dielectric loss curve Λα for the α relaxation of PS/Tol solutions is independent of temperature, but the values of Λα for solutions in EtBz, PrBz, and BuBz increase with decreasing tempe..."
                },
                {
                    "title": "Handbook of Diabetes",
                    "abstract": "Introduction to diabetes History of diabetes Diagnosis of diabetes Classification of diabetes Public health aspects of diabetes Normal physiology of insulin secretion and action Epidemiology and aetiology of type 1 diabetes Epidemiology and aetiology of type 2 diabetes Other types of diabetes Assessing control in diabetes Management of type 1 diabetes Management of type 2 diabetes Diabetic ketoacidosis and hyperosmolar non-ketotic coma Hypertension in diabetes Control and complications Diabetic eye disease Diabetic nephropathy Diabetic neuropathy Hyperlipidaemia in diabetes Hypertension in diabetes Macrovascular disease in diabetes The diabetic foot Sexual problems in diabetes Gastrointestinal problems in diabetes The skin in diabetes Psychological and psychiatric problems in diabetes Some intercurrent problems in diabetes Pregnancy and diabetes Diabetes in children Diabetes in the elderly Diabetes and lifestyle Organisation of diabetes care."
                },
                {
                    "title": "City of Light: The Story of Fiber Optics",
                    "abstract": "1. Introduction: Building a City of Light 2. Guiding Light and Luminous Fountains 3. Fibers of Glass 4. The Quest for Remote Viewing: Television and the Legacy of Sword Swallowers 5. A Critical Insight: The Birth of the Clad Optical Fiber 6. 99 Percent Perspiration: The Birth of an Industry 7. A Vision of the Futue: Communicating with Light 8. The Laser Stimulates the Emission of New Ideas 9. \"The Only Thing Left is Optical Fibers\" 10. Trying to Sell a Dream 11. Breakthrough: The Clearest Glass in the World 12. Recipes for Grains of Salt: The Semiconductor Laser 13. A Demonstration for the Queen 14. Three Generations in Five Years 15. Submarine Cables: \"The Ocean Floor Will Be Covered with Glass\" 16. The Last Mile: An Elusive Vision 17. Reflections on the City of Light Appendix A: Dramatis Personae: Cast of Characters Appendix B: A Fiber-Optic Technology"
                },
                {
                    "title": "Enhanced concentration fluctuations in polymer solutions under shear flow.",
                    "abstract": null
                },
                {
                    "title": "Dynamic light scattering studies of concentration fluctuations in aqueous t‐butyl alcohol solutions",
                    "abstract": "Dynamic light‐scattering measurements using photon correlation spectroscopy have been performed on four different concentrations of t‐butyl alcohol in water; 7.25, 13.2, 20.1, and 26.0 mol %. Temperatures ranged from as low as −16 °C in the supercooled regime to as high as 72 °C. Mutual diffusion constants of the concentration fluctuations were extracted from the light‐scattering data. Viscosity measurements were also performed on these solutions over these temperature ranges. The correlation length of the concentration fluctuations determined from these measurements increased with increasing temperature, leveled off near room temperature and then showed another increase at lower, especially supercooled temperatures. These behaviors suggested critical demixing or consolute points should exist at both temperatures above the equilibrium boiling and below the freezing points of the mixture. The high temperature critical point is probably due to t‐butyl alcohol and water association, whereas the low temperatu..."
                },
                {
                    "title": "Optical fiber telecommunications",
                    "abstract": null
                },
                {
                    "title": "Multi-mode optical fiber transmission with a deep learning network",
                    "abstract": null
                },
                {
                    "title": "Deep Learning",
                    "abstract": null
                },
                {
                    "title": "Laser speckle contrast imaging in biomedical optics.",
                    "abstract": "First introduced in the 1980s, laser speckle contrast imaging is a powerful tool for full-field imaging of blood flow. Recently laser speckle contrast imaging has gained increased attention, in part due to its rapid adoption for blood flow studies in the brain. We review the underlying physics of speckle contrast imaging and discuss recent developments to improve the quantitative accuracy of blood flow measures. We also review applications of laser speckle contrast imaging in neuroscience, dermatology and ophthalmology."
                }
            ]
        },
        "author_data": {
            "779a4212-e043-45fd-a625-8b0b26439a1f": {
                "pk": "779a4212-e043-45fd-a625-8b0b26439a1f",
                "project_name": null,
                "name": "Weiru Fan",
                "bio": "I am a researcher dedicated to advancing computational imaging through innovative deep learning frameworks. My recent work focuses on bridging the gap between local and nonlocal pattern processing, exemplified by my development of the deterministic diffusion model (DDM). This framework not only enhances image reconstruction from complex patterns but also integrates Bayesian inference for reliable uncertainty quantification, ensuring predictive accuracy in critical applications.\n\nIn addition to DDM, I have pioneered the SHIELD approach for wavefront sensing, leveraging phase-sensitive second harmonic signals to achieve high-resolution phase imaging with remarkable robustness against noise. My exploration of photonic Ising machines has led to the development of a Fourier-mask method, enabling the simulation of complex spin glass models, which has significant implications for material science and neural networks.\n\nI am also passionate about overcoming the challenges of optical phase imaging in scattering media. My speckle three-dimensional reconstruction network (STRN) allows for depth-resolving phase information retrieval, promising advancements in biomedical applications. Furthermore, my work on deep learning quantum holography (DL-QHUP) has transformed holographic imaging by enhancing spatial resolution and noise resilience, paving the way for breakthroughs in various scientific fields.\n\nThrough these contributions, I aim to push the boundaries of imaging technology, making it more efficient, reliable, and applicable across diverse disciplines, from life sciences to remote sensing.",
                "collaborators": [
                    "Da-Wei Wang",
                    "Xingqi Xu",
                    "Tianrun Chen",
                    "Delong Zhang",
                    "Shi-Yao Zhu",
                    "Ziyang Chen",
                    "Xiaobin Tang",
                    "Yiyi Liao",
                    "Eddie Gil",
                    "Shiyao Zhu"
                ],
                "pub_titles": [
                    "General Intelligent Imaging and Uncertainty Quantification by Deterministic Diffusion Model",
                    "Second Harmonic Imaging Enhanced by Deep Learning Decipher",
                    "Programmable Photonic Simulator for Spin Glass Models",
                    "Recognizing three-dimensional phase images with deep learning",
                    "Deep Learning Enhanced Quantum Holography with Undetected Photons"
                ],
                "pub_abstracts": [
                    "Computational imaging is crucial in many disciplines from autonomous driving to life sciences. However, traditional model-driven and iterative methods consume large computational power and lack scalability for imaging. Deep learning (DL) is effective in processing local-to-local patterns, but it struggles with handling universal global-to-local (nonlocal) patterns under current frameworks. To bridge this gap, we propose a novel DL framework that employs a progressive denoising strategy, named the deterministic diffusion model (DDM), to facilitate general computational imaging at a low cost. We experimentally demonstrate the efficient and faithful image reconstruction capabilities of DDM from nonlocal patterns, such as speckles from multimode fiber and intensity patterns of second harmonic generation, surpassing the capability of previous state-of-the-art DL algorithms. By embedding Bayesian inference into DDM, we establish a theoretical framework and provide experimental proof of its uncertainty quantification. This advancement ensures the predictive reliability of DDM, avoiding misjudgment in high-stakes scenarios. This versatile and integrable DDM framework can readily extend and improve the efficacy of existing DL-based imaging applications.",
                    "Wavefront sensing and reconstruction are widely used for adaptive optics, aberration correction, and high-resolution optical phase imaging. Traditionally, interference and/or microlens arrays are used to convert the optical phase into intensity variation. Direct imaging of distorted wavefront usually results in complicated phase retrieval with low contrast and low sensitivity. Here, a novel approach has been developed and experimentally demonstrated based on the phase-sensitive information encoded into second harmonic signals, which are intrinsically sensitive to wavefront modulations. By designing and implementing a deep neural network, we demonstrate the second harmonic imaging enhanced by deep learning decipher (SHIELD) for efficient and resilient phase retrieval. Inheriting the advantages of two-photon microscopy, SHIELD demonstrates single-shot, reference-free, and video-rate phase imaging with sensitivity better than {\\lambda}/100 and high robustness against noises, facilitating numerous applications from biological imaging to wavefront sensing.",
                    "Spin glasses featured by frustrated interactions and metastable states have important applications in chemistry, material sciences and artificial neural networks. However, the solution of the spin glass models is hindered by the computational complexity that exponentially increases with the sample size. Photonic Ising machines based on spatial light modulation can speed up the calculation by obtaining the Hamiltonian from the modulated light intensity. However, the large-scale generalization to various spin couplings and higher dimensions is still elusive. Here, we develop a Fourier-mask method to program the spin couplings in photonic Ising machines. We observe the phase transition of the two-dimensional Mattis model and the J$\\mathrm{_1}$-J$\\mathrm{_2}$ model and study the critical phenomena. We also demonstrate that the three-dimensional Ising model, which has not been analytically solved, can be effectively constructed and simulated in two-dimensional lattices with Fourier masks. Our strategy provides a flexible route to tuning couplings and dimensions of statistical spin models, and improves the applicability of optical simulation in neural networks and combinatorial optimization problems.",
                    "Optical phase contains key information for biomedical and astronomical imaging. However, it is often obscured by layers of heterogeneous and scattering media, which render optical phase imaging at different depths an utmost challenge. Limited by the memory effect, current methods for phase imaging in strong scattering media are inapplicable to retrieving phases at different depths. To address this challenge, we developed a speckle three-dimensional reconstruction network (STRN) to recognize phase objects behind scattering media, which circumvents the limitations of memory effect. From the single-shot, reference-free and scanning-free speckle pattern input, STRN distinguishes depth-resolving quantitative phase information with high fidelity. Our results promise broad applications in biomedical tomography and endoscopy.",
                    "Holography is an essential technique of generating three-dimensional images. Recently, quantum holography with undetected photons (QHUP) has emerged as a groundbreaking method capable of capturing complex amplitude images. Despite its potential, the practical application of QHUP has been limited by susceptibility to phase disturbances, low interference visibility, and limited spatial resolution. Deep learning, recognized for its ability in processing complex data, holds significant promise in addressing these challenges. In this report, we present an ample advancement in QHUP achieved by harnessing the power of deep learning to extract images from single-shot holograms, resulting in vastly reduced noise and distortion, alongside a notable enhancement in spatial resolution. The proposed and demonstrated deep learning QHUP (DL-QHUP) methodology offers a transformative solution by delivering high-speed imaging, improved spatial resolution, and superior noise resilience, making it suitable for diverse applications across an array of research fields stretching from biomedical imaging to remote sensing. DL-QHUP signifies a crucial leap forward in the realm of holography, demonstrating its immense potential to revolutionize imaging capabilities and pave the way for advancements in various scientific disciplines. The integration of DL-QHUP promises to unlock new possibilities in imaging applications, transcending existing limitations and offering unparalleled performance in challenging environments."
                ],
                "domain": [
                    "Computational Imaging",
                    "Deep Learning",
                    "Optical Phase Imaging",
                    "Holography"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "af86d6db-8644-4954-9dd5-738cb37a0759": {
                "pk": "af86d6db-8644-4954-9dd5-738cb37a0759",
                "project_name": null,
                "name": "Xiaobin Tang",
                "bio": "I am a researcher dedicated to advancing methodologies in natural language processing (NLP), knowledge graph alignment, and computational imaging. My recent work focuses on enhancing Consumer Price Index (CPI) prediction through innovative keyword expansion techniques using the PANGU model, which leverages zero-shot learning to generate reliable keywords without the constraints of traditional fine-tuning methods. \n\nIn addition to CPI prediction, I have developed a new index to measure China's investment activity, utilizing semantic representation models like NEZHA to analyze trends over the past five years. This research provides valuable insights for policymakers and stakeholders in understanding investment dynamics in China.\n\nMy exploration of cross-lingual knowledge alignment has led to the development of a joint framework that effectively merges attribute and structural alignments, significantly improving entity alignment accuracy. Furthermore, I have introduced a deterministic diffusion model (DDM) for computational imaging, which employs a progressive denoising strategy to enhance image reconstruction from nonlocal patterns, ensuring predictive reliability through Bayesian inference.\n\nLastly, I have tackled the challenge of expert finding by proposing CODE, a zero-shot expert linking model that utilizes contrastive learning to enhance representation and matching capabilities. This work not only improves expert linking performance but also demonstrates the potential for continuous improvement through active learning in online environments. My research aims to bridge gaps in various domains, providing practical solutions and insights that can drive innovation and informed decision-making.",
                "collaborators": [
                    "Bo Chen",
                    "Jing Zhang",
                    "Hong Chen",
                    "Cuiping Li",
                    "Nuo Lei",
                    "Tong Shen",
                    "Manru Dong",
                    "Weiru Fan",
                    "Yiyi Liao",
                    "Da-Wei Wang"
                ],
                "pub_titles": [
                    "Research on CPI Prediction Based on Natural Language Processing",
                    "Measurement of Investment activity in China based on Natural language processing technology",
                    "JarKA: Modeling Attribute Interactions for Cross-lingual Knowledge Alignment",
                    "General Intelligent Imaging and Uncertainty Quantification by Deterministic Diffusion Model",
                    "CODE: Contrastive Pre-training with Adversarial Fine-tuning for Zero-shot Expert Linking"
                ],
                "pub_abstracts": [
                    "In the past, the seed keywords for CPI prediction were often selected based on empirical summaries of research and literature studies, which were prone to select omitted and invalid variables. In this paper, we design a keyword expansion technique for CPI prediction based on the cutting-edge NLP model, PANGU. We improve the CPI prediction ability using the corresponding web search index. Compared with the unsupervised pre-training and supervised downstream fine-tuning natural language processing models such as BERT and NEZHA, the PANGU model can be expanded to obtain more reliable CPI-generated keywords by its excellent zero-sample learning capability without the limitation of the downstream fine-tuning data set. Finally, this paper empirically tests the keyword prediction ability obtained by this keyword expansion method with historical CPI data.",
                    "The purpose of this study is to propose a new index to measure and reflect China's investment activity in time, and to analyze the changes of China's investment activity in the past five years. This study first uses the NEZHA model for semantic representation, and expand the indicator system based on semantic similarity. Then we calculate China's investment activity index by using the network search data. This study shows that China's investment activity began to decline in 2019, rebounded for a period of time after the outbreak of COVID-19 in 2020, and then continued to maintain a downward trend. Private investment activity has declined significantly, while government investment activity has increased. Among the provinces in Chinese Mainland, the investment activity of economically developed provinces has decreased significantly, while the investment activity of some economically less developed provinces in the north and south is higher. After the outbreak of COVID-19, the investment period became shorter. Our research will provide timely investment information for the government, decision makers and managers, as well as provide other researchers who also pay attention to investment with a perspective other than investment in fixed asset.",
                    "Abstract. Cross-lingual knowledge alignment is the cornerstone in building a comprehensive knowledge graph (KG), which can benefit various knowledge-driven applications. As the structures of KGs are usually sparse, attributes of entities may play an important role in aligning the entities. However, the heterogeneity of the attributes across KGs prevents from accurately embedding and comparing entities. To deal with the issue, we propose to model the interactions between attributes, instead of globally embedding an entity with all the attributes. We further propose a joint framework to merge the alignments inferred from the attributes and the structures. Experimental results show that the proposed model outperforms the state-of-art baselines by up to 38.48% HitRatio@1. The results also demonstrate that our model can infer the alignments between attributes, relationships and values, in addition to entities.",
                    "Computational imaging is crucial in many disciplines from autonomous driving to life sciences. However, traditional model-driven and iterative methods consume large computational power and lack scalability for imaging. Deep learning (DL) is effective in processing local-to-local patterns, but it struggles with handling universal global-to-local (nonlocal) patterns under current frameworks. To bridge this gap, we propose a novel DL framework that employs a progressive denoising strategy, named the deterministic diffusion model (DDM), to facilitate general computational imaging at a low cost. We experimentally demonstrate the efficient and faithful image reconstruction capabilities of DDM from nonlocal patterns, such as speckles from multimode fiber and intensity patterns of second harmonic generation, surpassing the capability of previous state-of-the-art DL algorithms. By embedding Bayesian inference into DDM, we establish a theoretical framework and provide experimental proof of its uncertainty quantification. This advancement ensures the predictive reliability of DDM, avoiding misjudgment in high-stakes scenarios. This versatile and integrable DDM framework can readily extend and improve the efficacy of existing DL-based imaging applications.",
                    "Expert finding, a popular service provided by many online websites such as Expertise Finder, LinkedIn, and AMiner, is beneficial to seeking candidate qualifications, consultants, and collaborators. However, its quality is suffered from lack of ample sources of expert information. This paper employs AMiner as the basis with an aim at linking any external experts to the counterparts on AMiner. As it is infeasible to acquire sufficient linkages from arbitrary external sources, we explore the problem of zero-shot expert linking. In this paper, we propose CODE, which first pre-trains an expert linking model by contrastive learning on AMiner such that it can capture the representation and matching patterns of experts without supervised signals, then it is fine-tuned between AMiner and external sources to enhance the models transferability in an adversarial manner. For evaluation, we first design two intrinsic tasks, author identification and paper clustering, to validate the representation and matching capability endowed by contrastive learning. Then the final external expert linking performance on two genres of external sources also implies the superiority of the adversarial fine-tuning method. Additionally, we show the online deployment of CODE, and continuously improve its online performance via active learning."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Knowledge Graph",
                    "Deep Learning",
                    "Expert Finding"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "8f473d33-f9af-40ca-b282-4640f40c841b": {
                "pk": "8f473d33-f9af-40ca-b282-4640f40c841b",
                "project_name": null,
                "name": "Xingqi Xu",
                "bio": "I am a researcher deeply engaged in the exploration of topological matter and quantum phenomena, particularly through the lens of cold atoms and superradiance lattices. My recent work has focused on harnessing the unique properties of these systems to measure geometric phases and topological invariants at room temperature, overcoming the challenges posed by thermal noise. I have developed innovative techniques such as velocity scanning tomography to extract Zak phases and investigate the interplay between thermal motion and topological physics.\n\nIn my research, I have also pioneered methods to utilize thermal motion as a control mechanism in Floquet-modulated systems, enabling the observation of dynamic localization and chiral edge currents. My work on photonic Ising machines has advanced the simulation of spin glass models, providing a flexible approach to tuning couplings and dimensions, which is crucial for applications in neural networks and optimization problems.\n\nAdditionally, I have contributed to the field of quantum illumination, designing a quantum induced coherence LiDAR that enhances signal-to-noise ratios in challenging environments. My recent integration of deep learning with quantum holography has significantly improved imaging capabilities, demonstrating the potential for high-speed, high-resolution imaging across various scientific disciplines.\n\nOverall, my research aims to bridge the gap between fundamental physics and practical applications, paving the way for advancements in quantum information processing, photonic devices, and biomedical imaging. I am passionate about pushing the boundaries of what is possible in quantum simulation and topological physics, and I look forward to further exploring these exciting frontiers.",
                "collaborators": [
                    "Da-Wei Wang",
                    "Shi-Yao Zhu",
                    "Ruosong Mao",
                    "Jiefei Wang",
                    "Han Cai",
                    "Gewei Qian",
                    "Weiru Fan",
                    "Huizhu Hu",
                    "Chenran Xu",
                    "Jianhao Dai"
                ],
                "pub_titles": [
                    "Measuring Zak phase in room-temperature atoms",
                    "Floquet superradiance lattices in thermal atoms",
                    "Programmable Photonic Simulator for Spin Glass Models",
                    "Quantum Induced Coherence Light Detection and Ranging",
                    "Deep Learning Enhanced Quantum Holography with Undetected Photons",
                    "Recognizing three-dimensional phase images with deep learning",
                    "Zak Phase Induced Topological Nonreciprocity",
                    "Velocity Scanning Tomography for Room-Temperature Quantum Simulation"
                ],
                "pub_abstracts": [
                    "Cold atoms provide a flexible platform for synthesizing and characterizing topolog-ical matter, where geometric phases play a central role. However, cold atoms are intrinsically prone to thermal noise, which can overwhelm the topological response and hamper promised applications. On the other hand, geometric phases also de-termine the energy spectra of particles subjected to a static force, based on the po-larization relation between Wannier-Stark ladders and geometric Zak phases. By exploiting this relation, we develop a method to extract geometric phases from en-ergy spectra of room-temperature superradiance lattices, which are momentum-space lattices of timed Dicke states. In such momentum-space lattices the thermal motion of atoms, instead of being a source of noise, provides effective forces which lead to spectroscopic signatures of the Zak phases. We measure Zak phases direct-ly from the anti-crossings between Wannier-Stark ladders in the Doppler-broadened absorption spectra of superradiance lattices. Our approach paves the way of measuring topological invariants and developing their applications in room-temperature atoms.",
                    "Floquet modulation has been widely used in optical lattices for coherent control of quantum gases, in particular for synthesizing artificial gauge fields and simulating topological matters. However, such modulation induces heating which can overwhelm the signal of quantum dynamics in ultracold atoms. Here we report that the thermal motion, instead of being a noise source, provides a new control knob in Floquet-modulated superradiance lattices, which are momentum-space tight-binding lattices of collectively excited states of atoms. The Doppler shifts combined with Floquet modulation provide effective forces along arbitrary directions in a lattice in frequency and momentum dimensions. Dynamic localization, dynamic delocalization and chiral edge currents can be simultaneously observed from a single transport spectrum of superradiance lattices in thermal atoms. Our work paves a way for simulating Floquet topological matters in room-temperature atoms and facilitates their applications in photonic devices.",
                    "Spin glasses featured by frustrated interactions and metastable states have important applications in chemistry, material sciences and artificial neural networks. However, the solution of the spin glass models is hindered by the computational complexity that exponentially increases with the sample size. Photonic Ising machines based on spatial light modulation can speed up the calculation by obtaining the Hamiltonian from the modulated light intensity. However, the large-scale generalization to various spin couplings and higher dimensions is still elusive. Here, we develop a Fourier-mask method to program the spin couplings in photonic Ising machines. We observe the phase transition of the two-dimensional Mattis model and the J$\\mathrm{_1}$-J$\\mathrm{_2}$ model and study the critical phenomena. We also demonstrate that the three-dimensional Ising model, which has not been analytically solved, can be effectively constructed and simulated in two-dimensional lattices with Fourier masks. Our strategy provides a flexible route to tuning couplings and dimensions of statistical spin models, and improves the applicability of optical simulation in neural networks and combinatorial optimization problems.",
                    "Quantum illumination has been proposed and demonstrated to improve the signal-to-noise ratio (SNR) in light detection and ranging (LiDAR). When relying on coincidence detection, such a quantum LiDAR is limited by the response time of the detector and suffers from jamming noise. Inspired by the Zou-Wang-Mandel experiment, we design, construct and validate a quantum induced coherence (QuIC) LiDAR which is inherently immune to ambient and jamming noises. In traditional LiDAR the direct detection of the reflected probe photons suffers from deteriorating SNR for increasing background noise. In QuIC LiDAR we circumvent this obstacle by only detecting the entangled reference photons, whose single-photon interference fringes are used to obtain the distance of the object, while the reflected probe photons are used to erase path information of the reference photons. In consequence, the noise accompanying the reflected probe light has no effect on the detected signal. We demonstrate such noise resilience with both LED and laser light to mimic the background noise and jamming attack. The proposed method paves a new way of battling noise in precise quantum electromagnetic sensing and ranging.",
                    "Holography is an essential technique of generating three-dimensional images. Recently, quantum holography with undetected photons (QHUP) has emerged as a groundbreaking method capable of capturing complex amplitude images. Despite its potential, the practical application of QHUP has been limited by susceptibility to phase disturbances, low interference visibility, and limited spatial resolution. Deep learning, recognized for its ability in processing complex data, holds significant promise in addressing these challenges. In this report, we present an ample advancement in QHUP achieved by harnessing the power of deep learning to extract images from single-shot holograms, resulting in vastly reduced noise and distortion, alongside a notable enhancement in spatial resolution. The proposed and demonstrated deep learning QHUP (DL-QHUP) methodology offers a transformative solution by delivering high-speed imaging, improved spatial resolution, and superior noise resilience, making it suitable for diverse applications across an array of research fields stretching from biomedical imaging to remote sensing. DL-QHUP signifies a crucial leap forward in the realm of holography, demonstrating its immense potential to revolutionize imaging capabilities and pave the way for advancements in various scientific disciplines. The integration of DL-QHUP promises to unlock new possibilities in imaging applications, transcending existing limitations and offering unparalleled performance in challenging environments.",
                    "Optical phase contains key information for biomedical and astronomical imaging. However, it is often obscured by layers of heterogeneous and scattering media, which render optical phase imaging at different depths an utmost challenge. Limited by the memory effect, current methods for phase imaging in strong scattering media are inapplicable to retrieving phases at different depths. To address this challenge, we developed a speckle three-dimensional reconstruction network (STRN) to recognize phase objects behind scattering media, which circumvents the limitations of memory effect. From the single-shot, reference-free and scanning-free speckle pattern input, STRN distinguishes depth-resolving quantitative phase information with high fidelity. Our results promise broad applications in biomedical tomography and endoscopy.",
                    "Topological physics provides novel insights for designing functional photonic devices, such as magnetic-free optical diodes, which are important in optical engineering and quantum information processing. Past efforts mostly focus on the topological edge modes in two-dimensional (2D) photonic Chern lattices, which, however, require delicate fabrication and temporal modulation. In particular, the 1D nonreciprocal edge mode needs to be embedded in a 2D lattice, contradicting with the compactness of integrated photonics. To address these challenges, we investigate the optical nonreciprocity of the 1D Su-Schrieffer-Heeger (SSH) superradiance lattices in room-temperature atoms. The probe fields propagating in two opposite directions perceive two different SSH topological phases, which have different absorption spectra due to the interplay between the Zak phase and the thermal motion of atoms, resulting in optical nonreciprocity. Our findings reveal the relationship between 1D topological matter and optical nonreciprocity, simplifying the design of topologically resilient nonreciprocal devices.",
                    "Quantum simulation offers an analog approach for exploring exotic quantum phenomena using controllable platforms, typically necessitating ultracold temperatures to maintain the quantum coherence. Superradiance lattices (SLs) have been harnessed to simulate coherent topological physics at room temperature, but the thermal motion of atoms remains a notable challenge in accurately measuring the physical quantities. To overcome this obstacle, we invent and validate a velocity scanning tomography technique to discern the responses of atoms with different velocities, allowing cold-atom spectroscopic resolution within room-temperature SLs. By comparing absorption spectra with and without atoms moving at specific velocities, we can derive the Wannier-Stark ladders of the SL across various effective static electric fields, their strengths being proportional to the atomic velocities. We extract the Zak phase of the SL by monitoring the ladder frequency shift as a function of the atomic velocity, effectively demonstrating the topological winding of the energy bands. Our research signifies the feasibility of room-temperature quantum simulation and facilitates their applications in quantum information processing."
                ],
                "domain": [
                    "Quantum Physics",
                    "Topological Matter",
                    "Photonic Devices",
                    "Quantum Simulation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "e9803756-d718-4c98-bcf3-1b2881422d6c": {
                "pk": "e9803756-d718-4c98-bcf3-1b2881422d6c",
                "project_name": null,
                "name": "Huizhu Hu",
                "bio": "I am a researcher dedicated to exploring the fascinating intersection of optical trapping, nanomechanics, and quantum physics. My work primarily focuses on the manipulation and control of optically levitated particles, which hold immense potential for precision sensing and quantum applications. Through my investigations, I have developed innovative techniques for enhancing the performance of optical traps, such as introducing couplers to facilitate tunable interactions between nanoparticles and creating calibration methods that improve measurement accuracy in high-sensitivity force detection.\n\nMy recent studies have delved into the dynamics of optically levitated microspheres, revealing insights into their behavior under various conditions, including thermal effects and optical binding. I have also pioneered methods for using these systems in applications like acceleration sensing and electric field measurement, demonstrating significant improvements in sensitivity and performance.\n\nIn addition to practical applications, I am passionate about advancing our understanding of fundamental physics, particularly in the realm of quantum mechanics. My research on superradiance lattices and topological phenomena aims to bridge the gap between theoretical concepts and experimental realizations, paving the way for new technologies in quantum information processing.\n\nOverall, my work is driven by a commitment to pushing the boundaries of what is possible in optical manipulation and sensing, with the ultimate goal of contributing to advancements in both fundamental science and practical applications.",
                "collaborators": [
                    "Zhenhai Fu",
                    "Nan Li",
                    "Han Cai",
                    "Shaochong Zhu",
                    "Ying Dong",
                    "Xingfan Chen",
                    "Xiaowen Gao",
                    "Cuihong Li",
                    "Cheng Liu",
                    "Zhiming Chen"
                ],
                "pub_titles": [
                    "Launch and capture of a single particle in a pulse-laser-assisted dual-beam fiber-optic trap",
                    "Coupler enabled tunable dipole-dipole coupling between optically levitated nanoparticles",
                    "Understanding thermal induced escape mechanism of optically levitated sphere in vacuum",
                    "Force Detection Sensitivity Spectrum Calibration of Levitated Nanomechanical Sensor Using Harmonic Coulomb Force",
                    "Collective-motion-enhanced acceleration sensing via an optically levitated microsphere array",
                    "Morphological tracking and tuning of silica NPs for stable levitation in vacuum optomechanical systems",
                    "Displacement calibration of optical tweezers with absolute gravitational acceleration",
                    "Nanoscale Electric Field Sensing Using Levitated Nano-resonator with Net Charge",
                    "Realization of all-optical underdamped stochastic Stirling engine",
                    "Investigating and Controlling the Libration and Rotation Dynamics of Nanoparticles in an Optomechanical System",
                    "Recognizing three-dimensional phase images with deep learning",
                    "From photon momentum transfer to acceleration sensing",
                    "Zak Phase Induced Topological Nonreciprocity",
                    "Optically Levitated Nanoparticles as Receiving Antennas for Low Frequency Wireless Communication",
                    "Velocity Scanning Tomography for Room-Temperature Quantum Simulation"
                ],
                "pub_abstracts": [
                    "The rapid loading and manipulation of microspheres in optical trap is important for its applications in optomechanics and precision force sensing. We investigate the microsphere behavior under coaction of a dual-beam fiber-optic trap and a pulse laser beam, which reveals a launched microsphere can be effectively captured in a spatial region. A suitable order of pulse duration for launch is derived according to the calculated detachment energy threshold of pulse laser. Furthermore, we illustrate the effect of structural parameters on the launching process, including the spot size of pulse laser, the vertical displacement of beam waist and the initial position of microsphere. Our result will be instructive in the optimal design of the pulse-laser-assisted optical tweezers for controllable loading mechanism of optical trap.",
                    "Multiple optically levitated particles in vacuum can exhibit electrostatic interactions, optical binding, and non-reciprocal light-induced dipole-dipole interactions, making them promising platforms for exploring mesoscopic entanglement and complex interactions. However, in optical trap arrays, individually controlling the position and polarization of each trap is challenging, limiting the precise tuning of interactions between adjacent particles. This constraint hinders the study of complex interaction systems. In this work, we introduce a third nanoparticle as a coupler to two initially non-interacting nanoparticles, achieving tunable dipole-dipole coupling mediated by the third one. We investigated the effect of the particles' phases and positions on the interaction strength and demonstrated its broad tunability. Our method allows for precise control of interactions between any pair of adjacent particles in multi-particle systems, facilitating the further use of levitated nanoparticle arrays in macroscopic quantum mechanics and sensing.",
                    "The escape phenomenon, mainly caused by thermal effects, is known as an obstacle to the further practical application of optical levitation system in vacuum. Irregular photophoresis induced by thermal effects can act as an amplifier of Brownian motion. Studies on this topic provide interpretation for particle escaping phenomenon during the pressure decreasing process, as well as valuable insights into the micro- and nanoscale thermal effects in optical trap in vacuum. In this paper, we derive and test a dynamic model for the motion of an optically levitated particle in a non-equilibrium state and demonstrate the escaping mechanism of heated particles. The result of theoretical investigations is consistent with experimental escape at 0.1mbar. This work reveals and provides a theoretical basis for the stable operation of laser levitated oscillator in high vacuum and pave the way for the practicability of ultra-sensitive sensing devices.",
                    "Oscillators based on levitated particles are promising for the development of ultrasensitive force detectors. The theoretical performance of levitated nanomechanical sensors is usually characterized by the so-called thermal noise limit force detection sensitivity, which does not exhibit spectral specificity in practical measurements. To characterize the actual detection performance, we propose a method for the force detection sensitivity calibration of a levitated nanomechanical sensor based on the harmonic Coulomb force. Utilizing the measured transfer function, we obtained the force detection sensitivity spectrum from the position spectrum. Although the thermal noise limit force detection sensitivity of the system reached $\\rm\\left( {4.39 \\pm 0.62} \\right) \\times {10^{ - 20}} N/H{z^{1/2}}$ at $\\rm{2.4\\times10^{-6} mbar}$ with feedback cooling, the measured sensitivity away from the resonance was of the order of $\\rm10^{-17} N/Hz^{1/2}$ based on the existing detection noise level. The calibration method established in our study is applicable to the performance evaluation of any optical levitation system for high-sensitivity force measurements.",
                    "Optically levitated microspheres are an excellent candidate for force and acceleration sensing. Here, we propose an acceleration sensing protocol based on an optically levitated microsphere array (MSA). The system consists of an $N$-microsphere array levitated in a driven optical cavity via holographic optical tweezers. By positioning the microspheres suitably relative to the cavity, only one of the collective modes of the MSA is coupled to the cavity mode. The optomechanical interaction encodes the information of acceleration acting on the MSA onto the intracavity photons, which can then be detected directly at the output of the cavity. The optically levitated MSA forms an effective large mass-distributed particle, which not only circumvents the problem of levitating a large mass microsphere but also results in a significant improvement of sensitivity. Compared with the traditional single-microsphere measurement scheme, our method presents an improvement in sensitivity by a factor of $\\sqrt{N}$.",
                    "Optically levitated nanomechanical resonators in vacuum perform ultrahigh sensitivity for mechanical quantities by overcoming the limitations of clamped resonators. However, the generally levitated silica nanoparticles (NPs) with low absorption and high transparence still face difficulties surviving in high vacuum with unclear reason. By monitoring the physicochemical properties like scattering, mass and density of amorphous silica NPs during pumping process. we propose that the loss of NPs may arises from the motional instability induced by laser heating lead releasing at low pressure. In this work, two types of NPs are heat treated from 100 to 1200 degree Celsius to release impurities before being loaded into an optical trap. The high vacuum levitation ratio for both NPs increase obviously after heat treatment. In particular, for NPs heated to 600 degree Celsius, the ratio strikingly improves from ~30% to 100% and ~0 to 85% for two types of NPs. The loss mechanism is further confirmed by their relatively stable physicochemical parameters during pumping process. This work paves a way for wide application of levitated nano-resonators and indicates that levitated vacuum optomechanical systems could be a promising tool for dynamics and in-situ studying of small particles like aerosols and dusts.",
                    "In recent years, levitated particles of optical traps in vacuum have shown enormous potential in precision sensor development and searching for new physics. The accuracy of the calibration relating the detected signal to absolute displacement of the trapped particle is a critical factor for absolute measurement performance. In this paper, we suggest and experimentally demonstrate a novel calibration method for optical tweezers based on free-falling particles in vacuum, where the gravitational acceleration is introduced as an absolute reference. Our work provides a calibration protocol with great certainty and traceability, which is significant in improving the accuracy of precision sensing based on optically levitated particles.",
                    "Nanomechanical resonator based on levitated particle exhibits unique advantages in the development of ultrasensitive electric field detector. We demonstrate a three-dimensional, high-sensitivity electric field measurement technology using the optically levitated nanoparticle with a known net charge. By changing the relative position between nanoparticle and parallel electrodes, the three-dimensional electric field distribution is scanned. The measured noise equivalent electric intensity with charge amount of 100 reaches the order of {\\rm{1\\mu V/cm/H}}{{\\rm{z}}^{{\\rm{1/2}}}} at 1.4 \\times {10^{ - 7}}mbar. Linearity analysis near resonance frequency shows a measured linear range over 91dB limited only by the maximum output voltage of the driving equipment. This work may provide avenue for developing a high-sensitive electric field sensor based on optically levitated nano-resonator.",
                    "We experimentally demonstrate a nano-scale stochastic Stirling heat engine operating in the underdamped regime. The setup involves an optically levitated silica particle that is subjected to a power-varying optical trap and periodically coupled to a cold/hot reservoir via switching on/off active feedback cooling. We conduct a systematic investigation of the engine's performance and find that both the output work and efficiency approach their theoretical limits under quasi-static conditions. Furthermore, we examine the dependence of the output work fluctuation on the cycle time and temperature difference between the hot and cold reservoirs. We observe that the distribution has a Gaussian profile in the quasi-static regime, whereas it becomes asymmetric and non-Gaussian as the cycle duration time decreases. This non-Gaussianity is qualitatively attributed to the strong correlation of the particle's position within a cycle in the non-equilibrium regime. Our experiments provide valuable insights into stochastic thermodynamics in the underdamped regime and open up new possibilities for the design of future nano-machines.",
                    "In optomechanical systems, the libration and rotation of nanoparticles offer profound insights for ultrasensitive torque measurement and macroscopic quantum superpositions. Achievements include transitioning libration to rotation up to 6 GHz and cooling libration to millikelvin temperatures. It is undoubted that the libration and rotation are respectively driven by restoring and constant optical torques. The transition mechanisms between these two states, however, demand further exploration. In this perspective, it is demonstrated in this manuscript that monitoring lateral-scattered light allows real-time observation of libration/rotation transitions and associated hysteresis as ellipticities of trapping laser fields vary. By calculating optical torques and solving the Langevin equation, transitions are linked to the balance between anisotropic-polarization-induced sinusoidal optical torques and constant ones, with absorption identified as the main contributor to constant torques. These findings enable direct weak torque sensing and precise nanoparticle control in rotational degrees, paving the way for studying quantum effects like nonadiabatic phase shifts and macroscopic quantum superpositions, thereby enriching quantum optomechanics research.",
                    "Optical phase contains key information for biomedical and astronomical imaging. However, it is often obscured by layers of heterogeneous and scattering media, which render optical phase imaging at different depths an utmost challenge. Limited by the memory effect, current methods for phase imaging in strong scattering media are inapplicable to retrieving phases at different depths. To address this challenge, we developed a speckle three-dimensional reconstruction network (STRN) to recognize phase objects behind scattering media, which circumvents the limitations of memory effect. From the single-shot, reference-free and scanning-free speckle pattern input, STRN distinguishes depth-resolving quantitative phase information with high fidelity. Our results promise broad applications in biomedical tomography and endoscopy.",
                    "As a typical application of photon momentum transfer, optical levitation systems are known for their ideal isolation from mechanical dissipation and thermal noise. These characters offer extraordinary potential for acceleration precision sensing and have attracted extensive attention in both fundamental and applied physics. Although considerable improvements of optical levitation accelerometers has been reported, the dynamic testing of the sensing performance remains a crucial challenge before the utilization in practical application scenarios. In this work, we present a dual-beam optical levitation accelerometer and demonstrate the test with dynamic inputs for the first time. An acceleration sensing sensitivity of $0.1\\mu g$ and a measurement range of $ 1g$ are achieved. These advancements solidify the potential of optical levitation accelerometer for deployment in practical domains, including navigation, intelligent driving, and industrial automation, building a bridge between the laboratory systems and real-world applications.",
                    "Topological physics provides novel insights for designing functional photonic devices, such as magnetic-free optical diodes, which are important in optical engineering and quantum information processing. Past efforts mostly focus on the topological edge modes in two-dimensional (2D) photonic Chern lattices, which, however, require delicate fabrication and temporal modulation. In particular, the 1D nonreciprocal edge mode needs to be embedded in a 2D lattice, contradicting with the compactness of integrated photonics. To address these challenges, we investigate the optical nonreciprocity of the 1D Su-Schrieffer-Heeger (SSH) superradiance lattices in room-temperature atoms. The probe fields propagating in two opposite directions perceive two different SSH topological phases, which have different absorption spectra due to the interplay between the Zak phase and the thermal motion of atoms, resulting in optical nonreciprocity. Our findings reveal the relationship between 1D topological matter and optical nonreciprocity, simplifying the design of topologically resilient nonreciprocal devices.",
                    "Low-frequency (LF) wireless communications play a crucial role in ensuring anti-interference, long-range, and efficient communication across various environments. However, in conventional LF communication systems, their antenna size is required to be inversely proportional to the wavelength, so that their mobility and flexibility are greatly limited. Here we introduce a novel prototype of LF receiving antennas based on optically levitated nanoparticles, which overcomes the size-frequency limitation to reduce the antenna size to the hundred-nanometer scale. These charged particles are extremely sensitive to external electric field as mechanical resonators, and their resonant frequencies are adjustable. The effectiveness of these antennas was experimentally demonstrated by using the frequency shift keying (2FSK) modulation scheme. The experimental results indicate a correlation between error rate and factors such as transmission rate, signal strength, and vacuum degree with a signal strength of approximately 0.1V/m and a bit error rate below 0.1%. This advancement in leveraging levitated particle mechanical resonators (LPMRs) as LF antennas marks a significant stride in long-distance communication technology.",
                    "Quantum simulation offers an analog approach for exploring exotic quantum phenomena using controllable platforms, typically necessitating ultracold temperatures to maintain the quantum coherence. Superradiance lattices (SLs) have been harnessed to simulate coherent topological physics at room temperature, but the thermal motion of atoms remains a notable challenge in accurately measuring the physical quantities. To overcome this obstacle, we invent and validate a velocity scanning tomography technique to discern the responses of atoms with different velocities, allowing cold-atom spectroscopic resolution within room-temperature SLs. By comparing absorption spectra with and without atoms moving at specific velocities, we can derive the Wannier-Stark ladders of the SL across various effective static electric fields, their strengths being proportional to the atomic velocities. We extract the Zak phase of the SL by monitoring the ladder frequency shift as a function of the atomic velocity, effectively demonstrating the topological winding of the energy bands. Our research signifies the feasibility of room-temperature quantum simulation and facilitates their applications in quantum information processing."
                ],
                "domain": [
                    "Optical Trapping",
                    "Quantum Optomechanics",
                    "Nanomechanics",
                    "Sensing Technology"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "64b30e18-ed98-4d5d-b6fc-5949ac12e67f": {
                "pk": "64b30e18-ed98-4d5d-b6fc-5949ac12e67f",
                "project_name": null,
                "name": "Vladislav V. Yakovlev",
                "bio": "I am a researcher deeply engaged in the exploration of quantum optics and its applications in advanced imaging techniques. My recent work focuses on the fascinating interplay between atomic arrays and non-Hermitian physics, where I discovered a butterfly-like band structure that reveals topologically-protected edge states exhibiting robust super- and subradiance behaviors. This research not only enhances our understanding of quantum memory and information but also opens new avenues for quantum sensing and super-resolution imaging.\n\nIn addition to my theoretical pursuits, I have made significant strides in practical applications, particularly in the realm of Brillouin microscopy. I demonstrated the first instance of quantum-enhanced stimulated Brillouin scattering using low-power continuous-wave lasers, achieving a notable signal-to-noise ratio enhancement through the use of squeezed light. This advancement holds promise for bio-imaging applications, allowing for the probing of mechanical properties in biological samples while minimizing phototoxicity and thermal effects.\n\nFurthermore, I am exploring the potential of programmable phase-only spatial optimization to influence supercontinuum generation. My preliminary results indicate that we can achieve significant spectral control without energy loss, paving the way for more efficient utilization of supercontinuum power. Overall, my research aims to bridge theoretical insights with practical innovations, contributing to the evolving landscape of quantum technologies and their applications in science and medicine.",
                "collaborators": [
                    "Anwei Zhang",
                    "Xianfeng Chen",
                    "Luqi Yuan",
                    "Tian Li",
                    "Fu Li",
                    "Xinghua Liu",
                    "Girish S. Agarwal",
                    "Alexandra A. Zhdanova",
                    "Yujie Shen",
                    "Jonathan V. Thompson"
                ],
                "pub_titles": [
                    "Tunable Topologically-protected Super- and Subradiant Boundary States in One-Dimensional Atomic Arrays",
                    "Quantum-Enhanced Stimulated Brillouin Scattering Spectroscopy and Imaging",
                    "Controlled Supercontinua via Spatial Beam Shaping"
                ],
                "pub_abstracts": [
                    "Single-photon super- and subradiance are important for the quantum memory and quantum information. We investigate one-dimensional atomic arrays under the spatially periodic magnetic field with a tunable phase, which provides a distinctive physics aspect of revealing exotic two-dimensional topological phenomena with a synthetic dimension. A butterfly-like nontrivial bandstructure associated with the non-Hermitian physics involving strong long-range interactions has been discovered. It leads to pairs of topologically-protected edge states, which exhibit the robust super- or subradiance behavior, localized at the boundaries of the atomic arrays. This work opens an avenue of exploring an interacting quantum optical platform with synthetic dimensions pointing to potential implications for quantum sensing as well as the super-resolution imaging.",
                    "Brillouin microscopy is an emerging label-free imaging technique to assess local viscoelastic properties. Quantum-enhanced stimulated Brillouin scattering is demonstrated for the first time using low power continuous-wave lasers at 795~nm. A signal to noise ratio enhancement of 3.4~dB is reported by using two-mode intensity-difference squeezed light generated with the four-wave mixing process in atomic rubidium vapor. The low optical power and the excitation wavelengths in the water transparency window has the potential to provide a powerful bio-imaging technique for probing mechanical properties of biological samples prone to phototoxicity and thermal effects. The performance enhancement affordable through the use of quantum light may pave the way for significantly improved sensitivity that cannot be achieved classically. The proposed new way of utilizing squeezed light for enhanced stimulated Brillouin scattering can be easily adapted for both spectroscopic and imaging applications in materials science and biology.",
                    "Recently, optimization techniques have had a significant impact in a variety of fields, leading to a higher signal-to-noise and more streamlined techniques. We consider the possibility for using programmable phase-only spatial optimization of the pump beam to influence the supercontinuum generation process. Preliminary results show that significant broadening and rough control of the supercontinuum spectrum are possible without loss of input energy. This serves as a proof-of-concept demonstration that spatial effects can controllably influence the supercontinuum spectrum, leading to possibilities for utilizing supercontinuum power more efficiently and achieving arbitrary spectral control."
                ],
                "domain": [
                    "Quantum Optics",
                    "Superradiance",
                    "Imaging Techniques",
                    "Non-Hermitian Physics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "bf0af01e-37a0-4e23-8aa0-5f7312094494": {
                "pk": "bf0af01e-37a0-4e23-8aa0-5f7312094494",
                "project_name": null,
                "name": "Shi-Yao Zhu",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimizing performance across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for optimal model designs, making it more efficient and insightful.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and machine learning, with the goal of making these technologies more effective and accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "AutoML",
                    "Multi-task Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "7ea805a3-2b92-43a6-8ef7-7ff872601d4b": {
                "pk": "7ea805a3-2b92-43a6-8ef7-7ff872601d4b",
                "project_name": null,
                "name": "Da-Wei Wang",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, thereby enhancing their scalability and effectiveness.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 configurations to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge and improving efficiency.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and making them more accessible and effective for real-world applications. I am excited about the future of this field and the potential for my contributions to inspire further innovations.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "de6c0809-b388-4fc7-966a-e0ae7cfae6da": {
                "pk": "de6c0809-b388-4fc7-966a-e0ae7cfae6da",
                "project_name": null,
                "name": "Delong Zhang",
                "bio": "I am a researcher dedicated to exploring the dynamics of Hamiltonian systems and the intricacies of quantum walk-based search algorithms. My recent work focuses on developing methods to predict the dynamics of Hamiltonian systems from discrete observations, where I have evaluated various approaches to understand their efficiency and effectiveness in different contexts. \n\nIn addition, I have delved into the challenges posed by the soufflé problem in quantum walk-based search algorithms, particularly in relation to Grover's algorithm. My research has led to the creation of a new framework that enhances robustness without compromising the quantum speedup, allowing for successful searches on complete bipartite graphs even when the exact number of marked vertices is unknown. \n\nThrough my work, I aim to bridge theoretical insights with practical applications, contributing to the advancement of both Hamiltonian dynamics and quantum computing methodologies.",
                "collaborators": [
                    "Zi-Yu Khoo",
                    "Stéphane Bressan",
                    "Yongzhen Xu",
                    "Lvzhou Li"
                ],
                "pub_titles": [
                    "What's Next? Predicting Hamiltonian Dynamics from Discrete Observations of a Vector Field",
                    "Robust Quantum Walk Search Without Knowing the Number of Marked Vertices"
                ],
                "pub_abstracts": [
                    "We present several methods for predicting the dynamics of Hamiltonian systems from discrete observations of their vector field. Each method is either informed or uninformed of the Hamiltonian property. We empirically and comparatively evaluate the methods and observe that information that the system is Hamiltonian can be effectively informed, and that different methods strike different trade-offs between efficiency and effectiveness for different dynamical systems.",
                    "There has been a very large body of research on searching a marked vertex on a graph based on quantum walks, and Grover's algorithm can be regarded as a quantum walk-based search algorithm on a special graph. However, the existing quantum walk-based search algorithms suffer severely from the souffl\\'{e} problem which mainly means that the success probability of finding a marked vertex could shrink dramatically even to zero when the number of search steps is greater than the right one, thus heavily reducing the robustness and practicability of the algorithm. Surprisingly, while the souffl\\'{e} problem of Grover's algorithm has attracted enough attention, how to address this problem for general quantum walk-based search algorithms is missing in the literature. Here we initiate the study of overcoming the souffl\\'{e} problem for quantum walk-based search algorithms by presenting a new quantum walk-based search framework that achieves robustness without sacrificing the quantum speedup. In this framework, for any adjustable parameter $\\epsilon$, the quantum algorithm can find a marked vertex on an $N$-vertex {\\it complete bipartite graph} with probability at least $ 1-\\epsilon$, whenever the number of search steps $h$ satisfies $h \\geq \\ln(\\frac{2}{\\sqrt{\\epsilon}})\\sqrt{N} + 1$. Note that the algorithm need not know the exact number of marked vertices. Consequently, we obtain quantum search algorithms with stronger robustness and practicability."
                ],
                "domain": [
                    "Quantum Computing",
                    "Hamiltonian Dynamics",
                    "Graph Theory"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively extract and enhance information from speckle patterns generated by coherent waves passing through inhomogeneous media to improve imaging and sensing capabilities?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for advancing optical and ultrasound imaging techniques, particularly in turbid media where traditional methods struggle. By improving the fidelity of information extraction from speckle patterns, this research could lead to significant advancements in medical imaging, environmental monitoring, and optical communications. The implications extend to enhancing the accuracy of diagnostics in healthcare and improving the reliability of imaging systems in complex environments, thereby influencing future research directions in both theoretical and applied optics.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in addressing this problem stem from the inherent randomness and complexity of speckle patterns, which can obscure the underlying information. Naive approaches may fail because they do not account for the intricate interference effects and the varying intensity distributions that characterize speckle. Additionally, technical obstacles include the need for sophisticated algorithms to analyze and reconstruct images from noisy data, as well as the practical difficulties in controlling and measuring the scattering environment accurately.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on either the theoretical aspects of speckle or on specific applications without a comprehensive approach to information extraction. Limitations in computational methods and a lack of robust datasets for training models have hindered progress. Additionally, many existing solutions do not adequately address the dynamic nature of scattering environments, which can vary significantly in real-world applications. Our approach aims to integrate advanced machine learning techniques with experimental data to overcome these barriers, providing a more holistic solution.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves using a combination of machine learning algorithms and statistical analysis to process speckle images collected from various scattering media. We will utilize the MNIST dataset as a modulation phase map, applying metrics such as the Pearson correlation coefficient (PCC) and the structural similarity index measure (SSIM) to evaluate image fidelity. The expected outcomes include improved accuracy in reconstructing images from speckle patterns, leading to enhanced imaging capabilities in turbid media and practical applications in medical diagnostics and optical communications."
    },
    "2407.10990": {
        "paper_data": {
            "title": "MedBench: A Comprehensive, Standardized, and Reliable Benchmarking System for Evaluating Chinese Medical Large Language Models",
            "url": "http://arxiv.org/abs/2407.10990v1",
            "arxiv_id": "2407.10990",
            "authors": [
                "Mianxin Liu",
                "Jinru Ding",
                "Jie Xu",
                "Weiguo Hu",
                "Xiaoyang Li",
                "Lifeng Zhu",
                "Zhian Bai",
                "Xiaoming Shi",
                "Benyou Wang",
                "Haitao Song",
                "Pengfei Liu",
                "Xiaofan Zhang",
                "Shanshan Wang",
                "Kang Li",
                "Haofen Wang",
                "Tong Ruan",
                "Xuanjing Huang",
                "Xin Sun",
                "Shaoting Zhang"
            ],
            "abstract": "Ensuring the general efficacy and goodness for human beings from medical large language models (LLM) before real-world deployment is crucial. However, a widely accepted and accessible evaluation process for medical LLM, especially in the Chinese context, remains to be established. In this work, we introduce \"MedBench\", a comprehensive, standardized, and reliable benchmarking system for Chinese medical LLM. First, MedBench assembles the currently largest evaluation dataset (300,901 questions) to cover 43 clinical specialties and performs multi-facet evaluation on medical LLM. Second, MedBench provides a standardized and fully automatic cloud-based evaluation infrastructure, with physical separations for question and ground truth. Third, MedBench implements dynamic evaluation mechanisms to prevent shortcut learning and answer remembering. Applying MedBench to popular general and medical LLMs, we observe unbiased, reproducible evaluation results largely aligning with medical professionals' perspectives. This study establishes a significant foundation for preparing the practical applications of Chinese medical LLMs. MedBench is publicly accessible at https://medbench.opencompass.org.cn.",
            "introduction": " Introduction   Large Language Models (LLMs) play a n increasingly  critical role across various fields and can potentially reform the healthcare sector. Medical Large Language  Models (MLLMs) have thus emerged as a key area of  focus [ 1, 2]. To ensure the reliability of MLLMs  before they are deployed in real -world settings, a  thorough evaluation is essential [ 3]. It is vital to  establish a comprehensive, standardized, and reliable   benchmarking system to assess the general efficacy of MLLMs. Despite this need, there is a notable absence of a universally recognized benchmarking framework,  especially in the context of the Chinese context.     There has been a great endeavor to establish Chinese  benchmarks for MLLM, such as National Medical  Licensing Examination in China -Question Answering  (MLEC -QA) [ 4], Chinese National Medical Licensing  Examination (CMExam) [ 5], Chinese Biomedical  Language Understanding Evaluation (CBLUE) [ 6],  and Comprehensive Medical Benchmark in Chinese (CMB) [7]. However, existing benchmarks exhibit several limitations that hinder their suitability for evaluating MLLMs comprehensively. Firstly , existing  benchmarks mainly focus on general clinical knowledge and often fall short in providing sufficient coverage across medical specialties, limiting their applicability to diverse healthcare sectors, as  evidenced by the reported subpar performance of  LLMs in specific fields like nephrology [ 8] and  myopia care [ 9]. I t highlights the necessity of  benchmarks that encompass a broader spectrum of medical specialties. Second ly, while existing  benchmarks contribute valuable datasets, they neglect the establishment of a standardized evaluation  infrastructure. Currently, the benchmark procedure is  often chosen and operated by the user, resulting in inconsistent evaluation experiments, we test ChatGPT   (GPT3.5, https://chat.openai.com/ ), PULSE [ 13],  ChatGLM3 [ 14], BenTsao [ 15], an d BianQue2 [ 16]  using MedBench. We classify different types of tasks  into four:   • Multiple choice question (MCQ) is typically a  form of assessment or inquiry that provides  multiple predetermined answers among whic h the  respondent must select.  • Close question answering (QA) involves  a type of  problem where words or phrases are removed  (blanks) from a text, and the goal is to fill in the blanks accurately.   • Open QA refers to a more complex and dynamic  form of question answering where the answers are not limited to a predefined set of options.  • Information Extraction (IE) is the task of  automatically extracting structured information from unstructured and/or semi -structured  machine -readable documents.   For different datas ets, varying metrics are computed  according to the task, including accuracy, BiLingual  Evaluation Understudy (BLEU) score [ 17],  Recall -Oriented Understudy for Gisting  Improve -Longest (ROUGH -L) score [ 18], and  micro -averaged F1 -score (Micro -F1). BLEU measures  the similarity between a machine- generated text and  one or more reference texts. It counts the number of overlapping n- grams (sequences of n words) between  the machi ne-generated and reference texts. The high  overlapping associates with higher BLEU score. Rouge -L also measures the overlap between the  model -generated summary and reference summaries.  Specifically, it considers the longest common subsequence of words betw een the generated",
            "references": [
                {
                    "title": "OpenMEDLab: An Open-source Platform for Multi-modality Foundation Models in Medicine",
                    "abstract": "The emerging trend of advancing generalist artificial intelligence, such as GPTv4 and Gemini, has reshaped the landscape of research (academia and industry) in machine learning and many other research areas. However, domain-specific applications of such foundation models (e.g., in medicine) remain untouched or often at their very early stages. It will require an individual set of transfer learning and model adaptation techniques by further expanding and injecting these models with domain knowledge and data. The development of such technologies could be largely accelerated if the bundle of data, algorithms, and pre-trained foundation models were gathered together and open-sourced in an organized manner. In this work, we present OpenMEDLab, an open-source platform for multi-modality foundation models. It encapsulates not only solutions of pioneering attempts in prompting and fine-tuning large language and vision models for frontline clinical and bioinformatic applications but also building domain-specific foundation models with large-scale multi-modal medical data. Importantly, it opens access to a group of pre-trained foundation models for various medical image modalities, clinical text, protein engineering, etc. Inspiring and competitive results are also demonstrated for each collected approach and model in a variety of benchmarks for downstream tasks. We welcome researchers in the field of medical artificial intelligence to continuously contribute cutting-edge methods and models to OpenMEDLab, which can be accessed via https://github.com/openmedlab."
                },
                {
                    "title": "OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM",
                    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in various multimodal tasks. However, their potential in the medical domain re-mains largely unexplored. A significant challenge arises from the scarcity of diverse medical images spanning various modalities and anatomical regions, which is essential in real-world medical applications. To solve this problem, in this paper, we introduce OmniMedVQA, a novel comprehensive medical Visual Question Answering (VQA) benchmark. This benchmark is collected from 73 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions. Importantly, all images in this benchmark are sourced from authentic medical scenarios, ensuring alignment with the requirements of the medical field and suitability for evaluating LVLMs. Through our extensive experiments, we have found that existing LVLMs struggle to address these medical VQA problems effectively. Moreover, what surprises us is that medical-specialized LVLMs even exhibit inferior performance to those general-domain models, calling for a more versatile and robust LVLM in the biomedical field. The evaluation results not only reveal the current limitations of LVLM in understanding real medical images but also highlight our dataset's significance. Our code with dataset are available at https://github.com/OpenGVLab/ Multi Modality-Arena."
                },
                {
                    "title": "Benchmarking Open-Source Large Language Models, GPT-4 and Claude 2 on Multiple-Choice Questions in Nephrology",
                    "abstract": null
                },
                {
                    "title": "Data-Centric Foundation Models in Computational Healthcare: A Survey",
                    "abstract": "The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare-related foundation models and datasets at https://github.com/Yunkun-Zhang/Data-Centric-FM-Healthcare ."
                },
                {
                    "title": "Six ways large language models are changing healthcare",
                    "abstract": null
                },
                {
                    "title": "Fake Alignment: Are LLMs Really Aligned Well?",
                    "abstract": "The growing awareness of safety concerns in large language models (LLMs) has sparked considerable interest in the evaluation of safety. This study investigates an under-explored issue about the evaluation of LLMs, namely the substantial discrepancy in performance between multiple-choice questions and open-ended questions. Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization. That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs. We introduce a Fake alIgNment Evaluation (FINE) framework and two novel metrics——Consistency Score (CS) and Consistent Safety Score (CSS), which jointly assess two complementary forms of evaluation to quantify fake alignment and obtain corrected performance estimation. Applying FINE to 14 widely-used LLMs reveals several models with purported safety are poorly aligned in practice. Subsequently, we found that multiple-choice format data can also be used as high-quality contrast distillation-based fine-tuning data, which can strongly improve the alignment consistency of LLMs with minimal fine-tuning overhead. For data and code, see https://github.com/AIFlames/Fake-Alignment."
                },
                {
                    "title": "trRosettaRNA: automated prediction of RNA 3D structure with transformer network",
                    "abstract": null
                },
                {
                    "title": "Don't Make Your LLM an Evaluation Benchmark Cheater",
                    "abstract": "Large language models~(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, \\ie \\emph{benchmark leakage}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs."
                },
                {
                    "title": "A new paradigm for applying deep learning to protein–ligand interaction prediction",
                    "abstract": "Protein-ligand interaction prediction poses a significant challenge in the field of drug design. Numerous machine learning and deep learning models have been developed to identify the most accurate docking poses of ligands and active compounds against specific targets. However, the current models often suffer from inadequate accuracy and lack practical physical significance in their scoring systems. In this research paper, we introduce IGModel, a novel approach that leverages the geometric information of protein-ligand complexes as input for predicting the root mean square deviation (RMSD) of docking poses and the binding strength (the negative value of the logrithm of binding affinity, pKd) with the same prediction framework. By incorporating the geometric information, IGModel ensures that its scores carry intuitive meaning. The performance of IGModel has been extensively evaluated on various docking power test sets, including the CASF-2016 benchmark, PDBbind-CrossDocked-Core, and DISCO set, consistently achieving state-of-theart accuracies. Furthermore, we assess IGModel’s generalization ability and robustness by evaluating it on unbiased test sets and sets containing target structures generated by AlphaFold2. The exceptional performance of IGModel on these sets demonstrates its efficacy. Additionally, we visualize the latent space of protein-ligand interactions encoded by IGModel and conduct interpretability analysis, providing valuable insights. This study presents a novel framework for deep learning-based prediction of protein-ligand interactions, contributing to the advancement of this field. Key Messages We introduce the first framework for simultaneously predicting the RMSD of the ligand docking pose and its binding strength to the target. IGModel can effectively improve the accuracy of identifying the near-native binding poses of the ligands, and can still outperform most baseline models in scoring power, ranking power and screening power tasks. IGModel is still ahead of other state-of-the-art models in the unbiased data set and the target structure predicted by AlphaFold2, proving its excellent generalization ability. Latent space provided by IGModel learns the physical interactions, thus indicating the robustness of the model."
                },
                {
                    "title": "BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT",
                    "abstract": "Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health suggestions polished by ChatGPT. Experimental results demonstrate that the proposed BianQue can simultaneously balance the capabilities of both questioning and health suggestions, which will help promote the research and application of LLMs in the field of proactive health."
                },
                {
                    "title": "SAM-Med2D",
                    "abstract": "The Segment Anything Model (SAM) represents a state-of-the-art research advancement in natural image segmentation, achieving impressive results with input prompts such as points and bounding boxes. However, our evaluation and recent research indicate that directly applying the pretrained SAM to medical image segmentation does not yield satisfactory performance. This limitation primarily arises from significant domain gap between natural images and medical images. To bridge this gap, we introduce SAM-Med2D, the most comprehensive studies on applying SAM to medical 2D images. Specifically, we first collect and curate approximately 4.6M images and 19.7M masks from public and private datasets, constructing a large-scale medical image segmentation dataset encompassing various modalities and objects. Then, we comprehensively fine-tune SAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that only adopt bounding box or point prompts as interactive segmentation approach, we adapt SAM to medical image segmentation through more comprehensive prompts involving bounding boxes, points, and masks. We additionally fine-tune the encoder and decoder of the original SAM to obtain a well-performed SAM-Med2D, leading to the most comprehensive fine-tuning strategies to date. Finally, we conducted a comprehensive evaluation and analysis to investigate the performance of SAM-Med2D in medical image segmentation across various modalities, anatomical structures, and organs. Concurrently, we validated the generalization capability of SAM-Med2D on 9 datasets from MICCAI 2023 challenge. Overall, our approach demonstrated significantly superior performance and generalization capability compared to SAM."
                },
                {
                    "title": "Benchmarking large language models’ performances for myopia care: a comparative analysis of ChatGPT-3.5, ChatGPT-4.0, and Google Bard",
                    "abstract": null
                },
                {
                    "title": "CMB: A Comprehensive Medical Benchmark in Chinese",
                    "abstract": "Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in contextual incongruities to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. We hope this benchmark provide first-hand experience in existing LLMs for medicine and also facilitate the widespread adoption and enhancement of medical LLMs within China. Our data and code are publicly available at https://github.com/FreedomIntelligence/CMB."
                },
                {
                    "title": "Open-ended questions automated evaluation: proposal of a new generation",
                    "abstract": "Abstract. Exams grading for the knowledge validation to recognise competences is an essential element for any learning process. There are two main modes for their evaluation: subjective and automated. Subjective evaluation is accused of many flaws such as the inconsistency of the human corrector and the time it requires. Automating the assessment of open-ended questions saves a lot of time, provides quick feedback to learners and ensures the consistency expected from the human correctors. However, this is a challenging problem to implement because the computer does not have the same faculties as a human. To address this issue, we conducted a literature review on open-ended questions automated evaluation to implement an automatic exam grading system with similar or even higher accuracy than a human corrector. This study allows us to classify the different approaches in three generations: “bag of words” based approaches, classical semantic similarity-based approaches and machine learning based approaches. The third generation offers the best state-of-the-art results despite criticism of it. These approaches rely on neural networks which need to have a large dataset for effective training. To tackle this handicap, we propose a fourth generation (section 3). This contribution relies on the use of pre-trained models for which a dataset for training is not necessary knowing that they are zero-shot-learners. After implementing our architecture, we conducted our experiments with the three main French-speaking models available on Hugging Face. The best model agrees with the human corrector at 96%."
                },
                {
                    "title": "A Survey on Evaluation of Large Language Models",
                    "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey"
                },
                {
                    "title": "Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train",
                    "abstract": "Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downstream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art (SOTA) self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM."
                },
                {
                    "title": "A Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification",
                    "abstract": null
                },
                {
                    "title": "On the Challenges and Perspectives of Foundation Models for Medical Image Analysis",
                    "abstract": "This article discusses the opportunities, applications and future directions of large-scale pretrained models, i.e., foundation models, which promise to significantly improve the analysis of medical images. Medical foundation models have immense potential in solving a wide range of downstream tasks, as they can help to accelerate the development of accurate and robust models, reduce the dependence on large amounts of labeled data, preserve the privacy and confidentiality of patient data. Specifically, we illustrate the \"spectrum\" of medical foundation models, ranging from general imaging models, modality-specific models, to organ/task-specific models, and highlight their challenges, opportunities and applications. We also discuss how foundation models can be leveraged in downstream medical tasks to enhance the accuracy and efficiency of medical image analysis, leading to more precise diagnosis and treatment decisions."
                },
                {
                    "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
                    "abstract": "Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model."
                },
                {
                    "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
                    "abstract": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese."
                },
                {
                    "title": "The SCARE 2023 guideline: updating consensus Surgical CAse REport (SCARE) guidelines",
                    "abstract": "Background: The Surgical CAse REport (SCARE) guidelines were first published in 2016 as a tool for surgeons to document and report their surgical cases in a standardised and comprehensive manner. However, with advances in technology and changes in the healthcare landscape, it is important to revise and update these guidelines to ensure they remain relevant and valuable for surgeons. Materials and methods: The updated guidelines were produced through a Delphi consensus exercise. Members of the SCARE 2020 guidelines Delphi group, editorial board members, and peer reviewers were invited to participate. Potential contributors were contacted by e-mail. An online survey was completed to indicate their agreement with the proposed changes to the guideline items. Results: A total of 54 participants were invited to participate and 44 (81.5%) completed the survey. There was a high degree of agreement among reviewers, with 36 items (83.7%) meeting the threshold for inclusion. Conclusion: Through a completed Delphi consensus exercise we present the SCARE 2023 guidelines. This will provide surgeons with a comprehensive and up-to-date tool for documenting and reporting their surgical cases while highlighting the importance of patient-centred care."
                },
                {
                    "title": "Large language models encode clinical knowledge",
                    "abstract": null
                },
                {
                    "title": "GLM-130B: An Open Bilingual Pre-trained Model",
                    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}."
                },
                {
                    "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark",
                    "abstract": "Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models, and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling."
                },
                {
                    "title": "Automatic grading and hinting in open-ended text questions",
                    "abstract": null
                },
                {
                    "title": "Integrating exosomal microRNAs and electronic health data improved tuberculosis diagnosis",
                    "abstract": null
                },
                {
                    "title": "Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics",
                    "abstract": "In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n-grams automatically. The second method relaxes strict n-gram matching to skip-bigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency."
                },
                {
                    "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                    "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
                },
                {
                    "title": "Opencompass: A universal evaluation platform for foundation models",
                    "abstract": null
                },
                {
                    "title": "MMbench: Is your",
                    "abstract": null
                },
                {
                    "title": "MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset",
                    "abstract": "Question Answering (QA) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, QA systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present MLEC-QA, the largest-scale Chinese multi-choice biomedical QA dataset, collected from the National Medical Licensing Examination in China. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: Clinic, Stomatology, Public Health, Traditional Chinese Medicine, and Traditional Chinese Medicine Combined with Western Medicine. We implement eight representative control methods and open-domain QA methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40% to 55% on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the MLEC-QA dataset can serve as a valuable resource for research and evaluation in open-domain QA, and also make advances for biomedical QA systems."
                }
            ]
        },
        "author_data": {
            "2c6b5143-2b5f-4036-9b40-f98685e4f308": {
                "pk": "2c6b5143-2b5f-4036-9b40-f98685e4f308",
                "project_name": null,
                "name": "Mianxin Liu",
                "bio": "I am a dedicated researcher specializing in the intersection of artificial intelligence and medical imaging, with a particular focus on diagnosing brain disorders and other critical health conditions. My recent work has centered on developing innovative frameworks that leverage multi-modal data to enhance diagnostic accuracy while minimizing costs. For instance, I proposed a progressive Alzheimer's disease sub-type diagnosis framework that intelligently utilizes easier-to-access modalities in earlier stages, significantly improving diagnostic efficiency.\n\nI have also explored the potential of multiscale functional connectivity networks (FCNs) derived from fMRI data, leading to the creation of the Multiscale-Atlases-based Hierarchical Graph Convolutional Network (MAHGCN). This model effectively captures hierarchical relationships in brain data, achieving impressive accuracy in diagnosing conditions like Alzheimer's and autism spectrum disorder.\n\nMy research extends to the development of novel models for predicting disease progression, such as the Clinically-Informed Residual Diffusion model for idiopathic pulmonary fibrosis, which allows for timely intervention. Additionally, I have investigated the integration of pathology images with advanced AI models, highlighting the challenges and opportunities in this domain.\n\nThrough my work, I aim to bridge the gap between complex medical data and practical clinical applications, ultimately contributing to improved patient outcomes and a deeper understanding of brain disorders. I am passionate about advancing the field of medical AI and continuously seek innovative solutions to enhance diagnostic processes.",
                "collaborators": [
                    "Dinggang Shen",
                    "Han Zhang",
                    "Feng Shi",
                    "Shaoting Zhang",
                    "Jingyang Zhang",
                    "Lei Ma",
                    "Fang Yan",
                    "Zhe Wang",
                    "Caiwen Jiang",
                    "Xiaodan Xing"
                ],
                "pub_titles": [
                    "A Progressive Single-Modality to Multi-Modality Classification Framework for Alzheimer's Disease Sub-type Diagnosis",
                    "Hierarchical Graph Convolutional Network Built by Multiscale Atlases for Brain Disorder Diagnosis Using Functional Connectivity",
                    "Mining fMRI Dynamics with Parcellation Prior for Brain Disease Diagnosis",
                    "Exploring the Feasibility of Multimodal Chatbot AI as Copilot in Pathology Diagnostics: Generalist Model's Pitfall",
                    "Segregation, integration and balance of large-scale resting brain networks configure different cognitive abilities",
                    "CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting Idiopathic Pulmonary Fibrosis Progression",
                    "BrainMVP: Multi-modal Vision Pre-training for Brain Image Analysis using Multi-parametric MRI",
                    "A dual-task mutual learning framework for predicting post-thrombectomy cerebral hemorrhage",
                    "Deep learning reveals the common spectrum underlying multiple brain disorders in youth and elders from brain functional networks",
                    "Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites",
                    "Structure-aware registration network for liver DCE-CT images",
                    "Cost-effective Instruction Learning for Pathology Vision and Language Analysis"
                ],
                "pub_abstracts": [
                    "The current clinical diagnosis framework of Alzheimer's disease (AD) involves multiple modalities acquired from multiple diagnosis stages, each with distinct usage and cost. Previous AD diagnosis research has predominantly focused on how to directly fuse multiple modalities for an end-to-end one-stage diagnosis, which practically requires a high cost in data acquisition. Moreover, a significant part of these methods diagnose AD without considering clinical guideline and cannot offer accurate sub-type diagnosis. In this paper, by exploring inter-correlation among multiple modalities, we propose a novel progressive AD sub-type diagnosis framework, aiming to give diagnosis results based on easier-to-access modalities in earlier low-cost stages, instead of modalities from all stages. Specifically, first, we design 1) a text disentanglement network for better processing tabular data collected in the initial stage, and 2) a modality fusion module for fusing multi-modality features separately. Second, we align features from modalities acquired in earlier low-cost stage(s) with later high-cost stage(s) to give accurate diagnosis without actual modality acquisition in later-stage(s) for saving cost. Furthermore, we follow the clinical guideline to align features at each stage for achieving sub-type diagnosis. Third, we leverage a progressive classifier that can progressively include additional acquired modalities (if needed) for diagnosis, to achieve the balance between diagnosis cost and diagnosis performance. We evaluate our proposed framework on large diverse public and in-home datasets (8280 in total) and achieve superior performance over state-of-the-art methods. Our codes will be released after the acceptance.",
                    "Functional connectivity network (FCN) data from functional magnetic resonance imaging (fMRI) is increasingly used for the diagnoses of brain disorders. However, state-of-the-art studies used to build the FCN using a single brain parcellation atlas at a certain spatial scale, which largely neglected functional interactions across different spatial scales in hierarchical manners. In this study, we propose a novel framework to perform multiscale FCN analysis for brain disorder diagnosis. We first use a set of well-defined multiscale atlases to compute multiscale FCNs. Then, we utilize biologically meaningful brain hierarchical relationships among the regions in multiscale atlases to perform nodal pooling across multiple spatial scales, namely \"Atlas-guided Pooling\". Accordingly, we propose a Multiscale-Atlases-based Hierarchical Graph Convolutional Network (MAHGCN), built on the stacked layers of graph convolution and the atlas-guided pooling, for a comprehensive extraction of diagnostic information from multiscale FCNs. Experiments on neuroimaging data from 1792 subjects demonstrate the effectiveness of our proposed method in the diagnoses of Alzheimer's disease (AD), the prodromal stage of AD (i.e., mild cognitive impairment [MCI]), as well as autism spectrum disorder (ASD), with accuracy of 88.9%, 78.6%, and 72.7% respectively. All results show significant advantages of our proposed method over other competing methods. This study not only demonstrates the feasibility of brain disorder diagnosis using resting-state fMRI empowered by deep learning, but also highlights that the functional interactions in the multiscale brain hierarchy are worth being explored and integrated into deep learning network architectures for better understanding the neuropathology of brain disorders.",
                    "To characterize atypical brain dynamics under diseases, prevalent studies investigate functional magnetic resonance imaging (fMRI). However, most of the existing analyses compress rich spatial-temporal information as the brain functional networks (BFNs) and directly investigate the whole-brain network without neurological priors about functional subnetworks. We thus propose a novel graph learning framework to mine fMRI signals with topological priors from brain parcellation for disease diagnosis. Specifically, we 1) detect diagnosis-related temporal features using a \"Transformer\" for a higher-level BFN construction, and process it with a following graph convolutional network, and 2) apply an attention-based multiple instance learning strategy to emphasize the disease-affected subnetworks to further enhance the diagnosis performance and interpretability. Experiments demonstrate higher effectiveness of our method than compared methods in the diagnosis of early mild cognitive impairment. More importantly, our method is capable of localizing crucial brain subnetworks during the diagnosis, providing insights into the pathogenic source of mild cognitive impairment.",
                    "Pathology images are crucial for diagnosing and managing various diseases by visualizing cellular and tissue-level abnormalities. Recent advancements in artificial intelligence (AI), particularly multimodal models like ChatGPT, have shown promise in transforming medical image analysis through capabilities such as medical vision-language question answering. However, there remains a significant gap in integrating pathology image data with these AI models for clinical applications. This study benchmarks the performance of GPT on pathology images, assessing their diagnostic accuracy and efficiency in real-word clinical records. We observe significant deficits of GPT in bone diseases and a fair-level performance in diseases from other three systems. Despite offering satisfactory abnormality annotations, GPT exhibits consistent disadvantage in terminology accuracy and multimodal integration. Specifically, we demonstrate GPT's failures in interpreting immunohistochemistry results and diagnosing metastatic cancers. This study highlight the weakness of current generalist GPT model and contribute to the integration of pathology and advanced AI.",
                    "Diverse cognitive processes set different demands on locally segregated and globally integrated brain activity. However, it remains unclear how resting brains configure their functional organization to balance the demands on network segregation and integration to best serve cognition. Here, we use an eigenmode-based approach to identify hierarchical modules in functional brain networks, and quantify the functional balance between network segregation and integration. In a large sample of healthy young adults (n=991), we combine the whole-brain resting state functional magnetic resonance imaging (fMRI) data with a mean-filed model on the structural network derived from diffusion tensor imaging and demonstrate that resting brain networks are on average close to a balanced state. This state allows for a balanced time dwelling at segregated and integrated configurations, and highly flexible switching between them. Furthermore, we employ structural equation modelling to estimate general and domain-specific cognitive phenotypes from nine tasks, and demonstrate that network segregation, integration and their balance in resting brains predict individual differences in diverse cognitive phenotypes. More specifically, stronger integration is associated with better general cognitive ability, stronger segregation fosters crystallized intelligence and processing speed, and individual's tendency towards balance supports better memory. Our findings provide a comprehensive and deep understanding of the brain's functioning principles in supporting diverse functional demands and cognitive abilities, and advance modern network neuroscience theories of human cognition.",
                    "The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly correlates with higher patient mortality rates. Early detection of IPF progression is critical for initiating timely treatment, which can effectively slow down the advancement of the disease. However, the current clinical criteria define disease progression requiring two CT scans with a one-year interval, presenting a dilemma: a disease progression is identified only after the disease has already progressed. To this end, in this paper, we develop a novel diffusion model to accurately predict the progression of IPF by generating patient's follow-up CT scan from the initial CT scan. Specifically, from the clinical prior knowledge, we tailor improvements to the traditional diffusion model and propose a Clinically-Informed Residual Diffusion model, called CIResDiff. The key innovations of CIResDiff include 1) performing the target region pre-registration to align the lung regions of two CT scans at different time points for reducing the generation difficulty, 2) adopting the residual diffusion instead of traditional diffusion to enable the model focus more on differences (i.e., lesions) between the two CT scans rather than the largely identical anatomical content, and 3) designing the clinically-informed process based on CLIP technology to integrate lung function information which is highly relevant to diagnosis into the reverse process for assisting generation. Extensive experiments on clinical data demonstrate that our approach can outperform state-of-the-art methods and effectively predict the progression of IPF.",
                    "Accurate diagnosis of brain abnormalities is greatly enhanced by the inclusion of complementary multi-parametric MRI imaging data. There is significant potential to develop a universal pre-training model that can be quickly adapted for image modalities and various clinical scenarios. However, current models often rely on uni-modal image data, neglecting the cross-modal correlations among different image modalities or struggling to scale up pre-training in the presence of missing modality data. In this paper, we propose BrainMVP, a multi-modal vision pre-training framework for brain image analysis using multi-parametric MRI scans. First, we collect 16,022 brain MRI scans (over 2.4 million images), encompassing eight MRI modalities sourced from a diverse range of centers and devices. Then, a novel pre-training paradigm is proposed for the multi-modal MRI data, addressing the issue of missing modalities and achieving multi-modal information fusion. Cross-modal reconstruction is explored to learn distinctive brain image embeddings and efficient modality fusion capabilities. A modality-wise data distillation module is proposed to extract the essence representation of each MR image modality for both the pre-training and downstream application purposes. Furthermore, we introduce a modality-aware contrastive learning module to enhance the cross-modality association within a study. Extensive experiments on downstream tasks demonstrate superior performance compared to state-of-the-art pre-training methods in the medical domain, with Dice Score improvement of 0.28%-14.47% across six segmentation benchmarks and a consistent accuracy improvement of 0.65%-18.07% in four individual classification tasks.",
                    "Ischemic stroke is a severe condition caused by the blockage of brain blood vessels, and can lead to the death of brain tissue due to oxygen deprivation. Thrombectomy has become a common treatment choice for ischemic stroke due to its immediate effectiveness. But, it carries the risk of postoperative cerebral hemorrhage. Clinically, multiple CT scans within 0-72 hours post-surgery are used to monitor for hemorrhage. However, this approach exposes radiation dose to patients, and may delay the detection of cerebral hemorrhage. To address this dilemma, we propose a novel prediction framework for measuring postoperative cerebral hemorrhage using only the patient's initial CT scan. Specifically, we introduce a dual-task mutual learning framework to takes the initial CT scan as input and simultaneously estimates both the follow-up CT scan and prognostic label to predict the occurrence of postoperative cerebral hemorrhage. Our proposed framework incorporates two attention mechanisms, i.e., self-attention and interactive attention. Specifically, the self-attention mechanism allows the model to focus more on high-density areas in the image, which are critical for diagnosis (i.e., potential hemorrhage areas). The interactive attention mechanism further models the dependencies between the interrelated generation and classification tasks, enabling both tasks to perform better than the case when conducted individually. Validated on clinical data, our method can generate follow-up CT scans better than state-of-the-art methods, and achieves an accuracy of 86.37% in predicting follow-up prognostic labels. Thus, our work thus contributes to the timely screening of post-thrombectomy cerebral hemorrhage, and could significantly reform the clinical process of thrombectomy and other similar operations related to stroke.",
                    "Brain disorders in the early and late life of humans potentially share pathological alterations in brain functions. However, the key evidence from neuroimaging data for pathological commonness remains unrevealed. To explore this hypothesis, we build a deep learning model, using multi-site functional magnetic resonance imaging data (N=4,410, 6 sites), for classifying 5 different brain disorders from healthy controls, with a set of common features. Our model achieves 62.6(1.9)% overall classification accuracy on data from the 6 investigated sites and detects a set of commonly affected functional subnetworks at different spatial scales, including default mode, executive control, visual, and limbic networks. In the deep-layer feature representation for individual data, we observe young and aging patients with disorders are continuously distributed, which is in line with the clinical concept of the \"spectrum of disorders\". The revealed spectrum underlying early- and late-life brain disorders promotes the understanding of disorder comorbidities in the lifespan.",
                    "In clinical practice, a segmentation network is often required to continually learn on a sequential data stream from multiple sites rather than a consolidated set, due to the storage cost and privacy restriction. However, during the continual learning process, existing methods are usually restricted in either network memorizability on previous sites or generalizability on unseen sites. This paper aims to tackle the challenging problem of Synchronous Memorizability and Generalizability (SMG) and to simultaneously improve performance on both previous and unseen sites, with a novel proposed SMG-learning framework. First, we propose a Synchronous Gradient Alignment (SGA) objective, which not only promotes the network memorizability by enforcing coordinated optimization for a small exemplar set from previous sites (called replay buffer), but also enhances the generalizability by facilitating site-invariance under simulated domain shift. Second, to simplify the optimization of SGA objective, we design a Dual-Meta algorithm that approximates the SGA objective as dual meta-objectives for optimization without expensive computation overhead. Third, for efficient rehearsal, we configure the replay buffer comprehensively considering additional inter-site diversity to reduce redundancy. Experiments on prostate MRI data sequentially acquired from six institutes demonstrate that our method can simultaneously achieve higher memorizability and generalizability over state-of-the-art methods. Code is available at https://github.com/jingyzhang/SMG-Learning.",
                    "Image registration of liver dynamic contrast-enhanced computed tomography (DCE-CT) is crucial for diagnosis and image-guided surgical planning of liver cancer. However, intensity variations due to the flow of contrast agents combined with complex spatial motion induced by respiration brings great challenge to existing intensity-based registration methods. To address these problems, we propose a novel structure-aware registration method by incorporating structural information of related organs with segmentation-guided deep registration network. Existing segmentation-guided registration methods only focus on volumetric registration inside the paired organ segmentations, ignoring the inherent attributes of their anatomical structures. In addition, such paired organ segmentations are not always available in DCE-CT images due to the flow of contrast agents. Different from existing segmentation-guided registration methods, our proposed method extracts structural information in hierarchical geometric perspectives of line and surface. Then, according to the extracted structural information, structure-aware constraints are constructed and imposed on the forward and backward deformation field simultaneously. In this way, all available organ segmentations, including unpaired ones, can be fully utilized to avoid the side effect of contrast agent and preserve the topology of organs during registration. Extensive experiments on an in-house liver DCE-CT dataset and a public LiTS dataset show that our proposed method can achieve higher registration accuracy and preserve anatomical structure more effectively than state-of-the-art methods.",
                    "The advent of vision-language models fosters the interactive conversations between AI-enabled models and humans. Yet applying these models into clinics must deal with daunting challenges around large-scale training data, financial, and computational resources. Here we propose a cost-effective instruction learning framework for conversational pathology named as CLOVER. CLOVER only trains a lightweight module and uses instruction tuning while freezing the parameters of the large language model. Instead of using costly GPT-4, we propose well-designed prompts on GPT-3.5 for building generation-based instructions, emphasizing the utility of pathological knowledge derived from the Internet source. To augment the use of instructions, we construct a high-quality set of template-based instructions in the context of digital pathology. From two benchmark datasets, our findings reveal the strength of hybrid-form instructions in the visual question-answer in pathology. Extensive results show the cost-effectiveness of CLOVER in answering both open-ended and closed-ended questions, where CLOVER outperforms strong baselines that possess 37 times more training parameters and use instruction data generated from GPT-4. Through the instruction tuning, CLOVER exhibits robustness of few-shot learning in the external clinical dataset. These findings demonstrate that cost-effective modeling of CLOVER could accelerate the adoption of rapid conversational applications in the landscape of digital pathology."
                ],
                "domain": [
                    "Medical Imaging",
                    "Deep Learning",
                    "Alzheimer's Disease",
                    "Multi-modal Analysis"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b0111362-c83b-4f75-a36e-c401d5ae4545": {
                "pk": "b0111362-c83b-4f75-a36e-c401d5ae4545",
                "project_name": null,
                "name": "Jinru Ding",
                "bio": "I am a researcher dedicated to the intersection of heterogeneous networks, knowledge graphs, and the application of large language models (LLMs) in medical diagnostics. My work primarily focuses on representation learning in heterogeneous networks, where I have developed Heterogeneous Graph Convolutional Networks (HCNs) to overcome the limitations of traditional meta-path-based methods. This innovative approach allows for the effective capture of both structural and semantic relationships within diverse networks, significantly enhancing performance across various analytic tasks.\n\nIn addition to my work on heterogeneous networks, I have explored the profiling of entities within knowledge graphs, introducing a scalable representation learning model that identifies distinctive features of entities. This research aims to facilitate human understanding and improve the usability of knowledge graphs.\n\nRecently, I have shifted my focus to the evaluation of LLMs in medical contexts. I recognized the need for a unified evaluation criterion for assessing the diagnostic capabilities of these models. To address this, I established the LLM-specific Mini-CEX evaluation framework and developed a patient simulator to automate interactions with LLMs. My findings demonstrate that this approach not only streamlines the evaluation process but also enhances the reliability of diagnostic dialogues generated by LLMs.\n\nThrough my research, I strive to bridge the gap between complex network structures and practical applications, ultimately contributing to advancements in both artificial intelligence and healthcare.",
                "collaborators": [
                    "Jie Xu",
                    "Lu Lu",
                    "Jiali Pang",
                    "Xiaoming Shi",
                    "Shaoting Zhang",
                    "Jie Zhang",
                    "Suyuan Liu",
                    "Hongyan Wu",
                    "Xiang Zhang",
                    "Qingqing Yang"
                ],
                "pub_titles": [
                    "Meta-Path-Free Representation Learning on Heterogeneous Networks",
                    "Entity Profiling in Knowledge Graphs",
                    "MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine",
                    "LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation"
                ],
                "pub_abstracts": [
                    "Real-world networks and knowledge graphs are usually heterogeneous networks. Representation learning on heterogeneous networks is not only a popular but a pragmatic research field. The main challenge comes from the heterogeneity -- the diverse types of nodes and edges. Besides, for a given node in a HIN, the significance of a neighborhood node depends not only on the structural distance but semantics. How to effectively capture both structural and semantic relations is another challenge. The current state-of-the-art methods are based on the algorithm of meta-path and therefore have a serious disadvantage -- the performance depends on the arbitrary choosing of meta-path(s). However, the selection of meta-path(s) is experience-based and time-consuming. In this work, we propose a novel meta-path-free representation learning on heterogeneous networks, namely Heterogeneous graph Convolutional Networks (HCN). The proposed method fuses the heterogeneity and develops a $k$-strata algorithm ($k$ is an integer) to capture the $k$-hop structural and semantic information in heterogeneous networks. To the best of our knowledge, this is the first attempt to break out of the confinement of meta-paths for representation learning on heterogeneous networks. We carry out extensive experiments on three real-world heterogeneous networks. The experimental results demonstrate that the proposed method significantly outperforms the current state-of-the-art methods in a variety of analytic tasks.",
                    "Knowledge Graphs (KGs) are graph-structured knowledge bases storing factual information about real-world entities. Understanding the uniqueness of each entity is crucial to the analyzing, sharing, and reusing of KGs. Traditional profiling technologies encompass a vast array of methods to find distinctive features in various applications, which can help to differentiate entities in the process of human understanding of KGs. In this work, we present a novel profiling approach to identify distinctive entity features. The distinctiveness of features is carefully measured by a HAS model, which is a scalable representation learning model to produce a multi-pattern entity embedding. We fully evaluate the quality of entity profiles generated from real KGs. The results show that our approach facilitates human understanding of entities in KGs.",
                    "METHODS: First, a set of evaluation criteria is designed based on a comprehensive literature review. Second, existing candidate criteria are optimized for using a Delphi method by five experts in medicine and engineering. Third, three clinical experts design a set of medical datasets to interact with LLMs. Finally, benchmarking experiments are conducted on the datasets. The responses generated by chatbots based on LLMs are recorded for blind evaluations by five licensed medical experts. RESULTS: The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with sixteen detailed indicators. The medical datasets include twenty-seven medical dialogues and seven case reports in Chinese. Three chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both multiple-turn medical dialogue and case report scenarios.",
                    "There is an increasing interest in developing LLMs for medical diagnosis to improve diagnosis efficiency. Despite their alluring technological potential, there is no unified and comprehensive evaluation criterion, leading to the inability to evaluate the quality and potential risks of medical LLMs, further hindering the application of LLMs in medical treatment scenarios. Besides, current evaluations heavily rely on labor-intensive interactions with LLMs to obtain diagnostic dialogues and human evaluation on the quality of diagnosis dialogue. To tackle the lack of unified and comprehensive evaluation criterion, we first initially establish an evaluation criterion, termed LLM-specific Mini-CEX to assess the diagnostic capabilities of LLMs effectively, based on original Mini-CEX. To address the labor-intensive interaction problem, we develop a patient simulator to engage in automatic conversations with LLMs, and utilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental results show that the LLM-specific Mini-CEX is adequate and necessary to evaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual evaluation on the metrics of humanistic qualities and provides reproducible and automated comparisons between different LLMs."
                ],
                "domain": [
                    "Heterogeneous Networks",
                    "Knowledge Graphs",
                    "Large Language Models",
                    "Representation Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "8f0af34d-5659-410c-97e3-1edd7fb78bf1": {
                "pk": "8f0af34d-5659-410c-97e3-1edd7fb78bf1",
                "project_name": null,
                "name": "Jie Xu",
                "bio": "I am a researcher with a strong focus on differential geometry and the analysis of partial differential equations (PDEs), particularly in the context of the Yamabe problem and its applications to Riemannian geometry. My recent work has involved developing iterative methods to solve the Yamabe problem on both open domains and closed manifolds, extending traditional approaches by providing new proofs that unify the analysis for dimensions \\( n \\geq 3 \\).\n\nI have also explored the existence and uniqueness of complex-valued nonlinear elliptic PDEs, employing innovative techniques such as parametrix methods and perturbation approaches. My research extends to the study of scalar curvature problems on compact manifolds, where I have established conditions for prescribing scalar and mean curvature functions, contributing to a deeper understanding of the geometric properties of manifolds.\n\nIn addition to my theoretical work, I have ventured into network theory, proposing algorithms for real-time detection of super points and super hosts in high-speed networks. My algorithms leverage advanced data structures and parallel computing techniques to efficiently estimate cardinalities and detect anomalies in network traffic.\n\nOverall, my research bridges the gap between pure mathematics and practical applications, aiming to provide both theoretical insights and computational tools that address complex problems in geometry and network analysis.",
                "collaborators": [],
                "pub_titles": [
                    "Solving Yamabe Problem by An Iterative Method",
                    "A Local Method for Compact and Non-compact Yamabe Problems",
                    "Distributed super point cardinality estimation under sliding time window for high speed network",
                    "GPU based Real-time Super Hosts Detection at Distributed Edge Routers",
                    "Iterative Methods for Globally Lipschitz Nonlinear Laplace Equations",
                    "Wave Approximation of Backward Heat Equation with Ricci Flow",
                    "Classifying local anisotropy formed by rigid molecules: symmetries and tensors",
                    "Symmetry-consistent expansion of interaction kernels between rigid molecules",
                    "Quasi-entropy by log-determinant covariance matrix and application to liquid crystals",
                    "Prescribed Scalar Curvature Problem under Conformal Deformation of A Riemannian Metric with Dirichlet Boundary Condition",
                    "Trichotomy Theorem for Prescribed Scalar and Mean Curvatures on Compact Manifolds with Boundaries",
                    "Kazdan-Warner Problem on Compact Riemann Surfaces with Smooth Boundary",
                    "Stochastic Epidemic Networks with Strategic Link Formation",
                    "Cardinalities estimation under sliding time window by sharing HyperLogLog Counter",
                    "Regain Sliding super point from distributed edge routers by GPU",
                    "Economical and efficient network super points detection based on GPU",
                    "Memory efficient distributed sliding super point cardinality estimation by GPU",
                    "READ: a three-communicating-stage distributed super points detections algorithm",
                    "The Boundary Yamabe Problem, II: General Constant Mean Curvature Case",
                    "Prescribed Scalar Curvature on Compact Manifolds Under Conformal Deformation"
                ],
                "pub_abstracts": [
                    "We introduce an iterative scheme to prove the Yamabe problem $ - a\\Delta_{g} u + S u = \\lambda u^{p-1} $, firstly on open domain $ (\\Omega, g) $ with Dirichlet boundary conditions, and then on closed manifolds $ (M, g) $ by local argument. It is a new proof, which solves the Yamabe problem for $ n \\geqslant 3 $ in a uniform argument, beyonds the traditional analysis with respect to the minimization of functionals.",
                    "Let $ (M, g) $ be a compact manifold or a complete non-compact manifold without boundary, $ \\dim M \\geqslant 4 $, and not locally conformally flat. In this article, we introduce a new local method to resolve the Yamabe problem on compact manifold for dimensions at least $ 4 $, and the Yamabe problem on non-compact complete manifolds without boundary, which are pointwise conformal to subsets of some compact manifolds. In particular, the new local method applies to the hard cases--the Yamabe constants are positive. Our local method also generalizes Brezis and Nirenberg's nonlinear eigenvalue problem to subsets of manifolds.",
                    "Super point is a special kind of host whose cardinality, the number of contacting hosts in a certain period, is bigger than a threshold. Super point cardinality estimation plays important roles in network field. This paper proposes a super point cardinality estimation algorithm under sliding time window. To maintain the state of previous hosts with few updating operations, a novel counter, asynchronous time stamp (AT), is proposed. For a sliding time window containing k time slices, AT only needs to be updated every k time slices at the cost of 1 more bit than a previous state-of-art counter which requires $log_2(k+1)$ bits but updates every time slice. Fewer updating operations mean that more AT could be contained to acquire higher accuracy in real-time. This paper also devises a novel reversible hash function scheme to restore super point from a pool of AT. Experiments on several real-world network traffic illustrate that the algorithm proposed in this paper could detect super points and estimate their cardinalities under sliding time window in real time.",
                    "The super host is a special host on the network which contacts with many other hosts during a certain time window. They play important roles in network researches such as scanners detection, resource allocation, spam filtering and so on. How to find super hosts in real time is the foundation of these applications. In this paper, a novel algorithm, denoted as CBAA, is proposed to solve this problem at edge routers. CBAA divides network traffic into different parts. A cube of bits array is devised to store hosts' linking information of different traffic parts when scanning packets. At the end of each time window, CBAA restores super hosts very fast because there are only a fraction of super hosts in each traffic part. CBAA is also a parallel algorithm. It's easy to deploy CBAA in GPU to deal with high-speed network traffic in real time. Experiments on a real-world core network prove the advantage of our algorithm.",
                    "We introduce an iterative method to prove the existence and uniqueness of the complex-valued nonlinear elliptic PDE of the form $ -\\Delta u + F(u) = f $ with Dirichlet or Neumann boundary conditions on a precompact domain $ \\Omega \\subset \\mathbb{R}^{n}$, where $ F : \\mathbb{C} \\rightarrow \\mathbb{C} $ is Lipschitz. The same method gives a solution to $ - \\Delta_{g} u + F(u) = f $ for these boundary conditions on a smooth, compact Riemannian manifold $ (M, g) $ with $ \\mathcal{C}^{1} $ boundary, where $ - \\Delta_{g} $ is the Laplace-Beltrami operator. We also apply parametrix methods to discuss an integral version of these PDEs.",
                    "In this paper, we consider solutions of the backward heat equation with Ricci flow on manifolds as a type of infinite dimensional limit of solutions of a wave equation on a larger manifold with an analysis of wavefront set. Specifically, the projection of the solution of the wave equation $ \\left(\\frac{2t}{N} \\cdot \\frac{\\partial^{2}}{\\partial t^{2}} + \\frac{tR(t, x)}{N} \\frac{\\partial}{\\partial t} - \\Delta_{\\tilde{g}^{(N)}(t)} \\right) u = R(t, x) $ outside its wavefront set onto $ \\mathbb{R}_{t} \\times M_{x, g(t)} $ solves the backward heat equation $ \\partial_{t} u + \\Delta_{x, g(t)} u = -R(t, x) $ within some appropriate time interval. We discuss this approximation starting from Euclidean case, and then extend to the open Riemannian manifold situation. This idea partially comes from Perelman's original papers in proving Poincar\\'e conjecture as well as Terence Tao's Notes in UCLA.",
                    "We consider an infinitesimal volume where there are many rigid molecules of the same kind, and discuss the description and classification of the local anisotropy in this volume by tensors. First, we examine the symmetry of a rigid molecule, which is described by a point group in $SO(3)$. For each point group in $SO(3)$, we find the tensors invariant under the rotations in the group. These tensors shall be symmetric and traceless. We write down the explicit expressions. The order parameters to describe the local anisotropy are then chosen as some of the invariant tensors averaged about the density function. Next, we discuss the classification of local anisotropy by the symmetry of the whole infinitesimal volume. This mesoscopic symmetry can be recognized by the value of the order parameter tensors in the sense of maximum entropy state. For some sets of order parameter tensors involving different molecular symmetries, we give the classification of mesoscopic symmetries, in which the three-fold, four-fold and polyhedral symmetries are examined.",
                    "We discuss the expansion of interaction kernels between anisotropic rigid molecules. The expansion decouples the correlated orientational variables so that it can be utilized to derive macroscopic models. Symmetries of two types are considered. First, we examine the symmetry of the interacting cluster, including the translation and rotation of the whole cluster, and label permutation within the cluster. The expansion is expressed by symmetric traceless tensors, and the linearly independent terms are identified. Then, we study the molecular symmetry characterized by a point group in $O(3)$. The proper rotations determine what symmetric traceless tensors can appear. The improper rotations decompose these tensors into two subspaces and determine how the tensors in the two subspaces are coupled. For each point group, we identify the two subspaces, so that the expansion consistent with the point group is established.",
                    "A quasi-entropy is constructed for tensors averaged by a density function on $SO(3)$ using the log-determinant of a covariance matrix. It serves as a substitution of the entropy for tensors derived from a constrained minimization that involves integrals. The quasi-entropy is an elementary function that possesses the essential properties of the original entropy. It constrains the covariance matrix to be positive definite, is strictly convex, and is invariant under rotations. Moreover, when reduced by symmetries, it keeps the vanishing tensors of the symmetry zero. Explicit expressions are provided for axial symmetries up to four-fold, as well as tetrahedral and octahedral symmetries. The quasi-entropy is utilized to discuss phase transitions in several systems. The results are consistent with using the original entropy. Besides, some novel results are presented.",
                    "In this article, we first show that for all compact Riemannian manifolds with non-empty smooth boundary and dimension at least 3, there exists a metric, pointwise conformal to the original metric, with constant scalar curvature in the interior, and constant scalar curvature on the boundary by considering the boundary as a manifold of its own with dimension at least 2. We then show a series of prescribed scalar curvature results in the interior and on the boundary, with pointwise conformal deformation. These type of results is both an analogy and an extension of Kazdan and Warner's \"Trichotomy Theorem\" on a different type of manifolds. The key step of these problems is to obtain a positive, smooth solution of a Yamabe equation with Dirichlet boundary conditions.",
                    "In this article, we give results of prescribing scalar and mean curvature functions for metrics either pointwise conformal or conformally equivalent to a Riemannian metric that is equipped on a compact manifold with boundary, with dimensions at least $ 3 $. The results are classified by the sign of the first eigenvalue of the conformal Laplacian. This leads to a \"Trichotomy Theorem\" in terms of both scalar and mean curvature functions, which is a full extension of the \"Trichotomy Theorem\" given by Kazdan and Warner. We also discuss prescribing Gauss and geodesic curvature problems on compact Riemann surfaces with boundary for metrics either pointwise conformal or conformally equivalent to the original metric, provided that the Euler characteristic is negative. The key step is a general version of monotone iteration scheme which handle the zeroth order nonlinear term on the boundary conditions.",
                    "In this article, we show that (i) any smooth function on compact Riemann surface with non-empty smooth boundary $ (M, \\partial M, g) $ can be realized as a Gaussian curvature function; (ii) any smooth function on $ \\partial M $ can be realized as a geodesic curvature function for some metric $ \\tilde{g} \\in [g] $. The essential steps are the existence results of Brezis-Merle type equations $ -\\Delta_{g} u + Au = K e^{2u} \\; {\\rm in} \\; M $ and $ \\frac{\\partial u}{\\partial \\nu} + \\kappa u = \\sigma e^{u} \\; {\\rm on} \\; \\partial M $ with given functions $ K, \\sigma $ and some constants $ A, \\kappa $. In addition, we rely on the extension of the uniformization theorem given by Osgood, Phillips and Sarnak.",
                    "Understanding cascading failures or epidemics in networks is crucial for developing effective defensive mechanisms for many critical systems and infrastructures (e.g. biological, social and cyber networks). Most of the existing works treat the network topology as being exogenously given and study under what conditions an epidemic breaks out and/or extinguishes. However, if agents are able to strategically decide their connections according to their own self-interest, the network will instead be endogenously formed and evolving. In such systems, the epidemic, agents' strategic decisions and the network structure become complexly coupled and co-evolve. As a result, existing knowledge may no longer be applicable. Built on a continuous time Susceptible-Infected-Susceptible epidemic model with strong mixing, this paper studies stochastic epidemic networks consisting of strategic agents, who decide the number of links to form based on a careful evaluation of its current obtainable benefit and the potential future cost due to infection by forming links. A game theoretical framework is developed to analyze such networks and a number of important insights are obtained. One key result is that whereas an epidemic eventually dies out if the effective spreading rate is sufficiently low in exogenously given networks, it never dies out when agents are strategic regardless of the effective spreading rate. This property leads to reduced achievable system efficiency and considerably different optimal protection mechanisms. Without understanding the strategic behavior of agents, significant security cost may incur.",
                    "Cardinalities estimation is an important research topic in network management and security. How to solve this problem under sliding time window is a hot topic. HyperLogLog is a memory efficient algorithm work under a fixed time window. A sliding version of HyperLogLog can work under sliding time window by replacing every counter of HyperLogLog with a list of feature possible maxim (LFPM). But LFPM is a dynamic structure whose size is variable at running time. This paper proposes a novel counter for HyperLogLog which consumes smaller size of memory than that of LFPM. Our counter is called bit distance recorder BDR, because it maintains the distance of every left most \"1\" bit position. The size of BDR is fixed. Based on BDR, we design a multi hosts' cardinalities estimation algorithm under sliding time window, virtual bit distance recorder VBDR. VBDR allocate a virtual vector of BDR for every host and every physical BDR is shared by several hosts to improve the memory usage. After a small modifcation, we propose another two parallel versions of VBDR which can run on GPU to handle high speed traffic. One of these parallel VBDR is fast in IP pair scanning and the other one is memory efficient. BDR is also suitable for other cardinality estimation algorithms such as PCSA, LogLog.",
                    "Sliding super point is a special host defined under sliding time window with which there are huge other hosts contact. It plays important roles in network security and management. But how to detect them in real time from nowadays high-speed network which contains several distributed routers is a hard task. Distributed sliding super point detection requires an algorithm that can estimate the number of contacting hosts incrementally, scan packets faster than their flowing speed and reconstruct sliding super point at the end of a time period. But no existing algorithm satisfies these three requirements simultaneously. To solve this problem, this paper firstly proposed a distributed sliding super point detection algorithm running on GPU. The advantage of this algorithm comes from a novel sliding estimator, which can estimate contacting host number incrementally under a sliding window, and a set of reversible hash functions, by which sliding super points could be regained without storing additional data such as IP list. There are two main procedures in this algorithm: packets scanning and sliding super points reconstruction. Both could run parallel without any data reading conflict. When deployed on a low cost GPU, this algorithm could deal with traffic with bandwidth as high as 680 Gb/s. A real world core network traffic is used to evaluate the performance of this sliding super point detection algorithm on a cheap GPU, Nvidia GTX950 with 4 GB graphic memory. Experiments comparing with other algorithms under discrete time window show that this algorithm has the highest accuracy. Under sliding time widow, this algorithm has the same performance as in discrete time window, where no other algorithms can work.",
                    "Network super point is a kind of special host which plays an important role in network management and security. For a core network, detecting super points in real time is a burden task because it requires plenty computing resources to keep up with the high speed of packets. Previous works try to solve this problem by using expensive memory, such as static random access memory, and multi cores of CPU. But the number of cores in CPU is small and each core of CPU has a high price. In this work, we use a popular parallel computing platform, graphic processing unit GPU, to mining core network's super point. We propose a double direction hash functions group which can map hosts randomly and restore them from a dense structure. Because the high randomness and simple process of the double direction hash functions, our algorithm reduce the memory to smaller than one-fourth of other algorithms. Because the small memory requirement of our algorithm, a low cost GPU, only worth 200 dollars, is fast enough to deal with a high speed network such as 750 Gb/s. No other algorithm can cope with such a high bandwidth traffic as accuracy as our algorithm on such a cheap platform. Experiments on the traffic collecting from a core network demonstrate the advantage of our efficient algorithm.",
                    "Super point is a kind of special host in the network which contacts with huge of other hosts. Estimating its cardinality, the number of other hosts contacting with it, plays important roles in network management. But all of existing works focus on discrete time window super point cardinality estimation which has great latency and ignores many measuring periods. Sliding time window measures super point cardinality in a finer granularity than that of discrete time window but also more complex. This paper firstly introduces an algorithm to estimate super point cardinality under sliding time window from distributed edge routers. This algorithm's ability of sliding super point cardinality estimating comes from a novel method proposed in this paper which can record the time that a host appears. Based on this method, two sliding cardinality estimators, sliding rough estimator and sliding linear estimator, are devised for super points detection and their cardinalities estimation separately. When using these two estimators together, the algorithm consumes the smallest memory with the highest accuracy. This sliding super point cardinality algorithm can be deployed in distributed environment and acquire the global super points' cardinality by merging estimators of distributed nodes. Both of these estimators could process packets parallel which makes it becom possible to deal with high speed network in real time by GPU. Experiments on a real world traffic show that this algorithm have the highest accuracy and the smallest memory comparing with others when running under discrete time window. Under sliding time window, this algorithm also has the same performance as under discrete time window.",
                    "A super point is a host that interacts with a far larger number of counterparts in the network over a period of time. Super point detection plays an important role in network research and application. With the increase of network scale, distributed super point detection has become a hot research topic. Compared with single-node super point detection algorithm, the difficulty of super point detection in multi-node distributed environment is how to reduce communication overhead. Therefore, this paper proposes a three-stage communication distributed super point detection algorithm: Rough Estimator based Asynchronous Distributed super point detection algorithm (READ). READ uses a lightweight estimator, the Rough Estimator (RE), which is fast in computation and takes less memory to generate candidate super point. At the same time, the Linear Estimator (LE) is used to accurately estimate the cardinality of each candidate super point, so as to detect the super point correctly. In READ, each node scans IP address pairs asynchronously. When reaching the time window boundary, READ starts three-stage communication to detect the super point. In this paper, we proof that the accuracy of READ in distributed environment is no less than that in the single node environment. Four groups of 10 Gb/s and 40 Gb/s real-world high-speed network traffic are used to test READ. The experimental results show that READ not only has higher accuracy in distributed environment, but also has less than 5% of communication burden compared with existing algorithms.",
                    "This article uses the iterative schemes and perturbation methods to completely solve the Han-Li conjecture, i.e. the general boundary Yamabe problem with prescribed constant scalar curvature and constant mean curvature on compact manifolds $ (M, \\partial M, g) $ with non-empty smooth boundary, $ \\dim M \\geqslant 3 $. It is equivalent to show the existence of a real, positive, smooth solution of $ -\\frac{4(n -1)}{n - 2} \\Delta_{g} u + R_{g} u = \\lambda u^{\\frac{n+2}{n - 2}} $ in $ M $, $ \\frac{\\partial u}{\\partial \\nu} + \\frac{n-2}{2} h_{g} u = \\frac{n-2}{2} \\zeta u^{\\frac{n}{n - 2}} $ on $ \\partial M $ with some constants $ \\lambda, \\zeta \\in \\mathbb{R} $. This boundary Yamabe problem is solved in cases according to the sign of the first eigenvalue $ \\eta_{1} $ of the conformal Laplacian with homogeneous Robin condition. The signs of scalar curvature $ R_{g} $ and mean curvature $ h_{g} $ play an important role in this existence result. In contrast to the classical method, the Weyl tensor and classification of boundary points play no role in the proof. In addition, the method is not dimensional specific. The key steps include a new version of monotone iteration scheme for nonlinear elliptic PDEs and nonlinear boundary conditions, an introduction of the perturbed conformal Laplacian, a delicate gluing skill to construct a smooth super-solution and existence of solution of local Yamabe-type equations.",
                    "We give sufficient and \"almost\" necessary conditions for the prescribed scalar curvature problems within the conformal class of a Riemannian metric $ g $ for both closed manifolds and compact manifolds with boundary, including the interesting cases $ \\mathbb{S}^{n} $ or some quotient of $ \\mathbb{S}^{n} $, in dimensions $ n \\geqslant 3 $, provided that the first eigenvalues of conformal Laplacian (with appropriate boundary conditions if necessary) are positive. When the manifold is not some quotient of $ \\mathbb{S}^{n} $, we show that, on one hand, any smooth function that is a positive constant within some open subset of the manifold with arbitrary positive measure, and has no restriction on the rest of the manifold, is a prescribed scalar curvature function of some metric under conformal change; on the other hand, any smooth function $ S $ is almost a prescribed scalar curvature function of Yamabe metric within the conformal class $ [g] $ in the sense that an appropriate perturbation of $ S $ that defers with $ S $ within an arbitrarily small open subset is a prescribed scalar curvature function of Yamabe metric. When the manifold is either $ \\mathbb{S}^{n} $ or $ \\mathbb{S}^n / \\Gamma $ with Kleinian group $ \\Gamma $ we show that any positive function that satisfies a technical analytical condition, called CONDITION B, can be realized as a prescribed scalar curvature functions on these manifolds."
                ],
                "domain": [
                    "Geometric Analysis",
                    "Nonlinear PDE",
                    "Network Theory",
                    "Computational Mathematics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "7628668a-eae2-4362-8b39-7ee07c8f3daf": {
                "pk": "7628668a-eae2-4362-8b39-7ee07c8f3daf",
                "project_name": null,
                "name": "Weiguo Hu",
                "bio": "I am a researcher dedicated to enhancing medical image segmentation, particularly in the context of colorectal cancer (CRC). My recent work focuses on developing an Anatomy-Guided segmentation framework (AG-CRC) that leverages automatically generated organ masks to improve the accuracy of CRC segmentation from CT scans. Recognizing the importance of anatomical context in lesion delineation, I designed a multi-faceted approach that includes obtaining robust organ of interest masks, implementing an innovative training patch sampling strategy, and introducing a self-supervised learning scheme inspired by the unique topology of tubular organs like the colon.\n\nThrough extensive evaluations on CRC segmentation datasets, my framework has demonstrated significant performance improvements over existing state-of-the-art models, achieving up to a 9% increase in Dice scores. I am passionate about bridging the gap between advanced deep learning techniques and practical applications in medical imaging, striving to contribute to more effective diagnostic tools that can ultimately enhance patient outcomes. My research not only emphasizes the importance of anatomical awareness in segmentation tasks but also showcases the potential of integrating various learning strategies to tackle complex challenges in the medical domain.",
                "collaborators": [
                    "Rongzhao Zhang",
                    "Zhian Bai",
                    "Ruoying Yu",
                    "Wenrao Pang",
                    "Lingyun Wang",
                    "Lifeng Zhu",
                    "Xiaofan Zhang",
                    "Huan Zhang"
                ],
                "pub_titles": [
                    "AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with Imperfect Anatomical Knowledge"
                ],
                "pub_abstracts": [
                    "When delineating lesions from medical images, a human expert can always keep in mind the anatomical structure behind the voxels. However, although high-quality (though not perfect) anatomical information can be retrieved from computed tomography (CT) scans with modern deep learning algorithms, it is still an open problem how these automatically generated organ masks can assist in addressing challenging lesion segmentation tasks, such as the segmentation of colorectal cancer (CRC). In this paper, we develop a novel Anatomy-Guided segmentation framework to exploit the auto-generated organ masks to aid CRC segmentation from CT, namely AG-CRC. First, we obtain multi-organ segmentation (MOS) masks with existing MOS models (e.g., TotalSegmentor) and further derive a more robust organ of interest (OOI) mask that may cover most of the colon-rectum and CRC voxels. Then, we propose an anatomy-guided training patch sampling strategy by optimizing a heuristic gain function that considers both the proximity of important regions (e.g., the tumor or organs of interest) and sample diversity. Third, we design a novel self-supervised learning scheme inspired by the topology of tubular organs like the colon to boost the model performance further. Finally, we employ a masked loss scheme to guide the model to focus solely on the essential learning region. We extensively evaluate the proposed method on two CRC segmentation datasets, where substantial performance improvement (5% to 9% in Dice) is achieved over current state-of-the-art medical image segmentation models, and the ablation studies further evidence the efficacy of every proposed component."
                ],
                "domain": [
                    "Medical Image Segmentation",
                    "Deep Learning",
                    "Anatomy-Guided Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "8eb43a5d-5378-47f9-8009-04e993a53c7e": {
                "pk": "8eb43a5d-5378-47f9-8009-04e993a53c7e",
                "project_name": null,
                "name": "Xiaoyang Li",
                "bio": "I am a researcher dedicated to the intersection of reinforcement learning and motion generation, with a particular focus on aligning generated motions with human preferences. My recent work introduces MotionRL, a pioneering approach that leverages Multi-Reward Reinforcement Learning to optimize text-to-motion generation tasks. Unlike previous methods that primarily aimed at improving numerical performance metrics, I emphasize the importance of human feedback and variability in perception. \n\nThrough MotionRL, I have developed a novel multi-objective optimization strategy that seeks to achieve Pareto optimality among text adherence, motion quality, and human preferences. This approach not only enhances the quality of generated motions but also provides users with greater control over the outcomes. My extensive experiments and user studies demonstrate significant improvements in performance across various metrics, showcasing the potential of integrating human-centric considerations into motion generation. I am passionate about advancing this field and exploring new ways to bridge the gap between technology and human experience.",
                "collaborators": [
                    "Xiaoyang Liu",
                    "Yunyao Mao",
                    "Wengang Zhou",
                    "Houqiang Li"
                ],
                "pub_titles": [
                    "MotionRL: Align Text-to-Motion Generation to Human Preferences with Multi-Reward Reinforcement Learning"
                ],
                "pub_abstracts": [
                    "We introduce MotionRL, the first approach to utilize Multi-Reward Reinforcement Learning (RL) for optimizing text-to-motion generation tasks and aligning them with human preferences. Previous works focused on improving numerical performance metrics on the given datasets, often neglecting the variability and subjectivity of human feedback. In contrast, our novel approach uses reinforcement learning to fine-tune the motion generator based on human preferences prior knowledge of the human perception model, allowing it to generate motions that better align human preferences. In addition, MotionRL introduces a novel multi-objective optimization strategy to approximate Pareto optimality between text adherence, motion quality, and human preferences. Extensive experiments and user studies demonstrate that MotionRL not only allows control over the generated results across different objectives but also significantly enhances performance across these metrics compared to other algorithms."
                ],
                "domain": [
                    "Reinforcement Learning",
                    "Text-to-Motion Generation",
                    "Human-Centric AI"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "69a832cc-d0b6-4be6-b0e4-e0315e1ab37f": {
                "pk": "69a832cc-d0b6-4be6-b0e4-e0315e1ab37f",
                "project_name": null,
                "name": "Lifeng Zhu",
                "bio": "I am a researcher dedicated to exploring the intersection of computer graphics and machine learning, particularly in the realm of texture generation for 3D meshes. My recent work focuses on leveraging pre-trained diffusion models to automatically generate high-quality textures. I recognized a significant challenge in achieving multi-view consistency due to the way these models operate in screen space. To address this, I developed a novel optimization-based color-fusion method that enhances consistency across different camera views while modifying latent codes through gradient back-propagation.\n\nMy approach builds on the Denoising Diffusion Implicit Models (DDIM) framework, allowing for a more flexible treatment of camera views and improving the overall quality of generated textures. Through rigorous evaluation on various 3D models, I demonstrated that my method outperforms existing state-of-the-art techniques. I am passionate about pushing the boundaries of what is possible in texture generation and making my findings accessible to the community, as evidenced by my open-source implementation available on GitHub. My goal is to continue innovating in this space, contributing to advancements in both graphics and machine learning.",
                "collaborators": [
                    "Hongkun Zhang",
                    "Zherong Pan",
                    "Congyi Zhang",
                    "Xifeng Gao"
                ],
                "pub_titles": [
                    "TexPainter: Generative Mesh Texturing with Multi-view Consistency"
                ],
                "pub_abstracts": [
                    "The recent success of pre-trained diffusion models unlocks the possibility of the automatic generation of textures for arbitrary 3D meshes in the wild. However, these models are trained in the screen space, while converting them to a multi-view consistent texture image poses a major obstacle to the output quality. In this paper, we propose a novel method to enforce multi-view consistency. Our method is based on the observation that latent space in a pre-trained diffusion model is noised separately for each camera view, making it difficult to achieve multi-view consistency by directly manipulating the latent codes. Based on the celebrated Denoising Diffusion Implicit Models (DDIM) scheme, we propose to use an optimization-based color-fusion to enforce consistency and indirectly modify the latent codes by gradient back-propagation. Our method further relaxes the sequential dependency assumption among the camera views. By evaluating on a series of general 3D models, we find our simple approach improves consistency and overall quality of the generated textures as compared to competing state-of-the-arts. Our implementation is available at: https://github.com/Quantuman134/TexPainter"
                ],
                "domain": [
                    "Computer Vision",
                    "Texture Generation",
                    "Diffusion Models",
                    "3D Meshes"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "6134d3ae-ca4f-439b-bc99-697eb5f4d15f": {
                "pk": "6134d3ae-ca4f-439b-bc99-697eb5f4d15f",
                "project_name": null,
                "name": "Zhian Bai",
                "bio": "I am a researcher dedicated to enhancing medical image segmentation, particularly in the context of colorectal cancer (CRC). My recent work centers around developing innovative frameworks that leverage anatomical information to improve segmentation accuracy. In my latest publication, I introduced the Anatomy-Guided segmentation framework (AG-CRC), which utilizes auto-generated organ masks to assist in the challenging task of CRC segmentation from CT scans.\n\nMy approach begins with obtaining multi-organ segmentation masks and refining them to create a robust organ of interest mask that effectively encompasses the colon-rectum and CRC voxels. I then implement a unique training patch sampling strategy that optimizes for both the proximity of critical regions and sample diversity, ensuring that the model learns from the most relevant data. Additionally, I incorporate a self-supervised learning scheme inspired by the topology of tubular organs, which further enhances model performance.\n\nThrough extensive evaluations on CRC segmentation datasets, my framework has demonstrated significant improvements in performance, achieving up to 9% enhancement in Dice scores compared to existing state-of-the-art models. I am passionate about bridging the gap between deep learning and clinical applications, and I strive to contribute to advancements that can ultimately improve patient outcomes in cancer diagnosis and treatment.",
                "collaborators": [
                    "Rongzhao Zhang",
                    "Ruoying Yu",
                    "Wenrao Pang",
                    "Lingyun Wang",
                    "Lifeng Zhu",
                    "Xiaofan Zhang",
                    "Huan Zhang",
                    "Weiguo Hu"
                ],
                "pub_titles": [
                    "AG-CRC: Anatomy-Guided Colorectal Cancer Segmentation in CT with Imperfect Anatomical Knowledge"
                ],
                "pub_abstracts": [
                    "When delineating lesions from medical images, a human expert can always keep in mind the anatomical structure behind the voxels. However, although high-quality (though not perfect) anatomical information can be retrieved from computed tomography (CT) scans with modern deep learning algorithms, it is still an open problem how these automatically generated organ masks can assist in addressing challenging lesion segmentation tasks, such as the segmentation of colorectal cancer (CRC). In this paper, we develop a novel Anatomy-Guided segmentation framework to exploit the auto-generated organ masks to aid CRC segmentation from CT, namely AG-CRC. First, we obtain multi-organ segmentation (MOS) masks with existing MOS models (e.g., TotalSegmentor) and further derive a more robust organ of interest (OOI) mask that may cover most of the colon-rectum and CRC voxels. Then, we propose an anatomy-guided training patch sampling strategy by optimizing a heuristic gain function that considers both the proximity of important regions (e.g., the tumor or organs of interest) and sample diversity. Third, we design a novel self-supervised learning scheme inspired by the topology of tubular organs like the colon to boost the model performance further. Finally, we employ a masked loss scheme to guide the model to focus solely on the essential learning region. We extensively evaluate the proposed method on two CRC segmentation datasets, where substantial performance improvement (5% to 9% in Dice) is achieved over current state-of-the-art medical image segmentation models, and the ablation studies further evidence the efficacy of every proposed component."
                ],
                "domain": [
                    "Medical Image Segmentation",
                    "Deep Learning",
                    "Anatomy-Guided Framework",
                    "Colorectal Cancer"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "eb548f0a-2304-4f5e-805b-ecefd3b7c3f5": {
                "pk": "eb548f0a-2304-4f5e-805b-ecefd3b7c3f5",
                "project_name": null,
                "name": "Xiaoming Shi",
                "bio": "I am a researcher dedicated to enhancing wireless network capacity through innovative antenna design. My recent work focuses on the development of a six-dimensional movable antenna (6DMA) system, which allows for both 3D positioning and 3D rotation of antennas. This flexibility is crucial for maximizing network performance, yet I recognize the practical challenges posed by existing base station architectures that primarily utilize fixed-position antenna arrays.\n\nTo address this, I introduced a hybrid fixed and movable antenna (HFMA) architecture that integrates conventional fixed-position arrays with adjustable 6DMA surfaces. This design not only facilitates implementation but also optimizes network capacity by adapting the rotation angles of the 6DMA surfaces based on user spatial distribution. Given the combinatorial nature of this optimization problem, I developed an adaptive Markov Chain Monte Carlo method to efficiently find solutions without the prohibitive computational costs of exhaustive search.\n\nThrough simulations, I have demonstrated significant performance improvements with my proposed HFMA design compared to various benchmark schemes. My research aims to bridge the gap between theoretical advancements and practical applications in wireless communication, ultimately contributing to more efficient and flexible network infrastructures.",
                "collaborators": [
                    "Xiaodan Shao",
                    "Rui Zhang"
                ],
                "pub_titles": [
                    "Capacity Maximization for Base Station with Hybrid Fixed and Movable Antennas"
                ],
                "pub_abstracts": [
                    "Six-dimensional movable antenna (6DMA) is an effective solution for enhancing wireless network capacity through the adjustment of both 3D positions and 3D rotations of distributed antennas/antenna surfaces. Although freely positioning/rotating 6DMA surfaces offers the greatest flexibility and thus highest capacity improvement, its implementation may be challenging in practice due to the drastic architecture change required for existing base stations (BSs), which predominantly adopt fixed-position antenna (FPA) arrays (e.g., sector antenna arrays). Thus, we introduce in this letter a new BS architecture called hybrid fixed and movable antennas (HFMA), which consists of both conventional FPA arrays and position/rotation-adjustable 6DMA surfaces. For ease of implementation, we consider that all 6DMA surfaces can rotate along a circular track above the FPA arrays. We aim to maximize the network capacity via optimizing the rotation angles of all 6DMA surfaces based on the users' spatial distribution. Since this problem is combinatorial and its optimal solution requires prohibitively high computational complexity via exhaustive search, we propose an alternative adaptive Markov Chain Monte Carlo based method to solve it more efficiently. Finally, we present simulation results that show significant performance gains achieved by our proposed design over various benchmark schemes."
                ],
                "domain": [
                    "Wireless Communication",
                    "Antenna Design",
                    "Network Optimization"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f3cb028e-fa42-4d11-87c3-29b05be589aa": {
                "pk": "f3cb028e-fa42-4d11-87c3-29b05be589aa",
                "project_name": null,
                "name": "Benyou Wang",
                "bio": "I am a researcher deeply engaged in the field of Natural Language Processing (NLP), with a particular focus on text representation and understanding. My work spans a variety of innovative approaches, from developing unified benchmarks for text classification models to exploring the intersection of quantum theory and language processing. I have proposed frameworks that leverage complex-valued representations to capture the nuances of human language, addressing challenges like semantic non-linearity and relational knowledge in pre-trained language models.\n\nMy recent contributions include the introduction of a multi-task learning framework for Aspect-based Sentiment Analysis, which enhances the extraction of opinion triplets, and the development of ElasticLM, a dynamic language model that adjusts its performance based on request streams. I have also pioneered methods for compressing large language models without sacrificing performance, making them more accessible for deployment in resource-constrained environments.\n\nI am particularly interested in the scalability of mathematical reasoning in language models, as demonstrated by my work on MathScale, which generated a comprehensive dataset for evaluating mathematical reasoning capabilities. Additionally, I have explored the potential of multi-agent frameworks to enhance the problem-solving abilities of large language models, showcasing the effectiveness of collaborative approaches in complex task environments.\n\nThrough my research, I aim to bridge theoretical insights with practical applications, pushing the boundaries of what is possible in NLP while addressing real-world challenges. My work not only contributes to academic discourse but also seeks to facilitate advancements in industry applications, making sophisticated NLP technologies more efficient and effective.",
                "collaborators": [
                    "Dawei Song",
                    "Qiuchi Li",
                    "Chen Zhang",
                    "Massimo Melucci",
                    "Hongbo Zhang",
                    "Xiang Wan",
                    "Zhengyang Tang",
                    "Fei Yu",
                    "Li Wang",
                    "Qikang Wei"
                ],
                "pub_titles": [
                    "TextZoo, a New Benchmark for Reconsidering Text Classification",
                    "Semantic Hilbert Space for Text Representation Learning",
                    "Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism and Synonymous Substitution",
                    "CNM: An Interpretable Complex-valued Network for Matching",
                    "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval",
                    "Quantum-inspired Complex Word Embedding",
                    "A Multi-task Learning Framework for Opinion Triplet Extraction",
                    "Natural Language Reasoning, A Survey",
                    "Rethinking The Uniformity Metric in Self-Supervised Learning",
                    "Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets",
                    "On Elastic Language Models",
                    "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning",
                    "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
                    "Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning",
                    "LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture",
                    "Exploring Extreme Parameter Compression for Pre-trained Language Models",
                    "MorphTE: Injecting Morphology in Tensorized Embeddings",
                    "Document-level Relation Extraction with Relation Correlations",
                    "Word Grounded Graph Convolutional Network",
                    "A Survey of Quantum-Cognitively Inspired Sentiment Analysis Models"
                ],
                "pub_abstracts": [
                    "Text representation is a fundamental concern in Natural Language Processing, especially in text classification. Recently, many neural network approaches with delicate representation model (e.g. FASTTEXT, CNN, RNN and many hybrid models with attention mechanisms) claimed that they achieved state-of-art in specific text classification datasets. However, it lacks an unified benchmark to compare these models and reveals the advantage of each sub-components for various settings. We re-implement more than 20 popular text representation models for classification in more than 10 datasets. In this paper, we reconsider the text classification task in the perspective of neural network and get serval effects with analysis of the above results.",
                    "Capturing the meaning of sentences has long been a challenging task. Current models tend to apply linear combinations of word features to conduct semantic composition for bigger-granularity units e.g. phrases, sentences, and documents. However, the semantic linearity does not always hold in human language. For instance, the meaning of the phrase `ivory tower' can not be deduced by linearly combining the meanings of `ivory' and `tower'. To address this issue, we propose a new framework that models different levels of semantic units (e.g. sememe, word, sentence, and semantic abstraction) on a single \\textit{Semantic Hilbert Space}, which naturally admits a non-linear semantic composition by means of a complex-valued vector word representation. An end-to-end neural network~\\footnote{https://github.com/wabyking/qnn} is proposed to implement the framework in the text classification task, and evaluation results on six benchmarking text classification datasets demonstrate the effectiveness, robustness and self-explanation power of the proposed model. Furthermore, intuitive case studies are conducted to help end users to understand how the framework works.",
                    "Pre-trained language models (PLMs) were considered to be able to store relational knowledge present in the training data. However, some relational knowledge seems to be discarded unsafely in PLMs due to \\textbf{report bias}: low-frequency relational knowledge might be underexpressed compared to high-frequency one in PLMs. This gives us a hint that relational knowledge might not be redundant to the stored knowledge of PLMs, but rather be complementary. To additionally inject relational knowledge into PLMs, we propose a simple-yet-effective approach to inject relational knowledge into PLMs, which is inspired by three observations (namely, polymorphism, synonymous substitution, and association). In particular, we switch entities in the training corpus to related entities (either hypernyms/hyponyms/synonyms, or arbitrarily-related concepts). Experimental results show that the proposed approach could not only better capture relational knowledge, but also improve the performance in various biomedical downstream tasks. Our model is available in \\url{https://github.com/StevenZHB/BioPLM_InjectingKnowledge}.",
                    "This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.",
                    "Deep prompt tuning (DPT) has gained great success in most natural language processing~(NLP) tasks. However, it is not well-investigated in dense retrieval where fine-tuning~(FT) still dominates. When deploying multiple retrieval tasks using the same backbone model~(e.g., RoBERTa), FT-based methods are unfriendly in terms of deployment cost: each new retrieval model needs to repeatedly deploy the backbone model without reuse. To reduce the deployment cost in such a scenario, this work investigates applying DPT in dense retrieval. The challenge is that directly applying DPT in dense retrieval largely underperforms FT methods. To compensate for the performance drop, we propose two model-agnostic and task-agnostic strategies for DPT-based retrievers, namely retrieval-oriented intermediate pretraining and unified negative mining, as a general approach that could be compatible with any pre-trained language model and retrieval task. The experimental results show that the proposed method (called DPTDR) outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions. We also conduct ablation studies to examine the effectiveness of each strategy in DPTDR. We believe this work facilitates the industry, as it saves enormous efforts and costs of deployment and increases the utility of computing resources. Our code is available at https://github.com/tangzhy/DPTDR.",
                    "A challenging task for word embeddings is to capture the emergent meaning or polarity of a combination of individual words. For example, existing approaches in word embeddings will assign high probabilities to the words \"Penguin\" and \"Fly\" if they frequently co-occur, but it fails to capture the fact that they occur in an opposite sense - Penguins do not fly. We hypothesize that humans do not associate a single polarity or sentiment to each word. The word contributes to the overall polarity of a combination of words depending upon which other words it is combined with. This is analogous to the behavior of microscopic particles which exist in all possible states at the same time and interfere with each other to give rise to new states depending upon their relative phases. We make use of the Hilbert Space representation of such particles in Quantum Mechanics where we subscribe a relative phase to each word, which is a complex number, and investigate two such quantum inspired models to derive the meaning of a combination of words. The proposed models achieve better performances than state-of-the-art non-quantum models on the binary sentence classification task.",
                    "The state-of-the-art Aspect-based Sentiment Analysis (ABSA) approaches are mainly based on either detecting aspect terms and their corresponding sentiment polarities, or co-extracting aspect and opinion terms. However, the extraction of aspect-sentiment pairs lacks opinion terms as a reference, while co-extraction of aspect and opinion terms would not lead to meaningful pairs without determining their sentiment dependencies. To address the issue, we present a novel view of ABSA as an opinion triplet extraction task, and propose a multi-task learning framework to jointly extract aspect terms and opinion terms, and simultaneously parses sentiment dependencies between them with a biaffine scorer. At inference phase, the extraction of triplets is facilitated by a triplet decoding method based on the above outputs. We evaluate the proposed framework on four SemEval benchmarks for ASBA. The results demonstrate that our approach significantly outperforms a range of strong baselines and state-of-the-art approaches.",
                    "This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.",
                    "Uniformity plays an important role in evaluating learned representations, providing insights into self-supervised learning. In our quest for effective uniformity metrics, we pinpoint four principled properties that such metrics should possess. Namely, an effective uniformity metric should remain invariant to instance permutations and sample replications while accurately capturing feature redundancy and dimensional collapse. Surprisingly, we find that the uniformity metric proposed by \\citet{Wang2020UnderstandingCR} fails to satisfy the majority of these properties. Specifically, their metric is sensitive to sample replications, and can not account for feature redundancy and dimensional collapse correctly. To overcome these limitations, we introduce a new uniformity metric based on the Wasserstein distance, which satisfies all the aforementioned properties. Integrating this new metric in existing self-supervised learning methods effectively mitigates dimensional collapse and consistently improves their performance on downstream tasks involving CIFAR-10 and CIFAR-100 datasets. Code is available at \\url{https://github.com/statsle/WassersteinSSL}.",
                    "Over-parameterized models, typically pretrained language models (LMs), have shown an appealing expressive power due to their small learning bias. However, the huge learning capacity of LMs can also lead to large learning variance. In a pilot study, we find that, when faced with multiple domains, a critical portion of parameters behave unexpectedly in a domain-specific manner while others behave in a domain-general one. Motivated by this phenomenon, we for the first time posit that domain-general parameters can underpin a domain-general LM that can be derived from the original LM. To uncover the domain-general LM, we propose to identify domain-general parameters by playing lottery tickets (dubbed doge tickets). In order to intervene the lottery, we propose a domain-general score, which depicts how domain-invariant a parameter is by associating it with the variance. Comprehensive experiments are conducted on the Amazon, Mnli and OntoNotes datasets. The results show that the doge tickets obtains an improved out-of-domain generalization in comparison with a range of competitive baselines. Analysis results further hint the existence of domain-general parameters and the performance consistency of doge tickets.",
                    "Large-scale pretrained language models have achieved compelling performance in a wide range of language understanding and information retrieval tasks. Knowledge distillation offers an opportunity to compress a large language model to a small one, in order to reach a reasonable latency-performance tradeoff. However, for scenarios where the number of requests (e.g., queries submitted to a search engine) is highly variant, the static tradeoff attained by the compressed language model might not always fit. Once a model is assigned with a static tradeoff, it could be inadequate in that the latency is too high when the number of requests is large or the performance is too low when the number of requests is small. To this end, we propose an elastic language model (ElasticLM) that elastically adjusts the tradeoff according to the request stream. The basic idea is to introduce a compute elasticity to the compressed language model, so that the tradeoff could vary on-the-fly along scalable and controllable compute. Specifically, we impose an elastic structure to enable ElasticLM with compute elasticity and design an elastic optimization to learn ElasticLM under compute elasticity. To serve ElasticLM, we apply an elastic schedule. Considering the specificity of information retrieval, we adapt ElasticLM to dense retrieval and reranking and present ElasticDenser and ElasticRanker respectively. Offline evaluation is conducted on a language understanding benchmark GLUE; and several information retrieval tasks including Natural Question, Trivia QA, and MS MARCO. The results show that ElasticLM along with ElasticDenser and ElasticRanker can perform correctly and competitively compared with an array of static baselines. Furthermore, online simulation with concurrency is also carried out. The results demonstrate that ElasticLM can provide elastic tradeoffs with respect to varying request stream.",
                    "Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a $\\textit{value estimation}$ problem in planning.   Inspired by the findings that $\\textit{outcome supervision for guided decoding essentially acts as a value model}$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM eliminates the need for labor-intensive annotations of step-level correctness, thereby significantly enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our $\\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters}$; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training value models for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for guided decoding.",
                    "Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively.",
                    "The emergence of large language models (LLMs) has opened up unprecedented possibilities for automating complex tasks that are often comparable to human performance. Despite their capabilities, LLMs still encounter difficulties in completing tasks that require high levels of accuracy and complexity due to their inherent limitations in handling multifaceted problems single-handedly. This paper introduces `Smurfs', a cutting-edge multi-agent framework designed to revolutionize the application of LLMs. By seamlessly transforming a conventional LLM into a synergistic multi-agent ensemble, Smurfs can enhance the model's ability to solve complex tasks at no additional cost. This is achieved through innovative prompting strategies that allocate distinct roles within the model, thereby facilitating collaboration among specialized agents and forming an intelligent multi-agent system. Our empirical investigation on both open-ended task of StableToolBench and closed-ended task on HotpotQA showcases Smurfs' superior capability in intricate tool utilization scenarios. Notably, Smurfs outmatches all the baseline methods in both experiments, setting new state-of-the-art performance. Furthermore, through comprehensive ablation studies, we dissect the contribution of the core components of the multi-agent framework to its overall efficacy. This not only verifies the effectiveness of the framework, but also sets a route for future exploration of multi-agent LLM systems.",
                    "Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as \\textit{degraded performance with more images} and \\textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge \\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks.",
                    "Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing. This raises many concerns from various perspectives, e.g., financial costs and carbon emissions. Compressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention. In this work, we aim to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one. Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and efficiency during compression. Our compressed BERT with ${1}/{7}$ parameters in Transformer layers performs on-par with, sometimes slightly better than the original BERT in GLUE benchmark. A tiny version achieves $96.7\\%$ performance of BERT-base with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding the embedding layer) and $2.7 \\times$ faster on inference. To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the benefit of the proposed method on a distilled BERT.",
                    "In the era of deep learning, word embeddings are essential when dealing with text tasks. However, storing and accessing these embeddings requires a large amount of space. This is not conducive to the deployment of these models on resource-limited devices. Combining the powerful compression capability of tensor products, we propose a word embedding compression method with morphological augmentation, Morphologically-enhanced Tensorized Embeddings (MorphTE). A word consists of one or more morphemes, the smallest units that bear meaning or have a grammatical function. MorphTE represents a word embedding as an entangled form of its morpheme vectors via the tensor product, which injects prior semantic and grammatical knowledge into the learning of embeddings. Furthermore, the dimensionality of the morpheme vector and the number of morphemes are much smaller than those of words, which greatly reduces the parameters of the word embeddings. We conduct experiments on tasks such as machine translation and question answering. Experimental results on four translation datasets of different languages show that MorphTE can compress word embedding parameters by about 20 times without performance loss and significantly outperforms related embedding compression methods.",
                    "Document-level relation extraction faces two overlooked challenges: long-tail problem and multi-label problem. Previous work focuses mainly on obtaining better contextual representations for entity pairs, hardly address the above challenges. In this paper, we analyze the co-occurrence correlation of relations, and introduce it into DocRE task for the first time. We argue that the correlations can not only transfer knowledge between data-rich relations and data-scarce ones to assist in the training of tailed relations, but also reflect semantic distance guiding the classifier to identify semantically close relations for multi-label entity pairs. Specifically, we use relation embedding as a medium, and propose two co-occurrence prediction sub-tasks from both coarse- and fine-grained perspectives to capture relation correlations. Finally, the learned correlation-aware embeddings are used to guide the extraction of relational facts. Substantial experiments on two popular DocRE datasets are conducted, and our method achieves superior results compared to baselines. Insightful analysis also demonstrates the potential of relation correlations to address the above challenges.",
                    "Graph Convolutional Networks (GCNs) have shown strong performance in learning text representations for various tasks such as text classification, due to its expressive power in modeling graph structure data (e.g., a literature citation network). Most existing GCNs are limited to deal with documents included in a pre-defined graph, i.e., it cannot be generalized to out-of-graph documents. To address this issue, we propose to transform the document graph into a word graph, to decouple data samples (i.e., documents in training and test sets) and a GCN model by using a document-independent graph. Such word-level GCN could therefore naturally inference out-of-graph documents in an inductive way. The proposed Word-level Graph (WGraph) can not only implicitly learning word presentation with commonly-used word co-occurrences in corpora, but also incorporate extra global semantic dependency derived from inter-document relationships (e.g., literature citations). An inductive Word-grounded Graph Convolutional Network (WGCN) is proposed to learn word and document representations based on WGraph in a supervised manner. Experiments on text classification with and without citation networks evidence that the proposed WGCN model outperforms existing methods in terms of effectiveness and efficiency.",
                    "Quantum theory, originally proposed as a physical theory to describe the motions of microscopic particles, has been applied to various non-physics domains involving human cognition and decision-making that are inherently uncertain and exhibit certain non-classical, quantum-like characteristics. Sentiment analysis is a typical example of such domains. In the last few years, by leveraging the modeling power of quantum probability (a non-classical probability stemming from quantum mechanics methodology) and deep neural networks, a range of novel quantum-cognitively inspired models for sentiment analysis have emerged and performed well. This survey presents a timely overview of the latest developments in this fascinating cross-disciplinary area. We first provide a background of quantum probability and quantum cognition at a theoretical level, analyzing their advantages over classical theories in modeling the cognitive aspects of sentiment analysis. Then, recent quantum-cognitively inspired models are introduced and discussed in detail, focusing on how they approach the key challenges of the sentiment analysis task. Finally, we discuss the limitations of the current research and highlight future research directions."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Quantum Computing",
                    "Text Representation",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f9d9db8d-18aa-4c07-801a-0840bfd3b36a": {
                "pk": "f9d9db8d-18aa-4c07-801a-0840bfd3b36a",
                "project_name": null,
                "name": "Haitao Song",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge and enhancing search efficiency.\n\nThrough these contributions, I strive to bridge the gap between theoretical advancements and practical applications, ultimately pushing the boundaries of what GNNs can achieve in real-world scenarios.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "666643f7-fa09-4740-a077-ffbb11dce2bf": {
                "pk": "666643f7-fa09-4740-a077-ffbb11dce2bf",
                "project_name": null,
                "name": "Pengfei Liu",
                "bio": "I am a researcher with a diverse background in combinatorial algorithms, machine learning, and natural language processing (NLP). My recent work has focused on tackling complex problems such as the multi-commodity flow problem, where I introduced innovative concepts like equilibrium pseudo-flow and stable pseudo-flow to enhance algorithmic efficiency. I have also developed a twin Convolutional Neural Network (tCNNS) model for drug response prediction, achieving significant improvements in performance metrics.\n\nIn the realm of NLP, I proposed SimCLS, a framework for abstractive summarization that bridges the gap between learning objectives and evaluation metrics, leading to state-of-the-art results. My exploration of multi-task learning through Parameters Read-Write Networks (PRaWNs) has provided insights into feature entanglement and task communication, enhancing model performance across various tasks.\n\nAdditionally, I have contributed to the understanding of NLP technology development through the reStructured Pre-training (RST) paradigm, which emphasizes data storage and access, yielding superior results on numerous datasets. My work also extends to the analysis of self-similar singularities in fluid dynamics and the development of dynamic compositional neural networks to address underfitting in tree-structured models.\n\nOverall, my research is driven by a commitment to advancing algorithmic techniques and enhancing model performance across diverse applications, with a focus on practical implications and empirical validation.",
                "collaborators": [
                    "Xuanjing Huang",
                    "Xipeng Qiu",
                    "Yixin Liu",
                    "Yu Peng",
                    "Yonghong An",
                    "Weizhe Yuan",
                    "Thomas Y. Hou"
                ],
                "pub_titles": [
                    "A Combinatorial Algorithm for the Multi-commodity Flow Problem",
                    "A Localized Method for the Multi-commodity Flow Problem",
                    "Drug cell line interaction prediction",
                    "SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization",
                    "Meta-Learning Multi-task Communication",
                    "A superradiant laser based on two-photon Raman transition of caesium atoms",
                    "Eliciting Information from Sensitive Survey Questions",
                    "reStructured Pre-training",
                    "Self-similar Singularity of a 1D Model for the 3D Axisymmetric Euler Equations",
                    "Deep Multi-Task Learning with Shared Memory",
                    "Dynamic Compositional Neural Networks over Tree Structure"
                ],
                "pub_abstracts": [
                    "This paper researches combinatorial algorithms for the multi-commodity flow problem. We relax the capacity constraints and introduce a penalty function $h$ for each arc. If the flow exceeds the capacity on arc $a$, arc $a$ would have a penalty cost. Based on the penalty function $h$, a new conception, equilibrium pseudo-flow, is introduced. Then we design a combinatorial algorithm to obtain equilibrium pseudo-flow. If the equilibrium pseudo-flow is a nonzero-equilibrium pseudo-flow, there exists no feasible solution for the multi-commodity flow problem; if the equilibrium pseudo-flow is a zero-equilibrium pseudo-flow, there exists a feasible solution for the multi-commodity flow problem and the zero-equilibrium pseudo-flow is the feasible solution. At last, a non-linear description of the multi-commodity flow problem is given, whose solution is equilibrium pseudo-flow. Besides, the content in this paper can be easily generalized to minimum cost multi-commodity flow problem.",
                    "This paper presents a local-control approach to address the multicommodity flow problem. The method involves relaxing both capacity constraints and flow conservation constraints. If the flow exceeds the capacity on an edge, the edge would incur a congestion cost. If the flow into a vertex is not equal to that out of the vertex, the vertex would have a height. Subsequently, a new concept, stable pseudo-flow, is introduced. Potential difference reduction algorithms, which don't rely on any shortest path or augmenting path, are designed to obtain stable pseudo-flow. If the stable pseudo-flow is a nonzero-stable pseudo-flow, there exists no feasible solution for multicommodity flow problem. Conversely, if the stable pseudo-flow is a zero-stable pseudo-flow, the feasible solution exists and the zero-stable pseudo-flow is the feasible solution. Notably, the algorithms work in a localized manner and can be efficiently implemented in parallel, which would further enhance performance.",
                    "Understanding the phenotypic drug response on cancer cell lines plays a vital rule in anti-cancer drug discovery and re-purposing. The Genomics of Drug Sensitivity in Cancer (GDSC) database provides open data for researchers in phenotypic screening to test their models and methods. Previously, most research in these areas starts from the fingerprints or features of drugs, instead of their structures. In this paper, we introduce a model for phenotypic screening, which is called twin Convolutional Neural Network for drugs in SMILES format (tCNNS). tCNNS is comprised of CNN input channels for drugs in SMILES format and cancer cell lines respectively. Our model achieves $0.84$ for the coefficient of determinant($R^2$) and $0.92$ for Pearson correlation($R_p$), which are significantly better than previous works\\cite{ammad2014integrative,haider2015copula,menden2013machine}. Besides these statistical metrics, tCNNS also provides some insights into phenotypic screening.",
                    "In this paper, we present a conceptually simple while empirically powerful framework for abstractive summarization, SimCLS, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing top-scoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results: https://github.com/yixinL7/SimCLS. Results of our proposed models have been deployed into ExplainaBoard platform, which allows researchers to understand our systems in a more fine-grained way.",
                    "In this paper, we describe a general framework: Parameters Read-Write Networks (PRaWNs) to systematically analyze current neural models for multi-task learning, in which we find that existing models expect to disentangle features into different spaces while features learned in practice are still entangled in shared space, leaving potential hazards for other training or unseen tasks.   We propose to alleviate this problem by incorporating an inductive bias into the process of multi-task learning, that each task can keep informed of not only the knowledge stored in other tasks but the way how other tasks maintain their knowledge.   In practice, we achieve above inductive bias by allowing different tasks to communicate by passing both hidden variables and gradients explicitly.   Experimentally, we evaluate proposed methods on three groups of tasks and two types of settings (\\textsc{in-task} and \\textsc{out-of-task}). Quantitative and qualitative results show their effectiveness.",
                    "We propose a superradiant laser based on two-photon Raman transition of caesium-133 atoms which collectively emit photons on an ultra narrow transition into the mode of a low Q resonator known as optical bad-cavity regime. The spin-spin correlation which characterizes the collective effect is demonstrated. We theoretically predict that the optical radiation has an extremely narrow linewidth in the 98 (1) *10-2 mHz range, smaller than the transition itself due to collective effects, and a power level of 7 (1)*10-10 W is possible, which can provide a possible new way to realize an optical clock with a millihertz linewidth.",
                    "This paper considers how to elicit information from sensitive survey questions. First we thoroughly evaluate list experiments (LE), a leading method in the experimental literature on sensitive questions. Our empirical results demonstrate that the assumptions required to identify sensitive information in LE are violated for the majority of surveys. Next we propose a novel survey method, called Multiple Response Technique (MRT), for eliciting information from sensitive questions. We require all of the respondents to answer three questions related to the sensitive information. This technique recovers sensitive information at a disaggregated level while still allowing arbitrary misreporting in survey responses. An application of the MRT provides novel empirical evidence on sexual orientation and Lesbian, Gay, Bisexual, and Transgender (LGBT)-related sentiment.",
                    "In this work, we try to decipher the internal connection of NLP technology development in the past decades, searching for essence, which rewards us with a (potential) new learning paradigm for NLP tasks, dubbed as reStructured Pre-training (RST). In such a paradigm, the role of data will be re-emphasized, and model pre-training and fine-tuning of downstream tasks are viewed as a process of data storing and accessing. Based on that, we operationalize the simple principle that a good storage mechanism should not only have the ability to cache a large amount of data but also consider the ease of access. We achieve this by pre-training models over restructured data that consist of a variety of valuable information instead of raw data after overcoming several engineering challenges. Experimentally, RST models not only surpass strong competitors (e.g., T0) on 52/55 popular datasets from a variety of NLP tasks, but also achieve superior performance in National College Entrance Examination - English (Gaokao-English),the most authoritative examination in China. Specifically, the proposed system Qin achieves 40 points higher than the average scores made by students and 15 points higher than GPT3 with 1/16 parameters. In particular, Qin gets a high score of 138.5 (the full mark is 150) in the 2018 English exam (national paper III). We have released the Gaokao Benchmark with an online submission platform.   In addition, we test our model in the 2022 College Entrance Examination English that happened a few days ago (2022.06.08), and it gets a total score of 134 (v.s. GPT3's 108).",
                    "We investigate the self-similar singularity of a 1D model for the 3D axisymmetric Euler equations, which is motivated by a particular singularity formation scenario observed in numerical computation. We prove the existence of a discrete family of self-similar profiles for this model and analyze their far-field properties. The self-similar profiles we find agree with direct simulation of the model and seem to have some stability.",
                    "Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks.",
                    "Tree-structured neural networks have proven to be effective in learning semantic representations by exploiting syntactic information. In spite of their success, most existing models suffer from the underfitting problem: they recursively use the same shared compositional function throughout the whole compositional process and lack expressive power due to inability to capture the richness of compositionality. In this paper, we address this issue by introducing the dynamic compositional neural networks over tree structure (DC-TreeNN), in which the compositional function is dynamically generated by a meta network. The role of meta-network is to capture the metaknowledge across the different compositional rules and formulate them. Experimental results on two typical tasks show the effectiveness of the proposed models."
                ],
                "domain": [
                    "Combinatorial Optimization",
                    "Multi-Task Learning",
                    "Natural Language Processing",
                    "Neural Networks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "1c9d7673-34da-4049-a39b-8172e9acb500": {
                "pk": "1c9d7673-34da-4049-a39b-8172e9acb500",
                "project_name": null,
                "name": "Xiaofan Zhang",
                "bio": "I am a dedicated researcher specializing in deep learning and computer vision, with a particular focus on medical image analysis and scene recognition. My work has led to the development of innovative models such as CSRNet, which excels in congested scene recognition and counting tasks, achieving state-of-the-art performance across multiple datasets. I have also pioneered the Knowledge-injected U-Transformer (KiUT) for generating radiology reports, effectively bridging the gap between visual data and clinical knowledge.\n\nMy research extends to multi-modal brain tumor segmentation, where I introduced the Modality Aware and Shift Mixer to enhance the integration of diverse imaging modalities. I am passionate about improving diagnostic processes, as demonstrated by my work on converting H&E staining to IHC staining for breast cancer detection, which significantly enhances the accuracy of diagnostic imaging.\n\nIn addition to my contributions in medical imaging, I have explored the automation of deep neural network (DNN) hardware accelerators through DNNExplorer, optimizing performance and efficiency for various applications. My recent work on GuideGen showcases my commitment to addressing the annotation burden in medical datasets by generating high-fidelity synthetic images and masks.\n\nI am driven by the potential of deep learning to transform healthcare and improve patient outcomes, and I continuously seek to push the boundaries of what is possible in this exciting field. My research is characterized by a strong emphasis on practical applications, rigorous experimentation, and a collaborative spirit, as I aim to make impactful contributions to both academia and industry.",
                "collaborators": [
                    "Shaoting Zhang",
                    "Deming Chen",
                    "Zhongzhen Huang",
                    "Yuhong Li",
                    "Linda Wei",
                    "Wenhui Lei",
                    "Linrui Dai",
                    "Shengyi Hua",
                    "Yankai Jiang",
                    "Hanchen Ye"
                ],
                "pub_titles": [
                    "CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes",
                    "KiUT: Knowledge-injected U-Transformer for Radiology Report Generation",
                    "Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation",
                    "DeReStainer: H&E to IHC Pathological Image Translation via Decoupled Staining Channels",
                    "Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment",
                    "Being-ahead: Benchmarking and Exploring Accelerators for Hardware-Efficient AI Deployment",
                    "GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation",
                    "SiamVGG: Visual Tracking using Deeper Siamese Networks",
                    "Efficient Subclass Segmentation in Medical Images"
                ],
                "pub_abstracts": [
                    "We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.",
                    "Radiology report generation aims to automatically generate a clinically accurate and coherent paragraph from the X-ray image, which could relieve radiologists from the heavy burden of report writing. Although various image caption methods have shown remarkable performance in the natural image field, generating accurate reports for medical images requires knowledge of multiple modalities, including vision, language, and medical terminology. We propose a Knowledge-injected U-Transformer (KiUT) to learn multi-level visual representation and adaptively distill the information with contextual and clinical knowledge for word prediction. In detail, a U-connection schema between the encoder and decoder is designed to model interactions between different modalities. And a symptom graph and an injected knowledge distiller are developed to assist the report generation. Experimentally, we outperform state-of-the-art methods on two widely used benchmark datasets: IU-Xray and MIMIC-CXR. Further experimental results prove the advantages of our architecture and the complementary benefits of the injected knowledge.",
                    "Combining images from multi-modalities is beneficial to explore various information in computer vision, especially in the medical domain. As an essential part of clinical diagnosis, multi-modal brain tumor segmentation aims to delineate the malignant entity involving multiple modalities. Although existing methods have shown remarkable performance in the task, the information exchange for cross-scale and high-level representations fusion in spatial and modality are limited in these methods. In this paper, we present a novel Modality Aware and Shift Mixer that integrates intra-modality and inter-modality dependencies of multi-modal images for effective and robust brain tumor segmentation. Specifically, we introduce a Modality-Aware module according to neuroimaging studies for modeling the specific modality pair relationships at low levels, and a Modality-Shift module with specific mosaic patterns is developed to explore the complex relationships across modalities at high levels via the self-attention. Experimentally, we outperform previous state-of-the-art approaches on the public Brain Tumor Segmentation (BraTS 2021 segmentation) dataset. Further qualitative experiments demonstrate the efficacy and robustness of MASM.",
                    "Breast cancer is a highly fatal disease among cancers in women, and early detection is crucial for treatment. HER2 status, a valuable diagnostic marker based on Immunohistochemistry (IHC) staining, is instrumental in determining breast cancer status. The high cost of IHC staining and the ubiquity of Hematoxylin and Eosin (H&E) staining make the conversion from H&E to IHC staining essential. In this article, we propose a destain-restain framework for converting H&E staining to IHC staining, leveraging the characteristic that H&E staining and IHC staining of the same tissue sections share the Hematoxylin channel. We further design loss functions specifically for Hematoxylin and Diaminobenzidin (DAB) channels to generate IHC images exploiting insights from separated staining channels. Beyond the benchmark metrics on BCI contest, we have developed semantic information metrics for the HER2 level. The experimental results demonstrated that our method outperforms previous open-sourced methods in terms of image intrinsic property and semantic information.",
                    "Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia. Codes will be publicly available.",
                    "Customized hardware accelerators have been developed to provide improved performance and efficiency for DNN inference and training. However, the existing hardware accelerators may not always be suitable for handling various DNN models as their architecture paradigms and configuration tradeoffs are highly application-specific. It is important to benchmark the accelerator candidates in the earliest stage to gather comprehensive performance metrics and locate the potential bottlenecks. Further demands also emerge after benchmarking, which require adequate solutions to address the bottlenecks and improve the current designs for targeted workloads. To achieve these goals, in this paper, we leverage an automation tool called DNNExplorer for benchmarking customized DNN hardware accelerators and exploring novel accelerator designs with improved performance and efficiency. Key features include (1) direct support to popular machine learning frameworks for DNN workload analysis and accurate analytical models for fast accelerator benchmarking; (2) a novel accelerator design paradigm with high-dimensional design space support and fine-grained adjustability to overcome the existing design drawbacks; and (3) a design space exploration (DSE) engine to generate optimized accelerators by considering targeted AI workloads and available hardware resources. Results show that accelerators adopting the proposed novel paradigm can deliver up to 4.2X higher throughput (GOP/s) than the state-of-the-art pipeline design in DNNBuilder and up to 2.0X improved efficiency than the recently published generic design in HybridDNN given the same DNN model and resource budgets. With DNNExplorer's benchmarking and exploration features, we can be ahead at building and optimizing customized AI accelerators and enable more efficient AI applications.",
                    "The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \\textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corresponding mask slice to incorporate both style information and anatomical guidance. This pipeline guarantees high fidelity and variability as well as exact alignment between generated CT volumes and tissue masks. Both qualitative and quantitative experiments on 3D abdominal CTs demonstrate a high performance of our proposed pipeline, thereby proving our method can serve as a dataset generator and provide potential benefits to downstream tasks. It is hoped that our work will offer a promising solution on the multimodality generation of CT and its anatomical mask. Our source code is publicly available at https://github.com/OvO1111/JointImageGeneration.",
                    "Recently, we have seen a rapid development of Deep Neural Network (DNN) based visual tracking solutions. Some trackers combine the DNN-based solutions with Discriminative Correlation Filters (DCF) to extract semantic features and successfully deliver the state-of-the-art tracking accuracy. However, these solutions are highly compute-intensive, which require long processing time, resulting unsecured real-time performance. To deliver both high accuracy and reliable real-time performance, we propose a novel tracker called SiamVGG\\footnote{https://github.com/leeyeehoo/SiamVGG}. It combines a Convolutional Neural Network (CNN) backbone and a cross-correlation operator, and takes advantage of the features from exemplary images for more accurate object tracking. The architecture of SiamVGG is customized from VGG-16 with the parameters shared by both exemplary images and desired input video frames. We demonstrate the proposed SiamVGG on OTB-2013/50/100 and VOT 2015/2016/2017 datasets with the state-of-the-art accuracy while maintaining a decent real-time performance of 50 FPS running on a GTX 1080Ti. Our design can achieve 2% higher Expected Average Overlap (EAO) compared to the ECO and C-COT in VOT2017 Challenge.",
                    "As research interests in medical image analysis become increasingly fine-grained, the cost for extensive annotation also rises. One feasible way to reduce the cost is to annotate with coarse-grained superclass labels while using limited fine-grained annotations as a complement. In this way, fine-grained data learning is assisted by ample coarse annotations. Recent studies in classification tasks have adopted this method to achieve satisfactory results. However, there is a lack of research on efficient learning of fine-grained subclasses in semantic segmentation tasks. In this paper, we propose a novel approach that leverages the hierarchical structure of categories to design network architecture. Meanwhile, a task-driven data generation method is presented to make it easier for the network to recognize different subclass categories. Specifically, we introduce a Prior Concatenation module that enhances confidence in subclass segmentation by concatenating predicted logits from the superclass classifier, a Separate Normalization module that stretches the intra-class distance within the same superclass to facilitate subclass segmentation, and a HierarchicalMix model that generates high-quality pseudo labels for unlabeled samples by fusing only similar superclass regions from labeled and unlabeled images. Our experiments on the BraTS2021 and ACDC datasets demonstrate that our approach achieves comparable accuracy to a model trained with full subclass annotations, with limited subclass annotations and sufficient superclass annotations. Our approach offers a promising solution for efficient fine-grained subclass segmentation in medical images. Our code is publicly available here."
                ],
                "domain": [
                    "Medical Image Analysis",
                    "Deep Learning",
                    "Computer Vision",
                    "Multi-modal Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "e066a8ee-88da-4031-8f11-eda44f01b46c": {
                "pk": "e066a8ee-88da-4031-8f11-eda44f01b46c",
                "project_name": null,
                "name": "Shanshan Wang",
                "bio": "I am a researcher with a strong focus on the intersection of finance, statistics, and machine learning. My work primarily revolves around understanding the dynamics of stock price responses to trades, both at the individual stock level and across correlated markets. I have developed models that quantify the self- and cross-impact of trades, revealing how the trading behavior of one stock can transiently influence others. My research extends to exploring the complexities of trading strategies, particularly in minimizing costs associated with cross-impacts in paired stock trades.\n\nIn addition to my work in finance, I have made significant contributions to statistical methodologies, particularly in the realm of hypothesis testing and variable selection under sparse conditions. My development of the partial penalized likelihood approach has shown promising results in both theoretical and empirical settings, outperforming traditional methods.\n\nI am also passionate about advancing domain adaptation techniques in machine learning, proposing self-adaptive methods that enhance domain alignment and improve transfer learning outcomes. My recent work includes the development of robust regression algorithms for big data streams, which address the challenges of outliers and high-velocity data.\n\nOverall, my research aims to bridge theoretical insights with practical applications, providing tools and frameworks that enhance our understanding of complex systems in finance and beyond. I am committed to pushing the boundaries of knowledge in these fields and contributing to the development of innovative solutions.",
                "collaborators": [
                    "Thomas Guhr",
                    "Rudi Schäfer",
                    "Lei Zhang",
                    "Tingting Huang",
                    "Huiwen Wang",
                    "Sebastian Neusüß",
                    "Hengjian Cui",
                    "Chunbai Tao",
                    "Chenglong Xiao",
                    "Zsolt Lángi"
                ],
                "pub_titles": [
                    "Trading strategies for stock pairs regarding to the cross-impact cost",
                    "Partial Penalized Likelihood Ratio Test under Sparse Case",
                    "Self-adaptive Re-weighted Adversarial Domain Adaptation",
                    "Microscopic Understanding of Cross-Responses between Stocks: a Two-Component Price Impact Model",
                    "Local fluctuations of the signed traded volumes and the dependencies of demands: a copula analysis",
                    "Online Updating Huber Robust Regression for Big Data Streams",
                    "Novel Algorithms for Efficient Mining of Connected Induced Subgraphs of a Given Cardinality",
                    "The Honeycomb Conjecture in normed planes and an alpha-convex variant of a theorem of Dowker",
                    "A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis",
                    "Delay-Adaptive Control of First-order Hyperbolic PIDEs",
                    "Spatial Functional Linear Model and its Estimation Method",
                    "BP-Triplet Net for Unsupervised Domain Adaptation: A Bayesian Perspective",
                    "Price response in correlated financial markets: empirical results",
                    "Cross-response in correlated financial markets: individual stocks",
                    "Average cross-responses in correlated financial market",
                    "Grasping asymmetric information in market impacts",
                    "Statistical properties of market collective responses",
                    "A Flexible Spatial Autoregressive Modelling Framework for Mixed Covariates of Multiple Data Types"
                ],
                "pub_abstracts": [
                    "We extend the framework of trading strategies of Gatheral [2010] from single stocks to a pair of stocks. Our trading strategy with the executions of two round-trip trades can be described by the trading rates of the paired stocks and the ratio of their trading periods. By minimizing the potential cost arising from cross-impacts, i.e., the price change of one stock due to the trades of another stock, we can find out an optimal strategy for executing a sequence of trades from different stocks. We further apply the model of the strategy to a specific case, where we quantify the cross-impacts of traded volumes and of time lag with empirical data for the computation of costs. We thus picture the influence of cross-impacts on the trading strategy.",
                    "This work is concern with testing the low-dimensional parameters of interest with divergent dimensional data and variable selection for the rest under the sparse case. A consistent test via the partial penalized likelihood approach, called the partial penalized likelihood ratio test statistic is derived, and its asymptotic distributions under the null hypothesis and the local alternatives of order $n^{-1/2}$ are obtained under some regularity conditions. Meanwhile, the oracle property of the partial penalized likelihood estimator also holds. The proposed partial penalized likelihood ratio test statistic outperforms the full penalized likelihood ratio test statistic in term of size and power, and performs as well as the classical likelihood ratio test statistic. Moreover, the proposed method obtains the variable selection results as well as the p-values of testing. Numerical simulations and an analysis of Prostate Cancer data confirm our theoretical findings and demonstrate the promising performance of the proposed partial penalized likelihood in hypothesis testing and variable selection.",
                    "Existing adversarial domain adaptation methods mainly consider the marginal distribution and these methods may lead to either under transfer or negative transfer. To address this problem, we present a self-adaptive re-weighted adversarial domain adaptation approach, which tries to enhance domain alignment from the perspective of conditional distribution. In order to promote positive transfer and combat negative transfer, we reduce the weight of the adversarial loss for aligned features while increasing the adversarial force for those poorly aligned measured by the conditional entropy. Additionally, triplet loss leveraging source samples and pseudo-labeled target samples is employed on the confusing domain. Such metric loss ensures the distance of the intra-class sample pairs closer than the inter-class pairs to achieve the class-level alignment. In this way, the high accurate pseudolabeled target samples and semantic alignment can be captured simultaneously in the co-training process. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David's theorem. Empirical evidence demonstrates that the proposed model outperforms state of the arts on standard domain adaptation datasets.",
                    "We construct a price impact model between stocks in a correlated market. For the price change of a given stock induced by the short-run liquidity of this stock itself and of the information about other stocks, we introduce a self- and a cross-impact function of the time lag. We model the average cross-response functions for individual stocks employing the impact functions of the time lag, the impact functions of traded volumes and the trade-sign correlators. To quantify the self- and cross-impacts, we propose a construction to fix the parameters in the impact functions. These parameters are further corroborated by a diffusion function that measures the correlated motion of prices from different stocks. This construction is mainly ad hoc and alternative ones are not excluded. It turns out that both the sign cross- and self-correlators are connected with the cross-responses. The self- and cross-impact functions are indispensable to compensate amplification effects which are due to the sign correlators integrated over time. We further quantify and interpret the price impacts of time lag in terms of temporary and permanent components. To support our model, we also analyze empirical data, in particular the memory properties of the sign self- and average cross-correlators. The relation between the average cross-responses and the traded volumes which are smaller than their average is of power-law form.",
                    "We investigate how the local fluctuations of the signed traded volumes affect the dependence of demands between stocks. We analyze the empirical dependence of demands using copulas and show that they are well described by a bivariate $\\mathcal{K}$ copula density function. We find that large local fluctuations strongly increase the positive dependence but lower slightly the negative one in the copula density. This interesting feature is due to cross-correlations of volume imbalances between stocks. Also, we explore the asymmetries of tail dependencies of the copula density, which are moderate for the negative dependencies but strong for the positive ones. For the latter, we reveal that large local fluctuations of the signed traded volumes trigger stronger dependencies of demands than of supplies, probably indicating a bull market with persistent raising of prices.",
                    "Big data streams are grasping increasing attention with the development of modern science and information technology. Due to the incompatibility of limited computer memory to high volume of streaming data, real-time methods without historical data storage is worth investigating. Moreover, outliers may occur with high velocity data streams generating, calling for more robust analysis. Motivated by these concerns, a novel Online Updating Huber Robust Regression algorithm is proposed in this paper. By extracting key features of new data subsets, it obtains a computational efficient online updating estimator without historical data storage. Meanwhile, by integrating Huber regression into the framework, the estimator is robust to contaminated data streams, such as heavy-tailed or heterogeneous distributed ones as well as cases with outliers. Moreover, the proposed online updating estimator is asymptotically equivalent to Oracle estimator obtained by the entire data and has a lower computation complexity. Extensive numerical simulations and a real data analysis are also conducted to evaluate the estimation and calculation efficiency of the proposed method.",
                    "Mining subgraphs with interesting structural properties from networks (or graphs) is a computationally challenging task. In this paper, we propose two algorithms for enumerating all connected induced subgraphs of a given cardinality from networks (or connected undirected graphs in networks). The first algorithm is a variant of a previous well-known algorithm. The algorithm enumerates all connected induced subgraphs of cardinality $k$ in a bottom-up manner. The data structures that lead to unit time element checking and linear space are presented. Different from previous algorithms that either work in a bottom-up manner or a reverse search manner, an algorithm that enumerates all connected induced subgraphs of cardinality $k$ in a top-down manner is proposed. The correctness and complexity of the top-down algorithm are theoretically analyzed and proven. In the experiments, we evaluate the efficiency of the algorithms using a set of real-world networks from various fields. Experimental results show that the variant bottom-up algorithm outperforms the state-of-the-art algorithms for enumerating connected induced subgraphs of small cardinality, and the top-down algorithm can achieve an order of magnitude speedup over the state-of-the-art algorithms for enumerating connected induced subgraphs of large cardinality.",
                    "The Honeycomb Conjecture states that among tilings with unit area cells in the Euclidean plane, the average perimeter of a cell is minimal for a regular hexagonal tiling. This conjecture was proved by L. Fejes T\\'oth for convex tilings, and by Hales for not necessarily convex tilings. In this paper we investigate the same question for tilings of a given normed plane, and show that among normal, convex tilings in a normed plane, the average squared perimeter of a cell is minimal for a tiling whose cells are translates of a centrally symmetric hexagon. We also show that the question whether the same statement is true for the average perimeter of a cell is closely related to an $\\alpha$-convex variant of a theorem of Dowker on the area of polygons circumscribed about a convex disk. Exploring this connection we find families of norms in which the average perimeter of a cell of a tiling is minimal for a hexagonal tiling, and prove some additonal related results. Finally, we apply our method to give a partial answer to a problem of Steinhaus about the isoperimetric ratios of cells of certain tilings in the Euclidean plane, appeared in an open problem book of Croft, Falconer and Guy.",
                    "This paper introduces a curated dataset of urban scenes for audio-visual scene analysis which consists of carefully selected and recorded material. The data was recorded in multiple European cities, using the same equipment, in multiple locations for each scene, and is openly available. We also present a case study for audio-visual scene recognition and show that joint modeling of audio and visual modalities brings significant performance gain compared to state of the art uni-modal systems. Our approach obtained an 84.8% accuracy compared to 75.8% for the audio-only and 68.4% for the video-only equivalent systems.",
                    "We develop a delay-adaptive controller for a class of first-order hyperbolic partial integro-differential equations (PIDEs) with an unknown input delay. By employing a transport PDE to represent delayed actuator states, the system is transformed into a transport partial differential equation (PDE) with unknown propagation speed cascaded with a PIDE. A parameter update law is designed using a Lyapunov argument and the infinite-dimensional backstepping technique to establish global stability results. Furthermore, the well-posedness of the closed-loop system is analyzed. Finally, the effectiveness of the proposed method was validated through numerical simulations",
                    "The classical functional linear regression model (FLM) and its extensions, which are based on the assumption that all individuals are mutually independent, have been well studied and are used by many researchers. This independence assumption is sometimes violated in practice, especially when data with a network structure are collected in scientific disciplines including marketing, sociology and spatial economics. However, relatively few studies have examined the applications of FLM to data with network structures. We propose a novel spatial functional linear model (SFLM), that incorporates a spatial autoregressive parameter and a spatial weight matrix into FLM to accommodate spatial dependencies among individuals. The proposed model is relatively flexible as it takes advantage of FLM in handling high-dimensional covariates and spatial autoregressive (SAR) model in capturing network dependencies. We develop an estimation method based on functional principal component analysis (FPCA) and maximum likelihood estimation. Simulation studies show that our method performs as well as the FPCA-based method used with FLM when no network structure is present, and outperforms the latter when network structure is present. A real weather data is also employed to demonstrate the utility of the SFLM.",
                    "Triplet loss, one of the deep metric learning (DML) methods, is to learn the embeddings where examples from the same class are closer than examples from different classes. Motivated by DML, we propose an effective BP-Triplet Loss for unsupervised domain adaption (UDA) from the perspective of Bayesian learning and we name the model as BP-Triplet Net. In previous metric learning based methods for UDA, sample pairs across domains are treated equally, which is not appropriate due to the domain bias. In our work, considering the different importance of pair-wise samples for both feature learning and domain alignment, we deduce our BP-Triplet loss for effective UDA from the perspective of Bayesian learning. Our BP-Triplet loss adjusts the weights of pair-wise samples in intra domain and inter domain. Especially, it can self attend to the hard pairs (including hard positive pair and hard negative pair). Together with the commonly used adversarial loss for domain alignment, the quality of target pseudo labels is progressively improved. Our method achieved low joint error of the ideal source and target hypothesis. The expected target error can then be upper bounded following Ben-David s theorem. Comprehensive evaluations on five benchmark datasets, handwritten digits, Office31, ImageCLEF-DA, Office-Home and VisDA-2017 demonstrate the effectiveness of the proposed approach for UDA.",
                    "Previous studies of the stock price response to individual trades focused on single stocks. We empirically investigate the price response of one stock to the trades of other stocks. How large is the impact of one stock on others and vice versa? -- This impact of trades on the price change across stocks appears to be transient instead of permanent. Performing different averages, we distinguish active and passive responses. The two average responses show different characteristic dependences on the time lag. The passive response exhibits a shorter response period with sizeable volatilities, and the active response a longer period. We also study the response for a given stock with respect to different sectors and to the whole market. Furthermore, we compare the self-response with the various cross-responses. The correlation of the trade signs is a short-memory process for a pair of stocks, but it turns into a long-memory process when averaged over different pairs of stocks.",
                    "Previous studies of the stock price response to trades focused on the dynamics of single stocks, i.e. they addressed the self-response. We empirically investigate the price response of one stock to the trades of other stocks in a correlated market, i.e. the cross-responses. How large is the impact of one stock on others and vice versa? -- This impact of trades on the price change across stocks appears to be transient instead of permanent as we discuss from the viewpoint of market efficiency. Furthermore, we compare the self-responses on different scales and the self- and cross-responses on the same scale. We also find that the cross-correlation of the trade signs turns out to be a short-memory process.",
                    "There are non-vanishing price responses across different stocks in correlated financial markets. We further study this issue by performing different averages, which identify active and passive cross-responses. The two average cross-responses show different characteristic dependences on the time lag. The passive cross-response exhibits a shorter response period with sizeable volatilities, while the corresponding period for the active cross-response is longer. The average cross-responses for a given stock are evaluated either with respect to the whole market or to different sectors. Using the response strength, the influences of individual stocks are identified and discussed. Moreover, the various cross-responses as well as the average cross-responses are compared with the self-responses. In contrast, the short memory of trade sign cross-correlation for stock pairs, the sign cross-correlation has long memory when averaged over different pairs of stocks.",
                    "The price impact for a single trade is estimated by the immediate response on an event time scale, i.e., the immediate change of midpoint prices before and after a trade. We work out the price impacts across a correlated financial market. We quantify the asymmetries of the distributions and of the market structures of cross-impacts, and find that the impacts across the market are asymmetric and non-random. Using spectral statistics and Shannon entropy, we visualize the asymmetric information in price impacts. Also, we introduce an entropy of impacts to estimate the randomness between stocks. We show that the useful information is encoded in the impacts corresponding to small entropy. The stocks with large number of trades are more likely to impact others, while the less traded stocks have higher probability to be impacted by others.",
                    "We empirically analyze the price and liquidity responses to trade signs, traded volumes and signed traded volumes. Utilizing the singular value decomposition, we explore the interconnections of price responses and of liquidity responses across the whole market. The statistical characteristics of their singular vectors are well described by the $t$ location-scale distribution. Furthermore, we discuss the relation between prices and liquidity with respect to their overlapping factors. The factors of price and liquidity changes are non-random when these factors are related to the traded volumes. This means that the traded volumes play a critical role in the price change induced by the liquidity change. In contrast, the two kinds of factors are weakly overlapping when they are related to the trade signs and signed traded volumes. Hence, an imbalance of liquidity is related to the price change.",
                    "Mixed spatial autoregressive (SAR) models with numerical covariates have been well studied. However, as non-numerical data, such as functional data and compositional data, receive substantial amounts of attention and are applied to economics, medicine and meteorology, it becomes necessary to develop flexible SAR models with multiple data types. In this article, we integrate three types of covariates, functional, compositional and numerical, in an SAR model. The new model has the merits of classical functional linear models and compositional linear models with scalar responses. Moreover, we develop an estimation method for the proposed model, which is based on functional principal component analysis (FPCA), the isometric logratio (ilr) transformation and the maximum likelihood estimation method. Monte Carlo experiments demonstrate the effectiveness of the estimators. A real dataset is also used to illustrate the utility of the proposed model."
                ],
                "domain": [
                    "Statistical Modeling",
                    "Financial Markets",
                    "Machine Learning",
                    "Domain Adaptation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "1017f538-2e43-48f5-abec-271eb5658c8a": {
                "pk": "1017f538-2e43-48f5-abec-271eb5658c8a",
                "project_name": null,
                "name": "Kang Li",
                "bio": "I am a researcher with a strong focus on the intersection of quantum mechanics, electromagnetic theory, and machine learning applications in imaging. My recent work has delved into the duality of electromagnetic fields, revealing how electric and magnetic charges are interdependent under specific conditions. I have also explored the potential of masked autoencoders (MAE) in ultrasound imaging, proposing a novel deblurring approach that significantly enhances classification performance.\n\nMy research extends into non-commutative quantum mechanics, where I have investigated various effects such as the Aharonov-Casher and Aharonov-Bohm effects, employing innovative methods like Bopp's shift to derive new insights into quantum phases. Additionally, I have contributed to the understanding of reduced C*-algebras associated with étale groupoids, establishing important connections between algebraic structures and topological properties.\n\nI am particularly interested in the practical applications of my theoretical findings, such as optimizing microgrid scheduling through demand response from electric vehicles, which balances operational costs and user participation. My work aims to bridge theoretical physics with real-world applications, fostering advancements in both fields. Through my research, I strive to uncover deeper connections within the fabric of quantum mechanics and its implications for technology and energy systems.",
                "collaborators": [
                    "Jianhua Wang",
                    "Sayipjamal Dulat",
                    "Qingbo Kang",
                    "Jun Gao",
                    "Qicheng Lao",
                    "Yang Li",
                    "Christian Bönicke",
                    "Xiaomin Yu",
                    "Carlos Naon"
                ],
                "pub_titles": [
                    "Comments on the dependence between electric charge and magnetic charge",
                    "Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition",
                    "The HMW effect in Noncommutative Quantum Mechanics",
                    "The topological AC effect on noncommutative phase space",
                    "Incorporating demand response of electric vehicles in scheduling of isolated microgrids with renewables using a bi-level programming approach",
                    "Ideal structure and pure infiniteness of ample groupoid $C^*$-algebras",
                    "Two-dimensional Noncommutative atom Gas with Anandan interaction",
                    "Equivalent bosonic theory for the massive Thirring model with non-local interaction",
                    "Commutator Anomaly in Noncommutative Quantum Mechanics",
                    "The Aharonov-Bohm Effect in Noncommutative Quantum Mechanics"
                ],
                "pub_abstracts": [
                    "By using the two 4-dimensional potential formulation of electromagnetic (EM) field theory introduced in [1], we found that the SO(2) duality symmetric EM field theory can be reduced to the magnetic source free case by a special choice of SO(2) parameter,this special case we called nature picture of the EM field theory, the reduction condition led to a result, i.e. the electric charge and magnetic charge are no more independent. Some comments to paper [SD] are also mentioned.",
                    "Masked autoencoder (MAE) has attracted unprecedented attention and achieves remarkable performance in many vision tasks. It reconstructs random masked image patches (known as proxy task) during pretraining and learns meaningful semantic representations that can be transferred to downstream tasks. However, MAE has not been thoroughly explored in ultrasound imaging. In this work, we investigate the potential of MAE for ultrasound image recognition. Motivated by the unique property of ultrasound imaging in high noise-to-signal ratio, we propose a novel deblurring MAE approach that incorporates deblurring into the proxy task during pretraining. The addition of deblurring facilitates the pretraining to better recover the subtle details presented in the ultrasound images, thus improving the performance of the downstream classification task. Our experimental results demonstrate the effectiveness of our deblurring MAE, achieving state-of-the-art performance in ultrasound image classification. Overall, our work highlights the potential of MAE for ultrasound image recognition and presents a novel approach that incorporates deblurring to further improve its effectiveness.",
                    "The HMW effect in non-commutative quantum mechanics is studied. By solving the Dirac equations on non-commutative (NC) space and non-commutative phase space, we obtain topological HMW phase on NC space and NC phase space respectively, where the additional terms related to the space-space and momentum-momentum non-commutativity are given explicitly.",
                    "The Aharonov-Casher (AC) effect in non-commutative(NC) quantum mechanics is studied. Instead of using the star product method, we use a generalization of Bopp's shift method. After solving the Dirac equations both on noncommutative space and noncommutative phase space by the new method, we obtain the corrections to AC phase on NC space and NC phase space respectively.",
                    "In this work, a novel optimal scheduling approach is proposed for isolated microgrids (MGs) with renewable generations by incorporating demand response of electric vehicles (EVs). First, a bi-level programming-based MG scheduling model is proposed under real-time pricing environments, where the upper- and lower- levels seek to minimize the MG net operating cost and the EV charging cost. Second, a hybrid solution algorithm called JAYA-interior point method is put forward to solve the model. And finally, the simulation results demonstrate that incorporating demand response of electric vehicles is able to guide EV users to actively participate in MG scheduling and achieve the peak load shaving, which offers a fundamental way to balance the interests between MG and EV users.",
                    "In this paper, we study the ideal structure of reduced $C^*$-algebras $C^*_r(G)$ associated to \\'etale groupoids $G$. In particular, we characterize when there is a one-to-one correspondence between the closed, two-sided ideals in $C_r^*(G)$ and the open invariant subsets of the unit space $G^{(0)}$ of $G$. As a consequence, we show that if $G$ is an inner exact, essentially principal, ample groupoid, then $C_r^*(G)$ is (strongly) purely infinite if and only if every non-zero projection in $C_0(G^{(0)})$ is properly infinite in $C_r^*(G)$. We also establish a sufficient condition on the ample groupoid $G$ that ensures pure infiniteness of $C_r^*(G)$ in terms of paradoxicality of compact open subsets of the unit space $G^{(0)}$.   Finally, we introduce the type semigroup for ample groupoids and also obtain a dichotomy result: Let $G$ be an ample groupoid with compact unit space which is minimal and topologically principal. If the type semigroup is almost unperforated, then $C_r^*(G)$ is a simple $C^*$-algebra which is either stably finite or strongly purely infinite.",
                    "Landau like quantization of the Anandan system in a special electromagnetic field is studied. Unlike the cases of the AC system and the HMW system, the torques of the system on the magnetic dipole and the electric dipole don't vanish. By constructing Heisenberg algebra, the Landau analog levels and eigenstates on commutative space, NC space and NC phase space are obtained respectively. By using the coherent state method, some statistical properties of such free atom gas are studied and the expressions of some thermodynamic quantities related to revolution direction are obtained. Two particular cases of temperature are discussed and the more simple expressions of the free energy on the three spaces are obtained. We give the relation between the value of $\\sigma$ and revolution direction clearly, and find Landau like levels of the Anandan system are invariant and the levels between the AC system and the HMW system are interchanged each other under Maxwell dual transformations on the three spaces. The two sets of eigenstates labelled by $\\sigma$ can be related by a supersymmetry transformation on commutative space, but the phenomenon don't occur on NC situation. We emphasize that some results relevant to Anandan interaction are suitable for the cases of AC interaction and HMW interaction under special conditions.",
                    "We study, through path-integral methods, an extension of the massive Thirring model in which the interaction between currents is non-local. By examining the mass-expansion of the partition function we show that this non-local massive Thirring model is equivalent to a certain non-local extension of the sine-Gordon theory. Thus, we establish a non-local generalization of the famous Coleman's equivalence. We also discuss some possible applications of this result in the context of one-dimensional strongly correlated systems and finite-size Quantum Field Theories.",
                    "In this letter, firstly, the Schr$\\ddot{o}$dinger equation on noncommutative phase space is given by using a generalized Bopp's shift. Then the anomaly term of commutator of arbitrary physical observable operators on noncommutative phase space is obtained. Finally, the basic uncertainty relations for space-space and space-momentum as well as momentum-momentum operators in noncommutative quantum mechanics (NCQM), and uncertainty relation for arbitrary physical observable operators in NCQM are discussed.",
                    "The Aharonov-Bohm (AB) effect in non-commutative quantum mechanics (NCQM) is studied. First, by introducing a shift for the magnetic vector potential we give the Schr$\\ddot{o}$dinger equations in the presence of a magnetic field on NC space and NC phase space, respectively. Then by solving the Schr$\\ddot{o}$dinger equations, we obtain the Aharonov-Bohm (AB) phase on NC space and NC phase space, respectively."
                ],
                "domain": [
                    "Quantum Mechanics",
                    "Non-Commutative Geometry",
                    "Machine Learning",
                    "Electromagnetic Theory"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "3e97e9f9-19cb-4e9c-8046-c9ac6ba106c1": {
                "pk": "3e97e9f9-19cb-4e9c-8046-c9ac6ba106c1",
                "project_name": null,
                "name": "Haofen Wang",
                "bio": "I am a researcher dedicated to advancing the fields of natural language processing and artificial intelligence, with a particular focus on dialogue systems, recommendation systems, and knowledge representation. My recent work has centered on developing innovative frameworks that enhance the capabilities of medical dialogue systems and improve the efficiency of medicine recommendation through the integration of electronic medical records and medical knowledge graphs.\n\nOne of my notable contributions is the introduction of a multi-hierarchical state structure for dialogue state tracking, which addresses the complexities of interpreting medical conversations. I also pioneered the Safe Medicine Recommendation (SMR) framework, which leverages graph embedding techniques to create a robust patient-disease-medicine graph for more accurate recommendations.\n\nIn addition to my work in healthcare, I have explored the potential of retrieval-augmented generation (RAG) systems, proposing a modular RAG framework that enhances the integration of retrieval and generation processes. My research also delves into the optimization of prompt engineering for text-to-image models, ensuring that user inputs align with model preferences for better outcomes.\n\nI am passionate about bridging the gap between AI technologies and real-world applications, as evidenced by my work on conversational recommender systems and the development of the Materials Knowledge Graph, which organizes extensive scientific literature for efficient material discovery. My goal is to create systems that not only perform well but also enhance user experience and understanding in various domains.",
                "collaborators": [
                    "Meng Wang",
                    "Yun Xiong",
                    "Yunfan Gao",
                    "Wenqiang Zhang",
                    "Huifang Du",
                    "Xuejing Feng",
                    "Qianyu Guo",
                    "Xinyu Gao",
                    "Tao Sheng",
                    "Yuan-Fang Li"
                ],
                "pub_titles": [
                    "Prompt-based Generative Approach towards Multi-Hierarchical Medical Dialogue State Tracking",
                    "SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation",
                    "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
                    "A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis",
                    "Large Language Model Enhanced Knowledge Representation Learning: A Survey",
                    "KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation",
                    "Preference-Guided Refactored Tuning for Retrieval Augmented Code Generation",
                    "ReCo: A Dataset for Residential Community Layout Planning",
                    "Adversarial Learning for Chinese NER from Crowd Annotations",
                    "A Graph Traversal Based Approach to Answer Non-Aggregation Questions Over DBpedia",
                    "Plug-and-Play Feature Generation for Few-Shot Medical Image Classification",
                    "Rewarding What Matters: Step-by-Step Reinforcement Learning for Task-Oriented Dialogue",
                    "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
                    "Revealing Secrets in SPARQL Session Level",
                    "Towards Proactive Interactions for In-Vehicle Conversational Assistants Utilizing Large Language Models",
                    "Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model",
                    "Development and Evaluation Study of Intelligent Cockpit in the Age of Large Models",
                    "Retrieval-Augmented Generation for Large Language Models: A Survey"
                ],
                "pub_abstracts": [
                    "The medical dialogue system is a promising application that can provide great convenience for patients. The dialogue state tracking (DST) module in the medical dialogue system which interprets utterances into the machine-readable structure for downstream tasks is particularly challenging. Firstly, the states need to be able to represent compound entities such as symptoms with their body part or diseases with degrees of severity to provide enough information for decision support. Secondly, these named entities in the utterance might be discontinuous and scattered across sentences and speakers. These also make it difficult to annotate a large corpus which is essential for most methods. Therefore, we first define a multi-hierarchical state structure. We annotate and publish a medical dialogue dataset in Chinese. To the best of our knowledge, there are no publicly available ones before. Then we propose a Prompt-based Generative Approach which can generate slot values with multi-hierarchies incrementally using a top-down approach. A dialogue style prompt is also supplemented to utilize the large unlabeled dialogue corpus to alleviate the data scarcity problem. The experiments show that our approach outperforms other DST methods and is rather effective in the scenario with little data.",
                    "Most of the existing medicine recommendation systems that are mainly based on electronic medical records (EMRs) are significantly assisting doctors to make better clinical decisions benefiting both patients and caregivers. Even though the growth of EMRs is at a lighting fast speed in the era of big data, content limitations in EMRs restrain the existed recommendation systems to reflect relevant medical facts, such as drug-drug interactions. Many medical knowledge graphs that contain drug-related information, such as DrugBank, may give hope for the recommendation systems. However, the direct use of these knowledge graphs in the systems suffers from robustness caused by the incompleteness of the graphs. To address these challenges, we stand on recent advances in graph embedding learning techniques and propose a novel framework, called Safe Medicine Recommendation (SMR), in this paper. Specifically, SMR first constructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and medical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly embeds diseases, medicines, patients, and their corresponding relations into a shared lower dimensional space. Finally, SMR uses the embeddings to decompose the medicine recommendation into a link prediction process while considering the patient's diagnoses and adverse drug reactions. To our best knowledge, SMR is the first to learn embeddings of a patient-disease-medicine graph for medicine recommendation in the world. Extensive experiments on real datasets are conducted to evaluate the effectiveness of proposed framework.",
                    "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of \"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
                    "Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.",
                    "The integration of Large Language Models (LLM) with Knowledge Representation Learning (KRL) signifies a significant advancement in the field of artificial intelligence (AI), enhancing the ability to capture and utilize both structure and textual information. Despite the increasing research on enhancing KRL with LLMs, a thorough survey that analyse processes of these enhanced models is conspicuously absent. Our survey addresses this by categorizing these models based on three distinct Transformer architectures, and by analyzing experimental data from various KRL downstream tasks to evaluate the strengths and weaknesses of each approach. Finally, we identify and explore potential future research directions in this emerging yet underexplored domain.",
                    "Commit messages are natural language descriptions of code changes, which are important for software evolution such as code understanding and maintenance. However, previous methods are trained on the entire dataset without considering the fact that a portion of commit messages adhere to good practice (i.e., good-practice commits), while the rest do not. On the basis of our empirical study, we discover that training on good-practice commits significantly contributes to the commit message generation. Motivated by this finding, we propose a novel knowledge-aware denoising learning method called KADEL. Considering that good-practice commits constitute only a small proportion of the dataset, we align the remaining training samples with these good-practice commits. To achieve this, we propose a model that learns the commit knowledge by training on good-practice commits. This knowledge model enables supplementing more information for training samples that do not conform to good practice. However, since the supplementary information may contain noise or prediction errors, we propose a dynamic denoising training method. This method composes a distribution-aware confidence function and a dynamic distribution list, which enhances the effectiveness of the training process. Experimental results on the whole MCMD dataset demonstrate that our method overall achieves state-of-the-art performance compared with previous methods. Our source code and data are available at https://github.com/DeepSoftwareAnalytics/KADEL",
                    "Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.   To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28% on EM, 13% on BLEU, and 6.8% on CodeBLEU.",
                    "Layout planning is centrally important in the field of architecture and urban design. Among the various basic units carrying urban functions, residential community plays a vital part for supporting human life. Therefore, the layout planning of residential community has always been of concern, and has attracted particular attention since the advent of deep learning that facilitates the automated layout generation and spatial pattern recognition. However, the research circles generally suffer from the insufficiency of residential community layout benchmark or high-quality datasets, which hampers the future exploration of data-driven methods for residential community layout planning. The lack of datasets is largely due to the difficulties of large-scale real-world residential data acquisition and long-term expert screening. In order to address the issues and advance a benchmark dataset for various intelligent spatial design and analysis applications in the development of smart city, we introduce Residential Community Layout Planning (ReCo) Dataset, which is the first and largest open-source vector dataset related to real-world community to date. ReCo Dataset is presented in multiple data formats with 37,646 residential community layout plans, covering 598,728 residential buildings with height information. ReCo can be conveniently adapted for residential community layout related urban design tasks, e.g., generative layout design, morphological pattern recognition and spatial evaluation. To validate the utility of ReCo in automated residential community layout planning, two Generative Adversarial Network (GAN) based generative models are further applied to the dataset. We expect ReCo Dataset to inspire more creative and practical work in intelligent design and beyond. The ReCo Dataset is published at: https://www.kaggle.com/fdudsde/reco-dataset.",
                    "To quickly obtain new labeled data, we can choose crowdsourcing as an alternative way at lower cost in a short time. But as an exchange, crowd annotations from non-experts may be of lower quality than those from experts. In this paper, we propose an approach to performing crowd annotation learning for Chinese Named Entity Recognition (NER) to make full use of the noisy sequence labels from multiple annotators. Inspired by adversarial learning, our approach uses a common Bi-LSTM and a private Bi-LSTM for representing annotator-generic and -specific information. The annotator-generic information is the common knowledge for entities easily mastered by the crowd. Finally, we build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we create two data sets for Chinese NER tasks from two domains. The experimental results show that our system achieves better scores than strong baseline systems.",
                    "We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems.",
                    "Few-shot learning (FSL) presents immense potential in enhancing model generalization and practicality for medical image classification with limited training data; however, it still faces the challenge of severe overfitting in classifier training due to distribution bias caused by the scarce training samples. To address the issue, we propose MedMFG, a flexible and lightweight plug-and-play method designed to generate sufficient class-distinctive features from limited samples. Specifically, MedMFG first re-represents the limited prototypes to assign higher weights for more important information features. Then, the prototypes are variationally generated into abundant effective features. Finally, the generated features and prototypes are together to train a more generalized classifier. Experiments demonstrate that MedMFG outperforms the previous state-of-the-art methods on cross-domain benchmarks involving the transition from natural images to medical images, as well as medical images with different lesions. Notably, our method achieves over 10% performance improvement compared to several baselines. Fusion experiments further validate the adaptability of MedMFG, as it seamlessly integrates into various backbones and baselines, consistently yielding improvements of over 2.9% across all results.",
                    "Reinforcement learning (RL) is a powerful approach to enhance task-oriented dialogue (TOD) systems. However, existing RL methods tend to mainly focus on generation tasks, such as dialogue policy learning (DPL) or response generation (RG), while neglecting dialogue state tracking (DST) for understanding. This narrow focus limits the systems to achieve globally optimal performance by overlooking the interdependence between understanding and generation. Additionally, RL methods face challenges with sparse and delayed rewards, which complicates training and optimization. To address these issues, we extend RL into both understanding and generation tasks by introducing step-by-step rewards throughout the token generation. The understanding reward increases as more slots are correctly filled in DST, while the generation reward grows with the accurate inclusion of user requests. Our approach provides a balanced optimization aligned with task completion. Experimental results demonstrate that our approach effectively enhances the performance of TOD systems and achieves new state-of-the-art results on three widely used datasets, including MultiWOZ2.0, MultiWOZ2.1, and In-Car. Our approach also shows superior few-shot ability in low-resource settings compared to current models.",
                    "Large language models (LLMs) have demonstrated their significant potential to be applied for addressing various application tasks. However, traditional recommender systems continue to face great challenges such as poor interactivity and explainability, which actually also hinder their broad deployment in real-world systems. To address these limitations, this paper proposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender System) that innovatively augments LLMs for building conversational recommender systems by converting user profiles and historical interactions into prompts. Chat-Rec is demonstrated to be effective in learning user preferences and establishing connections between users and products through in-context learning, which also makes the recommendation process more interactive and explainable. What's more, within the Chat-Rec framework, user's preferences can transfer to different products for cross-domain recommendations, and prompt-based injection of information into LLMs can also handle the cold-start scenarios with new items. In our experiments, Chat-Rec effectively improve the results of top-k recommendations and performs better in zero-shot rating prediction task. Chat-Rec offers a novel approach to improving recommender systems and presents new practical scenarios for the implementation of AIGC (AI generated content) in recommender system studies.",
                    "Based on Semantic Web technologies, knowledge graphs help users to discover information of interest by using live SPARQL services. Answer-seekers often examine intermediate results iteratively and modify SPARQL queries repeatedly in a search session. In this context, understanding user behaviors is critical for effective intention prediction and query optimization. However, these behaviors have not yet been researched systematically at the SPARQL session level. This paper reveals secrets of session-level user search behaviors by conducting a comprehensive investigation over massive real-world SPARQL query logs. In particular, we thoroughly assess query changes made by users w.r.t. structural and data-driven features of SPARQL queries. To illustrate the potentiality of our findings, we employ an application example of how to use our findings, which might be valuable to devise efficient SPARQL caching, auto-completion, query suggestion, approximation, and relaxation techniques in the future.",
                    "Research demonstrates that the proactivity of in-vehicle conversational assistants (IVCAs) can help to reduce distractions and enhance driving safety, better meeting users' cognitive needs. However, existing IVCAs struggle with user intent recognition and context awareness, which leads to suboptimal proactive interactions. Large language models (LLMs) have shown potential for generalizing to various tasks with prompts, but their application in IVCAs and exploration of proactive interaction remain under-explored. These raise questions about how LLMs improve proactive interactions for IVCAs and influence user perception. To investigate these questions systematically, we establish a framework with five proactivity levels across two dimensions-assumption and autonomy-for IVCAs. According to the framework, we propose a \"Rewrite + ReAct + Reflect\" strategy, aiming to empower LLMs to fulfill the specific demands of each proactivity level when interacting with users. Both feasibility and subjective experiments are conducted. The LLM outperforms the state-of-the-art model in success rate and achieves satisfactory results for each proactivity level. Subjective experiments with 40 participants validate the effectiveness of our framework and show the proactive level with strong assumptions and user confirmation is most appropriate.",
                    "Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges for efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques, integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs.",
                    "The development of Artificial Intelligence (AI) Large Models has a great impact on the application development of automotive Intelligent cockpit. The fusion development of Intelligent Cockpit and Large Models has become a new growth point of user experience in the industry, which also creates problems for related scholars, practitioners and users in terms of their understanding and evaluation of the user experience and the capability characteristics of the Intelligent Cockpit Large Models (ICLM). This paper aims to analyse the current situation of Intelligent cockpit, large model and AI Agent, to reveal the key of application research focuses on the integration of Intelligent Cockpit and Large Models, and to put forward a necessary limitation for the subsequent development of an evaluation system for the capability of automotive ICLM and user experience. The evaluation system, P-CAFE, proposed in this paper mainly proposes five dimensions of perception, cognition, action, feedback and evolution as the first-level indicators from the domains of cognitive architecture, user experience, and capability characteristics of large models, and many second-level indicators to satisfy the current status of the application and research focuses are selected. After expert evaluation, the weights of the indicators were determined, and the indicator system of P-CAFE was established. Finally, a complete evaluation method was constructed based on Fuzzy Hierarchical Analysis. It will lay a solid foundation for the application and evaluation of the automotive ICLM, and provide a reference for the development and improvement of the future ICLM.",
                    "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Knowledge Graph",
                    "Dialogue Systems",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "4aecdb80-f86f-4201-bb3c-25f6309def06": {
                "pk": "4aecdb80-f86f-4201-bb3c-25f6309def06",
                "project_name": null,
                "name": "Tong Ruan",
                "bio": "I am a researcher dedicated to advancing the field of medical natural language processing (NLP), with a particular focus on clinical terminology normalization, named entity recognition, and dialogue systems. My recent work has centered on developing innovative frameworks and models that enhance the accuracy and efficiency of processing medical texts, especially in the Chinese language context. \n\nOne of my notable contributions is the combined recall and rank framework for Chinese procedure terminology normalization, which significantly improves the identification of relevant medical terms. I have also pioneered a multi-hierarchical state structure for dialogue state tracking in medical dialogue systems, addressing the challenges of compound entities and data scarcity through a novel prompt-based generative approach.\n\nIn the realm of clinical named entity recognition (CNER), I have explored various architectures, including Residual Dilated Convolutional Neural Networks and Bi-directional LSTM models, to effectively capture contextual features and handle rare entities. My work on a recurrent capsule network for coronary artery disease severity classification exemplifies my commitment to applying advanced techniques to real-world medical challenges.\n\nAdditionally, I have contributed to the development of a continual pretraining method for domain-specific language models, addressing the issue of catastrophic forgetting. My recent projects also include constructing a comprehensive dataset for medical diagnostic decision-making and evaluating long-context capabilities of large language models in the medical domain.\n\nThrough my research, I aim to bridge the gap between advanced NLP techniques and practical applications in healthcare, ultimately improving clinical decision-making and patient outcomes.",
                "collaborators": [
                    "Yangming Zhou",
                    "Kui Xue",
                    "Qi Wang",
                    "Jiahui Qiu",
                    "Daqi Gao",
                    "Ping He",
                    "Huanhuan Zhang",
                    "Ju Gao",
                    "Zhiyuan Ma",
                    "Xiaoming Shi"
                ],
                "pub_titles": [
                    "A multi-perspective combined recall and rank framework for Chinese procedure terminology normalization",
                    "Prompt-based Generative Approach towards Multi-Hierarchical Medical Dialogue State Tracking",
                    "Fast and Accurate Recognition of Chinese Clinical Named Entities with Residual Dilated Convolutions",
                    "Incorporating Dictionaries into Deep Neural Networks for the Chinese Clinical Named Entity Recognition",
                    "Automatic Severity Classification of Coronary Artery Disease via Recurrent Capsule Network",
                    "An attention-based Bi-GRU-CapsNet model for hypernymy detection between compound entities",
                    "Question Answering based Clinical Text Structuring Using Pre-trained Language Model",
                    "Fine-tuning BERT for Joint Entity and Relation Extraction in Chinese Medical Text",
                    "AF Adapter: Continual Pretraining for Building Chinese Biomedical Language Model",
                    "MedDM:LLM-executable clinical guidance tree for clinical decision-making",
                    "MedOdyssey: A Medical Domain Benchmark for Long Context Evaluation Up to 200K Tokens"
                ],
                "pub_abstracts": [
                    "Medical terminology normalization aims to map the clinical mention to terminologies come from a knowledge base, which plays an important role in analyzing Electronic Health Record(EHR) and many downstream tasks. In this paper, we focus on Chinese procedure terminology normalization. The expression of terminologies are various and one medical mention may be linked to multiple terminologies. Previous study explores some methods such as multi-class classification or learning to rank(LTR) to sort the terminologies by literature and semantic information. However, these information is inadequate to find the right terminologies, particularly in multi-implication cases. In this work, we propose a combined recall and rank framework to solve the above problems. This framework is composed of a multi-task candidate generator(MTCG), a keywords attentive ranker(KAR) and a fusion block(FB). MTCG is utilized to predict the mention implication number and recall candidates with semantic similarity. KAR is based on Bert with a keywords attentive mechanism which focuses on keywords such as procedure sites and procedure types. FB merges the similarity come from MTCG and KAR to sort the terminologies from different perspectives. Detailed experimental analysis shows our proposed framework has a remarkable improvement on both performance and efficiency.",
                    "The medical dialogue system is a promising application that can provide great convenience for patients. The dialogue state tracking (DST) module in the medical dialogue system which interprets utterances into the machine-readable structure for downstream tasks is particularly challenging. Firstly, the states need to be able to represent compound entities such as symptoms with their body part or diseases with degrees of severity to provide enough information for decision support. Secondly, these named entities in the utterance might be discontinuous and scattered across sentences and speakers. These also make it difficult to annotate a large corpus which is essential for most methods. Therefore, we first define a multi-hierarchical state structure. We annotate and publish a medical dialogue dataset in Chinese. To the best of our knowledge, there are no publicly available ones before. Then we propose a Prompt-based Generative Approach which can generate slot values with multi-hierarchies incrementally using a top-down approach. A dialogue style prompt is also supplemented to utilize the large unlabeled dialogue corpus to alleviate the data scarcity problem. The experiments show that our approach outperforms other DST methods and is rather effective in the scenario with little data.",
                    "Clinical Named Entity Recognition (CNER) aims to identify and classify clinical terms such as diseases, symptoms, treatments, exams, and body parts in electronic health records, which is a fundamental and crucial task for clinical and translation research. In recent years, deep learning methods have achieved significant success in CNER tasks. However, these methods depend greatly on Recurrent Neural Networks (RNNs), which maintain a vector of hidden activations that are propagated through time, thus causing too much time to train models. In this paper, we propose a Residual Dilated Convolutional Neural Network with Conditional Random Field (RD-CNN-CRF) to solve it. Specifically, Chinese characters and dictionary features are first projected into dense vector representations, then they are fed into the residual dilated convolutional neural network to capture contextual features. Finally, a conditional random field is employed to capture dependencies between neighboring tags. Computational results on the CCKS-2017 Task 2 benchmark dataset show that our proposed RD-CNN-CRF method competes favorably with state-of-the-art RNN-based methods both in terms of computational performance and training time.",
                    "Clinical Named Entity Recognition (CNER) aims to identify and classify clinical terms such as diseases, symptoms, treatments, exams, and body parts in electronic health records, which is a fundamental and crucial task for clinical and translational research. In recent years, deep neural networks have achieved significant success in named entity recognition and many other Natural Language Processing (NLP) tasks. Most of these algorithms are trained end to end, and can automatically learn features from large scale labeled datasets. However, these data-driven methods typically lack the capability of processing rare or unseen entities. Previous statistical methods and feature engineering practice have demonstrated that human knowledge can provide valuable information for handling rare and unseen cases. In this paper, we address the problem by incorporating dictionaries into deep neural networks for the Chinese CNER task. Two different architectures that extend the Bi-directional Long Short-Term Memory (Bi-LSTM) neural network and five different feature representation schemes are proposed to handle the task. Computational results on the CCKS-2017 Task 2 benchmark dataset show that the proposed method achieves the highly competitive performance compared with the state-of-the-art deep learning methods.",
                    "Coronary artery disease (CAD) is one of the leading causes of cardiovascular disease deaths. CAD condition progresses rapidly, if not diagnosed and treated at an early stage may eventually lead to an irreversible state of the heart muscle death. Invasive coronary arteriography is the gold standard technique for CAD diagnosis. Coronary arteriography texts describe which part has stenosis and how much stenosis is in details. It is crucial to conduct the severity classification of CAD. In this paper, we employ a recurrent capsule network (RCN) to extract semantic relations between clinical named entities in Chinese coronary arteriography texts, through which we can automatically find out the maximal stenosis for each lumen to inference how severe CAD is according to the improved method of Gensini. Experimental results on the corpus collected from Shanghai Shuguang Hospital show that our proposed method achieves an accuracy of 97.0\\% in the severity classification of CAD.",
                    "Named entities are usually composable and extensible. Typical examples are names of symptoms and diseases in medical areas. To distinguish these entities from general entities, we name them \\textit{compound entities}. In this paper, we present an attention-based Bi-GRU-CapsNet model to detect hypernymy relationship between compound entities. Our model consists of several important components. To avoid the out-of-vocabulary problem, English words or Chinese characters in compound entities are fed into the bidirectional gated recurrent units. An attention mechanism is designed to focus on the differences between the two compound entities. Since there are some different cases in hypernymy relationship between compound entities, capsule network is finally employed to decide whether the hypernymy relationship exists or not. Experimental results demonstrate",
                    "Clinical text structuring is a critical and fundamental task for clinical research. Traditional methods such as taskspecific end-to-end models and pipeline models usually suffer from the lack of dataset and error propagation. In this paper, we present a question answering based clinical text structuring (QA-CTS) task to unify different specific tasks and make dataset shareable. A novel model that aims to introduce domain-specific features (e.g., clinical named entity information) into pre-trained language model is also proposed for QA-CTS task. Experimental results on Chinese pathology reports collected from Ruijing Hospital demonstrate our presented QA-CTS task is very effective to improve the performance on specific tasks. Our proposed model also competes favorably with strong baseline models in specific tasks.",
                    "Entity and relation extraction is the necessary step in structuring medical text. However, the feature extraction ability of the bidirectional long short term memory network in the existing model does not achieve the best effect. At the same time, the language model has achieved excellent results in more and more natural language processing tasks. In this paper, we present a focused attention model for the joint entity and relation extraction task. Our model integrates well-known BERT language model into joint learning through dynamic range attention mechanism, thus improving the feature representation ability of shared parameter layer. Experimental results on coronary angiography texts collected from Shuguang Hospital show that the F1-score of named entity recognition and relation classification tasks reach 96.89% and 88.51%, which are better than state-of-the-art methods 1.65% and 1.22%, respectively.",
                    "Continual pretraining is a popular way of building a domain-specific pretrained language model from a general-domain language model. In spite of its high efficiency, continual pretraining suffers from catastrophic forgetting, which may harm the model's performance in downstream tasks. To alleviate the issue, in this paper, we propose a continual pretraining method for the BERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a small number of attention heads and hidden units inside each self-attention layer and feed-forward network. Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain. In experiments, models are applied to downstream tasks for evaluation. The results demonstrate that with only about 17% of model parameters trained, AF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong baselines. Further experimental results show that our method alleviates the catastrophic forgetting problem by 11% compared to the fine-tuning method.",
                    "It is becoming increasingly emphasis on the importance of LLM participating in clinical diagnosis decision-making. However, the low specialization refers to that current medical LLMs can not provide specific medical advice, which are more like a medical Q\\&A. And there is no suitable clinical guidance tree data set that can be used directly with LLM. To address this issue, we first propose LLM-executavle clinical guidance tree(CGT), which can be directly used by large language models, and construct medical diagnostic decision-making dataset (MedDM), from flowcharts in clinical practice guidelines. We propose an approach to screen flowcharts from medical literature, followed by their identification and conversion into standardized diagnostic decision trees. Constructed a knowledge base with 1202 decision trees, which came from 5000 medical literature and covered 12 hospital departments, including internal medicine, surgery, psychiatry, and over 500 diseases.Moreover, we propose a method for reasoning on LLM-executable CGT and a Patient-LLM multi-turn dialogue framework.",
                    "Numerous advanced Large Language Models (LLMs) now support context lengths up to 128K, and some extend to 200K. Some benchmarks in the generic domain have also followed up on evaluating long-context capabilities. In the medical domain, tasks are distinctive due to the unique contexts and need for domain expertise, necessitating further evaluation. However, despite the frequent presence of long texts in medical scenarios, evaluation benchmarks of long-context capabilities for LLMs in this field are still rare. In this paper, we propose MedOdyssey, the first medical long-context benchmark with seven length levels ranging from 4K to 200K tokens. MedOdyssey consists of two primary components: the medical-context \"needles in a haystack\" task and a series of tasks specific to medical applications, together comprising 10 datasets. The first component includes challenges such as counter-intuitive reasoning and novel (unknown) facts injection to mitigate knowledge leakage and data contamination of LLMs. The second component confronts the challenge of requiring professional medical expertise. Especially, we design the ``Maximum Identical Context'' principle to improve fairness by guaranteeing that different LLMs observe as many identical contexts as possible. Our experiment evaluates advanced proprietary and open-source LLMs tailored for processing long contexts and presents detailed performance analyses. This highlights that LLMs still face challenges and need for further research in this area. Our code and data are released in the repository: \\url{https://github.com/JOHNNY-fans/MedOdyssey.}"
                ],
                "domain": [
                    "Natural Language Processing",
                    "Medical Informatics",
                    "Named Entity Recognition",
                    "Dialogue Systems"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "9dcb4372-dadf-48c0-82da-91275b233f45": {
                "pk": "9dcb4372-dadf-48c0-82da-91275b233f45",
                "project_name": null,
                "name": "Xuanjing Huang",
                "bio": "I am a dedicated researcher in the field of natural language processing (NLP) with a strong focus on multi-task learning and neural network architectures. My work has primarily revolved around enhancing the performance of NLP models by leveraging shared knowledge across tasks. For instance, I developed Parameters Read-Write Networks (PRaWNs) to address the entanglement of features in multi-task learning, allowing tasks to communicate more effectively through hidden variables and gradients.\n\nI have also explored innovative approaches to word sense disambiguation (WSD) by integrating gloss knowledge into BERT-based models, achieving state-of-the-art results. My research extends to sentence ordering, where I proposed an end-to-end neural approach that utilizes pointer networks to improve contextual understanding and reduce error propagation.\n\nIn addition, I have contributed to adversarial multi-task learning frameworks that separate shared and private latent feature spaces, demonstrating significant improvements across various text classification tasks. My work on dynamic compositional neural networks and Gaussian mixture embeddings has further advanced the understanding of syntactic and semantic representations in NLP.\n\nThrough my extensive experimentation and development of novel architectures, I aim to push the boundaries of what is possible in NLP, making my findings accessible to the research community. I am passionate about creating models that not only perform well but also provide insights into the underlying mechanisms of language understanding.",
                "collaborators": [
                    "Xipeng Qiu",
                    "Pengfei Liu",
                    "Xinchi Chen",
                    "Chi Sun",
                    "Jingjing Gong",
                    "Zhan Shi",
                    "Yige Xu",
                    "Luyao Huang",
                    "PengFei Liu",
                    "Jinyue Su"
                ],
                "pub_titles": [
                    "Meta-Learning Multi-task Communication",
                    "GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge",
                    "Overview of the NLPCC 2017 Shared Task: Chinese News Headline Categorization",
                    "Recurrent Neural Network for Text Classification with Multi-Task Learning",
                    "Syntax-based Attention Model for Natural Language Inference",
                    "Neural Sentence Ordering",
                    "Deep Multi-Task Learning with Shared Memory",
                    "Adversarial Multi-Criteria Learning for Chinese Word Segmentation",
                    "Dynamic Compositional Neural Networks over Tree Structure",
                    "Incorporating Discriminator in Sentence Generation: a Gibbs Sampling Method",
                    "Deformable Stacked Structure for Named Entity Recognition",
                    "End-to-End Neural Sentence Ordering Using Pointer Network",
                    "DAG-based Long Short-Term Memory for Neural Word Segmentation",
                    "VCWE: Visual Character-Enhanced Word Embeddings",
                    "How to Fine-Tune BERT for Text Classification?",
                    "Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation",
                    "A Re-ranking Model for Dependency Parser with Recursive Convolutional Neural Network",
                    "Gaussian Mixture Embeddings for Multiple Word Prototypes",
                    "Modelling Interaction of Sentence Pair with coupled-LSTMs",
                    "Adversarial Multi-task Learning for Text Classification"
                ],
                "pub_abstracts": [
                    "In this paper, we describe a general framework: Parameters Read-Write Networks (PRaWNs) to systematically analyze current neural models for multi-task learning, in which we find that existing models expect to disentangle features into different spaces while features learned in practice are still entangled in shared space, leaving potential hazards for other training or unseen tasks.   We propose to alleviate this problem by incorporating an inductive bias into the process of multi-task learning, that each task can keep informed of not only the knowledge stored in other tasks but the way how other tasks maintain their knowledge.   In practice, we achieve above inductive bias by allowing different tasks to communicate by passing both hidden variables and gradients explicitly.   Experimentally, we evaluate proposed methods on three groups of tasks and two types of settings (\\textsc{in-task} and \\textsc{out-of-task}). Quantitative and qualitative results show their effectiveness.",
                    "Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lexical resources like WordNet, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incorporating gloss (sense definition) into neural networks for WSD. However, compared with traditional word expert supervised methods, they have not achieved much improvement. In this paper, we focus on how to better leverage gloss knowledge in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT-based models for WSD. We fine-tune the pre-trained BERT model on SemCor3.0 training corpus and the experimental results on several English all-words WSD benchmark datasets show that our approach outperforms the state-of-the-art systems.",
                    "In this paper, we give an overview for the shared task at the CCF Conference on Natural Language Processing \\& Chinese Computing (NLPCC 2017): Chinese News Headline Categorization. The dataset of this shared task consists 18 classes, 12,000 short texts along with corresponded labels for each class. The dataset and example code can be accessed at https://github.com/FudanNLP/nlpcc2017_news_headline_categorization.",
                    "Neural network based methods have obtained great progress on a variety of natural language processing tasks. However, in most previous works, the models are learned based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we use the multi-task learning framework to jointly learn across multiple related tasks. Based on recurrent neural network, we propose three different mechanisms of sharing information to model text with task-specific and shared layers. The entire network is trained jointly on all these tasks. Experiments on four benchmark text classification tasks show that our proposed models can improve the performance of a task with the help of other related tasks.",
                    "Introducing attentional mechanism in neural network is a powerful concept, and has achieved impressive results in many natural language processing tasks. However, most of the existing models impose attentional distribution on a flat topology, namely the entire input representation sequence. Clearly, any well-formed sentence has its accompanying syntactic tree structure, which is a much rich topology. Applying attention to such topology not only exploits the underlying syntax, but also makes attention more interpretable. In this paper, we explore this direction in the context of natural language inference. The results demonstrate its efficacy. We also perform extensive qualitative analysis, deriving insights and intuitions of why and how our model works.",
                    "Sentence ordering is a general and critical task for natural language generation applications. Previous works have focused on improving its performance in an external, downstream task, such as multi-document summarization. Given its importance, we propose to study it as an isolated task. We collect a large corpus of academic texts, and derive a data driven approach to learn pairwise ordering of sentences, and validate the efficacy with extensive experiments. Source codes and dataset of this paper will be made publicly available.",
                    "Neural network based models have achieved impressive results on various specific tasks. However, in previous works, most models are learned separately based on single-task supervised objectives, which often suffer from insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks.",
                    "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.",
                    "Tree-structured neural networks have proven to be effective in learning semantic representations by exploiting syntactic information. In spite of their success, most existing models suffer from the underfitting problem: they recursively use the same shared compositional function throughout the whole compositional process and lack expressive power due to inability to capture the richness of compositionality. In this paper, we address this issue by introducing the dynamic compositional neural networks over tree structure (DC-TreeNN), in which the compositional function is dynamically generated by a meta network. The role of meta-network is to capture the metaknowledge across the different compositional rules and formulate them. Experimental results on two typical tasks show the effectiveness of the proposed models.",
                    "Generating plausible and fluent sentence with desired properties has long been a challenge. Most of the recent works use recurrent neural networks (RNNs) and their variants to predict following words given previous sequence and target label. In this paper, we propose a novel framework to generate constrained sentences via Gibbs Sampling. The candidate sentences are revised and updated iteratively, with sampled new words replacing old ones. Our experiments show the effectiveness of the proposed method to generate plausible and diverse sentences.",
                    "Neural architecture for named entity recognition has achieved great success in the field of natural language processing. Currently, the dominating architecture consists of a bi-directional recurrent neural network (RNN) as the encoder and a conditional random field (CRF) as the decoder. In this paper, we propose a deformable stacked structure for named entity recognition, in which the connections between two adjacent layers are dynamically established. We evaluate the deformable stacked structure by adapting it to different layers. Our model achieves the state-of-the-art performances on the OntoNotes dataset.",
                    "Sentence ordering is one of important tasks in NLP. Previous works mainly focused on improving its performance by using pair-wise strategy. However, it is nontrivial for pair-wise models to incorporate the contextual sentence information. In addition, error prorogation could be introduced by using the pipeline strategy in pair-wise models. In this paper, we propose an end-to-end neural approach to address the sentence ordering problem, which uses the pointer network (Ptr-Net) to alleviate the error propagation problem and utilize the whole contextual information. Experimental results show the effectiveness of the proposed model. Source codes and dataset of this paper are available.",
                    "Neural word segmentation has attracted more and more research interests for its ability to alleviate the effort of feature engineering and utilize the external resource by the pre-trained character or word embeddings. In this paper, we propose a new neural model to incorporate the word-level information for Chinese word segmentation. Unlike the previous word-based models, our model still adopts the framework of character-based sequence labeling, which has advantages on both effectiveness and efficiency at the inference stage. To utilize the word-level information, we also propose a new long short-term memory (LSTM) architecture over directed acyclic graph (DAG). Experimental results demonstrate that our model leads to better performances than the baseline models.",
                    "Chinese is a logographic writing system, and the shape of Chinese characters contain rich syntactic and semantic information. In this paper, we propose a model to learn Chinese word embeddings via three-level composition: (1) a convolutional neural network to extract the intra-character compositionality from the visual shape of a character; (2) a recurrent neural network with self-attention to compose character representation into word embeddings; (3) the Skip-Gram framework to capture non-compositionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks: word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.",
                    "Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.",
                    "Fine-tuning pre-trained language models like BERT has become an effective way in NLP and yields state-of-the-art results on many downstream tasks. Recent studies on adapting BERT to new tasks mainly focus on modifying the model structure, re-designing the pre-train tasks, and leveraging external data and knowledge. The fine-tuning strategy itself has yet to be fully explored. In this paper, we improve the fine-tuning of BERT with two effective mechanisms: self-ensemble and self-distillation. The experiments on text classification and natural language inference tasks show our proposed methods can significantly improve the adaption of BERT without any external data or knowledge.",
                    "In this work, we address the problem to model all the nodes (words or phrases) in a dependency tree with the dense representations. We propose a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, we introduce the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Based on RCNN, we use a discriminative model to re-rank a $k$-best list of candidate dependency parsing trees. The experiments show that RCNN is very effective to improve the state-of-the-art dependency parsing on both English and Chinese datasets.",
                    "Recently, word representation has been increasingly focused on for its excellent properties in representing the word semantics. Previous works mainly suffer from the problem of polysemy phenomenon. To address this problem, most of previous models represent words as multiple distributed vectors. However, it cannot reflect the rich relations between words by representing words as points in the embedded space. In this paper, we propose the Gaussian mixture skip-gram (GMSG) model to learn the Gaussian mixture embeddings for words based on skip-gram framework. Each word can be regarded as a gaussian mixture distribution in the embedded space, and each gaussian component represents a word sense. Since the number of senses varies from word to word, we further propose the Dynamic GMSG (D-GMSG) model by adaptively increasing the sense number of words during training. Experiments on four benchmarks show the effectiveness of our proposed model.",
                    "Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architecture and its superiority to state-of-the-art methods.",
                    "Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks. The datasets of all 16 tasks are publicly available at \\url{http://nlp.fudan.edu.cn/data/}"
                ],
                "domain": [
                    "Natural Language Processing",
                    "Multi-task Learning",
                    "Neural Networks",
                    "Word Embeddings"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "424d4ffa-ed2d-4a27-b132-b5aeec8df8c6": {
                "pk": "424d4ffa-ed2d-4a27-b132-b5aeec8df8c6",
                "project_name": null,
                "name": "Xin Sun",
                "bio": "As a researcher, my work primarily focuses on the intersection of public finance and entrepreneurship, particularly in the context of crises such as the COVID-19 pandemic. My recent study delves into the significant fiscal pressures faced by Chinese local governments due to lockdown measures, which have led to declines in revenue and increased public health expenditures. I am particularly interested in how these challenges have impacted startups, which have struggled with reduced funding and profitability during this tumultuous period.\n\nThrough my research, I aim to highlight the urgent need for policymakers to implement targeted support measures for startups, such as financial assistance, tax incentives, and regulatory flexibility. I believe that fostering innovation and entrepreneurship is crucial for economic recovery, and my work advocates for a balanced approach that considers both health and economic development. By addressing the challenges faced by local governments and the startup ecosystem, I hope to contribute to the formulation of effective policies that can help navigate the complexities of recovery in the post-pandemic landscape. My goal is to provide insights that not only inform academic discourse but also guide practical policy decisions that support sustainable economic growth.",
                "collaborators": [],
                "pub_titles": [
                    "Impact of COVID-19 Lockdown Measures on Chinese Startups and Local Government Public Finance: Challenges and Policy Implications"
                ],
                "pub_abstracts": [
                    "This paper aims to assess the impact of COVID-19 on the public finance of Chinese local governments, with a particular focus on the effect of lockdown measures on startups during the pandemic. The outbreak has placed significant fiscal pressure on local governments, as containment measures have led to declines in revenue and increased expenses related to public health and social welfare. In tandem, startups have faced substantial challenges, including reduced funding and profitability, due to the negative impact of lockdown measures on entrepreneurship. Moreover, the pandemic has generated short- and long-term economic shocks, affecting both employment and economic recovery. To address these challenges, policymakers must balance health concerns with economic development. In this regard, the government should consider implementing more preferential policies that focus on startups to ensure their survival and growth. Such policies may include financial assistance, tax incentives, and regulatory flexibility to foster innovation and entrepreneurship. By and large, the COVID-19 pandemic has had a profound impact on both the public finance of Chinese local governments and the startup ecosystem. Addressing the challenges faced by local governments and startups will require a comprehensive approach that balances health and economic considerations and includes targeted policies to support entrepreneurship and innovation."
                ],
                "domain": [
                    "Public Finance",
                    "COVID-19",
                    "Entrepreneurship",
                    "Economic Policy"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "e6a7bb0a-3621-44b7-973b-f994a5f0c6a7": {
                "pk": "e6a7bb0a-3621-44b7-973b-f994a5f0c6a7",
                "project_name": null,
                "name": "Shaoting Zhang",
                "bio": "I am a dedicated researcher specializing in the intersection of medical imaging and deep learning, with a strong focus on developing innovative models that enhance diagnostic accuracy and efficiency. My recent work explores the vast potential of foundation models in medical image analysis, addressing challenges such as data scarcity, privacy concerns, and the need for robust, multi-modal approaches.\n\nOne of my notable contributions is the development of the Knowledge-injected U-Transformer (KiUT) for radiology report generation, which integrates contextual and clinical knowledge to improve the accuracy of automated report writing. I have also pioneered the Modality Aware and Shift Mixer (MASM) for brain tumor segmentation, effectively capturing intra- and inter-modality dependencies to enhance segmentation performance.\n\nMy research extends to the creation of the Endo-FM, a foundation model tailored for endoscopic video analysis, which significantly outperforms existing methods across various downstream tasks. Additionally, I have tackled the long-tailed distribution problem in medical image analysis with the zero-shot pan-tumor segmentation framework (ZePT), demonstrating its ability to segment unseen tumor categories.\n\nI am passionate about leveraging advanced techniques such as multi-modal prompt tuning and diffusion models to bridge gaps in medical imaging tasks. My work aims to not only push the boundaries of what is possible in medical image analysis but also to ensure that these advancements translate into real-world clinical applications, ultimately improving patient outcomes.",
                "collaborators": [
                    "Xiaofan Zhang",
                    "Zhongzhen Huang",
                    "Yankai Jiang",
                    "Dimitris Metaxas",
                    "Linda Wei",
                    "Wenhui Lei",
                    "Xiaosong Wang",
                    "Rongzhao Zhang",
                    "Shu Wang",
                    "Dimitris N. Metaxas"
                ],
                "pub_titles": [
                    "On the Challenges and Perspectives of Foundation Models for Medical Image Analysis",
                    "KiUT: Knowledge-injected U-Transformer for Radiology Report Generation",
                    "Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation",
                    "DeReStainer: H&E to IHC Pathological Image Translation via Decoupled Staining Channels",
                    "Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment",
                    "Interactive Reinforcement Learning for Object Grounding via Self-Talking",
                    "Multi-modal Data Binding for Survival Analysis Modeling with Incomplete Data and Annotations",
                    "Learning Unknown from Correlations: Graph Neural Network for Inter-novel-protein Interaction Prediction",
                    "UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation",
                    "Exploring Data Redundancy in Real-world Image Classification through Data Selection",
                    "Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train",
                    "Embedding Label Structures for Fine-Grained Feature Representation",
                    "ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting",
                    "CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation",
                    "PathoTune: Adapting Visual Foundation Model to Pathological Specialists",
                    "MedDiff-FM: A Diffusion-based Foundation Model for Versatile Medical Image Applications",
                    "MedLSAM: Localize and Segment Anything Model for 3D CT Images",
                    "Visual Tracking via Reliable Memories",
                    "Multispectral Deep Neural Networks for Pedestrian Detection"
                ],
                "pub_abstracts": [
                    "This article discusses the opportunities, applications and future directions of large-scale pre-trained models, i.e., foundation models, for analyzing medical images. Medical foundation models have immense potential in solving a wide range of downstream tasks, as they can help to accelerate the development of accurate and robust models, reduce the large amounts of required labeled data, preserve the privacy and confidentiality of patient data. Specifically, we illustrate the \"spectrum\" of medical foundation models, ranging from general vision models, modality-specific models, to organ/task-specific models, highlighting their challenges, opportunities and applications. We also discuss how foundation models can be leveraged in downstream medical tasks to enhance the accuracy and efficiency of medical image analysis, leading to more precise diagnosis and treatment decisions.",
                    "Radiology report generation aims to automatically generate a clinically accurate and coherent paragraph from the X-ray image, which could relieve radiologists from the heavy burden of report writing. Although various image caption methods have shown remarkable performance in the natural image field, generating accurate reports for medical images requires knowledge of multiple modalities, including vision, language, and medical terminology. We propose a Knowledge-injected U-Transformer (KiUT) to learn multi-level visual representation and adaptively distill the information with contextual and clinical knowledge for word prediction. In detail, a U-connection schema between the encoder and decoder is designed to model interactions between different modalities. And a symptom graph and an injected knowledge distiller are developed to assist the report generation. Experimentally, we outperform state-of-the-art methods on two widely used benchmark datasets: IU-Xray and MIMIC-CXR. Further experimental results prove the advantages of our architecture and the complementary benefits of the injected knowledge.",
                    "Combining images from multi-modalities is beneficial to explore various information in computer vision, especially in the medical domain. As an essential part of clinical diagnosis, multi-modal brain tumor segmentation aims to delineate the malignant entity involving multiple modalities. Although existing methods have shown remarkable performance in the task, the information exchange for cross-scale and high-level representations fusion in spatial and modality are limited in these methods. In this paper, we present a novel Modality Aware and Shift Mixer that integrates intra-modality and inter-modality dependencies of multi-modal images for effective and robust brain tumor segmentation. Specifically, we introduce a Modality-Aware module according to neuroimaging studies for modeling the specific modality pair relationships at low levels, and a Modality-Shift module with specific mosaic patterns is developed to explore the complex relationships across modalities at high levels via the self-attention. Experimentally, we outperform previous state-of-the-art approaches on the public Brain Tumor Segmentation (BraTS 2021 segmentation) dataset. Further qualitative experiments demonstrate the efficacy and robustness of MASM.",
                    "Breast cancer is a highly fatal disease among cancers in women, and early detection is crucial for treatment. HER2 status, a valuable diagnostic marker based on Immunohistochemistry (IHC) staining, is instrumental in determining breast cancer status. The high cost of IHC staining and the ubiquity of Hematoxylin and Eosin (H&E) staining make the conversion from H&E to IHC staining essential. In this article, we propose a destain-restain framework for converting H&E staining to IHC staining, leveraging the characteristic that H&E staining and IHC staining of the same tissue sections share the Hematoxylin channel. We further design loss functions specifically for Hematoxylin and Diaminobenzidin (DAB) channels to generate IHC images exploiting insights from separated staining channels. Beyond the benchmark metrics on BCI contest, we have developed semantic information metrics for the HER2 level. The experimental results demonstrated that our method outperforms previous open-sourced methods in terms of image intrinsic property and semantic information.",
                    "Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia. Codes will be publicly available.",
                    "Humans are able to identify a referred visual object in a complex scene via a few rounds of natural language communications. Success communication requires both parties to engage and learn to adapt for each other. In this paper, we introduce an interactive training method to improve the natural language conversation system for a visual grounding task. During interactive training, both agents are reinforced by the guidance from a common reward function. The parametrized reward function also cooperatively updates itself via interactions, and contribute to accomplishing the task. We evaluate the method on GuessWhat?! visual grounding task, and significantly improve the task success rate. However, we observe language drifting problem during training and propose to use reward engineering to improve the interpretability for the generated conversations. Our result also indicates evaluating goal-ended visual conversation tasks require semantic relevant metrics beyond task success rate.",
                    "Survival analysis stands as a pivotal process in cancer treatment research, crucial for predicting patient survival rates accurately. Recent advancements in data collection techniques have paved the way for enhancing survival predictions by integrating information from multiple modalities. However, real-world scenarios often present challenges with incomplete data, particularly when dealing with censored survival labels. Prior works have addressed missing modalities but have overlooked incomplete labels, which can introduce bias and limit model efficacy. To bridge this gap, we introduce a novel framework that simultaneously handles incomplete data across modalities and censored survival labels. Our approach employs advanced foundation models to encode individual modalities and align them into a universal representation space for seamless fusion. By generating pseudo labels and incorporating uncertainty, we significantly enhance predictive accuracy. The proposed method demonstrates outstanding prediction accuracy in two survival analysis tasks on both employed datasets. This innovative approach overcomes limitations associated with disparate modalities and improves the feasibility of comprehensive survival analysis using multiple large foundation models.",
                    "The study of multi-type Protein-Protein Interaction (PPI) is fundamental for understanding biological processes from a systematic perspective and revealing disease mechanisms. Existing methods suffer from significant performance degradation when tested in unseen dataset. In this paper, we investigate the problem and find that it is mainly attributed to the poor performance for inter-novel-protein interaction prediction. However, current evaluations overlook the inter-novel-protein interactions, and thus fail to give an instructive assessment. As a result, we propose to address the problem from both the evaluation and the methodology. Firstly, we design a new evaluation framework that fully respects the inter-novel-protein interactions and gives consistent assessment across datasets. Secondly, we argue that correlations between proteins must provide useful information for analysis of novel proteins, and based on this, we propose a graph neural network based method (GNN-PPI) for better inter-novel-protein interaction prediction. Experimental results on real-world datasets of different scales demonstrate that GNN-PPI significantly outperforms state-of-the-art PPI prediction methods, especially for the inter-novel-protein interaction prediction.",
                    "Accurate segmentation of the fetal brain from Magnetic Resonance Image (MRI) is important for prenatal assessment of fetal development. Although deep learning has shown the potential to achieve this task, it requires a large fine annotated dataset that is difficult to collect. To address this issue, weakly-supervised segmentation methods with image-level labels have gained attention, which are commonly based on class activation maps from a classification network trained with image tags. However, most of these methods suffer from incomplete activation regions, due to the low-resolution localization without detailed boundary cues. To this end, we propose a novel weakly-supervised method with image-level labels based on semantic features and context information exploration. We first propose an Uncertainty-weighted Multi-resolution Class Activation Map (UM-CAM) to generate high-quality pixel-level supervision. Then, we design a Geodesic distance-based Seed Expansion (GSE) method to provide context information for rectifying the ambiguous boundaries of UM-CAM. Extensive experiments on a fetal brain dataset show that our UM-CAM can provide more accurate activation regions with fewer false positive regions than existing CAM variants, and our proposed method outperforms state-of-the-art weakly-supervised methods with image-level labels.",
                    "Deep learning models often require large amounts of data for training, leading to increased costs. It is particularly challenging in medical imaging, i.e., gathering distributed data for centralized training, and meanwhile, obtaining quality labels remains a tedious job. Many methods have been proposed to address this issue in various training paradigms, e.g., continual learning, active learning, and federated learning, which indeed demonstrate certain forms of the data valuation process. However, existing methods are either overly intuitive or limited to common clean/toy datasets in the experiments. In this work, we present two data valuation metrics based on Synaptic Intelligence and gradient norms, respectively, to study the redundancy in real-world image data. Novel online and offline data selection algorithms are then proposed via clustering and grouping based on the examined data values. Our online approach effectively evaluates data utilizing layerwise model parameter updates and gradients in each epoch and can accelerate model training with fewer epochs and a subset (e.g., 19%-59%) of data while maintaining equivalent levels of accuracy in a variety of datasets. It also extends to the offline coreset construction, producing subsets of only 18%-30% of the original. The codes for the proposed adaptive data selection and coreset computation are available (https://github.com/ZhenyuTANG2023/data_selection).",
                    "Foundation models have exhibited remarkable success in various applications, such as disease diagnosis and text report generation. To date, a foundation model for endoscopic video analysis is still lacking. In this paper, we propose Endo-FM, a foundation model specifically developed using massive endoscopic video data. First, we build a video transformer, which captures both local and global long-range dependencies across spatial and temporal dimensions. Second, we pre-train our transformer model using global and local views via a self-supervised manner, aiming to make it robust to spatial-temporal variations and discriminative across different scenes. To develop the foundation model, we construct a large-scale endoscopy video dataset by combining 9 publicly available datasets and a privately collected dataset from Baoshan Branch of Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K video clips with up to 5 million frames, encompassing various protocols, target organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a given downstream task via fine-tuning by serving as the backbone. With experiments on 3 different types of downstream tasks, including classification, segmentation, and detection, our Endo-FM surpasses the current state-of-the-art (SOTA) self-supervised pre-training and adapter-based transfer learning methods by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6% Dice, and 9.9% F1 for classification, segmentation, and detection). Code, datasets, and models are released at https://github.com/med-air/Endo-FM.",
                    "Recent algorithms in convolutional neural networks (CNN) considerably advance the fine-grained image classification, which aims to differentiate subtle differences among subordinate classes. However, previous studies have rarely focused on learning a fined-grained and structured feature representation that is able to locate similar images at different levels of relevance, e.g., discovering cars from the same make or the same model, both of which require high precision. In this paper, we propose two main contributions to tackle this problem. 1) A multi-task learning framework is designed to effectively learn fine-grained feature representations by jointly optimizing both classification and similarity constraints. 2) To model the multi-level relevance, label structures such as hierarchy or shared attributes are seamlessly embedded into the framework by generalizing the triplet loss. Extensive and thorough experiments have been conducted on three fine-grained datasets, i.e., the Stanford car, the car-333, and the food datasets, which contain either hierarchical labels or shared attributes. Our proposed method has achieved very competitive performance, i.e., among state-of-the-art classification accuracy. More importantly, it significantly outperforms previous fine-grained feature representations for image retrieval at different levels of relevance.",
                    "The long-tailed distribution problem in medical image analysis reflects a high prevalence of common conditions and a low prevalence of rare ones, which poses a significant challenge in developing a unified model capable of identifying rare or novel tumor categories not encountered during training. In this paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT) based on query-disentangling and self-prompting to segment unseen tumor categories beyond the training set. ZePT disentangles the object queries into two subsets and trains them in two stages. Initially, it learns a set of fundamental queries for organ segmentation through an object-aware feature grouping strategy, which gathers organ-level visual features. Subsequently, it refines the other set of advanced queries that focus on the auto-generated visual prompts for unseen tumor segmentation. Moreover, we introduce query-knowledge alignment at the feature level to enhance each query's discriminative representation and generalizability. Extensive experiments on various tumor segmentation tasks demonstrate the performance superiority of ZePT, which surpasses the previous counterparts and evidence the promising ability for zero-shot tumor segmentation in real-world settings.",
                    "Existing promptable segmentation methods in the medical imaging field primarily consider either textual or visual prompts to segment relevant objects, yet they often fall short when addressing anomalies in medical images, like tumors, which may vary greatly in shape, size, and appearance. Recognizing the complexity of medical scenarios and the limitations of textual or visual prompts, we propose a novel dual-prompt schema that leverages the complementary strengths of visual and textual prompts for segmenting various organs and tumors. Specifically, we introduce CAT, an innovative model that Coordinates Anatomical prompts derived from 3D cropped images with Textual prompts enriched by medical domain knowledge. The model architecture adopts a general query-based design, where prompt queries facilitate segmentation queries for mask prediction. To synergize two types of prompts within a unified framework, we implement a ShareRefiner, which refines both segmentation and prompt queries while disentangling the two types of prompts. Trained on a consortium of 10 public CT datasets, CAT demonstrates superior performance in multiple segmentation tasks. Further validation on a specialized in-house dataset reveals the remarkable capacity of segmenting tumors across multiple cancer stages. This approach confirms that coordinating multimodal prompts is a promising avenue for addressing complex scenarios in the medical domain.",
                    "As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality prompt tuning approaches. Significantly, PathoTune facilitates the direct adaptation of natural visual foundation models to pathological tasks, drastically outperforming pathological foundation models with simple linear probing. The code is available at https://github.com/openmedlab/PathoDuet.",
                    "Diffusion models have achieved significant success in both the natural image and medical image domains, encompassing a wide range of applications. Previous investigations in medical images have often been constrained to specific anatomical regions, particular applications, and limited datasets, resulting in isolated diffusion models. This paper introduces a diffusion-based foundation model to address a diverse range of medical image tasks, namely MedDiff-FM. MedDiff-FM leverages 3D CT images from multiple publicly available datasets, covering anatomical regions from head to abdomen, to pre-train a diffusion foundation model, and explores the capabilities of the diffusion foundation model across a variety of application scenarios. The diffusion foundation model handles multi-level image processing both at the image-level and patch-level, and utilizes position embedding to establish multi-level spatial relationships as well as anatomical structures and region classes to control certain anatomical regions. MedDiff-FM manages several downstream tasks seamlessly, including image denoising, anomaly detection, and image synthesis. MedDiff-FM is also capable of performing lesion generation and lesion inpainting by rapidly fine-tuning the diffusion foundation model using ControlNet with task-specific conditions. Experimental results demonstrate the effectiveness of MedDiff-FM in addressing diverse downstream medical image tasks.",
                    "Recent advancements in foundation models have shown significant potential in medical image analysis. However, there is still a gap in models specifically designed for medical image localization. To address this, we introduce MedLAM, a 3D medical foundation localization model that accurately identifies any anatomical part within the body using only a few template scans. MedLAM employs two self-supervision tasks: unified anatomical mapping (UAM) and multi-scale similarity (MSS) across a comprehensive dataset of 14,012 CT scans. Furthermore, we developed MedLSAM by integrating MedLAM with the Segment Anything Model (SAM). This innovative framework requires extreme point annotations across three directions on several templates to enable MedLAM to locate the target anatomical structure in the image, with SAM performing the segmentation. It significantly reduces the amount of manual annotation required by SAM in 3D medical imaging scenarios. We conducted extensive experiments on two 3D datasets covering 38 distinct organs. Our findings are twofold: 1) MedLAM can directly localize anatomical structures using just a few template scans, achieving performance comparable to fully supervised models; 2) MedLSAM closely matches the performance of SAM and its specialized medical adaptations with manual prompts, while minimizing the need for extensive point annotations across the entire dataset. Moreover, MedLAM has the potential to be seamlessly integrated with future 3D SAM models, paving the way for enhanced segmentation performance. Our code is public at \\href{https://github.com/openmedlab/MedLSAM}",
                    "In this paper, we propose a novel visual tracking framework that intelligently discovers reliable patterns from a wide range of video to resist drift error for long-term tracking tasks. First, we design a Discrete Fourier Transform (DFT) based tracker which is able to exploit a large number of tracked samples while still ensures real-time performance. Second, we propose a clustering method with temporal constraints to explore and memorize consistent patterns from previous frames, named as reliable memories. By virtue of this method, our tracker can utilize uncontaminated information to alleviate drifting issues. Experimental results show that our tracker performs favorably against other state of-the-art methods on benchmark datasets. Furthermore, it is significantly competent in handling drifts and able to robustly track challenging long videos over 4000 frames, while most of others lose track at early frames.",
                    "Multispectral pedestrian detection is essential for around-the-clock applications, e.g., surveillance and autonomous driving. We deeply analyze Faster R-CNN for multispectral pedestrian detection task and then model it into a convolutional network (ConvNet) fusion problem. Further, we discover that ConvNet-based pedestrian detectors trained by color or thermal images separately provide complementary information in discriminating human instances. Thus there is a large potential to improve pedestrian detection by using color and thermal images in DNNs simultaneously. We carefully design four ConvNet fusion architectures that integrate two-branch ConvNets on different DNNs stages, all of which yield better performance compared with the baseline detector. Our experimental results on KAIST pedestrian benchmark show that the Halfway Fusion model that performs fusion on the middle-level convolutional features outperforms the baseline method by 11% and yields a missing rate 3.5% lower than the other proposed architectures."
                ],
                "domain": [
                    "Medical Imaging",
                    "Foundation Models",
                    "Deep Learning",
                    "Computer Vision"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we establish a comprehensive and standardized benchmarking framework for evaluating Medical Large Language Models (MLLMs) in the Chinese healthcare context?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it will provide a reliable means to assess the efficacy of MLLMs, which are increasingly being integrated into healthcare. A standardized benchmarking framework will facilitate consistent evaluations, enabling researchers to compare results across different models and studies. This advancement could lead to improved MLLM performance, ultimately enhancing patient care and outcomes in various medical specialties. Furthermore, it will encourage further research into specialized applications of MLLMs, driving innovation and practical applications in healthcare.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem include the need for a comprehensive coverage of diverse medical specialties, as existing benchmarks primarily focus on general clinical knowledge. This limitation results in inadequate evaluation of MLLMs in specific fields, such as nephrology and myopia care. Additionally, the lack of a standardized evaluation infrastructure leads to inconsistent benchmarking practices, making it difficult to draw reliable conclusions. Naive approaches may fail because they do not account for the complexity of medical language and the nuances of different specialties, which require tailored evaluation metrics and methodologies.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has been limited by a focus on general clinical knowledge without addressing the specific needs of various medical specialties. Existing benchmarks have not established a standardized evaluation process, leading to variability in how MLLMs are assessed. Barriers include the absence of a comprehensive dataset that covers a wide range of medical fields and the lack of collaboration among researchers to create a unified framework. My approach differs by proposing a systematic methodology that incorporates diverse medical specialties and establishes a consistent evaluation infrastructure, addressing the gaps in prior work.\n\n**[Question 5] - What are the key components of my approach and results?**  \nMy proposed methodology involves developing a comprehensive benchmarking framework that includes a diverse set of tasks: Multiple Choice Questions (MCQ), Close Question Answering (QA), Open QA, and Information Extraction (IE). I will utilize datasets such as the National Medical Licensing Examination in China and others, applying metrics like accuracy, BLEU score, ROUGH-L score, and micro-averaged F1-score to evaluate MLLM performance. The expected outcomes include a standardized evaluation process that enhances the reliability of MLLM"
    },
    "2404.08001": {
        "paper_data": {
            "title": "Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics",
            "url": "http://arxiv.org/abs/2404.08001v1",
            "arxiv_id": "2404.08001",
            "authors": [
                "Zhengde Zhang",
                "Yiyu Zhang",
                "Haodong Yao",
                "Jianwen Luo",
                "Rui Zhao",
                "Bo Huang",
                "Jiameng Zhao",
                "Yipu Liao",
                "Ke Li",
                "Lina Zhao",
                "Jun Cao",
                "Fazhi Qi",
                "Changzheng Yuan"
            ],
            "abstract": "Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with state-of-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific field, it's challenging to acquire unique domain knowledge while keeping the model itself advanced. To address this challenge, a sophisticated large language model system named as Xiwu has been developed, allowing you switch between the most advanced foundation models and quickly teach the model domain knowledge. In this work, we will report on the best practices for applying LLMs in the field of high-energy physics (HEP), including: a seed fission technology is proposed and some data collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system has been developed to facilitate rapid training under a specified foundation model. The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna, ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model on the HEP knowledge question-and-answering and code generation. This strategy significantly enhances the potential for growth of our model's performance, with the hope of surpassing GPT-4 as it evolves with the development of open-source models. This work provides a customized LLM for the field of HEP, while also offering references for applying LLM to other fields, the corresponding codes are available on Github.",
            "introduction": "   1 Introduction  Nam dui ligula, fringilla a, euismod sodales, sollicitudin vel, wisi. Morbi auctor lorem non justo. Nam lacus libero, pretium at, lobortis vitae, ultricies et, tellus. Donec aliquet, tortor sed accumsan bibendum, erat ligula aliquet magna, vitae ornare odio metus a mi. Morbi ac orci et nisl hendrerit mollis. Suspendisse ut massa. Cras nec ante. Pellentesque a nulla. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam tincidunt urna. Nulla ullamcorper vestibulum turpis. Pellentesque cursus luctus mauris. Nulla malesuada porttitor diam. Donec felis erat, congue non, volutpat at, tincidunt tristique, libero. Vivamus viverra fermentum felis. Donec nonummy pellentesque ante. Phasellus adipiscing semper elit. Proin fermentum massa ac quam. Sed diam turpis, molestie vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum ligula, eleifend at, accumsan nec, suscipit a, ipsum. Morbi blandit ligula feugiat magna. Nunc eleifend consequat lorem. Sed lacinia nulla vitae enim. Pellentesque tincidunt purus vel magna. Integer non enim. Praesent euismod nunc eu purus. Donec bibendum quam in tellus. Nullam cursus pulvinar lectus. Donec et mi. Nam vulputate metus eu enim. Vestibulum pellentesque felis eu massa.     2 Headings: first level  Quisque ullamcorper placerat ipsum. Cras nibh. Morbi vel justo vitae lacus tincidunt ultrices. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. In hac habitasse platea dictumst. Integer tempus convallis augue. Etiam facilisis. Nunc elementum fermentum wisi. Aenean placerat. Ut imperdiet, enim sed gravida sollicitudin, felis odio placerat quam, ac pulvinar elit purus eget enim. Nunc vitae tortor. Proin tempus nibh sit amet nisl. Vivamus quis tortor vitae risus porta vehicula. See Section 2.    2.1 Headings: second level  Fusce mauris. Vestibulum luctus nibh at lectus. Sed bibendum, nulla a faucibus semper, leo velit ultricies tellus, ac venenatis arcu wisi vel nisl. Vestibulum diam. Aliquam pellentesque, augue quis sagittis posuere, turpis lacus congue quam, in hendrerit risus eros eget felis. Maecenas eget erat in sapien mattis porttitor. Vestibulum porttitor. Nulla facilisi. Sed a turpis eu lacus commodo facilisis. Morbi fringilla, wisi in dignissim interdum, justo lectus sagittis dui, et vehicula libero dui cursus dui. Mauris tempor ligula sed lacus. Duis cursus enim ut augue. Cras ac magna. Cras nulla. Nulla egestas. Curabitur a leo. Quisque egestas wisi eget nunc. Nam feugiat lacus vel est. Curabitur consectetuer.    ξi⁢j⁢(t)=P⁢(xt=i,xt+1=j|y,v,w;θ)=αi⁢(t)⁢ai⁢jwt⁢βj⁢(t+1)⁢bjvt+1⁢(yt+1)∑i=1N∑j=1Nαi⁢(t)⁢ai⁢jwt⁢βj⁢(t+1)⁢bjvt+1⁢(yt+1)subscript𝜉𝑖𝑗𝑡𝑃formulae-sequencesubscript𝑥𝑡𝑖subscript𝑥𝑡1conditional𝑗𝑦𝑣𝑤𝜃subscript𝛼𝑖𝑡subscriptsuperscript𝑎subscript𝑤𝑡𝑖𝑗subscript𝛽𝑗𝑡1subscriptsuperscript𝑏subscript𝑣𝑡1𝑗subscript𝑦𝑡1superscriptsubscript𝑖1𝑁superscriptsubscript𝑗1𝑁subscript𝛼𝑖𝑡subscriptsuperscript𝑎subscript𝑤𝑡𝑖𝑗subscript𝛽𝑗𝑡1subscriptsuperscript𝑏subscript𝑣𝑡1𝑗subscript𝑦𝑡1\\xi_{ij}(t)=P(x_{t}=i,x_{t+1}=j|y,v,w;\\theta)={\\frac{\\alpha_{i}(t)a^{w_{t}}_{% ij}\\beta_{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}{\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\alpha_% {i}(t)a^{w_{t}}_{ij}\\beta_{j}(t+1)b^{v_{t+1}}_{j}(y_{t+1})}}italic_ξ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_t ) = italic_P ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_i , italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_j | italic_y , italic_v , italic_w ; italic_θ ) = divide start_ARG italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) italic_a start_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_β start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t + 1 ) italic_b start_POSTSUPERSCRIPT italic_v start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) italic_a start_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_β start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t + 1 ) italic_b start_POSTSUPERSCRIPT italic_v start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) end_ARG  (1)      2.1.1 Headings: third level  Suspendisse vel felis. Ut lorem lorem, interdum eu, tincidunt sit amet, laoreet vitae, arcu. Aenean faucibus pede eu ante. Praesent enim elit, rutrum at, molestie",
            "references": []
        },
        "author_data": {
            "dfad56f6-4b85-42ee-9f47-e910217d3c2b": {
                "pk": "dfad56f6-4b85-42ee-9f47-e910217d3c2b",
                "project_name": null,
                "name": "Zhengde Zhang",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. Recently, I developed Position-aware GNNs (P-GNNs), which significantly improve node embedding by capturing a node's position within the broader graph structure. This innovation has proven effective in various prediction tasks, showcasing up to a 66% improvement in performance metrics.\n\nI am also passionate about exploring the relationship between the architecture of neural networks and their predictive performance. My work on relational graphs has revealed critical insights into how the graph structure influences outcomes, leading to the identification of a \"sweet spot\" for optimal performance.\n\nIn addition, I have introduced Identity-aware GNNs (ID-GNNs), which enhance the expressive power of message-passing frameworks by considering node identities during aggregation. This approach has yielded substantial accuracy improvements across multiple prediction tasks.\n\nMy recent endeavors include the ROLAND framework for dynamic graphs, which allows for the seamless adaptation of static GNNs to dynamic environments, and the development of AutoTransfer, an AutoML solution that leverages prior architectural knowledge to improve search efficiency for new tasks.\n\nThrough my research, I aim to bridge the gap between theoretical advancements and practical applications, ultimately contributing to the evolution of GNNs and their integration into real-world scenarios.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "aab329c0-a723-42f2-844a-dc47d022ada4": {
                "pk": "aab329c0-a723-42f2-844a-dc47d022ada4",
                "project_name": null,
                "name": "Yiyu Zhang",
                "bio": "As a researcher deeply engaged in the field of two-dimensional transition metal dichalcogenides (TMDs), I have dedicated my work to addressing the significant challenges that hinder their full potential. My recent review highlights the critical need for compatible and scalable dielectric materials and integration techniques, which are essential for enhancing device performance and facilitating commercial applications. I explore various dielectric synthesis and integration methods, assessing their applicability to atomically thin materials, which are often difficult to work with using conventional techniques.\n\nIn my research, I delve into the dielectric integration for a range of applications, including nanoelectronics, optoelectronics, flexible electronics, valleytronics, biosensing, and quantum information processing. I aim to provide a comprehensive understanding of the basic working principles of these devices, the specific dielectric requirements they entail, and the current progress in the field. By discussing key challenges and future prospects, I strive to inspire further innovation and exploration in the integration of TMDs with dielectric materials. My goal is to contribute to the advancement of this exciting area of research, ultimately unlocking the vast potential of two-dimensional materials for next-generation technologies.",
                "collaborators": [
                    "Chit Siong Lau",
                    "Sarthak Das",
                    "Ivan A. Verzhbitskiy",
                    "Ding Huang",
                    "Teymour Talha-Dean",
                    "Wei Fu",
                    "Dasari Venkatakrishnarao",
                    "Kuan Eng Johnson Goh"
                ],
                "pub_titles": [
                    "Dielectrics for Two-Dimensional Transition Metal Dichalcogenide Applications"
                ],
                "pub_abstracts": [
                    "Despite over a decade of intense research efforts, the full potential of two-dimensional transition metal dichalcogenides continues to be limited by major challenges. The lack of compatible and scalable dielectric materials and integration techniques restrict device performances and their commercial applications Conventional dielectric integration techniques for bulk semiconductors are difficult to adapt for atomically thin two-dimensional materials. This review provides a brief introduction into various common and emerging dielectric synthesis and integration techniques and discusses their applicability for 2D transition metal dichalcogenides. Dielectric integration for various applications is reviewed in subsequent sections including nanoelectronics, optoelectronics, flexible electronics, valleytronics, biosensing, quantum information processing, and quantum sensing. For each application, we introduce basic device working principles, discuss the specific dielectric requirements, review current progress, present key challenges, and offer insights into future prospects and opportunities."
                ],
                "domain": [
                    "2D Materials",
                    "Dielectric Integration",
                    "Nanoelectronics",
                    "Quantum Sensing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "a347ea91-acb2-4f0c-87e3-1495f891a1cd": {
                "pk": "a347ea91-acb2-4f0c-87e3-1495f891a1cd",
                "project_name": null,
                "name": "Haodong Yao",
                "bio": "I am a researcher with a diverse background in mathematics, computer vision, and materials science, focusing on the intersection of theoretical and applied methodologies. My recent work includes proving the Kudla-Rapoport conjecture for \\(\\mathcal{Y}\\)-cycles on exotic smooth unitary Rapoport-Zink spaces, where I explored the intricate relationship between arithmetic intersection numbers and local representation density. This research not only deepens our understanding of these mathematical structures but also bridges connections between geometric and analytic perspectives.\n\nIn the realm of computer vision, I developed a neural network-based method for aligning partially-overlapped 3D line reconstructions. This innovative approach integrates a Multilayer Perceptron for feature extraction, an Optimal Transport layer for estimating match probabilities, and a RANSAC framework for robust transformation estimation. My experiments demonstrate significant improvements in registration precision, showcasing the potential of combining deep learning with geometric principles.\n\nAdditionally, I have ventured into materials science by constructing physics-informed Graph Neural Networks and Transformer models to analyze X-ray absorption spectroscopy (XAS) data. This work aims to simplify the quantitative analysis of three-dimensional structures, making it accessible without requiring extensive user expertise. My goal is to enhance the understanding of structure-function relationships in energy and catalysis, ultimately contributing to advancements in these critical fields. I am passionate about leveraging interdisciplinary approaches to solve complex problems and drive innovation in research.",
                "collaborators": [
                    "Liu Liu",
                    "Hongdong Li",
                    "Ruyi Zha",
                    "Fei Zhan",
                    "Lirong Zheng",
                    "Zhi Geng",
                    "Can Yu",
                    "Xue Han",
                    "Xueqi Song",
                    "Shuguang Chen"
                ],
                "pub_titles": [
                    "A Kudla-Rapoport Formula for Exotic Smooth Models of Odd Dimension",
                    "PlueckerNet: Learn to Register 3D Line Reconstructions",
                    "A graph neural network and transformer based approach to XANES data analyis"
                ],
                "pub_abstracts": [
                    "In this article, we prove a Kudla-Rapoport conjecture for $\\mathcal{Y}$-cycles on exotic smooth unitary Rapoport-Zink spaces of odd arithmetic dimension, i.e. the arithmetic intersection numbers for $\\mathcal{Y}$-cycles equals the derivatives of local representation density. We also compare $\\mathcal{Z}$-cycles and $\\mathcal{Y}$-cycles on these RZ spaces. The method is to relate both geometric and analytic sides to the even dimensional case and reduce the conjecture to the results in arXiv:2101.09485.",
                    "Aligning two partially-overlapped 3D line reconstructions in Euclidean space is challenging, as we need to simultaneously solve correspondences and relative pose between line reconstructions. This paper proposes a neural network based method and it has three modules connected in sequence: (i) a Multilayer Perceptron (MLP) based network takes Pluecker representations of lines as inputs, to extract discriminative line-wise features and matchabilities (how likely each line is going to have a match), (ii) an Optimal Transport (OT) layer takes two-view line-wise features and matchabilities as inputs to estimate a 2D joint probability matrix, with each item describes the matchness of a line pair, and (iii) line pairs with Top-K matching probabilities are fed to a 2-line minimal solver in a RANSAC framework to estimate a six Degree-of-Freedom (6-DoF) rigid transformation. Experiments on both indoor and outdoor datasets show that the registration (rotation and translation) precision of our method outperforms baselines significantly.",
                    "X-ray absorption spectroscopy (XAS) is an indispensable tool to characterize the atomic-scale three-dimensional local structure of the system, in which XANES is the most important energy region to reflect the three-dimensional structure. However quantitative analysis of three-dimensional structure from XANES requires users to have a deep understanding and accurate judgment of structural information and summarize several structural parameters, which is often difficult to achieve. In this work, We construct \\textbf{physics-informed Graph neural network} and \\textbf{Transformer} models for calculating XANES from the input three-dimensional structure; we improve the efficiency of the model based on the physical meaning of XAS; then we combine the model and optimization algorithm to fit the three-dimensional structure of given system. This method does not require users to summarize the structural parameters, has wide application range. It can be applied to the three-dimensional structure analysis of solid materials and has positive significance for the study of structure-function relationship in the fields of energy and catalysis. In addition, this method is expected to be developed into an online three-dimensional structure analysis method for XAS related beamlines."
                ],
                "domain": [
                    "Graph Neural Network",
                    "X-ray Absorption Spectroscopy",
                    "Computer Vision",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "fd720bba-26f4-45f8-b2d6-b2dafd4939b2": {
                "pk": "fd720bba-26f4-45f8-b2d6-b2dafd4939b2",
                "project_name": null,
                "name": "Jianwen Luo",
                "bio": "I am a researcher dedicated to advancing imaging technologies and robotic systems, with a particular focus on ultrasound imaging, photoacoustic tomography, and quadrupedal robotics. My recent work has centered on enhancing image reconstruction techniques, leveraging deep learning and novel algorithms to improve the quality and speed of ultrasound and photoacoustic imaging. For instance, I developed a convolutional neural network (CNN) framework that effectively estimates missing components in synthetic transmit aperture (STA) datasets, significantly reducing artifacts and improving image quality.\n\nIn the realm of robotics, I have designed a high payload-capacity quadruped robot, Kirin, which utilizes innovative mechanical designs to enhance its payload-carrying capabilities. My research also explores adaptive control strategies for quadrupedal locomotion, focusing on online identification of payload parameters to improve stability and performance on uneven terrains.\n\nAdditionally, I have contributed to the development of self-supervised learning frameworks for inverse problems in medical imaging, allowing for improved reconstruction accuracy without the need for extensive ground truth data. My work in photoacoustic imaging has led to the creation of a score-based generative model that enhances image recovery from limited sensor data, demonstrating significant improvements over traditional methods.\n\nOverall, my research aims to bridge the gap between advanced imaging techniques and practical applications in robotics and medical diagnostics, striving for innovations that enhance both performance and efficiency in these fields.",
                "collaborators": [
                    "Hengrong Lan",
                    "Qiong He",
                    "Jingke Zhang",
                    "Chaoyang Song",
                    "Zhiqiang Li",
                    "Yueheng Zhou",
                    "Ming Liu",
                    "Lijie Huang",
                    "Bingchen Jin",
                    "Ye Zhao"
                ],
                "pub_titles": [
                    "Deep Learning Improves Dataset Recovery for High Frame Rate Synthetic Transmit Aperture Imaging",
                    "A Unified Convergence Theorem",
                    "Kirin: A Quadruped Robot with High Payload Carrying Capability",
                    "Fast Sampling generative model for Ultrasound image reconstruction",
                    "Single PW takes a shortcut to compound PW in US imaging",
                    "Cross-domain Self-supervised Framework for Photoacoustic Computed Tomography Image Reconstruction",
                    "An Adaptive Control Algorithm for Quadruped Locomotion with Proprioceptive Linear Legs",
                    "Score-based Generative Models for Photoacoustic Image Reconstruction with Rotation Consistency Constraints",
                    "A General Framework for Inverse Problem Solving using Self-Supervised Deep Learning: Validations in Ultrasound and Photoacoustic Image Reconstruction",
                    "Partial Hadamard Encoded Synthetic Transmit Aperture for High Frame Rate Imaging with Minimal l2-Norm Least Square Method",
                    "A semi-automatic ultrasound image analysis system for the grading diagnosis of COVID-19 pneumonia",
                    "Terrain-perception-free Quadrupedal Spinning Locomotion on Versatile Terrains: Modeling, Analysis, and Experimental Validation",
                    "Maximize the Foot Clearance for a Hopping Robotic Leg Considering Motor Saturation",
                    "Mapping Human Muscle Force to Supernumerary Robotics Device for Overhead Task Assistance",
                    "Reconstructing undersampled photoacoustic microscopy images using deep learning"
                ],
                "pub_abstracts": [
                    "Synthetic transmit aperture (STA) imaging can achieve optimal lateral resolution in the full field of view, at the cost of low frame rate (FR) and low signal-to-noise ratio (SNR). In our previous studies, compressed sensing based synthetic transmit aperture (CS-STA) and minimal l2-norm least squares (LS-STA) methods were proposed to recover the complete STA dataset from fewer Hadamard-encoded plane wave (PW) transmissions. Results demonstrated that, compared with STA imaging, CS/LS-STA can maintain the high resolution of STA and improve the contrast in the deep region with increased FR. However, these methods would introduce errors to the recovered STA datasets and subsequently produce severe artifacts to the beamformed images. Recently, we discovered that the theoretical explanation for the error introduced in the LS-STA-based recovery is that LS-STA method neglects the null space component of the real STA data. To deal with this problem, we propose to train a convolutional neural network (CNN) under the null space learning framework (to estimate the missing null space component) for high-accuracy recovery of the STA dataset from fewer Hadamard-encoded PW transmissions. The mapping between the low-quality STA dataset (recovered using the LS-STA method) and the corresponding high-quality STA dataset (obtained using full Hadamard-encoded STA imaging, HE-STA) was learned from phantom and in vivo samples. The performance of the proposed CNN-STA method was compared with the LS-STA, STA, and HE-STA methods, in terms of visual quality, NRMSE, gCNR, and FWHM. The results demonstrate that the proposed method can improve the recovery accuracy of the STA datasets and therefore effectively suppress the artifacts presented in the images obtained using the LS-STA method. In addition, the proposed method can maintain the high lateral resolution of STA with fewer PW transmissions, as LS-STA does.",
                    "We prove a unified convergence theorem, which presents in four equivalent forms of the famous Antosik-Mikusinski Theorems. In particular, we show that Swartz' three uniform convergence principles are all equivalent to the Antosik-Mikusinski Theorems.",
                    "The quadruped robot is a versatile mobile platform with potential ability for high payload carrying. However, most of the existing quadruped robots aim at high maneuverability, highly dynamic and agile locomotion. In spite of this, payload carrying is still an indispensable ability for the quadruped robots. Design of a quadruped robot with high payload capacity is yet deeply explored. In this study, a 50 kg electrically-actuated quadruped robot, Kirin, is presented to leverage the payload carrying capability. Kirin is an characterized with prismatic quasi-direct-drive (QDD) leg. This mechanism greatly augments the payload carrying capability. This study presents several design principles for the payload-carrying-oriented quadruped robots, including the mechanical design, actuator parameters selection, and locomotion control method. The theoretical analysis implies that the lifting task tends to be a bottleneck for the existing robots with the articulated knee joints. By using prismatic QDD leg, the payload carrying capability of Kirin is enhanced greatly. To demonstrate Kirin's payload carrying capability, in preliminary experiment, up to 125 kg payload lifting in static stance and 50 kg payload carrying in dynamic trotting are tested. Whole body compliance with payload carrying is also demonstrated.",
                    "Image reconstruction from radio-frequency data is pivotal in ultrafast plane wave ultrasound imaging. Unlike the conventional delay-and-sum (DAS) technique, which relies on somewhat imprecise assumptions, deep learning-based methods perform image reconstruction by training on paired data, leading to a notable enhancement in image quality. Nevertheless, these strategies often exhibit limited generalization capabilities. Recently, denoising diffusion models have become the preferred paradigm for image reconstruction tasks. However, their reliance on an iterative sampling procedure results in prolonged generation time. In this paper, we propose a novel sampling framework that concurrently enforces data consistency of ultrasound signals and data-driven priors. By leveraging the advanced diffusion model, the generation of high-quality images is substantially expedited. Experimental evaluations on an in-vivo dataset indicate that our approach with a single plane wave surpasses DAS with spatial coherent compounding of 75 plane waves.",
                    "Reconstruction of ultrasound (US) images from radio-frequency data can be conceptualized as a linear inverse problem. Traditional deep learning approaches, which aim to improve the quality of US images by directly learning priors, often encounter challenges in generalization. Recently, diffusion-based generative models have received significant attention within the research community due to their robust performance in image reconstruction tasks. However, a limitation of these models is their inherent low speed in generating image samples from pure Gaussian noise progressively. In this study, we exploit the inherent similarity between the US images reconstructed from a single plane wave (PW) and PW compounding PWC). We hypothesize that a single PW can take a shortcut to reach the diffusion trajectory of PWC, removing the need to begin with Gaussian noise. By employing an advanced diffusion model, we demonstrate its effectiveness in US image reconstruction, achieving a substantial reduction in sampling steps. In-vivo experimental results indicate that our approach can reduce sampling steps by 60%, while preserving comparable performance metrics with the conventional diffusion model.",
                    "Accurate image reconstruction is crucial for photoacoustic (PA) computed tomography (PACT). Recently, deep learning has been used to reconstruct the PA image with a supervised scheme, which requires high-quality images as ground truth labels. In practice, there are inevitable trade-offs between cost and performance since the use of more channels is an expensive strategy to access more measurements. Here, we propose a cross-domain unsupervised reconstruction (CDUR) strategy with a pure transformer model, which overcomes the lack of ground truth labels from limited PA measurements. The proposed approach exploits the equivariance of PACT to achieve high performance with a smaller number of channels. We implement a self-supervised reconstruction in a model-based form. Meanwhile, we also leverage the self-supervision to enforce the measurement and image consistency on three partitions of measured PA data, by randomly masking different channels. We find that dynamically masking a high proportion of the channels, e.g., 80%, yields nontrivial self-supervisors in both image and signal domains, which decrease the multiplicity of the pseudo solution to efficiently reconstruct the image from fewer PA measurements with minimum error of the image. Experimental results on in-vivo PACT dataset of mice demonstrate the potential of our unsupervised framework. In addition, our method shows a high performance (0.83 structural similarity index (SSIM) in the extreme sparse case with 13 channels), which is close to that of supervised scheme (0.77 SSIM with 16 channels). On top of all the advantages, our method may be deployed on different trainable models in an end-to-end manner.",
                    "Quadruped robots manifest great potential to traverse rough terrains with payload. Numerous traditional control methods for legged dynamic locomotion are model-based and exhibit high sensitivity to model uncertainties and payload variations. Therefore, high-performance model parameter estimation becomes indispensable. However, the inertia parameters of payload are usually unknown and dynamically changing when the quadruped robot is deployed in versatile tasks. To address this problem, online identification of the inertia parameters and the Center of Mass (CoM) position of the payload for the quadruped robots draw an increasing interest. This study presents an adaptive controller based on the online payload identification for the high payload capacity (the ratio between payload and robot's self-weight) quadruped locomotion. We name it as Adaptive Controller for Quadruped Locomotion (ACQL), which consists of a recursive update law and a control law. ACQL estimates the external forces and torques induced by the payload online. The estimation is incorporated in inverse-dynamics-based Quadratic Programming (QP) to realize a trotting gait. As such, the tracking accuracy of the robot's CoM and orientation trajectories are improved. The proposed method, ACQL, is verified in a real quadruped robot platform. Experiments prove the estimation efficacy for the payload weighing from 20 $kg$ to 75 $kg$ and loaded at different locations of the robot's torso.",
                    "Photoacoustic tomography (PAT) is a newly emerged imaging modality which enables both high optical contrast and acoustic depth of penetration. Reconstructing images of photoacoustic tomography from limited amount of senser data is among one of the major challenges in photoacoustic imaging. Previous works based on deep learning were trained in supervised fashion, which directly map the input partially known sensor data to the ground truth reconstructed from full field of view. Recently, score-based generative models played an increasingly significant role in generative modeling. Leveraging this probabilistic model, we proposed Rotation Consistency Constrained Score-based Generative Model (RCC-SGM), which recovers the PAT images by iterative sampling between Langevin dynamics and a constraint term utilizing the rotation consistency between the images and the measurements. Our proposed method can generalize to different measurement processes (32.29 PSNR with 16 measurements under random sampling, whereas 28.50 for supervised counterpart), while supervised methods need to train on specific inverse mappings.",
                    "The image reconstruction process in medical imaging can be treated as solving an inverse problem. The inverse problem is usually solved using time-consuming iterative algorithms with sparsity or other constraints. Recently, deep neural network (DNN)-based methods have been developed to accelerate the inverse-problem-solving process. However, these methods typically adopt supervised learning scheme, which requires ground truths, or labels of the solutions, for training. In many applications, it would be challenging or even impossible to obtain the ground truth, such as the tissue reflectivity function in ultrasound beamforming. In this study, a general framework based on self-supervised learning (SSL) scheme is proposed to train a DNN to solve the inverse problems. In this way, the measurements can be used as both the inputs and the labels during the training of DNN. The proposed SSL method is applied to four typical linear inverse problems for validation, i.e., plane wave ultrasound and photoacoustic image reconstructions, compressed sensing-based synthetic transmit aperture dataset recovery and deconvolution in ultrasound localization microscopy. Results show that, using the proposed framework, the trained DNN can achieve improved reconstruction accuracy with reduced computational time, compared with conventional methods.",
                    "Synthetic transmit aperture (STA) ultrasound imaging is well known for ideal focusing in the full field of view. However, it suffers from low signal-to-noise ratio (SNR) and low frame rate, because each array element must be activated individually. In our previous study, we encoded all the array elements with partial Hadamard matrix and reconstructed the complete STA dataset with compressed sensing (CS) algorithm (CS-STA). As all the elements are activated in each transmission and the number of transmissions is smaller than that of STA, this method can achieve higher SNR and higher frame rate. Its main drawback is the time-consuming CS reconstruction. In this study, we accelerate the complete STA dataset reconstruction with minimal l2-norm least square method. Thanks of the orthogonality of partial Hadamard matrix, the minimal l2-norm least square solution can be easily calculated. The proposed method is tested with simulation data and experimental phantom and in-vivo data. The results demonstrate that the proposed method achieves ~5*10^3 times faster reconstruction speed than CS algorithm. The simulation results demonstrate that the proposed method is capable of achieving the same accuracy for STA dataset reconstruction as conventional CS-STA method. The simulations, phantom and in-vivo experiments show that the proposed method is capable of improving the generalized contrast-to-noise ratio (gCNR) and SNR with maintained spatial resolution and fewer transmissions, compared with STA. In conclusion, the improved image quality and reduced computational time of LS-STA pave the way for its real-time applications in the clinics.",
                    "This paper proposes a semi-automatic system based on quantitative characterization of the specific image patterns in lung ultrasound (LUS) images, in order to assess the lung conditions of patients with COVID-19 pneumonia, as well as to differentiate between the severe / and no-severe cases. Specifically, four parameters are extracted from each LUS image, namely the thickness (TPL) and roughness (RPL) of the pleural line, and the accumulated with (AWBL) and acoustic coefficient (ACBL) of B lines. 27 patients are enrolled in this study, which are grouped into 13 moderate patients, 7 severe patients and 7 critical patients. Furthermore, the severe and critical patients are regarded as the severe cases, and the moderate patients are regarded as the non-severe cases. Biomarkers among different groups are compared. Each single biomarker and a classifier with all the biomarkers as input are utilized for the binary diagnosis of severe case and non-severe case, respectively. The classifier achieves the best classification performance among all the compared methods (area under the receiver operating characteristics curve = 0.93, sensitivity = 0.93, specificity = 0.85). The proposed image analysis system could be potentially applied to the grading and prognosis evaluation of patients with COVID-19 pneumonia.",
                    "Dynamic quadrupedal locomotion over rough terrains reveals remarkable progress over the last few decades. Small-scale quadruped robots are adequately flexible and adaptable to traverse uneven terrains along sagittal direction, such as slopes and stairs. To accomplish autonomous locomotion navigation in complex environments, spinning is a fundamental yet indispensable functionality for legged robots. However, spinning behaviors of quadruped robots on uneven terrain often exhibit position drifts. Motivated by this problem, this study presents an algorithmic method to enable accurate spinning motions over uneven terrain and constrain the spinning radius of the Center of Mass (CoM) to be bounded within a small range to minimize the drift risks. A modified spherical foot kinematics representation is proposed to improve the foot kinematic model and rolling dynamics of the quadruped during locomotion. A CoM planner is proposed to generate stable spinning motion based on projected stability margins. Accurate motion tracking is accomplished with Linear Quadratic Regulator (LQR) to bound the position drift during the spinning movement. Experiments are conducted on a small-scale quadruped robot and the effectiveness of the proposed method is verified on versatile terrains including flat ground, stairs and slopes.",
                    "A hopping leg, no matter in legged animals or humans, usually behaves like a spring during the periodic hopping. Hopping like a spring is efficient and without the requirement of complicated control algorithms. Position and force control are two main methods to realize such a spring-like behaviour. The position control usually consumes the torque resources to ensure the position accuracy and compensate the tracking errors. In comparison, the force control strategy is able to maintain a high elasticity. Currently, the position and force control both leads to the discount of motor saturation ratio as well as the bandwidth of the control system, and thus attenuates the performance of the actuator. To augment the performance, this letter proposes a motor saturation strategy based on the force control to maximize the output torque of the actuator and realize the continuous hopping motion with natural dynamics. The proposed strategy is able to maximize the saturation ratio of motor and thus maximize the foot clearance of the single leg. The dynamics of the two-mass model is utilized to increase the force bandwidth and the performance of the actuator. A single leg with two degrees of freedom is designed as the experiment platform. The actuator consists of a powerful electric motor, a harmonic gear and encoder. The effectiveness of this method is verified through simulations and experiments using a robotic leg actuated by powerful high reduction ratio actuators.",
                    "Supernumerary Robotics Device (SRD) is an ideal solution to provide robotic assistance in overhead manual manipulation. Since two arms are occupied for the overhead task, it is desired to have additional arms to assist us in achieving other subtasks such as supporting the far end of a long plate and pushing it upward to fit in the ceiling. In this study, a method that maps human muscle force to SRD for overhead task assistance is proposed. Our methodology is to utilize redundant DoFs such as the idle muscles in the leg to control the supporting force of the SRD. A sEMG device is worn on the operator's shank where muscle signals are measured, parsed, and transmitted to SRD for control. In the control aspect, we adopted stiffness control in the task space based on torque control at the joint level. We are motivated by the fact that humans can achieve daily manipulation merely through simple inherent compliance property in joint driven by muscles. We explore to estimate the force of some particular muscles in humans and control the SRD to imitate the behaviors of muscle and output supporting forces to accomplish the subtasks such as overhead supporting. The sEMG signals detected from human muscles are extracted, filtered, rectified, and parsed to estimate the muscle force. We use this force information as the intent of the operator for proper overhead supporting force. As one of the well-known compliance control methods, stiffness control is easy to achieve using a few of straightforward parameters such as stiffness and equilibrium point. Through tuning the stiffness and equilibrium point, the supporting force of SRD in task space can be easily controlled. The muscle force estimated by sEMG is mapped to the desired force in the task space of the SRD. The desired force is transferred into stiffness or equilibrium point to output the corresponding supporting force.",
                    "One primary technical challenge in photoacoustic microscopy (PAM) is the necessary compromise between spatial resolution and imaging speed. In this study, we propose a novel application of deep learning principles to reconstruct undersampled PAM images and transcend the trade-off between spatial resolution and imaging speed. We compared various convolutional neural network (CNN) architectures, and selected a fully dense U-net (FD U-net) model that produced the best results. To mimic various undersampling conditions in practice, we artificially downsampled fully-sampled PAM images of mouse brain vasculature at different ratios. This allowed us to not only definitively establish the ground truth, but also train and test our deep learning model at various imaging conditions. Our results and numerical analysis have collectively demonstrated the robust performance of our model to reconstruct PAM images with as few as 2% of the original pixels, which may effectively shorten the imaging time without substantially sacrificing the image quality."
                ],
                "domain": [
                    "Medical Imaging",
                    "Deep Learning",
                    "Robotics",
                    "Ultrasound Imaging"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "df396928-c613-4bc0-8a35-b449a74ad86f": {
                "pk": "df396928-c613-4bc0-8a35-b449a74ad86f",
                "project_name": null,
                "name": "Rui Zhao",
                "bio": "I am a researcher dedicated to advancing the fields of artificial intelligence, data governance, and reinforcement learning. My recent work has focused on developing innovative frameworks such as Dr.Aid, a logic-based AI system for automated compliance checking of data governance rules over data-flow graphs. This framework leverages formal languages to ensure adherence to complex data rules in decentralized contexts, addressing real-world challenges in data management.\n\nIn addition to data governance, I have explored the security vulnerabilities of deep learning systems, particularly concerning adversarial examples. My comprehensive analysis of attack and defense methods has contributed to a deeper understanding of how to safeguard AI models against malicious inputs.\n\nI am also passionate about enhancing user privacy in decentralized web architectures. My work on Data Terms of Use (DToU) introduces a formal description and reasoning system that empowers users to manage their data permissions effectively, promoting a more user-centric approach to data sharing.\n\nIn the realm of reinforcement learning, I have developed novel methodologies to improve sample efficiency and performance in goal-oriented dialogue agents and robotic manipulation tasks. My research on Curiosity-Driven Prioritization and Energy-Based Prioritization frameworks has shown promising results, demonstrating significant improvements in learning efficiency and task performance.\n\nOverall, my research aims to bridge the gap between theoretical advancements and practical applications, ensuring that intelligent systems are not only effective but also ethical and user-friendly.",
                "collaborators": [
                    "Volker Tresp",
                    "Jun Zhao",
                    "Malcolm Atkinson",
                    "Qingfeng Sun",
                    "Zimeng Zhou",
                    "Raymond H. Chan",
                    "Xudong Sun",
                    "Kecheng Zheng",
                    "Zheng-jun Zha"
                ],
                "pub_titles": [
                    "An Automated Framework for Supporting Data-Governance Rule Compliance in Decentralized MIMO Contexts",
                    "The Vulnerability of the Neural Networks Against Adversarial Examples in Deep Learning Algorithms",
                    "Towards a computer-interpretable actionable formal model to encode data governance rules",
                    "Perennial Semantic Data Terms of Use for Decentralized Web",
                    "Learning Goal-Oriented Visual Dialog via Tempered Policy Gradient",
                    "Efficient Dialog Policy Learning via Positive Memory Retention",
                    "Curiosity-Driven Experience Prioritization via Density Estimation",
                    "Bounds for $GL_3$ $L$-functions in depth aspect",
                    "Energy-Based Hindsight Experience Prioritization",
                    "Long-living Service for Cooperative Knowledge Use in Decentralized Data Stores",
                    "A Nuclear-norm Model for Multi-Frame Super-Resolution Reconstruction from Video Clips",
                    "Maximum Entropy-Regularized Multi-Goal Reinforcement Learning",
                    "Stacked Convolutional Deep Encoding Network for Video-Text Retrieval"
                ],
                "pub_abstracts": [
                    "We propose Dr.Aid, a logic-based AI framework for automated compliance checking of data governance rules over data-flow graphs. The rules are modelled using a formal language based on situation calculus and are suitable for decentralized contexts with multi-input-multi-output (MIMO) processes. Dr.Aid models data rules and flow rules and checks compliance by reasoning about the propagation, combination, modification and application of data rules over the data flow graphs. Our approach is driven and evaluated by real-world datasets using provenance graphs from data-intensive research.",
                    "With further development in the fields of computer vision, network security, natural language processing and so on so forth, deep learning technology gradually exposed certain security risks. The existing deep learning algorithms cannot effectively describe the essential characteristics of data, making the algorithm unable to give the correct result in the face of malicious input. Based on current security threats faced by deep learning, this paper introduces the problem of adversarial examples in deep learning, sorts out the existing attack and defense methods of the black box and white box, and classifies them. It briefly describes the application of some adversarial examples in different scenarios in recent years, compares several defense technologies of adversarial examples, and finally summarizes the problems in this research field and prospects for its future development. This paper introduces the common white box attack methods in detail, and further compares the similarities and differences between the attack of the black and white box. Correspondingly, the author also introduces the defense methods, and analyzes the performance of these methods against the black and white box attack.",
                    "With the needs of science and business, data sharing and re-use has become an intensive activity for various areas. In many cases, governance imposes rules concerning data use, but there is no existing computational technique to help data-users comply with such rules. We argue that intelligent systems can be used to improve the situation, by recording provenance records during processing, encoding the rules and performing reasoning. We present our initial work, designing formal models for data rules and flow rules and the reasoning system, as the first step towards helping data providers and data users sustain productive relationships.",
                    "In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal `Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a ``perennial'' DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.",
                    "Learning goal-oriented dialogues by means of deep reinforcement learning has recently become a popular research topic. However, commonly used policy-based dialogue agents often end up focusing on simple utterances and suboptimal policies. To mitigate this problem, we propose a class of novel temperature-based extensions for policy gradient methods, which are referred to as Tempered Policy Gradients (TPGs). On a recent AI-testbed, i.e., the GuessWhat?! game, we achieve significant improvements with two innovations. The first one is an extension of the state-of-the-art solutions with Seq2Seq and Memory Network structures that leads to an improvement of 7%. The second one is the application of our newly developed TPG methods, which improves the performance additionally by around 5% and, even more importantly, helps produce more convincing utterances.",
                    "This paper is concerned with the training of recurrent neural networks as goal-oriented dialog agents using reinforcement learning. Training such agents with policy gradients typically requires a large amount of samples. However, the collection of the required data in form of conversations between chat-bots and human agents is time-consuming and expensive. To mitigate this problem, we describe an efficient policy gradient method using positive memory retention, which significantly increases the sample-efficiency. We show that our method is 10 times more sample-efficient than policy gradients in extensive experiments on a new synthetic number guessing game. Moreover, in a real-word visual object discovery game, the proposed method is twice as sample-efficient as policy gradients and shows state-of-the-art performance.",
                    "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.",
                    "Let $f$ be a Hecke-Maass cusp form for $SL_3(\\mathbb{Z})$ and $\\chi$ a primitive Dirichlet character of prime power conductor $\\mathfrak{q}=p^{\\kappa}$ with $p$ prime and $\\kappa\\geq 10$. We prove a subconvexity bound $$ L\\left(\\frac{1}{2},\\pi\\otimes \\chi\\right)\\ll_{p,\\pi,\\varepsilon} \\mathfrak{q}^{3/4-3/40+\\varepsilon} $$ for any $\\varepsilon>0$, where the dependence of the implied constant on $p$ is explicit and polynomial. We obtain this result by applying the circle method of Kloosterman's version, summation formulas of Poisson and Voronoi's type and a conductor lowering mechanism introduced by Munshi [14]. The main new technical estimates are the essentially square root bounds for some twisted multi-dimensional character sums, which are proved by an elementary method.",
                    "In Hindsight Experience Replay (HER), a reinforcement learning agent is trained by treating whatever it has achieved as virtual goals. However, in previous work, the experience was replayed at random, without considering which episode might be the most valuable for learning. In this paper, we develop an energy-based framework for prioritizing hindsight experience in robotic manipulation tasks. Our approach is inspired by the work-energy principle in physics. We define a trajectory energy function as the sum of the transition energy of the target object over the trajectory. We hypothesize that replaying episodes that have high trajectory energy is more effective for reinforcement learning in robotics. To verify our hypothesis, we designed a framework for hindsight experience prioritization based on the trajectory energy of goal states. The trajectory energy function takes the potential, kinetic, and rotational energy into consideration. We evaluate our Energy-Based Prioritization (EBP) approach on four challenging robotic manipulation tasks in simulation. Our empirical results show that our proposed method surpasses state-of-the-art approaches in terms of both performance and sample-efficiency on all four tasks, without increasing computational time. A video showing experimental results is available at https://youtu.be/jtsF2tTeUGQ",
                    "Personal Data Stores (PDS) like SoLiD is an emerging data and knowledge management solution in recent years. They promise to give back ownership and control of data to the user, and provide protocols for developers to build applications using the data. However, existing Solid-based applications often focus on using a single-user's data. In this article, we use a simple but realistic calendar-and-meeting-scheduling scenario to demonstrate the feasibility and design considerations for enabling cooperative data-use across multiple users' SoLiD Pods. This scenario identifies the bottleneck for certain cooperative use cases, namely those involving offline-changing and synchronization of knowledge information. We demonstrate a viable approach to mediate this issue, introducing a long-living thin service, the orchestrator. We describe our implementation and discuss its applicability to other ecosystems. We conclude by discussing the implication of such services, in particular their risks and challenges for building decentralised applications.",
                    "We propose a variational approach to obtain super-resolution images from multiple low-resolution frames extracted from video clips. First the displacement between the low-resolution frames and the reference frame are computed by an optical flow algorithm. Then a low-rank model is used to construct the reference frame in high-resolution by incorporating the information of the low-resolution frames. The model has two terms: a 2-norm data fidelity term and a nuclear-norm regularization term. Alternating direction method of multipliers is used to solve the model. Comparison of our methods with other models on synthetic and real video clips show that our resulting images are more accurate with less artifacts. It also provides much finer and discernable details.",
                    "In Multi-Goal Reinforcement Learning, an agent learns to achieve multiple goals with a goal-conditioned policy. During learning, the agent first collects the trajectories into a replay buffer, and later these trajectories are selected randomly for replay. However, the achieved goals in the replay buffer are often biased towards the behavior policies. From a Bayesian perspective, when there is no prior knowledge about the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first propose a novel multi-goal RL objective based on weighted entropy. This objective encourages the agent to maximize the expected return, as well as to achieve more diverse goals. Secondly, we developed a maximum entropy-based prioritization framework to optimize the proposed objective. For evaluation of this framework, we combine it with Deep Deterministic Policy Gradient, both with or without Hindsight Experience Replay. On a set of multi-goal robotic tasks of OpenAI Gym, we compare our method with other baselines and show promising improvements in both performance and sample-efficiency.",
                    "Existing dominant approaches for cross-modal video-text retrieval task are to learn a joint embedding space to measure the cross-modal similarity. However, these methods rarely explore long-range dependency inside video frames or textual words leading to insufficient textual and visual details. In this paper, we propose a stacked convolutional deep encoding network for video-text retrieval task, which considers to simultaneously encode long-range and short-range dependency in the videos and texts. Specifically, a multi-scale dilated convolutional (MSDC) block within our approach is able to encode short-range temporal cues between video frames or text words by adopting different scales of kernel size and dilation size of convolutional layer. A stacked structure is designed to expand the receptive fields by repeatedly adopting the MSDC block, which further captures the long-range relations between these cues. Moreover, to obtain more robust textual representations, we fully utilize the powerful language model named Transformer in two stages: pretraining phrase and fine-tuning phrase. Extensive experiments on two different benchmark datasets (MSR-VTT, MSVD) show that our proposed method outperforms other state-of-the-art approaches."
                ],
                "domain": [
                    "Reinforcement Learning",
                    "Data Governance",
                    "Adversarial Machine Learning",
                    "Natural Language Processing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "26eda785-faee-4975-8483-7e26074fe748": {
                "pk": "26eda785-faee-4975-8483-7e26074fe748",
                "project_name": null,
                "name": "Bo Huang",
                "bio": "I am a researcher deeply engaged in the qualitative theory of differential systems, particularly focusing on the study of limit cycles and their bifurcations. My work employs algorithmic approaches to analyze polynomial differential systems, utilizing symbolic computation methods to derive insights into their dynamic behavior. I have developed several algorithms that implement the averaging method, allowing for the transformation of differential systems into normal forms and the computation of averaged functions. \n\nMy recent research has expanded into the realm of ultracold atomic gases, where I investigate the interactions within Bose-Fermi mixtures and their implications for understanding phase separation and dynamic phenomena. I have also explored the integrability and linearizability of three-dimensional polynomial systems, providing criteria and algorithms that enhance our understanding of their behavior.\n\nIn addition to theoretical advancements, I have ventured into practical applications, such as developing a deep learning system for Chinese font generation and proposing a novel loss function for pansharpening tasks in remote sensing. My interdisciplinary approach combines rigorous mathematical analysis with innovative computational techniques, aiming to bridge the gap between theory and application in both mathematics and physics. Through my work, I strive to contribute to the understanding of complex systems and their underlying dynamics, while also addressing real-world challenges through advanced computational methods.",
                "collaborators": [
                    "Dongming Wang",
                    "Chee Yap",
                    "Jiajun Cai",
                    "Fenxi Xiao",
                    "Xia Wu",
                    "Ivan Mastev",
                    "Valery Romanovski",
                    "Leonid A. Sidorenkov",
                    "Rudolf Grimm"
                ],
                "pub_titles": [
                    "Algorithmic Averaging for Studying Periodic Orbits of Planar Differential Systems",
                    "BEC immersed in a Fermi sea: Theory of static and dynamic behavior across phase separation",
                    "Using Symbolic Computation to Analyze Zero-Hopf Bifurcations of Polynomial Differential Systems",
                    "Zero-Hopf Bifurcation of Limit Cycles in Certain Differential Systems",
                    "An Algorithmic Approach to Limit Cycles of Nonlinear Differential Systems: the Averaging Method Revisited",
                    "An Inter- and Intra-Band Loss for Pansharpening Convolutional Neural Networks",
                    "Automatic Generation of Chinese Handwriting via Fonts Style Representation Learning",
                    "Integrability and Linearizability of a Family of Three-Dimensional Polynomial Systems",
                    "Finite-temperature effects on a triatomic Efimov resonance in ultracold cesium"
                ],
                "pub_abstracts": [
                    "One of the main open problems in the qualitative theory of real planar differential systems is the study of limit cycles. In this article, we present an algorithmic approach for detecting how many limit cycles can bifurcate from the periodic orbits of a given polynomial differential center when it is perturbed inside a class of polynomial differential systems via the averaging method. We propose four symbolic algorithms to implement the averaging method. The first algorithm is based on the change of polar coordinates that allows one to transform a considered differential system to the normal form of averaging. The second algorithm is used to derive the solutions of certain differential systems associated to the unperturbed term of the normal of averaging. The third algorithm exploits the partial Bell polynomials and allows one to compute the integral formula of the averaged functions at any order. The last algorithm is based on the aforementioned algorithms and determines the exact expressions of the averaged functions for the considered differential systems. The implementation of our algorithms is discussed and evaluated using several examples. The experimental results have extended the existing relevant results for certain classes of differential systems.",
                    "We theoretically study the static and dynamic behavior of a BEC immersed in a large Fermi sea of ultracold atoms under conditions of tunable interspecies interaction. The degenerate Bose-Fermi mixture is kept in an elongated trap, typical for a single-beam optical dipole trap. We focus on the case of repulsive Bose-Fermi interaction and develop mean-field models to simulate the system over a wide range of repulsion strength. We further get analytical solutions in the regimes of phase separation and weak interaction. We obtain static density profiles and the frequency of the radial breathing mode, which is an elementary dynamic phenomenon of the mixture. Our results unveil the structure of the Bose-Fermi interface and describe the origin of the frequency shift of the breathing mode when the components become phase-separated at strong repulsion. We show that the mediated interaction between bosons induced by the Fermi sea can be understood as an adiabatic second-order mean-field effect, which is valid also beyond the weak-interaction regime for relevant experimental conditions. These results are consistent with our recent observations in a mixture of $^{41}$K and $^6$Li.",
                    "This paper is devoted to the study of infinitesimal limit cycles that can bifurcate from zero-Hopf equilibria of differential systems based on the averaging method. We develop an efficient symbolic program using Maple for computing the averaged functions of any order for continuous differential systems in arbitrary dimension. The program allows us to systematically analyze zero-Hopf bifurcations of polynomial differential systems using symbolic computation methods. We show that for the first-order averaging, $\\ell\\in\\{0,1,\\ldots,2^{n-3}\\}$ limit cycles can bifurcate from the zero-Hopf equilibrium for the general class of perturbed differential systems and up to the second-order averaging, the maximum number of limit cycles can be determined by computing the mixed volume of a polynomial system obtained from the averaged functions. A number of examples are presented to demonstrate the effectiveness of the proposed algorithmic approach.",
                    "This paper studies the number of limit cycles that may bifurcate from an equilibrium of an autonomous system of differential equations. The system in question is assumed to be of dimension $n$, have a zero-Hopf equilibrium at the origin, and consist only of homogeneous terms of order $m$. Denote by $H_k(n,m)$ the maximum number of limit cycles of the system that can be detected by using the averaging method of order $k$. We prove that $H_1(n,m)\\leq(m-1)\\cdot m^{n-2}$ and $H_k(n,m)\\leq(km)^{n-1}$ for generic $n\\geq3$, $m\\geq2$ and $k>1$. The exact numbers of $H_k(n,m)$ or tight bounds on the numbers are determined by computing the mixed volumes of some polynomial systems obtained from the averaged functions. Based on symbolic and algebraic computation, a general and algorithmic approach is proposed to derive sufficient conditions for a given differential system to have a prescribed number of limit cycles. The effectiveness of the proposed approach is illustrated by a family of third-order differential equations and by a four-dimensional hyperchaotic differential system.",
                    "This paper introduces an algorithmic approach to the analysis of bifurcation of limit cycles from the centers of nonlinear continuous differential systems via the averaging method. We develop three algorithms to implement the averaging method. The first algorithm allows to transform the considered differential systems to the normal formal of averaging. Here, we restricted the unperturbed term of the normal form of averaging to be identically zero. The second algorithm is used to derive the computational formulae of the averaged functions at any order. The third algorithm is based on the first two algorithms that determines the exact expressions of the averaged functions for the considered differential systems. The proposed approach is implemented in Maple and its effectiveness is shown by several examples. Moreover, we report some incorrect results in published papers on the averaging method.",
                    "Pansharpening aims to fuse panchromatic and multispectral images from the satellite to generate images with both high spatial and spectral resolution. With the successful applications of deep learning in the computer vision field, a lot of scholars have proposed many convolutional neural networks (CNNs) to solve the pansharpening task. These pansharpening networks focused on various distinctive structures of CNNs, and most of them are trained by L2 loss between fused images and simulated desired multispectral images. However, L2 loss is designed to directly minimize the difference of spectral information of each band, which does not consider the inter-band relations in the training process. In this letter, we propose a novel inter- and intra-band (IIB) loss to overcome the drawback of original L2 loss. Our proposed IIB loss can effectively preserve both inter- and intra-band relations and can be directly applied to different pansharpening CNNs.",
                    "In this paper, we propose and end-to-end deep Chinese font generation system. This system can generate new style fonts by interpolation of latent style-related embeding variables that could achieve smooth transition between different style. Our method is simpler and more effective than other methods, which will help to improve the font design efficiency",
                    "We investigate the local integrability and linearizability of a family of three-dimensional polynomial systems with the matrix of the linear approximation having the eigenvalues $1, \\zeta, \\zeta^2 $, where $\\zeta$ is a primitive cubic root of unity. We establish a criterion for the convergence of the Poincar\\'e--Dulac normal form of the systems and examine the relationship between the normal form and integrability. Additionally, we introduce an efficient algorithm to determine the necessary conditions for the integrability of the systems. This algorithm is then applied to a quadratic subfamily of the systems to analyze its integrability and linearizability. Our findings offer insights into the integrability properties of three-dimensional polynomial systems.",
                    "We report a thorough investigation of finite-temperature effects on three-body recombination near a triatomic Efimov resonance in an ultracold gas of cesium atoms. Our measurements cover a wide range from a near-ideal realization of the zero-temperature limit to a strongly temperature-dominated regime. The experimental results are analyzed within a recently introduced theoretical model based on a universal zero-range theory. The temperature-induced shift of the resonance reveals a contribution that points to an energy-dependence of the three-body parameter. We interpret this contribution in terms of the finite range of the van der Waals interaction in real atomic systems and we quantify it in an empirical way based on length scale arguments. A universal character of the corresponding resonance shift is suggested by observations related to other Efimov resonances and the comparison with a theoretical finite-temperature approach that explicitly takes the van der Waals interaction into account. Our findings are of importance for the precise determination of Efimov resonance positions from experiments at finite temperatures."
                ],
                "domain": [
                    "Differential Equations",
                    "Bifurcation Theory",
                    "Machine Learning",
                    "Image Processing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "764554f2-7196-4937-af38-3d030757001d": {
                "pk": "764554f2-7196-4937-af38-3d030757001d",
                "project_name": null,
                "name": "Jiameng Zhao",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I am passionate about understanding the broader design space of GNNs. My work on AutoTransfer and FALCON aims to streamline the process of finding optimal model designs by leveraging prior knowledge and efficiently navigating the design space. I believe that by systematically studying these dimensions, we can unlock new potentials in machine learning applications.\n\nOverall, my research is driven by a desire to push the boundaries of GNNs, making them more effective and applicable to real-world challenges while fostering a deeper understanding of their underlying principles.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "216dfb74-3c2d-4298-afdc-dd3faabea763": {
                "pk": "216dfb74-3c2d-4298-afdc-dd3faabea763",
                "project_name": null,
                "name": "Yipu Liao",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance on tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge and enhancing search efficiency.\n\nThrough these contributions, I strive to push the boundaries of what GNNs can achieve, fostering a deeper understanding of their structure and performance while making them more accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "5cf8230a-1b98-4d4a-ac86-01e1f28b48fa": {
                "pk": "5cf8230a-1b98-4d4a-ac86-01e1f28b48fa",
                "project_name": null,
                "name": "Ke Li",
                "bio": "I am a researcher deeply engaged in the fields of multi-objective optimization, quantum information theory, and machine learning. My work primarily focuses on the development and refinement of algorithms that enhance decision-making processes and optimize complex systems. I have extensively explored the multi-objective evolutionary algorithm based on decomposition (MOEA/D), providing comprehensive surveys that detail its evolution and core design components. \n\nIn the realm of quantum hypothesis testing, I have contributed to understanding the error probabilities associated with multiple quantum hypotheses, proving significant conjectures and establishing new bounds that have implications for quantum information theory. My research also delves into the challenges of amodal instance segmentation, where I developed a novel method that leverages standard annotations to predict occluded object regions.\n\nI am particularly passionate about automating algorithm design through reinforcement learning, where I have successfully created methods that outperform traditional optimization algorithms. Additionally, I have investigated multi-fidelity optimization strategies, presenting a systematic exploration of their methodologies and applications across various domains.\n\nMy recent work emphasizes the importance of addressing the limitations of existing algorithms, such as those for k-nearest neighbor searches, by proposing innovative strategies that enhance efficiency and adaptability. Overall, I strive to bridge theoretical advancements with practical applications, fostering collaborations that push the boundaries of what is possible in optimization and machine learning.",
                "collaborators": [
                    "Jitendra Malik",
                    "Bing Zhu",
                    "Fan Li",
                    "Graeme Smith"
                ],
                "pub_titles": [
                    "Decomposition Multi-Objective Evolutionary Optimization: From State-of-the-Art to Future Opportunities",
                    "A Survey of Decomposition-Based Evolutionary Multi-Objective Optimization: Part I-Past and Future",
                    "Second-order asymptotics for quantum hypothesis testing",
                    "Discriminating quantum states: the multiple Chernoff distance",
                    "Optimal Partitioned-Interval Detection Binary Quantum Receiver with Practical Devices",
                    "Amodal Instance Segmentation",
                    "Learning to Optimize",
                    "On the Implicit Assumptions of GANs",
                    "Multi-Fidelity Methods for Optimization: A Survey",
                    "Quantum de Finetti theorem under fully-one-way adaptive measurements",
                    "Bandit Label Inference for Weakly Supervised Learning",
                    "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing",
                    "Fast k-Nearest Neighbour Search via Prioritized DCI",
                    "Learning to Optimize Neural Nets"
                ],
                "pub_abstracts": [
                    "Decomposition has been the mainstream approach in the classic mathematical programming for multi-objective optimization and multi-criterion decision-making. However, it was not properly studied in the context of evolutionary multi-objective optimization until the development of multi-objective evolutionary algorithm based on decomposition (MOEA/D). In this article, we present a comprehensive survey of the development of MOEA/D from its origin to the current state-of-the-art approaches. In order to be self-contained, we start with a step-by-step tutorial that aims to help a novice quickly get onto the working mechanism of MOEA/D. Then, selected major developments of MOEA/D are reviewed according to its core design components including weight vector settings, sub-problem formulations, selection mechanisms and reproduction operators. Besides, we also overviews some further developments for constraint handling, computationally expensive objective functions, preference incorporation, and real-world applications. In the final part, we shed some lights on emerging directions for future developments.",
                    "Decomposition has been the mainstream approach in classic mathematical programming for multi-objective optimization and multi-criterion decision-making. However, it was not properly studied in the context of evolutionary multi-objective optimization (EMO) until the development of multi-objective evolutionary algorithm based on decomposition (MOEA/D). In this two-part survey series, we use MOEA/D as the representative of decomposition-based EMO to review the up-to-date development in this area, and systematically and comprehensively analyze its research landscape. In the first part, we present a comprehensive survey of the development of MOEA/D from its origin to the current state-of-the-art approaches. In order to be self-contained, we start with a step-by-step tutorial that aims to help a novice quickly get onto the working mechanism of MOEA/D. Then, selected major developments of MOEA/D are reviewed according to its core design components including weight vector settings, subproblem formulations, selection mechanisms and reproduction operators. Besides, we also overview some selected advanced topics for constraint handling, optimization in dynamic and uncertain environments, computationally expensive objective functions, and preference incorporation. In the final part, we shed some light on emerging directions for future developments.",
                    "In the asymptotic theory of quantum hypothesis testing, the minimal error probability of the first kind jumps sharply from zero to one when the error exponent of the second kind passes by the point of the relative entropy of the two states in an increasing way. This is well known as the direct part and strong converse of quantum Stein's lemma. Here we look into the behavior of this sudden change and have make it clear how the error of first kind grows smoothly according to a lower order of the error exponent of the second kind, and hence we obtain the second-order asymptotics for quantum hypothesis testing. This actually implies quantum Stein's lemma as a special case. Meanwhile, our analysis also yields tight bounds for the case of finite sample size. These results have potential applications in quantum information theory. Our method is elementary, based on basic linear algebra and probability theory. It deals with the achievability part and the optimality part in a unified fashion.",
                    "We consider the problem of testing multiple quantum hypotheses $\\{\\rho_1^{\\otimes n},\\ldots,\\rho_r^{\\otimes n}\\}$, where an arbitrary prior distribution is given and each of the $r$ hypotheses is $n$ copies of a quantum state. It is known that the average error probability $P_e$ decays exponentially to zero, that is, $P_e=\\exp\\{-\\xi n+o(n)\\}$. However, this error exponent $\\xi$ is generally unknown, except for the case that $r=2$.   In this paper, we solve the long-standing open problem of identifying the above error exponent, by proving Nussbaum and Szko\\l a's conjecture that $\\xi=\\min_{i\\neq j}C(\\rho_i,\\rho_j)$. The right-hand side of this equality is called the multiple quantum Chernoff distance, and $C(\\rho_i,\\rho_j):=\\max_{0\\leq s\\leq 1}\\{-\\log\\operatorname{Tr}\\rho_i^s\\rho_j^{1-s}\\}$ has been previously identified as the optimal error exponent for testing two hypotheses, $\\rho_i^{\\otimes n}$ versus $\\rho_j^{\\otimes n}$.   The main ingredient of our proof is a new upper bound for the average error probability, for testing an ensemble of finite-dimensional, but otherwise general, quantum states. This upper bound, up to a states-dependent factor, matches the multiple-state generalization of Nussbaum and Szko\\l a's lower bound. Specialized to the case $r=2$, we give an alternative proof to the achievability of the binary-hypothesis Chernoff distance, which was originally proved by Audenaert et al.",
                    "Partitioned-interval detection binary quantum receiver with non-ideal devices is theoretically analyzed. Using global optimized partition strategy, relatively large gain over standard quantum limit (SQL) is obtained with small partition number for certain mean photon number.",
                    "We consider the problem of amodal instance segmentation, the objective of which is to predict the region encompassing both visible and occluded parts of each object. Thus far, the lack of publicly available amodal segmentation annotations has stymied the development of amodal segmentation methods. In this paper, we sidestep this issue by relying solely on standard modal instance segmentation annotations to train our model. The result is a new method for amodal instance segmentation, which represents the first such method to the best of our knowledge. We demonstrate the proposed method's effectiveness both qualitatively and quantitatively.",
                    "Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.",
                    "Generative adversarial nets (GANs) have generated a lot of excitement. Despite their popularity, they exhibit a number of well-documented issues in practice, which apparently contradict theoretical guarantees. A number of enlightening papers have pointed out that these issues arise from unjustified assumptions that are commonly made, but the message seems to have been lost amid the optimism of recent years. We believe the identified problems deserve more attention, and highlight the implications on both the properties of GANs and the trajectory of research on probabilistic models. We recently proposed an alternative method that sidesteps these problems.",
                    "Real-world black-box optimization often involves time-consuming or costly experiments and simulations. Multi-fidelity optimization (MFO) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach. This survey presents a systematic exploration of MFO, underpinned by a novel text mining framework based on a pre-trained language model. We delve deep into the foundational principles and methodologies of MFO, focusing on three core components -- multi-fidelity surrogate models, fidelity management strategies, and optimization techniques. Additionally, this survey highlights the diverse applications of MFO across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of MFO in tackling complex computational challenges. Furthermore, we also envision several emerging challenges and prospects in the MFO landscape, spanning scalability, the composition of lower fidelities, and the integration of human-in-the-loop approaches at the algorithmic level. We also address critical issues related to benchmarking and the advancement of open science within the MFO community. Overall, this survey aims to catalyze further research and foster collaborations in MFO, setting the stage for future innovations and breakthroughs in the field.",
                    "We prove a version of the quantum de Finetti theorem: permutation-invariant quantum states are well approximated as a probabilistic mixture of multi-fold product states. The approximation is measured by distinguishability under fully one-way LOCC (local operations and classical communication) measurements. Our result strengthens Brand\\~{a}o and Harrow's de Finetti theorem where a kind of partially one-way LOCC measurements was used for measuring the approximation, with essentially the same error bound. As main applications, we show (i) a quasipolynomial-time algorithm which detects multipartite entanglement with amount larger than an arbitrarily small constant (measured with a variant of the relative entropy of entanglement), and (ii) a proof that in quantum Merlin-Arthur proof systems, polynomially many provers are not more powerful than a single prover when the verifier is restricted to one-way LOCC operations.",
                    "The scarcity of data annotated at the desired level of granularity is a recurring issue in many applications. Significant amounts of effort have been devoted to developing weakly supervised methods tailored to each individual setting, which are often carefully designed to take advantage of the particular properties of weak supervision regimes, form of available data and prior knowledge of the task at hand. Unfortunately, it is difficult to adapt these methods to new tasks and/or forms of data, which often require different weak supervision regimes or models. We present a general-purpose method that can solve any weakly supervised learning problem irrespective of the weak supervision regime or the model. The proposed method turns any off-the-shelf strongly supervised classifier into a weakly supervised classifier and allows the user to specify any arbitrary weakly supervision regime via a loss function. We apply the method to several different weak supervision regimes and demonstrate competitive results compared to methods specifically engineered for those settings.",
                    "Existing methods for retrieving k-nearest neighbours suffer from the curse of dimensionality. We argue this is caused in part by inherent deficiencies of space partitioning, which is the underlying strategy used by most existing methods. We devise a new strategy that avoids partitioning the vector space and present a novel randomized algorithm that runs in time linear in dimensionality of the space and sub-linear in the intrinsic dimensionality and the size of the dataset and takes space constant in dimensionality of the space and linear in the size of the dataset. The proposed algorithm allows fine-grained control over accuracy and speed on a per-query basis, automatically adapts to variations in data density, supports dynamic updates to the dataset and is easy-to-implement. We show appealing theoretical properties and demonstrate empirically that the proposed algorithm outperforms locality-sensitivity hashing (LSH) in terms of approximation quality, speed and space efficiency.",
                    "Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) offers a promising way of circumventing the curse and successfully reduces the dependence of query time on intrinsic dimensionality from exponential to sublinear. In this paper, we propose a variant of DCI, which we call Prioritized DCI, and show a remarkable improvement in the dependence of query time on intrinsic dimensionality. In particular, a linear increase in intrinsic dimensionality, or equivalently, an exponential increase in the number of points near a query, can be mostly counteracted with just a linear increase in space. We also demonstrate empirically that Prioritized DCI significantly outperforms prior methods. In particular, relative to Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumption by a factor of 21.",
                    "Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100."
                ],
                "domain": [
                    "Multi-objective Optimization",
                    "Quantum Information Theory",
                    "Weakly Supervised Learning",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "66a1e535-3bf3-4263-bf7e-ffa301bbaaf5": {
                "pk": "66a1e535-3bf3-4263-bf7e-ffa301bbaaf5",
                "project_name": null,
                "name": "Lina Zhao",
                "bio": "I am a researcher specializing in numerical methods for fluid dynamics and porous media, with a particular focus on developing robust and efficient computational techniques. My recent work has centered on mixed-type discontinuous Galerkin (DG) methods for complex coupled problems, such as the Stokes-Darcy and Brinkman-Darcy flows. I have pioneered novel formulations that ensure mass conservation and stability, even in challenging scenarios involving highly heterogeneous media and non-matching grids.\n\nMy research emphasizes the importance of rigorous convergence analysis and the development of adaptive mesh refinement strategies to enhance computational efficiency. I have also explored the application of multiscale finite element methods (MsFEM) to address steady-state problems, demonstrating their effectiveness in capturing multiscale features in porous media.\n\nIn addition to my theoretical contributions, I have conducted extensive numerical experiments that validate my methods and showcase their practical applicability. I am particularly interested in the interplay between mathematical modeling and computational implementation, striving to create methods that are not only theoretically sound but also efficient and user-friendly for real-world applications. My goal is to advance the field of computational fluid dynamics and contribute to solving complex engineering problems through innovative numerical techniques.",
                "collaborators": [
                    "Eric Chung",
                    "Eun-Jae Park",
                    "Ming Fai Lam",
                    "Shuyu Sun",
                    "Eric T. Chung",
                    "Zhiwen Zeng",
                    "Xiangke Wang",
                    "Zhiqiang Zheng",
                    "Yiran Wang",
                    "Shubin Fu"
                ],
                "pub_titles": [
                    "A Robin-type domain decomposition method for a novel mixed-type DG method for the coupled Stokes-Darcy problem",
                    "A posteriori error estimates for the mortar staggered DG method",
                    "Adaptive staggered DG method for Darcy flows in fractured porous media",
                    "A strongly mass conservative method for the coupled Brinkman-Darcy flow and transport",
                    "Constraint energy minimizing generalized multiscale finite element method for convection diffusion equation",
                    "Locking free staggered DG method for the Biot system of poroelasticity on general polygonal meshes",
                    "An analysis of the NLMC upscaling method for high contrast problems",
                    "Staggered discontinuous Galerkin methods for the Helmholtz equations with large wave number",
                    "A pressure robust staggered discontinuous Galerkin method for the Stokes equations",
                    "A uniformly robust staggered DG method for the unsteady Darcy-Forchheimer-Brinkman problem",
                    "Edge Agreement of Second-order Multi-agent System with Dynamic Quantization via Directed Edge Laplacian",
                    "Constraint energy minimization generalized multiscale finite element method in mixed formulation for parabolic equations",
                    "Generalized multiscale finite element method for highly heterogeneous compressible flow",
                    "Multiscale finite element method for Stokes-Darcy model",
                    "Staggered DG method for coupling of the Stokes and Darcy-Forchheimer problems",
                    "A new staggered DG method for the Brinkman problem robust in the Darcy and Stokes limits",
                    "Staggered DG method with small edges for Darcy flows in fractured porous media"
                ],
                "pub_abstracts": [
                    "In this paper, we first propose and analyze a novel mixed-type DG method for the coupled Stokes-Darcy problem on simplicial meshes. The proposed formulation is locally conservative. A mixed-type DG method in conjunction with the stress-velocity formulation is employed for the Stokes equations, where the symmetry of stress is strongly imposed. The staggered DG method is exploited to discretize the Darcy equations. As such, the discrete formulation can be easily adapted to account for the Beavers-Joseph-Saffman interface conditions without introducing additional variables. Importantly, the continuity of normal velocity is satisfied exactly at the discrete level. A rigorous convergence analysis is performed for all the variables. Then we devise and analyze a domain decomposition method via the use of Robin-type interface boundary conditions, which allows us to solve the Stokes subproblem and the Darcy subproblem sequentially with low computational costs. The convergence of the proposed iterative method is analyzed rigorously. In particular, the proposed iterative method also works for very small viscosity coefficient. Finally, several numerical experiments are carried out to demonstrate the capabilities and accuracy of the novel mixed-type scheme, and the convergence of the domain decomposition method.",
                    "Two residual-type error estimators for the mortar staggered discontinuous Galerkin discretizations of second order elliptic equations are developed. Both error estimators are proved to be reliable and efficient. Key to the derivation of the error estimator in potential $L^2$ error is the duality argument. On the other hand, an auxiliary function is defined, making it capable of decomposing the energy error into conforming part and nonconforming part, which can be combined with the well-known Scott-Zhang local quasi-interpolation operator and the mortar discrete formulation yields an error estimator in energy error. Importantly, our analysis for both error estimators does not require any saturation assumptions which are often needed in the literature. Several numerical experiments are presented to confirm our proposed theories.",
                    "Modeling flows in fractured porous media is important in applications. One main challenge in numerical simulation is that the flow is strongly influenced by the fractures, so that the solutions typically contain complex features, which require high computational grid resolutions. Instead of using uniformly fine mesh, a more computationally efficient adaptively refined mesh is desirable. In this paper we design and analyze a novel residual-type a posteriori error estimator for staggered DG methods on general polygonal meshes for Darcy flows in fractured porous media. The method can handle fairly general meshes and hanging nodes can be simply incorporated into the construction of the method, which is highly appreciated for adaptive mesh refinement. The reliability and efficiency of the error estmator are proved. The derivation of the reliability hinges on the stability of the continuous setting in the primal formulation. A conforming counterpart that is continuous within each bulk domain for the discrete bulk pressure is defined to facilitate the derivation of the reliability. Finally, several numerical experiments including multiple non-intersecting fractures are carried out to confirm the proposed theories.",
                    "In this paper, a strongly mass conservative and stabilizer free scheme is designed and analyzed for the coupled Brinkman-Darcy flow and transport. The flow equations are discretized by using a strongly mass conservative scheme in mixed formulation with a suitable incorporation of the interface conditions. In particular, the interface conditions can be incorporated into the discrete formulation naturally without introducing additional variables. Moreover, the proposed scheme behaves uniformly robust for various values of viscosity. A novel upwinding staggered DG scheme in mixed form is exploited to solve the transport equation, where the boundary correction terms are added to improve the stability. A rigorous convergence analysis is carried out for the approximation of the flow equations. The velocity error is shown to be independent of the pressure and thus confirms the pressure-robustness. Stability and a priori error estimates are also obtained for the approximation of the transport equation; moreover, we are able to achieve a sharp stability and convergence error estimates thanks to the strong mass conservation preserved by our scheme. In particular, the stability estimate depends only on the true velocity on the inflow boundary rather than on the approximated velocity. Several numerical experiments are presented to verify the theoretical findings and demonstrate the performances of the method.",
                    "In this paper we present and analyze a constraint energy minimizing generalized multiscale finite element method for convection diffusion equation. To define the multiscale basis functions, we first build an auxiliary multiscale space by solving local spectral problems motivated by analysis. Then constraint energy minimization performed in oversampling domains is exploited to construct the multiscale space. The resulting multiscale basis functions have a good decay property even for high contrast diffusion and convection coefficients. Furthermore, if the number of oversampling layer is chosen properly, we can prove that the convergence rate is proportional to the coarse mesh size. Our analysis also indicates that the size of the oversampling domain weakly depends on the contrast of the heterogeneous coefficients. Several numerical experiments are presented illustrating the performances of our method.",
                    "In this paper we propose and analyze a staggered discontinuous Galerkin method for a five-field formulation of the Biot system of poroelasticity on general polygonal meshes. Elasticity is equipped with stress-displacement-rotation formulation with weak stress symmetry for arbitrary polynomial orders, which extends the piecewise constant approximation developed in (L. Zhao and E.-J. Park, SIAM J. Sci. Comput. 42 (2020), A2158-A2181). The proposed method is locking free and can handle highly distorted grids possibly including hanging nodes, which is desirable for practical applications. We prove the convergence estimates for the semi-discrete scheme and fully discrete scheme for all the variables in their natural norms. In particular, the stability and convergence analysis do not need a uniformly positive storativity coefficient. Moreover, to reduce the size of the global system, we propose a five-field formulation based fixed stress splitting scheme, where the linear convergence of the scheme is proved. Several numerical experiments are carried out to confirm the optimal convergence rates and the locking-free property of the proposed method.",
                    "In this paper we propose simple multiscale basis functions with constraint energy minimization to solve elliptic problems with high contrast medium. Our methodology is based on the recently developed non-local multicontinuum method (NLMC). The main ingredient of the method is the construction of suitable local basis functions with the capability of capturing multiscale features and non-local effects. In our method, each coarse block is decomposed into various regions according to the contrast ratio, and we require that the contrast ratio should be relatively small within each region. The basis functions are constructed by solving a local problem defined on the oversampling domains and they have mean value one on the chosen region and zero mean otherwise. Numerical analysis shows that the resulting basis functions can be localizable and have a decay property. The convergence of the multiscale solution is also proved. Finally, some numerical experiments are carried out to illustrate the performances of the proposed method. They show that the proposed method can solve problem with high contrast medium efficiently. In particular, if the oversampling size is large enough, then we can achieve the desired error.",
                    "In this paper we investigate staggered discontinuous Galerkin method for the Helmholtz equation with large wave number on general quadrilateral and polygonal meshes. The method is highly flexible by allowing rough grids such as the trapezoidal grids and highly distorted grids, and at the same time, is numerical flux free. Furthermore, it allows hanging nodes, which can be simply treated as additional vertices. By exploiting a modified duality argument, the stability and convergence can be proved under the condition that $\\kappa h$ is sufficiently small, where $\\kappa$ is the wave number and $h$ is the mesh size. Error estimates for both the scalar and vector variables in $L^2$ norm are established. Several numerical experiments are tested to verify our theoretical results and to present the capability of our method for capturing singular solutions.",
                    "In this paper we propose a pressure robust staggered discontinuous Galerkin method for the Stokes equations on general polygonal meshes by using piecewise constant approximations. We modify the right hand side of the body force in the discrete formulation by exploiting divergence preserving velocity reconstruction operator, which is the crux for pressure independent velocity error estimates. The optimal convergence for velocity gradient, velocity and pressure are proved. In addition, we are able to prove the superconvergence of velocity approximation by the incorporation of divergence preserving velocity reconstruction operator in the dual problem, which is also an important contribution of this paper. Finally, several numerical experiments are carried out to confirm the theoretical findings.",
                    "In this paper we propose and analyze a uniformly robust staggered DG method for the unsteady Darcy-Forchheimer-Brinkman problem. Our formulation is based on velocity gradient-velocity-pressure and the resulting scheme can be flexibly applied to fairly general polygonal meshes. We relax the tangential continuity for velocity, which is the key ingredient in achieving the uniform robustness. We present well-posedness and error analysis for both the semi-discrete scheme and the fully discrete scheme, and the theories indicate that the error estimates for velocity are independent of pressure. Several numerical experiments are presented to confirm the theoretical findings.",
                    "This work explores the edge agreement problem of second-order multi-agent system with dynamic quantization under directed communication. To begin with, by virtue of the directed edge laplacian, we derive a model reduction representation of the closed-loop multi-agent system depended on the spanning tree subgraph. Considering the limitations of the finite bandwidth channels, the quantization effects of second-order multi-agent system under directed graph are considered. Motivated by the observation that the static quantizer always lead to the practical stability rather than the asymptotic stability, the dynamic quantized communication strategy referring to the rooming in-rooming out scheme is employed. Based on the reduced model associated with the essential edge Laplacian, the asymptotic stability of second-order multi-agent system under dynamic quantized effects with only finite quantization level can be guaranteed. Finally, simulation results are provided to verify the theoretical analysis.",
                    "In this paper, we develop the constraint energy minimization generalized multiscale finite element method (CEM-GMsFEM) in mixed formulation applied to parabolic equations with heterogeneous diffusion coefficients. The construction of the method is based on two multiscale spaces: pressure multiscale space and velocity multiscale space. The pressure space is constructed via a set of well-designed local spectral problems, which can be solved independently. Based on the computed pressure multiscale space, we will construct the velocity multiscale space by applying constrained energy minimization. The convergence of the proposed method is proved.In particular, we prove that the convergence of the method depends only on the coarse grid size, and is independent of the heterogeneities and contrast of thediffusion coefficient. Four typical types of permeability fields are exploited in the numerical simulations, and the results indicate that our proposed method works well and gives efficient and accurate numerical solutions.",
                    "In this paper, we study the generalized multiscale finite element method (GMsFEM) for single phase compressible flow in highly heterogeneous porous media. We follow the major steps of the GMsFEM to construct permeability dependent offline basis for fast coarse-grid simulation. The offline coarse space is efficiently constructed only once based on the initial permeability field with parallel computing. A rigorous convergence analysis is performed for two types of snapshot spaces. The analysis indicates that the convergence rates of the proposed multiscale method depend on the coarse meshsize and the eigenvalue decay of the local spectral problem. To further increase the accuracy of multiscale method, residual driven online multiscale basis is added to the offline space. The construction of online multiscale basis is based on a carefully design error indicator motivated by the analysis. We find that online basis is particularly important for the singular source. Rich numerical tests on typical 3D highly heterogeneous medias are presented to demonstrate the impressive computational advantages of the proposed multiscale method.",
                    "This paper explores the application of the multiscale finite element method (MsFEM) to address steady-state Stokes-Darcy problems with BJS interface conditions in highly heterogeneous porous media. We assume the existence of multiscale features in the Darcy region and propose an algorithm for the multiscale Stokes-Darcy model. During the offline phase, we employ MsFEM to construct permeability-dependent offline bases for efficient coarse-grid simulation, with this process conducted in parallel to enhance its efficiency. In the online phase, we use the Robin-Robin algorithm to derive the model's solution. Subsequently, we conduct error analysis based on $L^2$ and $H^1$ norms, assuming certain periodic coefficients in the Darcy region. To validate our approach, we present extensive numerical tests on highly heterogeneous media, illustrating the results of the error analysis.",
                    "In this paper we develop a staggered discontinuous Galerkin method for the Stokes and Darcy-Forchheimer problems coupled with the \\Red{Beavers-Joseph-Saffman} conditions. The method is defined by imposing staggered continuity for all the variables involved and the interface conditions are enforced by switching the roles of the variables met on the interface, which eliminate the hassle of introducing additional variables. This method can be flexibly applied to rough grids such as the highly distorted grids and the polygonal grids. In addition, the method allows nonmatching grids on the interface thanks to the special inclusion of the interface conditions, which is highly appreciated from a practical point of view. A new discrete trace inequality and a generalized Poincar\\'{e}-Friedrichs inequality are proved, which enables us to prove the optimal convergence estimates under reasonable regularity assumptions. Finally, several numerical experiments are given to illustrate the performances of the proposed method, and the numerical results indicate that the proposed method is accurate and efficient, in addition, it is a good candidate for practical applications.",
                    "In this paper we propose a novel staggered discontinuous Galerkin method for the Brinkman problem on general quadrilateral and polygonal meshes. The proposed method is robust in the Stokes and Darcy limits, in addition, hanging nodes can be automatically incorporated in the construction of the method, which are desirable features in practical applications. There are three unknowns involved in our formulation, namely velocity gradient, velocity and pressure. Unlike the original staggered DG formulation proposed for the Stokes equations in \\cite{KimChung13}, we relax the tangential continuity of velocity and enforce different staggered continuity properties for the three unknowns, which is tailored to yield an optimal $L^2$ error estimates for velocity gradient, velocity and pressure independent of the viscosity coefficient. Moreover, by choosing suitable projection, superconvergence can be proved for $L^2$ error of velocity. Finally, several numerical results illustrating the good performances of the proposed method and confirming the theoretical findings are presented.",
                    "In this paper, we present and analyze a staggered discontinuous Galerkin method for Darcy flows in fractured porous media on fairly general meshes. A staggered discontinuous Galerkin method and a standard conforming finite element method with appropriate inclusion of interface conditions are exploited for the bulk region and the fracture, respectively. Our current analysis weakens the usual assumption on the polygonal mesh, which can integrate more general meshes such as elements with arbitrarily small edges into our theoretical framework. We prove the optimal convergence estimates in $L^2$ error for all the variables by exploiting the Ritz projection. Importantly, our error estimates are shown to be fully robust with respect to the heterogeneity and anisotropy of the permeability coefficients. Several numerical experiments including meshes with small edges and anisotropic meshes are carried out to confirm the theoretical findings. Finally, our method is applied in the framework of unfitted mesh."
                ],
                "domain": [
                    "Numerical Analysis",
                    "Finite Element Method",
                    "Computational Fluid Dynamics",
                    "Porous Media"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "91989b21-c5a8-4440-a8fb-9d1dc9bc021e": {
                "pk": "91989b21-c5a8-4440-a8fb-9d1dc9bc021e",
                "project_name": null,
                "name": "Jun Cao",
                "bio": "I am a researcher deeply engaged in the field of neutrino physics, with a particular focus on the Daya Bay Neutrino Experiment and its implications for understanding neutrino mixing angles, especially \\(\\theta_{13}\\). My work has contributed to the precise measurement of this critical parameter, which is essential for advancing our knowledge of neutrino oscillations. I have been involved in various aspects of reactor neutrino experiments, including the design and implementation of detectors, background estimation, and the analysis of neutrino flux, which is vital for reducing uncertainties in measurements.\n\nMy research also extends to the theoretical underpinnings of particle interactions, where I have explored the implications of perturbative QCD in analyzing glueball decays and deuteron form factors. I have developed models that align with experimental data, providing insights into the behavior of these particles under different conditions. Additionally, I have investigated the potential for reactor neutrino experiments to determine mass hierarchy, employing innovative Fourier analysis techniques to enhance sensitivity.\n\nThrough my publications, I aim to bridge the gap between experimental findings and theoretical models, contributing to a deeper understanding of fundamental particle physics. I am committed to advancing the field through collaborative efforts and innovative research methodologies, with the ultimate goal of unraveling the mysteries of neutrino behavior and their role in the universe.",
                "collaborators": [
                    "Hui-fang Wu",
                    "Dachun Yang",
                    "Tao Huang",
                    "Kam-Biu Luk",
                    "Jinnan Zhang",
                    "Yu Liu",
                    "Liang Zhan",
                    "Yifang Wang",
                    "Liangjian Wen"
                ],
                "pub_titles": [
                    "Daya Bay Neutrino Experiment",
                    "Reactor Neutrino Experiments",
                    "Determining Reactor Neutrino Flux",
                    "Hardy Spaces $H_L^p({\\mathbb R}^n)$ Associated to Operators Satisfying $k$-Davies-Gaffney Estimates",
                    "Narrow width of a glueball decay into two mesons",
                    "Light-cone QCD predictions for elastic ed-scattering in the intermediate energy region",
                    "An overview of the Daya Bay Reactor Neutrino Experiment",
                    "Towards a Sub-percent Precision Measurement of $\\sin^2θ_{13}$ with Reactor Antineutrinos",
                    "Deuteron Electromagnetic Form Factors in the Intermediate Energy Region",
                    "Corrigendum to \"Hardy Spaces $H_{\\mathcal L}^1({\\mathbb R}^n)$ Associated to Schrödinger Type Operators $(-Δ)^2+V^2$\" [Houston J. Math 36 (4) (2010), 1067-1095]",
                    "Determination of the Neutrino Mass Hierarchy at an Intermediate Baseline",
                    "A Phenomenological Expression for Deuteron Electromagnetic Form Factors Based on Perturbative QCD Predictions"
                ],
                "pub_abstracts": [
                    "The Daya Bay Neutrino Experiment is proposed to measure sin^2(2\\theta_{13}) to better than 0.01 at 90% C.L. in a three-year run. The experimental site, detector design, and background estimation are presented.",
                    "Precisely measuring $\\theta_{13}$ is one of the highest priority in neutrino oscillation study. Reactor experiments can cleanly determine $\\theta_{13}$. Past reactor neutrino experiments are reviewed and status of next precision $\\theta_{13}$ experiments are presented. Daya Bay is designed to measure $\\sin^22\\theta_{13}$ to better than 0.01 and Double Chooz and RENO are designed to measure it to 0.02-0.03. All are heading to full operation in 2010. Recent improvements in neutrino moment measurement are also briefed.",
                    "Flux is an important source of uncertainties for a reactor neutrino experiment. It is determined from thermal power measurements, reactor core simulation, and knowledge of neutrino spectra of fuel isotopes. Past reactor neutrino experiments have determined the flux to (2-3)% precision. Precision measurements of mixing angle $\\theta_{13}$ by reactor neutrino experiments in the coming years will use near-far detector configurations. Most uncertainties from reactor will be canceled out. Understanding of the correlation of uncertainties is required for $\\theta_{13}$ experiments. Precise determination of reactor neutrino flux will also improve the sensitivity of the non-proliferation monitoring and future reactor experiments. We will discuss the flux calculation and recent progresses.",
                    "Let $L$ be a one to one operator of type $\\omega$ having a bounded $H_\\infty$ functional calculus and satisfying the $k$-Davies-Gaffney estimates with $k\\in{\\mathbb N}$. In this paper, the authors introduce the Hardy space $H_L^p(\\mathbb{R}^n)$ with $p\\in (0,\\,1]$ associated to $L$ in terms of square functions defined via $\\{e^{-t^{2k}L}\\}_{t>0}$ and establish their molecular and generalized square function characterizations. Typical examples of such operators include the $2k$-order divergence form homogeneous elliptic operator $L_1$ with complex bounded measurable coefficients and the $2k$-order Schr\\\"odinger type operator $L_2\\equiv (-\\Delta)^k+V^k$, where $\\Delta$ is the Laplacian and $0\\le V\\in L^k_{\\mathop\\mathrm{loc}}(\\mathbb{R}^n)$. Moreover, as applications, for $i\\in\\{1,\\,2\\}$, the authors prove that the associated Riesz transform $\\nabla^k(L_i^{-1/2})$ is bounded from $H_{L_i}^p(\\mathbb{R}^n)$ to $H^p(\\mathbb{R}^n)$ for $p\\in(n/(n+k),\\,1]$ and establish the Riesz transform characterizations of $H_{L_1}^p(\\mathbb{R}^n)$ for $ p\\in(rn/(n+kr),\\,1]$ if $\\{e^{-tL_1}\\}_{t>0}$ satisfies the $L^r-L^2$ $k$-off-diagonal estimates with $r\\in (1,2]$. These results when $k\\equiv1$ and $L\\equiv L_1$ are known.",
                    "The widths of a glueball decay to two pions or kaons are analyzed in the pQCD framework. Our results show that the glueball ground state has small branching ratio for two-meson decay mode, which is around $10^{-2}$. The predicted values are consistent with the data of $\\xi\\to\\pi\\pi, KK$ if $\\xi$ particle is a $2^{++}$ glueball. Applicability of pQCD to the glueball decay and comparison with $\\chi_{cJ}$ decay are also discussed.",
                    "The contributions of helicity-flip matrix elements to the deuteron form factors are discussed in the light-cone frame. Normalized $A(Q^2)$, $B(Q^2)$, $G_Q(Q^2)$ and $T_{20}$ are obtained in a simple QCD-inspired model. We find that $G_{+-}^+$ plays an important role in $G_Q(Q^2)$. Our numerical results are consistent with the data in the intermediate energy region.",
                    "The Daya Bay Reactor Neutrino Experiment discovered an unexpectedly large neutrino oscillation related to the mixing angle $\\theta_{13}$ in 2012. This finding paved the way to the next generation of neutrino oscillation experiments. In this article, we review the history, featured design, and scientific results of Daya Bay. Prospects of the experiment are also described.",
                    "Measuring the neutrino mixing parameter \\ensuremath{\\sin^2\\theta_{13}} to the sub-percent precision level could be necessary in the next ten years for the precision unitary test of the PMNS matrix. In this work, we discuss the possibility of such a measurement with reactor antineutrinos. We find that a single liquid scintillator detector on a reasonable scale could achieve the goal. We propose to install a detector of $\\sim10$\\% energy resolution at about 2.0~km from the reactors with a JUNO-like overburden. The integrated luminosity requirement is about 150~${\\rm kton}\\cdot {\\rm GW}\\cdot {\\rm year}$, corresponding to 4 years' operation of a 4~kton detector near a reactor complex of 9.2 GW thermal power like Taishan reactor. Unlike the previous $\\theta_{13}$ experiments with identical near and far detectors, which can suppress the systematics especially the rate uncertainty by the near-far relative measurement and the optimal baseline is at the first oscillation maximum of about 1.8~km, a single-detector measurement prefers to offset the baseline from the oscillation maximum. At low statistics $\\lesssim 10$~${\\rm kton}\\cdot {\\rm GW}\\cdot {\\rm year}$, the rate uncertainty dominates the systematics, and the optimal baseline is about 1.3~km. At higher statistics, the spectral shape uncertainty becomes dominant, and the optimal baseline shifts to about 2.0~km. The optimal baseline keeps being $\\sim 2.0$~km for an integrated luminosity up to $10^6$~${\\rm kton}\\cdot {\\rm GW}\\cdot {\\rm year}$. We have assumed that the TAO experiment will improve our understanding of the spectral shape uncertainty, which gives the highest precision measurement of reactor antineutrino spectrum for neutrino energy in the range of 3--6~MeV. We find that the optimal baseline is $\\sim 2.9$~km with a flat input spectral shape uncertainty provided by the future summation or conversion methods' prediction.",
                    "Based on a Perturbative QCD analysis of the deuteron form factor, a model for the reduced form factor is suggested. The numerical result is consistent with the data in the intermediate energy region.",
                    "We rectify an incorrect citation of the reference in obtaining the Gaussian upper bound for heat kernels of the Schr\\\"odinger type operators $(-\\Delta)^2+V^2$.",
                    "It is generally believed that neutrino mass hierarchy can be determined at a long baseline experiment, often using accelerator neutrino beams. Reactor neutrino experiments at an intermediate baseline have the capability to distinguish normal or inverted hierarchy. Recently it has been demonstrated that the mass hierarchy could possibly be identified using Fourier transform to the L/E spectrum if the mixing angle $\\sin^2(2\\theta_{13})>0.02$. In this study a more sensitive Fourier analysis is introduced. We found that an ideal detector at an intermediate baseline ($\\sim 60$ km) could identify the mass hierarchy for a mixing angle $\\sin^2(2\\theta_{13}) > 0.005$, without requirements on accurate information of reactor neutrino spectra and the value of $\\Delta m^2_{32}$.",
                    "For deuteron electromagnetic form factors,perturbative QCD(pQCD) predicts that $G^{+}_{00}$ becomes the dominate helicity amplitude and that $G^{+}_{+0}$ and $G^{+}_{+-}$ are suppressed by factors $\\Lambda_{\\rm QCD}/Q$ and $\\Lambda_{\\rm QCD}^2/Q^2$ at large $Q^2$,respectively. We try to discuss the higher order corrections beyond the pQCD asymptotic predictions by interpolating an analytical form to the intermediate energy region. From fitting the data,our results show that the helicity-zero to zero matrix element $G^{+}_{00}$ dominates the gross structure function $A(Q^2)$ in both of the large and intermediate energy regions; it is a good approximation for $G^{+}_{+-}$ to ignore the higher order contributions and the higher order corrections to $G^{+}_{+0}$ should be taken into account due to sizeable contributions in the intermediate energy region."
                ],
                "domain": [
                    "Neutrino Physics",
                    "Quantum Field Theory",
                    "Particle Physics",
                    "Reactor Experiments"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "981d1c71-2f90-4092-9d1b-f32b3bacf29f": {
                "pk": "981d1c71-2f90-4092-9d1b-f32b3bacf29f",
                "project_name": null,
                "name": "Fazhi Qi",
                "bio": "I am a researcher dedicated to enhancing data analysis and visualization in the realm of X-ray experiments. My recent work centers around the development of Daisy, a cloud-native platform designed to meet the diverse needs of the Chinese radiation facilities community. Daisy addresses a wide spectrum of challenges, from algorithmic issues to the establishment of robust scientific computing infrastructure. \n\nWith a focus on providing on-site data analysis services, Daisy facilitates rapid feedback and interaction, which is crucial for managing the high throughput data generated by next-generation facilities like the High Energy Photon Source (HEPS). The architecture of Daisy is built to support parallel processing through a plug-in based application, ensuring that users can efficiently handle complex data flows. My goal is to empower researchers with the tools they need to maximize the potential of their experiments, and I am excited about the impact Daisy will have on the scientific community.",
                "collaborators": [
                    "Yu Hu",
                    "Ling Li",
                    "Haolai Tian",
                    "Zhibing Liu",
                    "Qiulan Huang",
                    "Yi Zhang",
                    "Hao Hu"
                ],
                "pub_titles": [
                    "Daisy: Data analysis integrated software system for X-ray experiments"
                ],
                "pub_abstracts": [
                    "Daisy (Data Analysis Integrated Software System) has been designed for the analysis and visualization of the X-ray experiments. To address an extensive range of Chinese radiation facilities community's requirements from purely algorithmic problems to scientific computing infrastructure, Daisy sets up a cloud-native platform to support on-site data analysis services with fast feedback and interaction. The plugs-in based application is convenient to process the expected high throughput data flow in parallel at next-generation facilities such as the High Energy Photon Source (HEPS). The objectives, functionality and architecture of Daisy are described in this article."
                ],
                "domain": [
                    "Data Analysis",
                    "Scientific Computing",
                    "Cloud Computing",
                    "Visualization"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "7d7b36d9-a21f-489b-bf23-daeda109b9ec": {
                "pk": "7d7b36d9-a21f-489b-bf23-daeda109b9ec",
                "project_name": null,
                "name": "Changzheng Yuan",
                "bio": "I am a researcher with a diverse background in particle physics and the application of large language models (LLMs) in medical contexts. My early work focused on charmonia decays at BES/BEPC, where I contributed to significant findings such as the observation of various decay processes and the analysis of tau decay branching fractions using ALEPH data. I have also explored the production of charmonium states at HERA-B and investigated the potential existence of bound states in proton-antiproton systems, contributing to the understanding of resonances like X(1860) and X(1835).\n\nRecently, I have shifted my focus towards the intersection of artificial intelligence and healthcare, particularly in enhancing the performance of LLMs in non-English clinical settings. My work on the Knowledge and Few-shot Enhancement In-context Learning (KFE) framework has demonstrated how LLMs can be effectively integrated with diverse medical knowledge sources to improve their performance in medical question-answering tasks. By evaluating these models against the China National Medical Licensing Examination, I have shown that even smaller models can achieve significant improvements, thereby addressing language-related disparities in healthcare applications.\n\nThrough my research, I aim to bridge the gap between advanced computational techniques and practical applications in both fundamental physics and healthcare, ensuring that cutting-edge technologies can benefit a broader audience.",
                "collaborators": [
                    "Michel Davier",
                    "Cong-Feng Qiao",
                    "Haibo Li",
                    "Maozhi Yang",
                    "Andreas Hoecker",
                    "Bogdan Malaescu",
                    "Zhiqing Zhang",
                    "Jiageng Wu",
                    "Xian Wu",
                    "Zhaopeng Qiu"
                ],
                "pub_titles": [
                    "Recent BES results on charmonium decays",
                    "Measurement of Branching Fractions in Tau Decays",
                    "Finding eta_c' and h_c at HERA-B",
                    "Proposal of Direct Search for Strongly Bound States of ppbar, npbar Systems with High Intensity and Collective pbar beam",
                    "Update of the ALEPH non-strange spectral functions from hadronic $τ$ decays",
                    "Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries"
                ],
                "pub_abstracts": [
                    "Recent results on charmonia decays at BES/BEPC are reported, including the observation of psi'-->K_S K_L, psi'--> Vector + Tensor for the measurement of the relative phase between the strong and electromagnetic decays of psi' and a test of the pQCD ``12% rule'' between psi' and J/psi decays; the study of psi'--> gamma gamma J/psi for the determination of psi'--> pi^0 J/psi, eta J/psi, gamma chi_c1 and gamma chi_c2 decay branching fractions; the test of the color-octet mechanism via chi_cJ--> p \\bar{p} and chi_cJ--> \\Lambda \\bar{\\Lambda}; and a search for the CP violating process psi' and J/psi--> K_S K_S.",
                    "Full LEP-I data collected by the ALEPH detector during 1991-1995 running are analyzed in order to measure the $\\tau$ decay branching fractions. The analysis follows the global method used in the published study based on 1991-1993 data, with several improvements, especially concerning the treatment of photons and $\\pi^0$'s. Extensive systematic studies are performed, in order to match the large statistics of the data sample corresponding to 327148 measured and identified $\\tau$ decays. Preliminary values for the branching fractions are obtained for the 2 leptonic channels and 11 hadronic channels defined by their respective numbers of charged particles and $\\pi^0$'s. Using previously published ALEPH results on final states with charged and neutral kaons, corrections are applied so that branching ratios for exclusive final states without kaons are derived. Some physics implications of the results are given, in particular concerning universality in the leptonic charged weak current, isospin invariance in $a_1$ decays, and the separation of vector and axial-vector components of the total hadronic rate.",
                    "The production of Charmonium states $\\etacp$ and $\\hc$ at fixed-target experiment of $pN$ collisions at HERA-$B$ is considered. It is found that the HERA-$B$ at DESY is one of the best machines in further confirming and detecting these two kinds of Charmonia in the near future.",
                    "In this letter, we discuss the possibility to look for the direct evidence of the existence of the ppbar and npbar bound states. Measurement of the single \\gamma ray from the ppbar and npbar systems at rest can directly confirm whether the X(1860) and X(1835) are the resonances which are strongly coupled to ppbar. In addition to the neutral candidate, a charged resonance $X^-$ is also proposed to be searched for in npbar channel. We find that the data from the Crystal Barrel experiment at LEAR/CERN can be used to confirm the X(1835) observed by BES Collaboration. The possibility of measuring the $\\gamma$ spectrum below 100 MeV at the new experiment with cold high intensity $\\pbar$ beam at GSI is discussed. These new techniques can be used to probe the structure of the X(1860) and X(1835) in the future.",
                    "An update of the ALEPH non-strange spectral functions from hadronic $\\tau$ decays is presented. Compared to the 2005 ALEPH publication, the main improvement is related to the use of a new method to unfold the measured mass spectra from detector effects. This procedure also corrects a previous problem in the correlations between the unfolded mass bins. Results from QCD studies and for the evaluation of the hadronic vacuum polarisation contribution to the anomalous muon magnetic moment are derived using the new spectral functions. They are found in agreement with published results based on the previous set of spectral functions.",
                    "$\\textbf{Objectives}$: Large Language Models (LLMs) such as ChatGPT and Med-PaLM have excelled in various medical question-answering tasks. However, these English-centric models encounter challenges in non-English clinical settings, primarily due to limited clinical knowledge in respective languages, a consequence of imbalanced training corpora. We systematically evaluate LLMs in the Chinese medical context and develop a novel in-context learning framework to enhance their performance.   $\\textbf{Materials and Methods}$: The latest China National Medical Licensing Examination (CNMLE-2022) served as the benchmark. We collected 53 medical books and 381,149 medical questions to construct the medical knowledge base and question bank. The proposed Knowledge and Few-shot Enhancement In-context Learning (KFE) framework leverages the in-context learning ability of LLMs to integrate diverse external clinical knowledge sources. We evaluated KFE with ChatGPT(GPT3.5), GPT4, Baichuan2(BC2)-7B, and BC2-13B in CNMLE-2022 and investigated the effectiveness of different pathways for incorporating LLMs with medical knowledge from 7 perspectives.   $\\textbf{Results}$: Directly applying ChatGPT failed to qualify for the CNMLE-2022 at a score of 51. Cooperated with the KFE, the LLMs with varying sizes yielded consistent and significant improvements. The ChatGPT's performance surged to 70.04 and GPT-4 achieved the highest score of 82.59. This surpasses the qualification threshold (60) and exceeds the average human score of 68.70. It also enabled a smaller BC2-13B to pass the examination, showcasing the great potential in low-resource settings.   $\\textbf{Conclusion}$: By synergizing medical knowledge through in-context learning, LLM can extend clinical insight beyond language barriers, significantly reducing language-related disparities of LLM applications and ensuring global benefit in healthcare."
                ],
                "domain": [
                    "Particle Physics",
                    "Medical AI",
                    "Natural Language Processing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "### [Question 1] - What is the problem?\nHow can we improve the efficiency and accuracy of machine learning models in processing sequential data?\n\n### [Question 2] - Why is it interesting and important?\nSolving this problem is crucial for the research community as it addresses the growing need for advanced machine learning techniques that can handle complex sequential data, such as time series, natural language, and video analysis. By enhancing model performance in these areas, we can unlock new applications in fields like healthcare, finance, and autonomous systems. This research could lead to more robust algorithms that not only improve predictive accuracy but also reduce computational costs, thereby influencing future research directions and practical implementations.\n\n### [Question 3] - Why is it hard?\nThe challenges in solving this problem stem from the inherent complexity of sequential data, which often involves dependencies across time steps that are difficult to model accurately. Naive approaches, such as treating each time step independently, fail to capture these dependencies, leading to suboptimal performance. Additionally, issues such as overfitting, the curse of dimensionality, and the need for large labeled datasets complicate the development of effective models. Theoretical obstacles include the need for new mathematical frameworks to better understand and represent these dependencies.\n\n### [Question 4] - Why hasn't it been solved before?\nPrevious research has often focused on simpler models or has not adequately addressed the intricacies of sequential data. Limitations in computational power and the availability of large datasets have also hindered progress. Many existing solutions lack the ability to generalize across different types of sequential data or fail to integrate recent advancements in deep learning. Our approach aims to combine state-of-the-art techniques with novel methodologies that specifically target the unique challenges posed by sequential data, thereby offering a more comprehensive solution.\n\n### [Question 5] - What are the key components of my approach and results?\nOur proposed methodology involves developing a hybrid model that integrates recurrent neural networks (RNNs) with attention mechanisms to better capture dependencies in sequential data. We will utilize a diverse dataset comprising time series and natural language sequences, and evaluate our model using metrics such as accuracy, precision, and recall. The expected outcomes include improved model performance on benchmark tasks, demonstrating the effectiveness of our approach in handling complex sequential data and providing insights into the underlying patterns within the data."
    },
    "2402.09588": {
        "paper_data": {
            "title": "Emerging Opportunities of Using Large Language Models for Translation Between Drug Molecules and Indications",
            "url": "http://arxiv.org/abs/2402.09588v2",
            "arxiv_id": "2402.09588",
            "authors": [
                "David Oniani",
                "Jordan Hilsman",
                "Chengxi Zang",
                "Junmei Wang",
                "Lianjin Cai",
                "Jan Zawala",
                "Yanshan Wang"
            ],
            "abstract": "A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test existing LLMs on this new task. Specifically, we consider nine variations of the T5 LLM and evaluate them on two public datasets obtained from ChEMBL and DrugBank. Our experiments show the early results of using LLMs for this task and provide a perspective on the state-of-the-art. We also emphasize the current limitations and discuss future work that has the potential to improve the performance on this task. The creation of molecules from indications, or vice versa, will allow for more efficient targeting of diseases and significantly reduce the cost of drug discovery, with the potential to revolutionize the field of drug discovery in the era of generative AI.",
            "introduction": "  Introduction  Drug discovery is a costly process [1] that identifies chemical entities with the potential to become therapeutic agents [2]. Due to its clear benefits and significance to health, drug discovery has become an active area of research, with researchers attempting to automate and streamline drug discovery [3, 4]. Approved drugs have indications, which refer to the use of that drug for treating a particular disease [5, Chapter 5]. The creation of molecules from indications, or vice versa, will allow for more efficient targeting of diseases and significantly reduce the cost of drug discovery, with the potential to revolutionize the field of drug discovery.   Large Language Models (LLMs) have become one of the major directions of generative Artificial Intelligence (AI) research, with highly performant models like GPT-3 [6], GPT-4 [7], LLaMA [8], and Mixtral [9] developed in the recent years and services like ChatGPT reaching over 100 million users [10, 11]. LLMs utilize deep learning methods to perform various Natural Language Processing (NLP) tasks, such as text generation [12, 13] and neural machine translation [14, 15]. The capabilities of LLMs are due in part to their training on large-scale textual data, making the models familiar with a wide array of topics. LLMs have also demonstrated promising performance in a variety of tasks across different scientific fields [16, 17, 18, 19]. Since LLMs work with textual data, the first step is usually finding a way to express a problem in terms of text or language.   An image or a diagram is a typical way to present a molecule, but methods for obtaining textual representations of molecules do exist. One such method is the Simplified Molecular-Input Line-Entry System (SMILES) [20], which is usually considered as a language for describing molecules. As SMILES strings represent drugs in textual form, we can assess the viability of LLMs in translating between drug molecules and their indications. In this paper, we consider two tasks: drug-to-indication and indication-to-drug, where we seek to generate indications from the SMILES strings of existing drugs, and SMILES strings from indications, respectively. Translation between drugs and the corresponding indication will allow for finding a cure for diseases that have no current treatment, and give clinicians more avenues for patient care. We also release the codebase for the study111https://github.com/PittNAIL/drug-to-indication.   Research efforts have attempted de-novo drug discovery through the use of AI and, more recently, forms of generative AI [21]. There are numerous existing efforts for molecular design and drug discovery using AI, such as GPT-based models using scaffold SMILES strings accompanied with desired properties of the output molecule [22]. Others have used T5 architecture for various tasks, such as reaction prediction [23] and converting between molecular captions and SMILES strings [24]. Additional work in the field is centered around the generation of new molecules from gene expression signatures using generative adversarial networks [25], training recurrent neural networks on known compounds and their SMILES strings, then fine-tuning for specific agonists of certain receptors [26], or using graph neural networks to predict drugs and their corresponding indications from SMILES [27]. As such, there is an established promise in using AI for drug discovery and molecular design. Efforts to make data more friendly for AI generation of drugs include the development of the Self-Referencing Embedded Strings (SELFIES) [28], which can represent every valid molecule. The reasoning is that such a format will allow generative AI to construct valid molecules while maintaining crucial structural information in the string. The collection of these efforts sets the stage for our attempt at generating",
            "references": [
                {
                    "title": "Drug-drug interactions prediction based on deep learning and knowledge graph: A review",
                    "abstract": null
                },
                {
                    "title": "Autonomous chemical research with large language models",
                    "abstract": null
                },
                {
                    "title": "A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing",
                    "abstract": "We evaluate a range of recent LLMs on English creative writing, a challenging and complex task that requires imagination, coherence, and style. We use a difficult, open-ended scenario chosen to avoid training data reuse: an epic narration of a single combat between Ignatius J. Reilly, the protagonist of the Pulitzer Prize-winning novel A Confederacy of Dunces (1980), and a pterodactyl, a prehistoric flying reptile. We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style. Our results show that some state-of-the-art commercial LLMs match or slightly outperform our writers in most dimensions; whereas open-source LLMs lag behind. Humans retain an edge in creativity, while humor shows a binary divide between LLMs that can handle it comparably to humans and those that fail at it. We discuss the implications and limitations of our study and suggest directions for future research."
                },
                {
                    "title": "Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI",
                    "abstract": null
                },
                {
                    "title": "TCMBank: bridges between the largest herbal medicines, chemical ingredients, target proteins, and associated diseases with intelligence text mining",
                    "abstract": "Traditional Chinese Medicine (TCM) has long been viewed as a precious source of modern drug discovery. AI-assisted drug discovery (AIDD) has been investigated extensively. However, there are still two challenges in applying AIDD to guide TCM drug discovery: the lack of a large amount of standardized TCM-related information and AIDD is prone to pathological failures in out-of-domain data. We have released TCM Database@Taiwan in 2011, and it has been widely disseminated and used. Now, we developed TCMBank, the largest systematic free TCM database, which is an extension of TCM Database@Taiwan. TCMBank contains 9192 herbs, 61 966 ingredients (unduplicated), 15 179 targets, 32 529 diseases, and their pairwise relationships. By integrating multiple data sources, TCMBank provides 3D structure information of ingredients and provides a standard list and detailed information on herbs, ingredients, targets and diseases. TCMBank has an intelligent document identification module that continuously adds TCM-related information retrieved from the literature in PubChem. In addition, driven by TCMBank big data, we developed an ensemble learning-based drug discovery protocol for identifying potential leads and drug repurposing. We take colorectal cancer and Alzheimer's disease as examples to demonstrate how to accelerate drug discovery by artificial intelligence. Using TCMBank, researchers can view literature-driven relationship mapping between herbs/ingredients and genes/diseases, allowing the understanding of molecular action mechanisms for ingredients and identification of new potentially effective treatments. TCMBank is available at https://TCMBank.CN/."
                },
                {
                    "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
                    "abstract": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user’s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."
                },
                {
                    "title": "Augmenting Large Language Model Translators via Translation Memories",
                    "abstract": "Using translation memories (TMs) as prompts is a promising approach to in-context learning of machine translation models. In this work, we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to ``understand'' prompts is indeed helpful for making better use of TMs. Experiments show that the results of a pre-trained LLM translator can be greatly improved by using high-quality TM-based prompts. These results are even comparable to those of the state-of-the-art NMT systems which have access to large-scale in-domain bilingual data and are well tuned on the downstream tasks."
                },
                {
                    "title": "3D graph neural network with few-shot learning for predicting drug-drug interactions in scaffold-based cold start scenario",
                    "abstract": null
                },
                {
                    "title": "Computational approaches streamlining drug discovery",
                    "abstract": null
                },
                {
                    "title": "TCMBank-the largest TCM database provides deep learning-based Chinese-Western medicine exclusion prediction",
                    "abstract": null
                },
                {
                    "title": "Meta Learning With Graph Attention Networks for Low-Data Drug Discovery",
                    "abstract": "Finding candidate molecules with favorable pharmacological activity, low toxicity, and proper pharmacokinetic properties is an important task in drug discovery. Deep neural networks have made impressive progress in accelerating and improving drug discovery. However, these techniques rely on a large amount of label data to form accurate predictions of molecular properties. At each stage of the drug discovery pipeline, usually, only a few biological data of candidate molecules and derivatives are available, indicating that the application of deep neural networks for low-data drug discovery is still a formidable challenge. Here, we propose a meta learning architecture with graph attention network, Meta-GAT, to predict molecular properties in low-data drug discovery. The GAT captures the local effects of atomic groups at the atom level through the triple attentional mechanism and implicitly captures the interactions between different atomic groups at the molecular level. GAT is used to perceive molecular chemical environment and connectivity, thereby effectively reducing sample complexity. Meta-GAT further develops a meta learning strategy based on bilevel optimization, which transfers meta knowledge from other attribute prediction tasks to low-data target tasks. In summary, our work demonstrates how meta learning can reduce the amount of data required to make meaningful predictions of molecules in low-data scenarios. Meta learning is likely to become the new learning paradigm in low-data drug discovery. The source code is publicly available at: https://github.com/lol88/Meta-GAT."
                },
                {
                    "title": "LLaMA: Open and Efficient Foundation Language Models",
                    "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
                },
                {
                    "title": "Adaptive Machine Translation with Large Language Models",
                    "abstract": "Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES)."
                },
                {
                    "title": "Large language models encode clinical knowledge",
                    "abstract": null
                },
                {
                    "title": "Molecular Graph Generation by Decomposition and Reassembling",
                    "abstract": "Designing molecular structures with desired chemical properties is an essential task in drug discovery and materials design. However, finding molecules with the optimized desired properties is still a challenging task due to combinatorial explosion of the candidate space of molecules. Here we propose a novel decomposition-and-reassembling-based approach, which does not include any optimization in hidden space, and our generation process is highly interpretable. Our method is a two-step procedure: In the first decomposition step, we apply frequent subgraph mining to a molecular database to collect a smaller size of subgraphs as building blocks of molecules. In the second reassembling step, we search desirable building blocks guided via reinforcement learning and combine them to generate new molecules. Our experiments show that our method not only can find better molecules in terms of two standard criteria, the penalized log P and druglikeness, but also can generate drug molecules showing the valid intermediate molecules."
                },
                {
                    "title": "Factuality Enhanced Language Models for Open-Ended Text Generation",
                    "abstract": "Pretrained language models (LMs) are susceptible to generate text with nonfactual information. In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation. We design the FactualityPrompts test set and metrics to measure the factuality of LM generations. Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B. Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions. In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ''uniform randomness'' introduced at every sampling step. We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality. Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia). We propose a factuality-enhanced training method that uses TopicPrefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors. We release our code and FactualityPrompts benchmark at: https://github.com/nayeon7lee/FactualityPrompt."
                },
                {
                    "title": "Translation between Molecules and Natural Language",
                    "abstract": "We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality."
                },
                {
                    "title": "Unified Deep Learning Model for Multitask Reaction Predictions with Explanation",
                    "abstract": "There is significant interest and importance to develop robust machine learning models to assist organic chemistry synthesis. Typically, task-specific machine learning models for distinct reaction prediction tasks have been developed. In this work, we develop a unified deep learning model, T5Chem, for a variety of chemical reaction predictions tasks by adapting the \"Text-to-Text Transfer Transformer\" (T5) framework in natural language processing (NLP). On the basis of self-supervised pretraining with PubChem molecules, the T5Chem model can achieve state-of-the-art performances for four distinct types of task-specific reaction prediction tasks using four different open-source data sets, including reaction type classification on USPTO_TPL, forward reaction prediction on USPTO_MIT, single-step retrosynthesis on USPTO_50k, and reaction yield prediction on high-throughput C-N coupling reactions. Meanwhile, we introduced a new unified multitask reaction prediction data set USPTO_500_MT, which can be used to train and test five different types of reaction tasks, including the above four as well as a new reagent suggestion task. Our results showed that models trained with multiple tasks are more robust and can benefit from mutual learning on related tasks. Furthermore, we demonstrated the use of SHAP (SHapley Additive exPlanations) to explain T5Chem predictions at the functional group level, which provides a way to demystify sequence-based deep learning models in chemistry. T5Chem is accessible through https://yzhang.hpc.nyu.edu/T5Chem."
                },
                {
                    "title": "A review of molecular representation in the age of machine learning",
                    "abstract": "Research in chemistry increasingly requires interdisciplinary work prompted by, among other things, advances in computing, machine learning, and artificial intelligence. Everyone working with molecules, whether chemist or not, needs an understanding of the representation of molecules in a machine‐readable format, as this is central to computational chemistry. Four classes of representations are introduced: string, connection table, feature‐based, and computer‐learned representations. Three of the most significant representations are simplified molecular‐input line‐entry system (SMILES), International Chemical Identifier (InChI), and the MDL molfile, of which SMILES was the first to successfully be used in conjunction with a variational autoencoder (VAE) to yield a continuous representation of molecules. This is noteworthy because a continuous representation allows for efficient navigation of the immensely large chemical space of possible molecules. Since 2018, when the first model of this type was published, considerable effort has been put into developing novel and improved methodologies. Most, if not all, researchers in the community make their work easily accessible on GitHub, though discussion of computation time and domain of applicability is often overlooked. Herein, we present questions for consideration in future work which we believe will make chemical VAEs even more accessible."
                },
                {
                    "title": "SmileGNN: Drug–Drug Interaction Prediction Based on the SMILES and Graph Neural Network",
                    "abstract": "Concurrent use of multiple drugs can lead to unexpected adverse drug reactions. The interaction between drugs can be confirmed by routine in vitro and clinical trials. However, it is difficult to test the drug–drug interactions widely and effectively before the drugs enter the market. Therefore, the prediction of drug–drug interactions has become one of the research priorities in the biomedical field. In recent years, researchers have been using deep learning to predict drug–drug interactions by exploiting drug structural features and graph theory, and have achieved a series of achievements. A drug–drug interaction prediction model SmileGNN is proposed in this paper, which can be characterized by aggregating the structural features of drugs constructed by SMILES data and the topological features of drugs in knowledge graphs obtained by graph neural networks. The experimental results show that the model proposed in this paper combines a variety of data sources and has a better prediction performance compared with existing prediction models of drug–drug interactions. Five out of the top ten predicted new drug–drug interactions are verified from the latest database, which proves the credibility of SmileGNN."
                },
                {
                    "title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
                    "abstract": "Application of deep learning techniques for de novo generation of molecules, termed as inverse molecular design, has been gaining enormous traction in drug design. The representation of molecules in SMILES notation as a string of characters enables the usage of state of the art models in natural language processing, such as Transformers, for molecular design in general. Inspired by generative pre-training (GPT) models that have been shown to be successful in generating meaningful text, we train a transformer-decoder on the next token prediction task using masked self-attention for the generation of druglike molecules in this study. We show that our model, MolGPT, performs on par with other previously proposed modern machine learning frameworks for molecular generation in terms of generating valid, unique, and novel molecules. Furthermore, we demonstrate that the model can be trained conditionally to control multiple properties of the generated molecules. We also show that the model can be used to generate molecules with desired scaffolds as well as desired molecular properties by conditioning the generation on scaffold SMILES strings of desired scaffolds and property values. Using saliency maps, we highlight the interpretability of the generative process of the model."
                },
                {
                    "title": "Mol2Context-vec: learning molecular representation from context awareness for drug discovery",
                    "abstract": "With the rapid development of proteomics and the rapid increase of target molecules for drug action, computer-aided drug design (CADD) has become a basic task in drug discovery. One of the key challenges in CADD is molecular representation. High-quality molecular expression with chemical intuition helps to promote many boundary problems of drug discovery. At present, molecular representation still faces several urgent problems, such as the polysemy of substructures and unsmooth information flow between atomic groups. In this research, we propose a deep contextualized Bi-LSTM architecture, Mol2Context-vec, which can integrate different levels of internal states to bring dynamic representations of molecular substructures. And the obtained molecular context representation can capture the interactions between any atomic groups, especially a pair of atomic groups that are topologically distant. Experiments show that Mol2Context-vec achieves state-of-the-art performance on multiple benchmark datasets. In addition, the visual interpretation of Mol2Context-vec is very close to the structural properties of chemical molecules as understood by humans. These advantages indicate that Mol2Context-vec can be used as a reliable and effective tool for molecular expression. Availability: The source code is available for download in https://github.com/lol88/Mol2Context-vec."
                },
                {
                    "title": "GeoMol: Torsional Geometric Generation of Molecular 3D Conformer Ensembles",
                    "abstract": "Prediction of a molecule's 3D conformer ensemble from the molecular graph holds a key role in areas of cheminformatics and drug discovery. Existing generative models have several drawbacks including lack of modeling important molecular geometry elements (e.g. torsion angles), separate optimization stages prone to error accumulation, and the need for structure fine-tuning based on approximate classical force-fields or computationally expensive methods such as metadynamics with approximate quantum mechanics calculations at each geometry. We propose GeoMol--an end-to-end, non-autoregressive and SE(3)-invariant machine learning approach to generate distributions of low-energy molecular 3D conformers. Leveraging the power of message passing neural networks (MPNNs) to capture local and global graph information, we predict local atomic 3D structures and torsion angles, avoiding unnecessary over-parameterization of the geometric degrees of freedom (e.g. one angle per non-terminal bond). Such local predictions suffice both for the training loss computation, as well as for the full deterministic conformer assembly (at test time). We devise a non-adversarial optimal transport based loss function to promote diverse conformer generation. GeoMol predominantly outperforms popular open-source, commercial, or state-of-the-art machine learning (ML) models, while achieving significant speed-ups. We expect such differentiable 3D structure generators to significantly impact molecular modeling and related applications."
                },
                {
                    "title": "Molecular representation learning with language models and domain-relevant auxiliary tasks",
                    "abstract": "We apply a Transformer architecture, specifically BERT, to learn flexible and high quality molecular representations for drug discovery problems. We study the impact of using different combinations of self-supervised tasks for pre-training, and present our results for the established Virtual Screening and QSAR benchmarks. We show that: i) The selection of appropriate self-supervised task(s) for pre-training has a significant impact on performance in subsequent downstream tasks such as Virtual Screening. ii) Using auxiliary tasks with more domain relevance for Chemistry, such as learning to predict calculated molecular properties, increases the fidelity of our learnt representations. iii) Finally, we show that molecular representations learnt by our model `MolBert' improve upon the current state of the art on the benchmark datasets."
                },
                {
                    "title": "Artificial intelligence in drug discovery and development",
                    "abstract": null
                },
                {
                    "title": "Language Models are Few-Shot Learners",
                    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                {
                    "title": "One molecular fingerprint to rule them all: drugs, biomolecules, and the metabolome",
                    "abstract": null
                },
                {
                    "title": "Estimated Research and Development Investment Needed to Bring a New Medicine to Market, 2009-2018.",
                    "abstract": "Importance\nThe mean cost of developing a new drug has been the subject of debate, with recent estimates ranging from $314 million to $2.8 billion.\n\n\nObjective\nTo estimate the research and development investment required to bring a new therapeutic agent to market, using publicly available data.\n\n\nDesign and Setting\nData were analyzed on new therapeutic agents approved by the US Food and Drug Administration (FDA) between 2009 and 2018 to estimate the research and development expenditure required to bring a new medicine to market. Data were accessed from the US Securities and Exchange Commission, Drugs@FDA database, and ClinicalTrials.gov, alongside published data on clinical trial success rates.\n\n\nExposures\nConduct of preclinical and clinical studies of new therapeutic agents.\n\n\nMain Outcomes and Measures\nMedian and mean research and development spending on new therapeutic agents approved by the FDA, capitalized at a real cost of capital rate (the required rate of return for an investor) of 10.5% per year, with bootstrapped CIs. All amounts were reported in 2018 US dollars.\n\n\nResults\nThe FDA approved 355 new drugs and biologics over the study period. Research and development expenditures were available for 63 (18%) products, developed by 47 different companies. After accounting for the costs of failed trials, the median capitalized research and development investment to bring a new drug to market was estimated at $985.3 million (95% CI, $683.6 million-$1228.9 million), and the mean investment was estimated at $1335.9 million (95% CI, $1042.5 million-$1637.5 million) in the base case analysis. Median estimates by therapeutic area (for areas with ≥5 drugs) ranged from $765.9 million (95% CI, $323.0 million-$1473.5 million) for nervous system agents to $2771.6 million (95% CI, $2051.8 million-$5366.2 million) for antineoplastic and immunomodulating agents. Data were mainly accessible for smaller firms, orphan drugs, products in certain therapeutic areas, first-in-class drugs, therapeutic agents that received accelerated approval, and products approved between 2014 and 2018. Results varied in sensitivity analyses using different estimates of clinical trial success rates, preclinical expenditures, and cost of capital.\n\n\nConclusions and Relevance\nThis study provides an estimate of research and development costs for new therapeutic agents based on publicly available data. Differences from previous studies may reflect the spectrum of products analyzed, the restricted availability of data in the public domain, and differences in underlying assumptions in the cost calculations."
                },
                {
                    "title": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding",
                    "abstract": null
                },
                {
                    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                {
                    "title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
                    "abstract": "The discovery of novel materials and functional molecules can help to solve some of society’s most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering–generally denoted as inverse design–was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model’s internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models."
                },
                {
                    "title": "De novo generation of hit-like molecules from gene expression signatures using artificial intelligence",
                    "abstract": null
                },
                {
                    "title": "Fréchet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery",
                    "abstract": "The new wave of successful generative models in machine learning has increased the interest in deep learning driven de novo drug design. However, method comparison is difficult because of various flaws of the currently employed evaluation metrics. We propose an evaluation metric for generative models called Fréchet ChemNet distance (FCD). The advantage of the FCD over previous metrics is that it can detect whether generated molecules are diverse and have similar chemical and biological properties as real molecules."
                },
                {
                    "title": "Mol2vec: Unsupervised Machine Learning Approach with Chemical Intuition",
                    "abstract": "Inspired by natural language processing techniques, we here introduce Mol2vec, which is an unsupervised machine learning approach to learn vector representations of molecular substructures. Like the Word2vec models, where vectors of closely related words are in close proximity in the vector space, Mol2vec learns vector representations of molecular substructures that point in similar directions for chemically related substructures. Compounds can finally be encoded as vectors by summing the vectors of the individual substructures and, for instance, be fed into supervised machine learning approaches to predict compound properties. The underlying substructure vector embeddings are obtained by training an unsupervised machine learning approach on a so-called corpus of compounds that consists of all available chemical matter. The resulting Mol2vec model is pretrained once, yields dense vector representations, and overcomes drawbacks of common compound feature representations such as sparseness and bit collisions. The prediction capabilities are demonstrated on several compound property and bioactivity data sets and compared with results obtained for Morgan fingerprints as a reference compound representation. Mol2vec can be easily combined with ProtVec, which employs the same Word2vec concept on protein sequences, resulting in a proteochemometric approach that is alignment-independent and thus can also be easily used for proteins with low sequence similarities."
                },
                {
                    "title": "De Novo Design of Bioactive Small Molecules by Artificial Intelligence",
                    "abstract": "Generative artificial intelligence offers a fresh view on molecular design. We present the first‐time prospective application of a deep learning model for designing new druglike compounds with desired activities. For this purpose, we trained a recurrent neural network to capture the constitution of a large set of known bioactive compounds represented as SMILES strings. By transfer learning, this general model was fine‐tuned on recognizing retinoid X and peroxisome proliferator‐activated receptor agonists. We synthesized five top‐ranking compounds designed by the generative model. Four of the compounds revealed nanomolar to low‐micromolar receptor modulatory activity in cell‐based assays. Apparently, the computational model intrinsically captured relevant chemical and biological knowledge without the need for explicit rules. The results of this study advocate generative artificial intelligence for prospective de novo molecular design, and demonstrate the potential of these methods for future medicinal chemistry."
                },
                {
                    "title": "Automating drug discovery",
                    "abstract": null
                },
                {
                    "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation",
                    "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems."
                },
                {
                    "title": "ZINC 15 – Ligand Discovery for Everyone",
                    "abstract": "Many questions about the biological activity and availability of small molecules remain inaccessible to investigators who could most benefit from their answers. To narrow the gap between chemoinformatics and biology, we have developed a suite of ligand annotation, purchasability, target, and biology association tools, incorporated into ZINC and meant for investigators who are not computer specialists. The new version contains over 120 million purchasable “drug-like” compounds – effectively all organic molecules that are for sale – a quarter of which are available for immediate delivery. ZINC connects purchasable compounds to high-value ones such as metabolites, drugs, natural products, and annotated compounds from the literature. Compounds may be accessed by the genes for which they are annotated as well as the major and minor target classes to which those genes belong. It offers new analysis tools that are easy for nonspecialists yet with few limitations for experts. ZINC retains its original 3D roots – all molecules are available in biologically relevant, ready-to-dock formats. ZINC is freely available at http://zinc15.docking.org."
                },
                {
                    "title": "Get Your Atoms in Order - An Open-Source Implementation of a Novel and Robust Molecular Canonicalization Algorithm",
                    "abstract": "Finding a canonical ordering of the atoms in a molecule is a prerequisite for generating a unique representation of the molecule. The canonicalization of a molecule is usually accomplished by applying some sort of graph relaxation algorithm, the most common of which is the Morgan algorithm. There are known issues with that algorithm that lead to noncanonical atom orderings as well as problems when it is applied to large molecules like proteins. Furthermore, each cheminformatics toolkit or software provides its own version of a canonical ordering, most based on unpublished algorithms, which also complicates the generation of a universal unique identifier for molecules. We present an alternative canonicalization approach that uses a standard stable-sorting algorithm instead of a Morgan-like index. Two new invariants that allow canonical ordering of molecules with dependent chirality as well as those with highly symmetrical cyclic graphs have been developed. The new approach proved to be robust and fast when tested on the 1.45 million compounds of the ChEMBL 20 data set in different scenarios like random renumbering of input atoms or SMILES round tripping. Our new algorithm is able to generate a canonical order of the atoms of protein molecules within a few milliseconds. The novel algorithm is implemented in the open-source cheminformatics toolkit RDKit. With this paper, we provide a reference Python implementation of the algorithm that could easily be integrated in any cheminformatics toolkit. This provides a first step toward a common standard for canonical atom ordering to generate a universal unique identifier for molecules other than InChI."
                },
                {
                    "title": "ChEMBL web services: streamlining access to drug discovery data and utilities",
                    "abstract": "ChEMBL is now a well-established resource in the fields of drug discovery and medicinal chemistry research. The ChEMBL database curates and stores standardized bioactivity, molecule, target and drug data extracted from multiple sources, including the primary medicinal chemistry literature. Programmatic access to ChEMBL data has been improved by a recent update to the ChEMBL web services (version 2.0.x, https://www.ebi.ac.uk/chembl/api/data/docs), which exposes significantly more data from the underlying database and introduces new functionality. To complement the data-focused services, a utility service (version 1.0.x, https://www.ebi.ac.uk/chembl/api/utils/docs), which provides RESTful access to commonly used cheminformatics methods, has also been concurrently developed. The ChEMBL web services can be used together or independently to build applications and data processing workflows relevant to drug discovery and chemical biology."
                },
                {
                    "title": "Efficient Estimation of Word Representations in Vector Space",
                    "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities."
                },
                {
                    "title": "Extended-Connectivity Fingerprints",
                    "abstract": "Extended-connectivity fingerprints (ECFPs) are a novel class of topological fingerprints for molecular characterization. Historically, topological fingerprints were developed for substructure and similarity searching. ECFPs were developed specifically for structure-activity modeling. ECFPs are circular fingerprints with a number of useful qualities: they can be very rapidly calculated; they are not predefined and can represent an essentially infinite number of different molecular features (including stereochemical information); their features represent the presence of particular substructures, allowing easier interpretation of analysis results; and the ECFP algorithm can be tailored to generate different types of circular fingerprints, optimized for different uses. While the use of ECFPs has been widely adopted and validated, a description of their implementation has not previously been presented in the literature."
                },
                {
                    "title": "Levenshtein Distance: Information theory, Computer science, String (computer science), String metric, Damerau?Levenshtein distance, Spell checker, Hamming distance",
                    "abstract": "In information theory and computer science, the Levenshtein distance is a metric for measuring the amount of difference between two sequences (i.e., the so called edit distance). The Levenshtein distance between two strings is given by the minimum number of operations needed to transform one string into the other, where an operation is an insertion, deletion, or substitution of a single character. A generalization of the Levenshtein distance (Damerau?Levenshtein distance) allows the transposition of two characters as an operation. Some Translation Environment Tools, such as translation memory leveraging applications, use the Levenhstein algorithm to measure the edit distance between two fuzzy matching content segments.The metric is named after Vladimir Levenshtein, who considered this distance in 1965. It is often used in applications that need to determine how similar, or different, two strings are, such as spell checkers"
                },
                {
                    "title": "Commercializing Successful Biomedical Technologies",
                    "abstract": null
                },
                {
                    "title": "DrugBank: a comprehensive resource for in silico drug discovery and exploration",
                    "abstract": "DrugBank is a unique bioinformatics/cheminformatics resource that combines detailed drug (i.e. chemical) data with comprehensive drug target (i.e. protein) information. The database contains >4100 drug entries including >800 FDA approved small molecule and biotech drugs as well as >3200 experimental drugs. Additionally, >14,000 protein or drug target sequences are linked to these drug entries. Each DrugCard entry contains >80 data fields with half of the information being devoted to drug/chemical data and the other half devoted to drug target or protein data. Many data fields are hyperlinked to other databases (KEGG, PubChem, ChEBI, PDB, Swiss-Prot and GenBank) and a variety of structure viewing applets. The database is fully searchable supporting extensive text, sequence, chemical structure and relational query searches. Potential applications of DrugBank include in silico drug target discovery, drug design, drug docking or screening, drug metabolism prediction, drug interaction prediction and general pharmaceutical education. DrugBank is available at http://redpoll.pharmacy.ualberta.ca/drugbank/."
                },
                {
                    "title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments",
                    "abstract": "We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules."
                },
                {
                    "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                    "abstract": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST."
                },
                {
                    "title": "Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics",
                    "abstract": "In this paper we describe two new objective automatic evaluation methods for machine translation. The first method is based on longest common subsequence between a candidate translation and a set of reference translations. Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n-grams automatically. The second method relaxes strict n-gram matching to skip-bigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency."
                },
                {
                    "title": "Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics",
                    "abstract": "Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries. The results show that automatic evaluation using unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results."
                },
                {
                    "title": "Reoptimization of MDL Keys for Use in Drug Discovery",
                    "abstract": "For a number of years MDL products have exposed both 166 bit and 960 bit keysets based on 2D descriptors. These keysets were originally constructed and optimized for substructure searching. We report on improvements in the performance of MDL keysets which are reoptimized for use in molecular similarity. Classification performance for a test data set of 957 compounds was increased from 0.65 for the 166 bit keyset and 0.67 for the 960 bit keyset to 0.71 for a surprisal S/N pruned keyset containing 208 bits and 0.71 for a genetic algorithm optimized keyset containing 548 bits. We present an overview of the underlying technology supporting the definition of descriptors and the encoding of these descriptors into keysets. This technology allows definition of descriptors as combinations of atom properties, bond properties, and atomic neighborhoods at various topological separations as well as supporting a number of custom descriptors. These descriptors can then be used to set one or more bits in a keyset. We constructed various keysets and optimized their performance in clustering bioactive substances. Performance was measured using methodology developed by Briem and Lessel. \"Directed pruning\" was carried out by eliminating bits from the keysets on the basis of random selection, values of the surprisal of the bit, or values of the surprisal S/N ratio of the bit. The random pruning experiment highlighted the insensitivity of keyset performance for keyset lengths of more than 1000 bits. Contrary to initial expectations, pruning on the basis of the surprisal values of the various bits resulted in keysets which underperformed those resulting from random pruning. In contrast, pruning on the basis of the surprisal S/N ratio was found to yield keysets which performed better than those resulting from random pruning. We also explored the use of genetic algorithms in the selection of optimal keysets. Once more the performance was only a weak function of keyset size, and the optimizations failed to identify a single globally optimal keyset. Instead multiple, equally optimal keysets could be produced which had relatively low overlap of the descriptors they encoded."
                },
                {
                    "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                    "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations."
                },
                {
                    "title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules",
                    "abstract": "18-24."
                },
                {
                    "title": "The Generation of a Unique Machine Description for Chemical Structures-A Technique Developed at Chemical Abstracts Service.",
                    "abstract": null
                },
                {
                    "title": "Meta-MolNet: A Cross-Domain Benchmark for Few Examples Drug Discovery",
                    "abstract": null
                },
                {
                    "title": "Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting",
                    "abstract": ","
                },
                {
                    "title": "Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries",
                    "abstract": "We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning."
                },
                {
                    "title": "Large-scale self-supervised pretraining for molecular property prediction",
                    "abstract": null
                },
                {
                    "title": "Artificial Intelligence in Drug Discovery and Development",
                    "abstract": "Artificial Intelligence (AI) has recently been developed into a sizzling topic in the area of medical care industry. The biopharmaceutical industries are making efforts to approach AI to enhance drug discovery process, reduce research and development expenses, diminish failure rates in clinical trials and ultimately generate superior medicines. The accessibility of immense statistics in life sciences and a speedy development in machine learning algorithms led to an evolution of AI-based start-up companies focused on drug discovery over the recent years [1]. Numerous remarkable AIbiopharmaceutical alliance were declared in 2016-2017 that include Pfizer and IBM Watson, Sanofi Genzyme and Recursion Pharmaceuticals, AstraZeneca, Abbvie, Merck, Novartis, GSK and Exscientia, etc."
                },
                {
                    "title": "CHAPTER 28 – Drug Discovery",
                    "abstract": null
                },
                {
                    "title": "An Elementary Mathematical Theory of Classification and Prediction",
                    "abstract": null
                },
                {
                    "title": "Chatgpt sets record for fastest-growing user base - analyst note",
                    "abstract": null
                },
                {
                    "title": "Linear-time sequence modeling with selective state spaces (2023)",
                    "abstract": null
                },
                {
                    "title": "Chatgpt continues to be one of the fastest-growing services ever",
                    "abstract": null
                },
                {
                    "title": "All other authors declare no competing to Y.W. 10/10",
                    "abstract": null
                },
                {
                    "title": "OpenAI et al.",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "27379c3d-ad39-4ca5-ace6-62481f590503": {
                "pk": "27379c3d-ad39-4ca5-ace6-62481f590503",
                "project_name": null,
                "name": "David Oniani",
                "bio": "I am a researcher dedicated to advancing the intersection of artificial intelligence and healthcare, with a particular focus on natural language processing (NLP) and large language models (LLMs). My recent work explores the capabilities of LLMs in In-Context Learning (ICL) and their application in clinical settings, such as developing chatbots for COVID-19 information dissemination and enhancing clinical decision support systems. I have investigated the effectiveness of few-shot learning techniques, particularly through Siamese Neural Networks, to improve clinical NLP tasks despite limited annotated datasets.\n\nMy research also delves into the use of graph neural networks for cancer patient stratification, leveraging both phenotypic and genetic data to enhance classification accuracy. I am passionate about addressing health disparities through innovative AI solutions, such as developing ethical principles for generative AI in healthcare and creating tools to improve health literacy among patients.\n\nAdditionally, I have contributed to the development of ReDWINE, a clinical research datamart that facilitates rehabilitation research by standardizing electronic health record data. My goal is to harness the power of AI to transform healthcare delivery, making it more personalized and efficient while ensuring ethical considerations are at the forefront of technological advancements. Through my work, I aim to bridge the gap between cutting-edge AI research and practical applications that improve patient outcomes and promote health equity.",
                "collaborators": [
                    "Yanshan Wang",
                    "Sonish Sivarajkumar",
                    "Chen Wang",
                    "Yiqing Zhao",
                    "Andrew Wen",
                    "Hongfang Liu",
                    "Feichen Shen",
                    "Jordan Hilsman",
                    "Shyam Visweswaran",
                    "Hang Dong"
                ],
                "pub_titles": [
                    "In-Context Learning Functions with Varying Number of Minima",
                    "A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19",
                    "Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks",
                    "Comparisons of Graph Neural Networks on Cancer Classification Leveraging a Joint of Phenotypic and Genetic Features",
                    "Leveraging a Joint of Phenotypic and Genetic Features on Cancer Patient Subgrouping",
                    "Large Language Models Vote: Prompting for Rare Disease Identification",
                    "Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines",
                    "Toward Improving Health Literacy in Patient Education Materials with Neural Machine Translation Models",
                    "From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence",
                    "Extraction of Sleep Information from Clinical Notes of Patients with Alzheimer's Disease Using Natural Language Processing",
                    "Social and behavioral determinants of health in the era of artificial intelligence with electronic health records: A scoping review",
                    "ReDWINE: A Clinical Datamart with Text Analytical Capabilities to Facilitate Rehabilitation Research",
                    "Foundation Metrics for Evaluating Effectiveness of Healthcare Conversations Powered by Generative AI"
                ],
                "pub_abstracts": [
                    "Large Language Models (LLMs) have proven effective at In-Context Learning (ICL), an ability that allows them to create predictors from labeled examples. Few studies have explored the interplay between ICL and specific properties of functions it attempts to approximate. In our study, we use a formal framework to explore ICL and propose a new task of approximating functions with varying number of minima. We implement a method that allows for producing functions with given inputs as minima. We find that increasing the number of minima degrades ICL performance. At the same time, our evaluation shows that ICL outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster than 2NN in all settings. We validate the findings through a set of few-shot experiments across various hyperparameter configurations.",
                    "COVID-19 has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf, BERT, BioBERT, and USE to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot, we created a user-friendly interactive web application to be hosted online.",
                    "Clinical Natural Language Processing (NLP) has become an emerging technology in healthcare that leverages a large amount of free-text data in electronic health records (EHRs) to improve patient care, support clinical decisions, and facilitate clinical and translational science research. Recently, deep learning has achieved state-of-the-art performance in many clinical NLP tasks. However, training deep learning models usually requires large annotated datasets, which are normally not publicly available and can be time-consuming to build in clinical domains. Working with smaller annotated datasets is typical in clinical NLP and therefore, ensuring that deep learning models perform well is crucial for the models to be used in real-world applications. A widely adopted approach is fine-tuning existing Pre-trained Language Models (PLMs), but these attempts fall short when the training dataset contains only a few annotated samples. Few-Shot Learning (FSL) has recently been investigated to tackle this problem. Siamese Neural Network (SNN) has been widely utilized as an FSL approach in computer vision, but has not been studied well in NLP. Furthermore, the literature on its applications in clinical domains is scarce. In this paper, we propose two SNN-based FSL approaches for clinical NLP, including Pre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We evaluated the proposed approaches on two clinical tasks, namely clinical text classification and clinical named entity recognition. We tested three few-shot settings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP tasks were benchmarked using three PLMs, including BERT,BioBERT, and BioClinicalBERT. The experimental results verified the effectiveness of the proposed SNN-based FSL approaches in both NLP tasks.",
                    "Cancer is responsible for millions of deaths worldwide every year. Although significant progress hasbeen achieved in cancer medicine, many issues remain to be addressed for improving cancer therapy.Appropriate cancer patient stratification is the prerequisite for selecting appropriate treatment plan, ascancer patients are of known heterogeneous genetic make-ups and phenotypic differences. In thisstudy, built upon deep phenotypic characterizations extractable from Mayo Clinic electronic healthrecords (EHRs) and genetic test reports for a collection of cancer patients, we evaluated variousgraph neural networks (GNNs) leveraging a joint of phenotypic and genetic features for cancer typeclassification. Models were applied and fine-tuned on the Mayo Clinic cancer disease dataset. Theassessment was done through the reported accuracy, precision, recall, and F1 values as well as throughF1 scores based on the disease class. Per our evaluation results, GNNs on average outperformed thebaseline models with mean statistics always being higher that those of the baseline models (0.849 vs0.772 for accuracy, 0.858 vs 0.794 for precision, 0.843 vs 0.759 for recall, and 0.843 vs 0.855 for F1score). Among GNNs, ChebNet, GraphSAGE, and TAGCN showed the best performance, while GATshowed the worst. We applied and compared eight GNN models including AGNN, ChebNet, GAT,GCN, GIN, GraphSAGE, SGC, and TAGCN on the Mayo Clinic cancer disease dataset and assessedtheir performance as well as compared them with each other and with more conventional machinelearning models such as decision tree, gradient boosting, multi-layer perceptron, naive bayes, andrandom forest which we used as the baselines.",
                    "Cancer is responsible for millions of deaths worldwide every year. Although significant progress has been achieved in cancer medicine, many issues remain to be addressed for improving cancer therapy. Appropriate cancer patient stratification is the prerequisite for selecting appropriate treatment plan, as cancer patients are of known heterogeneous genetic make-ups and phenotypic differences. In this study, built upon deep phenotypic characterizations extractable from Mayo Clinic electronic health records (EHRs) and genetic test reports for a collection of cancer patients, we developed a system leveraging a joint of phenotypic and genetic features for cancer patient subgrouping.   The workflow is roughly divided into three parts: feature preprocessing, cancer patient classification, and cancer patient clustering based. In feature preprocessing step, we performed filtering, retaining the most relevant features. In cancer patient classification, we utilized joint categorical features to build a patient-feature matrix and applied nine different machine learning models, Random Forests (RF), Decision Tree (DT), Support Vector Machine (SVM), Naive Bayes (NB), Logistic Regression (LR), Multilayer Perceptron (MLP), Gradient Boosting (GB), Convolutional Neural Network (CNN), and Feedforward Neural Network (FNN), for classification purposes. Finally, in the cancer patient clustering step, we leveraged joint embeddings features and patient-feature associations to build an undirected feature graph and then trained the cancer feature node embeddings.",
                    "The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases affect a small fraction of the population. Rare disease identification from clinical notes inherently requires FSL techniques due to limited data availability. Manual data collection and annotation is both expensive and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FSL, available to those who signed the MIMIC-IV Data Use Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple times, substantially increasing the time needed for manual annotation, and to address this, we assess the feasibility of using JSON for automating generative LLM evaluation.",
                    "Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate superior performance, as compared to plain LLMs with ZSP, in providing accurate recommendations for COVID-19 outpatient treatment, which also highlights the potential for broader applications beyond the case study.",
                    "Health literacy is the central focus of Healthy People 2030, the fifth iteration of the U.S. national goals and objectives. People with low health literacy usually have trouble understanding health information, following post-visit instructions, and using prescriptions, which results in worse health outcomes and serious health disparities. In this study, we propose to leverage natural language processing techniques to improve health literacy in patient education materials by automatically translating illiterate languages in a given sentence. We scraped patient education materials from four online health information websites: MedlinePlus.gov, Drugs.com, Mayoclinic.org and Reddit.com. We trained and tested the state-of-the-art neural machine translation (NMT) models on a silver standard training dataset and a gold standard testing dataset, respectively. The experimental results showed that the Bidirectional Long Short-Term Memory (BiLSTM) NMT model outperformed Bidirectional Encoder Representations from Transformers (BERT)-based NMT models. We also verified the effectiveness of NMT models in translating health illiterate languages by comparing the ratio of health illiterate language in the sentence. The proposed NMT models were able to identify the correct complicated words and simplify into layman language while at the same time the models suffer from sentence completeness, fluency, readability, and have difficulty in translating certain medical terms.",
                    "In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about its application in healthcare, mainly due to concerns about transparency and related issues. Meanwhile, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI. In this paper, we propose GREAT PLEA ethical principles, encompassing governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.",
                    "Alzheimer's Disease (AD) is the most common form of dementia in the United States. Sleep is one of the lifestyle-related factors that has been shown critical for optimal cognitive function in old age. However, there is a lack of research studying the association between sleep and AD incidence. A major bottleneck for conducting such research is that the traditional way to acquire sleep information is time-consuming, inefficient, non-scalable, and limited to patients' subjective experience. A gold standard dataset is created from manual annotation of 570 randomly sampled clinical note documents from the adSLEEP, a corpus of 192,000 de-identified clinical notes of 7,266 AD patients retrieved from the University of Pittsburgh Medical Center (UPMC). We developed a rule-based Natural Language Processing (NLP) algorithm, machine learning models, and Large Language Model(LLM)-based NLP algorithms to automate the extraction of sleep-related concepts, including snoring, napping, sleep problem, bad sleep quality, daytime sleepiness, night wakings, and sleep duration, from the gold standard dataset. Rule-based NLP algorithm achieved the best performance of F1 across all sleep-related concepts. In terms of Positive Predictive Value (PPV), rule-based NLP algorithm achieved 1.00 for daytime sleepiness and sleep duration, machine learning models: 0.95 and for napping, 0.86 for bad sleep quality and 0.90 for snoring; and LLAMA2 with finetuning achieved PPV of 0.93 for Night Wakings, 0.89 for sleep problem, and 1.00 for sleep duration. The results show that the rule-based NLP algorithm consistently achieved the best performance for all sleep concepts. This study focused on the clinical notes of patients with AD, but could be extended to general sleep information extraction for other diseases.",
                    "Background: There is growing evidence that social and behavioral determinants of health (SBDH) play a substantial effect in a wide range of health outcomes. Electronic health records (EHRs) have been widely employed to conduct observational studies in the age of artificial intelligence (AI). However, there has been little research into how to make the most of SBDH information from EHRs. Methods: A systematic search was conducted in six databases to find relevant peer-reviewed publications that had recently been published. Relevance was determined by screening and evaluating the articles. Based on selected relevant studies, a methodological analysis of AI algorithms leveraging SBDH information in EHR data was provided. Results: Our synthesis was driven by an analysis of SBDH categories, the relationship between SBDH and healthcare-related statuses, and several NLP approaches for extracting SDOH from clinical literature. Discussion: The associations between SBDH and health outcomes are complicated and diverse; several pathways may be involved. Using Natural Language Processing (NLP) technology to support the extraction of SBDH and other clinical ideas simplifies the identification and extraction of essential concepts from clinical data, efficiently unlocks unstructured data, and aids in the resolution of unstructured data-related issues. Conclusion: Despite known associations between SBDH and disease, SBDH factors are rarely investigated as interventions to improve patient outcomes. Gaining knowledge about SBDH and how SBDH data can be collected from EHRs using NLP approaches and predictive models improves the chances of influencing health policy change for patient wellness, and ultimately promoting health and health equity.   Keywords: Social and Behavioral Determinants of Health, Artificial Intelligence, Electronic Health Records, Natural Language Processing, Predictive Model",
                    "Rehabilitation research focuses on determining the components of a treatment intervention, the mechanism of how these components lead to recovery and rehabilitation, and ultimately the optimal intervention strategies to maximize patients' physical, psychologic, and social functioning. Traditional randomized clinical trials that study and establish new interventions face several challenges, such as high cost and time commitment. Observational studies that use existing clinical data to observe the effect of an intervention have shown several advantages over RCTs. Electronic Health Records (EHRs) have become an increasingly important resource for conducting observational studies. To support these studies, we developed a clinical research datamart, called ReDWINE (Rehabilitation Datamart With Informatics iNfrastructure for rEsearch), that transforms the rehabilitation-related EHR data collected from the UPMC health care system to the Observational Health Data Sciences and Informatics (OHDSI) Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) to facilitate rehabilitation research. The standardized EHR data stored in ReDWINE will further reduce the time and effort required by investigators to pool, harmonize, clean, and analyze data from multiple sources, leading to more robust and comprehensive research findings. ReDWINE also includes deployment of data visualization and data analytics tools to facilitate cohort definition and clinical data analysis. These include among others the Open Health Natural Language Processing (OHNLP) toolkit, a high-throughput NLP pipeline, to provide text analytical capabilities at scale in ReDWINE. Using this comprehensive representation of patient data in ReDWINE for rehabilitation research will facilitate real-world evidence for health interventions and outcomes.",
                    "Generative Artificial Intelligence is set to revolutionize healthcare delivery by transforming traditional patient care into a more personalized, efficient, and proactive process. Chatbots, serving as interactive conversational models, will probably drive this patient-centered transformation in healthcare. Through the provision of various services, including diagnosis, personalized lifestyle recommendations, and mental health support, the objective is to substantially augment patient health outcomes, all the while mitigating the workload burden on healthcare providers. The life-critical nature of healthcare applications necessitates establishing a unified and comprehensive set of evaluation metrics for conversational models. Existing evaluation metrics proposed for various generic large language models (LLMs) demonstrate a lack of comprehension regarding medical and health concepts and their significance in promoting patients' well-being. Moreover, these metrics neglect pivotal user-centered aspects, including trust-building, ethics, personalization, empathy, user comprehension, and emotional support. The purpose of this paper is to explore state-of-the-art LLM-based evaluation metrics that are specifically applicable to the assessment of interactive conversational models in healthcare. Subsequently, we present an comprehensive set of evaluation metrics designed to thoroughly assess the performance of healthcare chatbots from an end-user perspective. These metrics encompass an evaluation of language processing abilities, impact on real-world clinical tasks, and effectiveness in user-interactive conversations. Finally, we engage in a discussion concerning the challenges associated with defining and implementing these metrics, with particular emphasis on confounding factors such as the target audience, evaluation methods, and prompt techniques involved in the evaluation process."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Machine Learning",
                    "Healthcare AI",
                    "Generative AI"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "617a13aa-d080-46b2-8b6c-56aac889ccb7": {
                "pk": "617a13aa-d080-46b2-8b6c-56aac889ccb7",
                "project_name": null,
                "name": "Jordan Hilsman",
                "bio": "As a researcher at the intersection of artificial intelligence and healthcare, my work focuses on leveraging generative Large Language Models (LLMs) to enhance Few-Shot Learning (FSL) applications, particularly in the context of rare disease identification. I recently developed the Models-Vote Prompting (MVP) approach, which aggregates outputs from multiple LLMs to improve classification accuracy in scenarios where data is scarce. This method not only enhances performance but also addresses the challenges of manual data annotation by automating the evaluation process.\n\nIn addition to my technical contributions, I am deeply invested in the ethical implications of generative AI in healthcare. Recognizing the parallels between military and medical decision-making, I have proposed the GREAT PLEA ethical principles to guide the responsible integration of generative AI technologies. These principles emphasize governance, reliability, equity, and accountability, aiming to mitigate biases and ensure that advancements in AI do not exacerbate existing health disparities.\n\nThrough my research, I strive to bridge the gap between innovative AI methodologies and their ethical application in real-world healthcare settings, ensuring that technology serves to enhance patient outcomes while upholding fundamental ethical standards.",
                "collaborators": [
                    "David Oniani",
                    "Yanshan Wang",
                    "Hang Dong",
                    "Fengyi Gao",
                    "Shiven Verma",
                    "Yifan Peng",
                    "COL",
                    "Ronald K. Poropatich",
                    "COL Jeremy C. Pamplin",
                    "LTC Gary L. Legault"
                ],
                "pub_titles": [
                    "Large Language Models Vote: Prompting for Rare Disease Identification",
                    "From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence"
                ],
                "pub_abstracts": [
                    "The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases affect a small fraction of the population. Rare disease identification from clinical notes inherently requires FSL techniques due to limited data availability. Manual data collection and annotation is both expensive and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FSL, available to those who signed the MIMIC-IV Data Use Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple times, substantially increasing the time needed for manual annotation, and to address this, we assess the feasibility of using JSON for automating generative LLM evaluation.",
                    "In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about its application in healthcare, mainly due to concerns about transparency and related issues. Meanwhile, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI. In this paper, we propose GREAT PLEA ethical principles, encompassing governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare."
                ],
                "domain": [
                    "Generative AI",
                    "Few-Shot Learning",
                    "Healthcare AI",
                    "Ethical AI"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ca3a0357-39e6-4933-b3eb-0ac21e0d0ff6": {
                "pk": "ca3a0357-39e6-4933-b3eb-0ac21e0d0ff6",
                "project_name": null,
                "name": "Chengxi Zang",
                "bio": "I am a researcher dedicated to leveraging deep learning and graph-based models to address complex challenges in drug discovery and health informatics. My recent work has focused on developing innovative generative models, such as MoFlow, which enables the generation of molecular graphs with desired chemical properties. This model not only ensures chemical validity but also excels in reconstructing training data and optimizing molecular properties, showcasing its potential to accelerate drug discovery.\n\nIn addition to generative modeling, I have explored the integration of Ordinary Differential Equations (ODEs) with Graph Neural Networks (GNNs) to learn continuous-time dynamics on complex networks. This approach allows for a unified framework that captures both the structure and dynamics of systems, facilitating predictions in various applications, including node classification and structured sequence prediction.\n\nI have also extended contrastive learning techniques to clinical risk prediction, proposing a novel supervised contrastive loss that enhances performance on imbalanced electronic health record data. This work demonstrates my commitment to improving clinical outcomes through advanced machine learning methodologies.\n\nFurthermore, I have developed visualization tools to enhance the interpretability of deep generative models in drug discovery, empowering researchers to better understand and evaluate model outputs. My research aims to bridge the gap between complex data-driven models and practical applications, ultimately contributing to advancements in healthcare and drug development.",
                "collaborators": [
                    "Fei Wang",
                    "Karan Yang",
                    "Peng Cui",
                    "Chaoming Song",
                    "Christos Faloutsos",
                    "Wenwu Zhu"
                ],
                "pub_titles": [
                    "MoFlow: An Invertible Flow Model for Generating Molecular Graphs",
                    "Neural Dynamics on Complex Networks",
                    "SCEHR: Supervised Contrastive Learning for Clinical Risk Prediction using Electronic Health Records",
                    "Visualizing Deep Graph Generative Models for Drug Discovery",
                    "Structural patterns of information cascades and their implications for dynamics and semantics"
                ],
                "pub_abstracts": [
                    "Generating molecular graphs with desired chemical properties driven by deep graph generative models provides a very promising way to accelerate drug discovery process. Such graph generative models usually consist of two steps: learning latent representations and generation of molecular graphs. However, to generate novel and chemically-valid molecular graphs from latent representations is very challenging because of the chemical constraints and combinatorial complexity of molecular graphs. In this paper, we propose MoFlow, a flow-based graph generative model to learn invertible mappings between molecular graphs and their latent representations. To generate molecular graphs, our MoFlow first generates bonds (edges) through a Glow based model, then generates atoms (nodes) given bonds by a novel graph conditional flow, and finally assembles them into a chemically valid molecular graph with a posthoc validity correction. Our MoFlow has merits including exact and tractable likelihood training, efficient one-pass embedding and generation, chemical validity guarantees, 100\\% reconstruction of training data, and good generalization ability. We validate our model by four tasks: molecular graph generation and reconstruction, visualization of the continuous latent space, property optimization, and constrained property optimization. Our MoFlow achieves state-of-the-art performance, which implies its potential efficiency and effectiveness to explore large chemical space for drug discovery.",
                    "Learning continuous-time dynamics on complex networks is crucial for understanding, predicting and controlling complex systems in science and engineering. However, this task is very challenging due to the combinatorial complexities in the structures of high dimensional systems, their elusive continuous-time nonlinear dynamics, and their structural-dynamic dependencies. To address these challenges, we propose to combine Ordinary Differential Equation Systems (ODEs) and Graph Neural Networks (GNNs) to learn continuous-time dynamics on complex networks in a data-driven manner. We model differential equation systems by GNNs. Instead of mapping through a discrete number of neural layers in the forward process, we integrate GNN layers over continuous time numerically, leading to capturing continuous-time dynamics on graphs. Our model can be interpreted as a Continuous-time GNN model or a Graph Neural ODEs model. Our model can be utilized for continuous-time network dynamics prediction, structured sequence prediction (a regularly-sampled case), and node semi-supervised classification tasks (a one-snapshot case) in a unified framework. We validate our model by extensive experiments in the above three scenarios. The promising experimental results demonstrate our model's capability of jointly capturing the structure and dynamics of complex systems in a unified framework.",
                    "Contrastive learning has demonstrated promising performance in image and text domains either in a self-supervised or a supervised manner. In this work, we extend the supervised contrastive learning framework to clinical risk prediction problems based on longitudinal electronic health records (EHR). We propose a general supervised contrastive loss $\\mathcal{L}_{\\text{Contrastive Cross Entropy} } + \\lambda \\mathcal{L}_{\\text{Supervised Contrastive Regularizer}}$ for learning both binary classification (e.g. in-hospital mortality prediction) and multi-label classification (e.g. phenotyping) in a unified framework. Our supervised contrastive loss practices the key idea of contrastive learning, namely, pulling similar samples closer and pushing dissimilar ones apart from each other, simultaneously by its two components: $\\mathcal{L}_{\\text{Contrastive Cross Entropy} }$ tries to contrast samples with learned anchors which represent positive and negative clusters, and $\\mathcal{L}_{\\text{Supervised Contrastive Regularizer}}$ tries to contrast samples with each other according to their supervised labels. We propose two versions of the above supervised contrastive loss and our experiments on real-world EHR data demonstrate that our proposed loss functions show benefits in improving the performance of strong baselines and even state-of-the-art models on benchmarking tasks for clinical risk predictions. Our loss functions work well with extremely imbalanced data which are common for clinical risk prediction problems. Our loss functions can be easily used to replace (binary or multi-label) cross-entropy loss adopted in existing clinical predictive models. The Pytorch code is released at \\url{https://github.com/calvin-zcx/SCEHR}.",
                    "Drug discovery aims at designing novel molecules with specific desired properties for clinical trials. Over past decades, drug discovery and development have been a costly and time consuming process. Driven by big chemical data and AI, deep generative models show great potential to accelerate the drug discovery process. Existing works investigate different deep generative frameworks for molecular generation, however, less attention has been paid to the visualization tools to quickly demo and evaluate model's results. Here, we propose a visualization framework which provides interactive visualization tools to visualize molecules generated during the encoding and decoding process of deep graph generative models, and provide real time molecular optimization functionalities. Our work tries to empower black box AI driven drug discovery models with some visual interpretabilities.",
                    "Information cascades are ubiquitous in both physical society and online social media, taking on large variations in structures, dynamics and semantics. Although the dynamics and semantics of information cascades have been studied, the structural patterns and their correlations with dynamics and semantics are largely unknown. Here we explore a large-scale dataset including $432$ million information cascades with explicit records of spreading traces, spreading behaviors, information content as well as user profiles. We find that the structural complexity of information cascades is far beyond the previous conjectures. We first propose a ten-dimensional metric to quantify the structural characteristics of information cascades, reflecting cascade size, silhouette, direction and activity aspects. We find that bimodal law governs majority of the metrics, information flows in cascades have four directions, and the self-loop number and average activity of cascades follows power law. We then analyze the high-order structural patterns of information cascades. Finally, we evaluate to what extent the structural features of information cascades can explain its dynamic patterns and semantics, and finally uncover some notable implications of structural patterns in information cascades. Our discoveries also provide a foundation for the microscopic mechanisms for information spreading, potentially leading to implications for cascade prediction and outlier detection."
                ],
                "domain": [
                    "Graph Generative Models",
                    "Drug Discovery",
                    "Contrastive Learning",
                    "Information Cascades"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b69ab5cf-8e66-45bb-b946-0bb1bfc6fb7b": {
                "pk": "b69ab5cf-8e66-45bb-b946-0bb1bfc6fb7b",
                "project_name": null,
                "name": "Junmei Wang",
                "bio": "I am a researcher with a strong focus on computational studies of membrane proteins, molecular dynamics simulations, and the application of deep learning in natural language processing (NLP). My work has led to the development of a novel continuum model for Poisson-Boltzmann calculations of membrane channel proteins, which significantly enhances the accuracy of water distribution predictions in these complex systems. This model optimizes parameters based on explicit-solvent molecular dynamics simulations, allowing for better alignment with experimental results, particularly in binding affinity calculations.\n\nIn addition to my work on membrane proteins, I have contributed to the advancement of polarizable force fields in biomolecular simulations, specifically through the development of the polarizable Gaussian Multipole (pGM) model. This model improves the efficiency and accuracy of electrostatic interactions in molecular simulations, facilitating more flexible and stable simulations of biomolecules.\n\nRecently, I have also explored the intersection of deep learning and information retrieval, particularly through the lens of BERT and its applications. My survey of BERT-based methods highlights their effectiveness in various NLP tasks and provides insights into their performance compared to generative large language models. I am passionate about bridging the gap between computational methods and practical applications, and I continuously seek to innovate and improve methodologies in both computational biology and machine learning.",
                "collaborators": [
                    "Ray Luo",
                    "Li Xiao",
                    "Jianxiong Diao",
                    "D Artagnan Greene",
                    "Haixin Wei",
                    "Ruxi Qi",
                    "Piotr Cieplak",
                    "Yong Duan",
                    "Jiajia Wang",
                    "Jimmy X. Huang"
                ],
                "pub_titles": [
                    "A Continuum Poisson-Boltzmann Model for Membrane Channel Proteins",
                    "Efficient Formulation of Polarizable Gaussian Multipole Electrostatics for Biomolecular Simulations",
                    "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges"
                ],
                "pub_abstracts": [
                    "Membrane proteins constitute a large portion of the human proteome and perform a variety of important functions as membrane receptors, transport proteins, enzymes, signaling proteins, and more. The computational studies of membrane proteins are usually much more complicated than those of globular proteins. Here we propose a new continuum model for Poisson-Boltzmann calculations of membrane channel proteins. Major improvements over the existing continuum slab model are as follows: 1) The location and thickness of the slab model are fine-tuned based on explicit-solvent MD simulations. 2) The highly different accessibility in the membrane and water regions are addressed with a two-step, two-probe grid labeling procedure, and 3) The water pores/channels are automatically identified. The new continuum membrane model is optimized (by adjusting the membrane probe, as well as the slab thickness and center) to best reproduce the distributions of buried water molecules in the membrane region as sampled in explicit water simulations. Our optimization also shows that the widely adopted water probe of 1.4 {\\AA} for globular proteins is a very reasonable default value for membrane protein simulations. It gives an overall minimum number of inconsistencies between the continuum and explicit representations of water distributions in membrane channel proteins, at least in the water accessible pore/channel regions that we focus on. Finally, we validate the new membrane model by carrying out binding affinity calculations for a potassium channel, and we observe a good agreement with experiment results.",
                    "Molecular dynamics simulations of biomolecules have been widely adopted in biomedical studies. As classical point-charge models continue to be used in routine biomolecular applications, there have been growing demands on developing polarizable force fields for handling more complicated biomolecular processes. Here we focus on a recently proposed polarizable Gaussian Multipole (pGM) model for biomolecular simulations. A key benefit of pGM is its screening of all short-range electrostatic interactions in a physically consistent manner, which is critical for stable charge-fitting and is needed to reproduce molecular anisotropy. Another advantage of pGM is that each atom's multipoles are represented by a single Gaussian function or its derivatives, allowing for more efficient electrostatics than other Gaussian-based models. In this study we present an efficient formulation for the pGM model defined with respect to a local frame formed with a set of covalent basis vectors. The covalent basis vectors are chosen to be along each atom's covalent bonding directions. The new local frame allows molecular flexibility during molecular simulations and facilitates an efficient formulation of analytical electrostatic forces without explicit torque computation. Subsequent numerical tests show that analytical atomic forces agree excellently with numerical finite-difference forces for the tested system. Finally, the new pGM electrostatics algorithm is interfaced with the PME implementation in Amber for molecular simulations under the periodic boundary conditions. To validate the overall pGM/PME electrostatics, we conducted an NVE simulation for a small water box of 512 water molecules. Our results show that, to achieve energy conservation in the polarizable model, it is important to ensure enough accuracy on both PME and induction iteration.",
                    "Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wide range of techniques of IR, and group them into six high-level categories: (i) handling long documents, (ii) integrating semantic information, (iii) balancing effectiveness and efficiency, (iv) predicting the weights of terms, (v) query expansion, and (vi) document expansion. We also provide links to resources, including datasets and toolkits, for BERT-based IR systems. A key highlight of our survey is the comparison between BERT's encoder-based models and the latest generative Large Language Models (LLMs), such as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we find that for specific tasks, finely tuned BERT encoders still outperform, and at a lower deployment cost. Finally, we summarize the comprehensive outcomes of the survey and suggest directions for future research in the area."
                ],
                "domain": [
                    "Computational Biology",
                    "Molecular Dynamics",
                    "Natural Language Processing",
                    "Deep Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "db3bf74d-db04-499d-9c91-6fa4fc230f7e": {
                "pk": "db3bf74d-db04-499d-9c91-6fa4fc230f7e",
                "project_name": null,
                "name": "Lianjin Cai",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nIn addition to architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimizing performance across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for optimal model designs, making it more efficient and insightful.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and machine learning, with the goal of making these technologies more effective and accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ea93f7ff-aadc-4845-8f3c-022a48fdd569": {
                "pk": "ea93f7ff-aadc-4845-8f3c-022a48fdd569",
                "project_name": null,
                "name": "Jan Zawala",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures and exploring innovative solutions.\n\nOne of my notable contributions is the development of Position-aware GNNs (P-GNNs), which effectively capture the positional context of nodes within a graph, significantly improving performance in tasks like link prediction and community detection. I also introduced Identity-aware GNNs (ID-GNNs), which enhance the expressive power of message-passing frameworks by incorporating node identities, leading to substantial accuracy improvements across various prediction tasks.\n\nRecognizing the challenges posed by dynamic graphs, I proposed the ROLAND framework, which allows for the seamless adaptation of static GNNs to dynamic environments, ensuring scalability and efficiency. My research also delves into the architectural design space of GNNs, where I systematically explored over 315,000 designs to provide guidelines for optimizing performance across different tasks.\n\nIn addition to my work on GNNs, I have ventured into automated machine learning (AutoML) with methods like FALCON and AutoTransfer, which aim to streamline the search for optimal model designs while leveraging prior knowledge to enhance efficiency.\n\nOverall, my research is driven by a passion for pushing the boundaries of what GNNs can achieve, fostering a deeper understanding of their structures, and making them more accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "AutoML",
                    "Multi-task Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ed6b29f5-3550-479e-943d-cdf93f46d380": {
                "pk": "ed6b29f5-3550-479e-943d-cdf93f46d380",
                "project_name": null,
                "name": "Yanshan Wang",
                "bio": "I am a researcher dedicated to advancing the intersection of natural language processing (NLP) and healthcare, with a focus on developing innovative solutions that enhance clinical decision-making and patient care. My recent work has explored a variety of methodologies, including prompt-based learning frameworks like HealthPrompt, which effectively utilize large language models (LLMs) for clinical NLP tasks without the need for extensive annotated datasets. \n\nI have also investigated the application of LLMs in patient-trial matching, demonstrating that open-source models can achieve performance parity with proprietary counterparts when fine-tuned on synthetic datasets. My research emphasizes the importance of fairness in healthcare AI, exemplified by my development of the Fair Patient Model (FPM), which aims to reduce bias in patient representations derived from electronic health records.\n\nAdditionally, I have contributed to the field of semantic textual similarity in clinical settings, creating the MedSTS resource to facilitate better information extraction from clinical texts. My work on assertion detection and the integration of clinical practice guidelines into LLMs showcases my commitment to improving the accuracy and efficiency of healthcare applications.\n\nThrough my research, I strive to bridge the gap between advanced AI techniques and practical healthcare applications, ultimately aiming to enhance patient outcomes and streamline clinical workflows. I am passionate about leveraging technology to address real-world challenges in healthcare, and I look forward to continuing this impactful journey.",
                "collaborators": [
                    "David Oniani",
                    "Sonish Sivarajkumar",
                    "Hongfang Liu",
                    "In-Chan Choi",
                    "Shyam Visweswaran",
                    "Yufei Huang",
                    "Mauro Nievas",
                    "Aditya Basu",
                    "Hrituraj Singh",
                    "Can Zheng"
                ],
                "pub_titles": [
                    "Stock price direction prediction by directly using prices data: an empirical study on the KOSPI and HSI",
                    "A novel soft keyboard for touchscreen phones: QWERT",
                    "MatLM: a Matrix Formulation for Probabilistic Language Models",
                    "HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing",
                    "A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19",
                    "In-Context Learning Functions with Varying Number of Minima",
                    "Generalized Ensemble Model for Document Ranking in Information Retrieval",
                    "Fair Patient Model: Mitigating Bias in the Patient Representation Learned from the Electronic Health Records",
                    "Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks",
                    "Distilling Large Language Models for Matching Patients to Clinical Trials",
                    "Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes",
                    "Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning",
                    "Indexing by Latent Dirichlet Allocation and Ensemble Model",
                    "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
                    "MedSTS: A Resource for Clinical Semantic Textual Similarity",
                    "Large Language Models Vote: Prompting for Rare Disease Identification",
                    "Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines"
                ],
                "pub_abstracts": [
                    "The prediction of a stock market direction may serve as an early recommendation system for short-term investors and as an early financial distress warning system for long-term shareholders. Many stock prediction studies focus on using macroeconomic indicators, such as CPI and GDP, to train the prediction model. However, daily data of the macroeconomic indicators are almost impossible to obtain. Thus, those methods are difficult to be employed in practice. In this paper, we propose a method that directly uses prices data to predict market index direction and stock price direction. An extensive empirical study of the proposed method is presented on the Korean Composite Stock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual constituents included in the indices. The experimental results show notably high hit ratios in predicting the movements of the individual constituents in the KOSPI and HIS.",
                    "The popularity of touchscreen phones has been growing around the world since the iPhones and Android phones were released. More and more mobile phones with large touchscreen have been produced, however, the phones with small size displays are still in the majority of touch phones. The foremost interface on touch smartphones is the information input module using soft keyboards. Traditional input methods on touch phones have either too small key buttons (such as QWERTY) or too many functions (such as 3$\\times$4 keyboard), which are inconvenient to use. Moreover, the conventional soft keyboards only use tapping to input texts while current touch smartphones allow various gestures on the touchscreen, such as sliding. In this paper, a novel soft keyboard called QWERT is proposed for touchscreen-based smartphones. The users can interact with phones via finger gestures of tapping or sliding when input text by using the QWERT. In doing so, the interactions between users and smartphones will be faster and easier. An experiment carried out on inexperienced human subjects shows that they can learn very fast due to their familiarities with QWERTY. A simulation experiment based on a cognitive architecture, ACT-R, was also conducted to predict the movement time (MT) of experienced human subjects. The simulation results show that the MT using QWERT outperforms other default keyboards. These outcomes imply that the novel QWERT is a viable option for touch smartphone users. Based on the novel design, an application is released on Android systems. This application is expected to give better user experience for customers who use touch smartphones.",
                    "Probabilistic language models are widely used in Information Retrieval (IR) to rank documents by the probability that they generate the query. However, the implementation of the probabilistic representations with programming languages that favor matrix calculations is challenging. In this paper, we utilize matrix representations to reformulate the probabilistic language models. The matrix representation is a superstructure for the probabilistic language models to organize the calculated probabilities and a potential formalism for standardization of language models and for further mathematical analysis. It facilitates implementations by matrix friendly programming languages. In this paper, we consider the matrix formulation of conventional language model with Dirichlet smoothing, and two language models based on Latent Dirichlet Allocation (LDA), i.e., LBDM and LDI. We release a Java software package--MatLM--implementing the proposed models. Code is available at: https://github.com/yanshanwang/JGibbLDA-v.1.0-MatLM.",
                    "Deep learning algorithms are dependent on the availability of large-scale annotated clinical text datasets. The lack of such publicly available datasets is the biggest bottleneck for the development of clinical Natural Language Processing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep learning models to classify instances from new classes of which no training data have been seen before. Prompt-based learning is an emerging ZSL technique where we define task-based templates for NLP tasks. We developed a novel prompt-based clinical NLP framework called HealthPrompt and applied the paradigm of prompt-based learning on clinical texts. In this technique, rather than fine-tuning a Pre-trained Language Model(PLM), the task definitions are tuned by defining a prompt template. We performed an in-depth analysis of HealthPrompt on six different PLMs in a no-data setting. Our experiments prove that prompts effectively capture the context of clinical texts and perform remarkably well without any training data.",
                    "COVID-19 has resulted in an ongoing pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The highly dynamic and rapidly evolving situation with COVID-19 has made it difficult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers from other members. However, due to the nature of such sites, there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. With the advancements in the field of natural language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate the generated responses. We utilized the GPT-2 language model and applied transfer learning to retrain it on the COVID-19 Open Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 different approaches, namely tf-idf, BERT, BioBERT, and USE to filter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence filtering tasks. Additionally, based on the chatbot, we created a user-friendly interactive web application to be hosted online.",
                    "Large Language Models (LLMs) have proven effective at In-Context Learning (ICL), an ability that allows them to create predictors from labeled examples. Few studies have explored the interplay between ICL and specific properties of functions it attempts to approximate. In our study, we use a formal framework to explore ICL and propose a new task of approximating functions with varying number of minima. We implement a method that allows for producing functions with given inputs as minima. We find that increasing the number of minima degrades ICL performance. At the same time, our evaluation shows that ICL outperforms 2-layer Neural Network (2NN) model. Furthermore, ICL learns faster than 2NN in all settings. We validate the findings through a set of few-shot experiments across various hyperparameter configurations.",
                    "A generalized ensemble model (gEnM) for document ranking is proposed in this paper. The gEnM linearly combines basis document retrieval models and tries to retrieve relevant documents at high positions. In order to obtain the optimal linear combination of multiple document retrieval models or rankers, an optimization program is formulated by directly maximizing the mean average precision. Both supervised and unsupervised learning algorithms are presented to solve this program. For the supervised scheme, two approaches are considered based on the data setting, namely batch and online setting. In the batch setting, we propose a revised Newton's algorithm, gEnM.BAT, by approximating the derivative and Hessian matrix. In the online setting, we advocate a stochastic gradient descent (SGD) based algorithm---gEnM.ON. As for the unsupervised scheme, an unsupervised ensemble model (UnsEnM) by iteratively co-learning from each constituent ranker is presented. Experimental study on benchmark data sets verifies the effectiveness of the proposed algorithms. Therefore, with appropriate algorithms, the gEnM is a viable option in diverse practical information retrieval applications.",
                    "Objective: To pre-train fair and unbiased patient representations from Electronic Health Records (EHRs) using a novel weighted loss function that reduces bias and improves fairness in deep representation learning models.   Methods: We defined a new loss function, called weighted loss function, in the deep representation learning model to balance the importance of different groups of patients and features. We applied the proposed model, called Fair Patient Model (FPM), to a sample of 34,739 patients from the MIMIC-III dataset and learned patient representations for four clinical outcome prediction tasks.   Results: FPM outperformed the baseline models in terms of three fairness metrics: demographic parity, equality of opportunity difference, and equalized odds ratio. FPM also achieved comparable predictive performance with the baselines, with an average accuracy of 0.7912. Feature analysis revealed that FPM captured more information from clinical features than the baselines.   Conclusion: FPM is a novel method to pre-train fair and unbiased patient representations from EHR data using a weighted loss function. The learned representations can be used for various downstream tasks in healthcare and can be extended to other domains where bias and fairness are important.",
                    "Clinical Natural Language Processing (NLP) has become an emerging technology in healthcare that leverages a large amount of free-text data in electronic health records (EHRs) to improve patient care, support clinical decisions, and facilitate clinical and translational science research. Recently, deep learning has achieved state-of-the-art performance in many clinical NLP tasks. However, training deep learning models usually requires large annotated datasets, which are normally not publicly available and can be time-consuming to build in clinical domains. Working with smaller annotated datasets is typical in clinical NLP and therefore, ensuring that deep learning models perform well is crucial for the models to be used in real-world applications. A widely adopted approach is fine-tuning existing Pre-trained Language Models (PLMs), but these attempts fall short when the training dataset contains only a few annotated samples. Few-Shot Learning (FSL) has recently been investigated to tackle this problem. Siamese Neural Network (SNN) has been widely utilized as an FSL approach in computer vision, but has not been studied well in NLP. Furthermore, the literature on its applications in clinical domains is scarce. In this paper, we propose two SNN-based FSL approaches for clinical NLP, including Pre-Trained SNN (PT-SNN) and SNN with Second-Order Embeddings (SOE-SNN). We evaluated the proposed approaches on two clinical tasks, namely clinical text classification and clinical named entity recognition. We tested three few-shot settings including 4-shot, 8-shot, and 16-shot learning. Both clinical NLP tasks were benchmarked using three PLMs, including BERT,BioBERT, and BioClinicalBERT. The experimental results verified the effectiveness of the proposed SNN-based FSL approaches in both NLP tasks.",
                    "The recent success of large language models (LLMs) has paved the way for their adoption in the high-stakes domain of healthcare. Specifically, the application of LLMs in patient-trial matching, which involves assessing patient eligibility against clinical trial's nuanced inclusion and exclusion criteria, has shown promise. Recent research has shown that GPT-3.5, a widely recognized LLM developed by OpenAI, can outperform existing methods with minimal 'variable engineering' by simply comparing clinical trial information against patient summaries. However, there are significant challenges associated with using closed-source proprietary LLMs like GPT-3.5 in practical healthcare applications, such as cost, privacy and reproducibility concerns. To address these issues, this study presents the first systematic examination of the efficacy of both proprietary (GPT-3.5, and GPT-4) and open-source LLMs (LLAMA 7B,13B, and 70B) for the task of patient-trial matching. Employing a multifaceted evaluation framework, we conducted extensive automated and human-centric assessments coupled with a detailed error analysis for each model. To enhance the adaptability of open-source LLMs, we have created a specialized synthetic dataset utilizing GPT-4, enabling effective fine-tuning under constrained data conditions. Our findings reveal that open-source LLMs, when fine-tuned on this limited and synthetic dataset, demonstrate performance parity with their proprietary counterparts. This presents a massive opportunity for their deployment in real-world healthcare applications. To foster further research and applications in this field, we release both the annotated evaluation dataset along with the fine-tuned LLM -- Trial-LLAMA -- for public use.",
                    "Semantic textual similarity (STS) in the clinical domain helps improve diagnostic efficiency and produce concise texts for downstream data mining tasks. However, given the high degree of domain knowledge involved in clinic text, it remains challenging for general language models to infer implicit medical relationships behind clinical sentences and output similarities correctly. In this paper, we present a graph-augmented cyclic learning framework for similarity estimation in the clinical domain. The framework can be conveniently implemented on a state-of-art backbone language model, and improve its performance by leveraging domain knowledge through co-training with an auxiliary graph convolution network (GCN) based network. We report the success of introducing domain knowledge in GCN and the co-training framework by improving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively.",
                    "In this study, we aim to address the task of assertion detection when extracting medical concepts from clinical notes, a key process in clinical natural language processing (NLP). Assertion detection in clinical NLP usually involves identifying assertion types for medical concepts in the clinical text, namely certainty (whether the medical concept is positive, negated, possible, or hypothetical), temporality (whether the medical concept is for present or the past history), and experiencer (whether the medical concept is described for the patient or a family member). These assertion types are essential for healthcare professionals to quickly and clearly understand the context of medical conditions from unstructured clinical texts, directly influencing the quality and outcomes of patient care. Although widely used, traditional methods, particularly rule-based NLP systems and machine learning or deep learning models, demand intensive manual efforts to create patterns and tend to overlook less common assertion types, leading to an incomplete understanding of the context. To address this challenge, our research introduces a novel methodology that utilizes Large Language Models (LLMs) pre-trained on a vast array of medical data for assertion detection. We enhanced the current method with advanced reasoning techniques, including Tree of Thought (ToT), Chain of Thought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank Adaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010 assertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11 improvements over the previous works. To further assess the generalizability of our approach, we extended our evaluation to a local dataset that focused on sleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31 higher than the previous method.",
                    "The contribution of this paper is two-fold. First, we present Indexing by Latent Dirichlet Allocation (LDI), an automatic document indexing method. The probability distributions in LDI utilize those in Latent Dirichlet Allocation (LDA), a generative topic model that has been previously used in applications for document retrieval tasks. However, the ad hoc applications, or their variants with smoothing techniques as prompted by previous studies in LDA-based language modeling, result in unsatisfactory performance as the document representations do not accurately reflect concept space. To improve performance, we introduce a new definition of document probability vectors in the context of LDA and present a novel scheme for automatic document indexing based on LDA. Second, we propose an Ensemble Model (EnM) for document retrieval. The EnM combines basis indexing models by assigning different weights and attempts to uncover the optimal weights to maximize the Mean Average Precision (MAP). To solve the optimization problem, we propose an algorithm, EnM.B, which is derived based on the boosting method. The results of our computational experiments on benchmark data sets indicate that both the proposed approaches are viable options for document retrieval.",
                    "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
                    "The wide adoption of electronic health records (EHRs) has enabled a wide range of applications leveraging EHR data. However, the meaningful use of EHR data largely depends on our ability to efficiently extract and consolidate information embedded in clinical text where natural language processing (NLP) techniques are essential. Semantic textual similarity (STS) that measures the semantic similarity between text snippets plays a significant role in many NLP applications. In the general NLP domain, STS shared tasks have made available a huge collection of text snippet pairs with manual annotations in various domains. In the clinical domain, STS can enable us to detect and eliminate redundant information that may lead to a reduction in cognitive burden and an improvement in the clinical decision-making process. This paper elaborates our efforts to assemble a resource for STS in the medical domain, MedSTS. It consists of a total of 174,629 sentence pairs gathered from a clinical corpus at Mayo Clinic. A subset of MedSTS (MedSTS_ann) containing 1,068 sentence pairs was annotated by two medical experts with semantic similarity scores of 0-5 (low to high similarity). We further analyzed the medical concepts in the MedSTS corpus, and tested four STS systems on the MedSTS_ann corpus. In the future, we will organize a shared task by releasing the MedSTS_ann corpus to motivate the community to tackle the real world clinical problems.",
                    "The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases affect a small fraction of the population. Rare disease identification from clinical notes inherently requires FSL techniques due to limited data availability. Manual data collection and annotation is both expensive and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FSL, available to those who signed the MIMIC-IV Data Use Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple times, substantially increasing the time needed for manual annotation, and to address this, we assess the feasibility of using JSON for automating generative LLM evaluation.",
                    "Background Large Language Models (LLMs), enhanced with Clinical Practice Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS). However, methods for incorporating CPGs into LLMs are not well studied. Methods We develop three distinct methods for incorporating CPGs into LLMs: Binary Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of the proposed methods, we create a set of synthetic patient descriptions and conduct both automatic and human evaluation of the responses generated by four LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was used as the baseline method. We focus on CDS for COVID-19 outpatient treatment as the case study. Results All four LLMs exhibit improved performance when enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP and PAGC in automatic evaluation. All of the proposed methods demonstrated high performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate superior performance, as compared to plain LLMs with ZSP, in providing accurate recommendations for COVID-19 outpatient treatment, which also highlights the potential for broader applications beyond the case study."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Machine Learning",
                    "Healthcare AI",
                    "Deep Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can Large Language Models (LLMs) be effectively utilized to translate between drug molecules represented as SMILES strings and their corresponding therapeutic indications?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem has significant implications for the research community as it could streamline the drug discovery process, making it more efficient and cost-effective. By automating the translation between drug molecules and their indications, researchers can identify potential treatments for diseases that currently lack effective therapies. This advancement could lead to a paradigm shift in drug discovery, fostering further research into generative AI applications in healthcare and potentially resulting in novel therapeutic agents that improve patient outcomes.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in addressing this problem include the complexity of accurately representing molecular structures in textual form and the inherent variability in how indications can be described. Naive approaches may fail due to the high dimensionality of the data and the need for precise mappings between SMILES strings and therapeutic indications. Additionally, the lack of extensive, high-quality datasets for training LLMs on this specific task poses a significant obstacle, as does the need for models to generalize well across diverse chemical entities and indications.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on either molecular design or indication prediction separately, often lacking a comprehensive approach that integrates both tasks. Existing solutions may have been limited by the types of models used or the datasets available, which did not adequately capture the complexity of the relationships between drugs and their indications. Our approach differs by leveraging the capabilities of LLMs to handle both tasks simultaneously, utilizing SMILES strings as a common language to bridge the gap between molecular structures and therapeutic uses.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves training a Large Language Model on a dataset of SMILES strings and their corresponding indications. We will use metrics such as accuracy and F1-score to evaluate the model's performance in both drug-to-indication and indication-to-drug translation tasks. The expected outcomes include a robust model capable of generating accurate therapeutic indications from drug molecules and vice versa, along with a publicly available codebase to facilitate further research and application in the field of drug discovery."
    },
    "2409.09825": {
        "paper_data": {
            "title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping",
            "url": "http://arxiv.org/abs/2409.09825v2",
            "arxiv_id": "2409.09825",
            "authors": [
                "Yanjun Lyu",
                "Zihao Wu",
                "Lu Zhang",
                "Jing Zhang",
                "Yiwei Li",
                "Wei Ruan",
                "Zhengliang Liu",
                "Xiaowei Yu",
                "Chao Cao",
                "Tong Chen",
                "Minheng Chen",
                "Yan Zhuang",
                "Xiang Li",
                "Rongjie Liu",
                "Chao Huang",
                "Wentao Li",
                "Tianming Liu",
                "Dajiang Zhu"
            ],
            "abstract": "Pre-trained large language models(LLMs) have attracted increasing attention in biomedical domains due to their success in natural language processing. However, the complex traits and heterogeneity of multi-sources genomics data pose significant challenges when adapting these models to the bioinformatics and biomedical field. To address these challenges, we present GP-GPT, the first specialized large language model for genetic-phenotype knowledge representation and genomics relation analysis. Our model is fine-tuned in two stages on a comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics, and medical genetics, derived from multiple large-scale validated datasets and scientific publications. GP-GPT demonstrates proficiency in accurately retrieving medical genetics information and performing common genomics analysis tasks, such as genomics information retrieval and relationship determination. Comparative experiments across domain-specific tasks reveal that GP-GPT outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These results highlight GP-GPT's potential to enhance genetic disease relation research and facilitate accurate and efficient analysis in the fields of genomics and medical genetics. Our investigation demonstrated the subtle changes of bio-factor entities' representations in the GP-GPT, which suggested the opportunities for the application of LLMs to advancing gene-phenotype research.",
            "introduction": "   1 Introduction  The relationship between genes, phenotypes and diseases is of fundamental importance but has not yet been fully understood. Due to the complex interplays and mutual regulations among genes, proteins, metabolomics, and phenotypes, finding the proper representations of their relationship has become a significant problem. There are numerous studies that focus on single-molecule or dual-molecule biological levels such as gene mutation, proteomic, gene expression, transcriptional regulation, pathway analysis, and clinical observation. These studies aim to address the questions in specific areas using simplified modelling. Existing research in human populations has elucidated the landscape of complex genomic relations. For instance, Genome-wide association study (GWAS) [1, 2, 3] is a well-established and effective approach for statistically discovering genetic loci associated with common diseases. Meta-analysis of genetic association provides new insights into cohorts of independent gene–disease associations study by combining them and resolving the discrepancies [4]. Genome annotation aims at understanding specific functional and structural properties which were conferred by nucleotide regions and amino acid sequences [5]. The Gene Ontology (GO) knowledge base (http://geneontology.org) comprehensively mapped the gene, functions of genes, and gene products into a formatted network [6] while human-specific gene networks like HumanNet v2 (http://www.inetbio.org/humannet) comprises a hierarchy of networks about gene-disease-drug associations [7]. The Online Mendelian Inheritance in Man(OMIM) [8] and DisGeNET [9] are essential knowledge platforms for disease genomics. Such large-scale genomics datasets and relevant studies have significantly expanded the scope of representative analysis of genes and diseases. However, holistic modeling at the whole genome-system scale remains challenging.   To better address this fundamental problem of representing and mapping genes and related phenotypes/diseases, the ideal approach should consider the entire genome system collectively across various molecular and biological levels. Additionally, it should incorporate experimental knowledge, bio-computational evidence, and biological text data, which have been overlooked in previous studies despite their considerable value. Large language models(LLMs) have emerged as transformative tools in natural language processing, demonstrating unprecedented capabilities in understanding and generating human-like text [10, 11, 12, 12]. These models, trained on extensive corpora, have shown remarkable versatility across diverse domains [13, 14, 15, 16, 17, 18, 19]. There are also significant successes in applying language models to the general biomedical field [20, 21, 22, 23, 24, 25]. LLM’s potential to process and analyze complex, unstructured data presents a unique opportunity to address long-standing challenges in bioinformatics. The ability of LLMs to capture intricate relationships and context from text aligns well with the complexities inherent in genetic and phenotype data. This synergy opens new avenues for knowledge discovery, hypothesis generation, and the elucidation of previously obscure gene-disease associations.   In this study, we focus on transforming the aforementioned problem into an AI-based system based on a gene-phenotype large language model, named GP-GPT. The main goal of GP-GPT is to integrate multiple sources of structured and unstructured genomic knowledge into a general LLM framework. Particularly, the model leverages both structured and unstructured data from multiple main data sources from OMIM, DisGeNET, and dbGaP [26]. By converting the data into bio-text, we identified and categorized gene entities, gene functions, protein entities, protein functions, phenotype entities, genotype-phenotype association analysis, and related biological mechanisms. These components were then integrated into",
            "references": [
                {
                    "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics",
                    "abstract": "Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learning between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, ranging from 50M up to 2.5B parameters and integrating information from 3,202 diverse human genomes, as well as 850 genomes selected across diverse phyla, including both model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the developed models can be fine-tuned at low cost and despite low available data regime to solve a variety of genomics applications. Despite no supervision, the transformer models learned to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence. Code and weights available on GitHub in Jax and HuggingFace in Pytorch. Example notebooks to apply these models to any downstream task are available on HuggingFace."
                },
                {
                    "title": "Large language model based framework for automated extraction of genetic interactions from unstructured data",
                    "abstract": "Extracting biological interactions from published literature helps us understand complex biological systems, accelerate research, and support decision-making in drug or treatment development. Despite efforts to automate the extraction of biological relations using text mining tools and machine learning pipelines, manual curation continues to serve as the gold standard. However, the rapidly increasing volume of literature pertaining to biological relations poses challenges in its manual curation and refinement. These challenges are further compounded because only a small fraction of the published literature is relevant to biological relation extraction, and the embedded sentences of relevant sections have complex structures, which can lead to incorrect inference of relationships. To overcome these challenges, we propose GIX, an automated and robust Gene Interaction Extraction framework, based on pre-trained Large Language models fine-tuned through extensive evaluations on various gene/protein interaction corpora including LLL and RegulonDB. GIX identifies relevant publications with minimal keywords, optimises sentence selection to reduce computational overhead, simplifies sentence structure while preserving meaning, and provides a confidence factor indicating the reliability of extracted relations. GIX’s Stage-2 relation extraction method performed well on benchmark protein/gene interaction datasets, assessed using 10-fold cross-validation, surpassing state-of-the-art approaches. We demonstrated that the proposed method, although fully automated, performs as well as manual relation extraction, with enhanced robustness. We also observed GIX’s capability to augment existing datasets with new sentences, incorporating newly discovered biological terms and processes. Further, we demonstrated GIX’s real-world applicability in inferring E. coli gene circuits."
                },
                {
                    "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
                    "abstract": "We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents."
                },
                {
                    "title": "Multimodal Learning for Mapping the Genotype-Phenotype Dynamics",
                    "abstract": "How complex phenotypes emerge from intricate gene expression patterns is a fundamental question in biology. Quantitative characterization of this relationship, however, is challenging due to the vast combinatorial possibilities and dynamic interplay between genotype and phenotype landscapes. Integrating high-content genotyping approaches such as single-cell RNA sequencing and advanced learning methods such as language models offers an opportunity for dissecting this complex relationship. Here, we present a computational integrated genetics framework designed to analyze and interpret the high-dimensional landscape of genotypes and their associated phenotypes simultaneously. We applied this approach to develop a multimodal foundation model to explore the genotype-phenotype relationship manifold for human transcriptomics at the cellular level. Analyzing this joint manifold showed a refined resolution of cellular heterogeneity, enhanced precision in phenotype annotating, and uncovered potential cross-tissue biomarkers that are undetectable through conventional gene expression analysis alone. Moreover, our results revealed that the gene networks are characterized by scale-free patterns and show context-dependent gene-gene interactions, both of which result in significant variations in the topology of the gene network, particularly evident during aging. Finally, utilizing contextualized embeddings, we investigated gene polyfunctionality which illustrates the multifaceted roles that genes play in different biological processes, and demonstrated that for VWF gene in endothelial cells. Overall, this study advances our understanding of the dynamic interplay between gene expression and phenotypic manifestation and demonstrates the potential of integrated genetics in uncovering new dimensions of cellular function and complexity."
                },
                {
                    "title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
                    "abstract": null
                },
                {
                    "title": "Advancing Multimodal Medical Capabilities of Gemini",
                    "abstract": "Many clinical tasks require an understanding of specialized data, such as medical images and genomics, which is not typically found in general-purpose large multimodal models. Building upon Gemini's multimodal models, we develop several models within the new Med-Gemini family that inherit core capabilities of Gemini and are optimized for medical use via fine-tuning with 2D and 3D radiology, histopathology, ophthalmology, dermatology and genomic data. Med-Gemini-2D sets a new standard for AI-based chest X-ray (CXR) report generation based on expert evaluation, exceeding previous best results across two separate datasets by an absolute margin of 1% and 12%, where 57% and 96% of AI reports on normal cases, and 43% and 65% on abnormal cases, are evaluated as\"equivalent or better\"than the original radiologists' reports. We demonstrate the first ever large multimodal model-based report generation for 3D computed tomography (CT) volumes using Med-Gemini-3D, with 53% of AI reports considered clinically acceptable, although additional research is needed to meet expert radiologist reporting quality. Beyond report generation, Med-Gemini-2D surpasses the previous best performance in CXR visual question answering (VQA) and performs well in CXR classification and radiology VQA, exceeding SoTA or baselines on 17 of 20 tasks. In histopathology, ophthalmology, and dermatology image classification, Med-Gemini-2D surpasses baselines across 18 out of 20 tasks and approaches task-specific model performance. Beyond imaging, Med-Gemini-Polygenic outperforms the standard linear polygenic risk score-based approach for disease risk prediction and generalizes to genetically correlated diseases for which it has never been trained. Although further development and evaluation are necessary in the safety-critical medical domain, our results highlight the potential of Med-Gemini across a wide range of medical tasks."
                },
                {
                    "title": "Bilingual Language Model for Protein Sequence and Structure",
                    "abstract": "Adapting large language models (LLMs) to protein sequences spawned the development of powerful protein language models (pLMs). Concurrently, AlphaFold2 broke through in protein structure prediction. Now we can systematically and comprehensively explore the dual nature of proteins that act and exist as three-dimensional (3D) machines and evolve as linear strings of one-dimensional (1D) sequences. Here, we leverage pLMs to simultaneously model both modalities by combining 1D sequences with 3D structure in a single model. We encode protein structures as token sequences using the 3Di-alphabet introduced by the 3D-alignment method Foldseek. This new foundation pLM extracts the features and patterns of the resulting “structure-sequence” representation. Toward this end, we built a non-redundant dataset from AlphaFoldDB and fine-tuned an existing pLM (ProtT5) to translate between 3Di and amino acid sequences. As a proof-of-concept for our novel approach, dubbed Protein structure-sequence T5 (ProstT5), we showed improved performance for subsequent prediction tasks, and for “inverse folding”, namely the generation of novel protein sequences adopting a given structural scaffold (“fold”). Our work showcased the potential of pLMs to tap into the information-rich protein structure revolution fueled by AlphaFold2. ProstT5 paves the way to develop new tools integrating the vast resource of 3D predictions, and opens new research avenues in the post-AlphaFold2 era. Our model is freely available for all at https://github.com/mheinzinger/ProstT5."
                },
                {
                    "title": "AI for Biomedicine in the Era of Large Language Models",
                    "abstract": "The capabilities of AI for biomedicine span a wide spectrum, from the atomic level, where it solves partial differential equations for quantum systems, to the molecular level, predicting chemical or protein structures, and further extending to societal predictions like infectious disease outbreaks. Recent advancements in large language models, exemplified by models like ChatGPT, have showcased significant prowess in natural language tasks, such as translating languages, constructing chatbots, and answering questions. When we consider biomedical data, we observe a resemblance to natural language in terms of sequences: biomedical literature and health records presented as text, biological sequences or sequencing data arranged in sequences, or sensor data like brain signals as time series. The question arises: Can we harness the potential of recent large language models to drive biomedical knowledge discoveries? In this survey, we will explore the application of large language models to three crucial categories of biomedical data: 1) textual data, 2) biological sequences, and 3) brain signals. Furthermore, we will delve into large language model challenges in biomedical research, including ensuring trustworthiness, achieving personalization, and adapting to multi-modal data representation"
                },
                {
                    "title": "BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning",
                    "abstract": "Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their textual descriptions, providing a more holistic understanding of biological entities, and largely improving the grounded reasoning of bio-text and bio-sequences. The model is pre-trained and fine-tuned with a large number of experiments, including \\emph{3 types of problems (classification, regression, generation), 15 kinds of tasks, and 21 total benchmark datasets}, demonstrating the remarkable performance and state-of-the-art results in most cases. BioT5+ stands out for its ability to capture intricate relationships in biological data, thereby contributing significantly to bioinformatics and computational biology. Our code is available at \\url{https://github.com/QizhiPei/BioT5}."
                },
                {
                    "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",
                    "abstract": "Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this\"world simulator\". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video generation models in general, and how advancements in the field could enable new ways of human-AI interaction, boosting productivity and creativity of video generation."
                },
                {
                    "title": "scGPT: toward building a foundation model for single-cell multi-omics using generative AI.",
                    "abstract": null
                },
                {
                    "title": "Surviving ChatGPT in healthcare",
                    "abstract": "At the dawn of of Artificial General Intelligence (AGI), the emergence of large language models such as ChatGPT show promise in revolutionizing healthcare by improving patient care, expanding medical access, and optimizing clinical processes. However, their integration into healthcare systems requires careful consideration of potential risks, such as inaccurate medical advice, patient privacy violations, the creation of falsified documents or images, overreliance on AGI in medical education, and the perpetuation of biases. It is crucial to implement proper oversight and regulation to address these risks, ensuring the safe and effective incorporation of AGI technologies into healthcare systems. By acknowledging and mitigating these challenges, AGI can be harnessed to enhance patient care, medical knowledge, and healthcare processes, ultimately benefiting society as a whole."
                },
                {
                    "title": "Large Language Models for Robotics: Opportunities, Challenges, and Perspectives",
                    "abstract": "Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction."
                },
                {
                    "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                    "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development."
                },
                {
                    "title": "Multimodality of AI for Education: Towards Artificial General Intelligence",
                    "abstract": "This paper presents a comprehensive examination of how multimodal artificial intelligence (AI) approaches are paving the way towards the realization of Artificial General Intelligence (AGI) in educational contexts. It scrutinizes the evolution and integration of AI in educational systems, emphasizing the crucial role of multimodality, which encompasses auditory, visual, kinesthetic, and linguistic modes of learning. This research delves deeply into the key facets of AGI, including cognitive frameworks, advanced knowledge representation, adaptive learning mechanisms, strategic planning, sophisticated language processing, and the integration of diverse multimodal data sources. It critically assesses AGI's transformative potential in reshaping educational paradigms, focusing on enhancing teaching and learning effectiveness, filling gaps in existing methodologies, and addressing ethical considerations and responsible usage of AGI in educational settings. The paper also discusses the implications of multimodal AI's role in education, offering insights into future directions and challenges in AGI development. This exploration aims to provide a nuanced understanding of the intersection between AI, multimodality, and education, setting a foundation for future research and development in AGI."
                },
                {
                    "title": "Disease2Vec: Encoding Alzheimer’s progression via disease embedding tree",
                    "abstract": null
                },
                {
                    "title": "Holistic Evaluation of GPT-4V for Biomedical Imaging",
                    "abstract": "In this paper, we present a large-scale evaluation probing GPT-4V's capabilities and limitations for biomedical image analysis. GPT-4V represents a breakthrough in artificial general intelligence (AGI) for computer vision, with applications in the biomedical domain. We assess GPT-4V's performance across 16 medical imaging categories, including radiology, oncology, ophthalmology, pathology, and more. Tasks include modality recognition, anatomy localization, disease diagnosis, report generation, and lesion detection. The extensive experiments provide insights into GPT-4V's strengths and weaknesses. Results show GPT-4V's proficiency in modality and anatomy recognition but difficulty with disease diagnosis and localization. GPT-4V excels at diagnostic report generation, indicating strong image captioning skills. While promising for biomedical imaging AI, GPT-4V requires further enhancement and validation before clinical deployment. We emphasize responsible development and testing for trustworthy integration of biomedical AGI. This rigorous evaluation of GPT-4V on diverse medical images advances understanding of multimodal large language models (LLMs) and guides future work toward impactful healthcare applications."
                },
                {
                    "title": "Structure Mapping Generative Adversarial Network for Multi-View Information Mapping Pattern Mining",
                    "abstract": "Multi-view learning is dedicated to integrating information from different views and improving the generalization performance of models. However, in most current works, learning under different views has significant independency, overlooking common information mapping patterns that exist between these views. This paper proposes a Structure Mapping Generative adversarial network (SM-GAN) framework, which utilizes the consistency and complementarity of multi-view data from the innovative perspective of information mapping. Specifically, based on network-structured multi-view data, a structural information mapping model is proposed to capture hierarchical interaction patterns among views. Subsequently, three different types of graph convolutional operations are designed in SM-GAN based on the model. Compared with regular GAN, we add a structural information mapping module between the encoder and decoder wthin the generator, completing the structural information mapping from the micro-view to the macro-view. This paper conducted sufficient validation experiments using public imaging genetics data in Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. It is shown that SM-GAN outperforms baseline and advanced methods in multi-label classification and evolution prediction tasks."
                },
                {
                    "title": "BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations",
                    "abstract": "Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at $\\href{https://github.com/QizhiPei/BioT5}{Github}$."
                },
                {
                    "title": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
                    "abstract": "The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44%, alongside a noteworthy context reduction of 96.45% across various graph reasoning tasks."
                },
                {
                    "title": "Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data",
                    "abstract": "Large language models (LLMs) have achieved impressive performance on many natural language processing tasks. However, their capabilities on graph-structured data remain relatively unexplored. In this paper, we conduct a series of experiments benchmarking leading LLMs on diverse graph prediction tasks spanning node, edge, and graph levels. We aim to assess whether LLMs can effectively process graph data and leverage topological structures to enhance performance, compared to specialized graph neural networks. Through varied prompt formatting and task/dataset selection, we analyze how well LLMs can interpret and utilize graph structures. By comparing LLMs' performance with specialized graph models, we offer insights into the strengths and limitations of employing LLMs for graph analytics. Our findings provide insights into LLMs' capabilities and suggest avenues for further exploration in applying them to graph analytics."
                },
                {
                    "title": "RadOnc-GPT: A Large Language Model for Radiation Oncology",
                    "abstract": "This paper presents RadOnc-GPT, a large language model specialized for radiation oncology through advanced tuning methods. RadOnc-GPT was finetuned on a large dataset of radiation oncology patient records and clinical notes from the Mayo Clinic in Arizona. The model employs instruction tuning on three key tasks - generating radiotherapy treatment regimens, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. Evaluations conducted by comparing RadOnc-GPT outputs to general large language model outputs showed that RadOnc-GPT generated outputs with significantly improved clarity, specificity, and clinical relevance. The study demonstrated the potential of using large language models fine-tuned using domain-specific knowledge like RadOnc-GPT to achieve transformational capabilities in highly specialized healthcare fields such as radiation oncology."
                },
                {
                    "title": "Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges",
                    "abstract": "Artificial General Intelligence (AGI), possessing the capacity to comprehend, learn, and execute tasks with human cognitive abilities, engenders significant anticipation and intrigue across scientific, commercial, and societal arenas. This fascination extends particularly to the Internet of Things (IoT), a landscape characterized by the interconnection of countless devices, sensors, and systems, collectively gathering and sharing data to enable intelligent decision-making and automation. This research embarks on an exploration of the opportunities and challenges towards achieving AGI in the context of the IoT. Specifically, it starts by outlining the fundamental principles of IoT and the critical role of Artificial Intelligence (AI) in IoT systems. Subsequently, it delves into AGI fundamentals, culminating in the formulation of a conceptual framework for AGI's seamless integration within IoT. The application spectrum for AGI-infused IoT is broad, encompassing domains ranging from smart grids, residential environments, manufacturing, and transportation to environmental monitoring, agriculture, healthcare, and education. However, adapting AGI to resource-constrained IoT settings necessitates dedicated research efforts. Furthermore, the paper addresses constraints imposed by limited computing resources, intricacies associated with large-scale IoT communication, as well as the critical concerns pertaining to security and privacy."
                },
                {
                    "title": "Radiology-Llama2: Best-in-Class Large Language Model for Radiology",
                    "abstract": "This paper introduces Radiology-Llama2, a large language model specialized for radiology through a process known as instruction tuning. Radiology-Llama2 is based on the Llama2 architecture and further trained on a large dataset of radiology reports to generate coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other generative language models, with a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of localized language models designed and tuned for specialized domains like radiology. When properly evaluated and deployed, such models can transform fields like radiology by automating rote tasks and enhancing human expertise."
                },
                {
                    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
                },
                {
                    "title": "Exploring Multimodal Approaches for Alzheimer's Disease Detection Using Patient Speech Transcript and Audio Data",
                    "abstract": "Alzheimer's disease (AD) is a common form of dementia that severely impacts patient health. As AD impairs the patient's language understanding and expression ability, the speech of AD patients can serve as an indicator of this disease. This study investigates various methods for detecting AD using patients' speech and transcripts data from the DementiaBank Pitt database. The proposed approach involves pre-trained language models and Graph Neural Network (GNN) that constructs a graph from the speech transcript, and extracts features using GNN for AD detection. Data augmentation techniques, including synonym replacement, GPT-based augmenter, and so on, were used to address the small dataset size. Audio data was also introduced, and WavLM model was used to extract audio features. These features were then fused with text features using various methods. Finally, a contrastive learning approach was attempted by converting speech transcripts back to audio and using it for contrastive learning with the original audio. We conducted intensive experiments and analysis on the above methods. Our findings shed light on the challenges and potential solutions in AD detection using speech and audio data."
                },
                {
                    "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
                    "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level, an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics for simple adaptation to novel tasks without updating pretrained model weights. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 17 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on all 8 datasets on average by +9 accuracy points."
                },
                {
                    "title": "Large Scale Foundation Model on Single-cell Transcriptomics",
                    "abstract": "Large-scale pretrained models have become foundation models leading to breakthroughs in natural language processing and related fields. Developing foundation models in life science for deciphering the “languages” of cells and facilitating biomedical research is promising yet challenging. We developed a large-scale pretrained model scFoundation with 100M parameters for this purpose. scFoundation was trained on over 50 million human single-cell transcriptomics data, which contain high-throughput observations on the complex molecular features in all known types of cells. scFoundation is currently the largest model in terms of the size of trainable parameters, dimensionality of genes and the number of cells used in the pre-training. Experiments showed that scFoundation can serve as a foundation model for single-cell transcriptomics and achieve state-of-the-art performances in a diverse array of downstream tasks, such as gene expression enhancement, tissue drug response prediction, single-cell drug response classification, and single-cell perturbation prediction."
                },
                {
                    "title": "AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology",
                    "abstract": "In this pioneering study, inspired by AutoGPT, the state-of-the-art open-source application based on the GPT-4 large language model, we develop a novel tool called AD-AutoGPT which can conduct data collection, processing, and analysis about complex health narratives of Alzheimer's Disease in an autonomous manner via users' textual prompts. We collated comprehensive data from a variety of news sources, including the Alzheimer's Association, BBC, Mayo Clinic, and the National Institute on Aging since June 2022, leading to the autonomous execution of robust trend analyses, intertopic distance maps visualization, and identification of salient terms pertinent to Alzheimer's Disease. This approach has yielded not only a quantifiable metric of relevant discourse but also valuable insights into public focus on Alzheimer's Disease. This application of AD-AutoGPT in public health signifies the transformative potential of AI in facilitating a data-rich understanding of complex health narratives like Alzheimer's Disease in an autonomous manner, setting the groundwork for future AI-driven investigations in global health landscapes."
                },
                {
                    "title": "Artificial General Intelligence for Medical Imaging",
                    "abstract": "In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond."
                },
                {
                    "title": "Transfer learning enables predictions in network biology",
                    "abstract": null
                },
                {
                    "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
                    "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
                },
                {
                    "title": "Community Graph Convolution Neural Network for Alzheimer's Disease Classification and Pathogenetic Factors Identification.",
                    "abstract": "As a complex neural network system, the brain regions and genes collaborate to effectively store and transmit information. We abstract the collaboration correlations as the brain region gene community network (BG-CN) and present a new deep learning approach, such as the community graph convolutional neural network (Com-GCN), for investigating the transmission of information within and between communities. The results can be used for diagnosing and extracting causal factors for Alzheimer's disease (AD). First, an affinity aggregation model for BG-CN is developed to describe intercommunity and intracommunity information transmission. Second, we design the Com-GCN architecture with intercommunity convolution and intracommunity convolution operations based on the affinity aggregation model. Through sufficient experimental validation on the AD neuroimaging initiative (ADNI) dataset, the design of Com-GCN matches the physiological mechanism better and improves the interpretability and classification performance. Furthermore, Com-GCN can identify lesioned brain regions and disease-causing genes, which may assist precision medicine and drug design in AD and serve as a valuable reference for other neurological disorders."
                },
                {
                    "title": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca",
                    "abstract": "Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward faithfully understanding the inner-workings of our ever-growing and most widely deployed language models. Our tool is extensible to larger LLMs and is released publicly at `https://github.com/stanfordnlp/pyvene`."
                },
                {
                    "title": "Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT",
                    "abstract": "Prompts have been proven to play a crucial role in large language models, and in recent years, vision models have also been using prompts to improve scalability for multiple downstream tasks. In this paper, we focus on adapting prompt design based on instruction tuning into a visual transformer model for image classification which we called Instruction-ViT. The key idea is to implement multi-modal prompts (text or image prompt) related to category information to guide the fine-tuning of the model. Based on the experiments of several image captionining tasks, the performance and domain adaptability were improved. Our work provided an innovative strategy to fuse multi-modal prompts with better performance and faster adaptability for visual classification models."
                },
                {
                    "title": "Prompt Engineering for Healthcare: Methodologies and Applications",
                    "abstract": "Prompt engineering is a critical technique in the field of natural language processing that involves designing and optimizing the prompts used to input information into models, aiming to enhance their performance on specific tasks. With the recent advancements in large language models, prompt engineering has shown significant superiority across various domains and has become increasingly important in the healthcare domain. However, there is a lack of comprehensive reviews specifically focusing on prompt engineering in the medical field. This review will introduce the latest advances in prompt engineering in the field of natural language processing for the medical field. First, we will provide the development of prompt engineering and emphasize its significant contributions to healthcare natural language processing applications such as question-answering systems, text summarization, and machine translation. With the continuous improvement of general large language models, the importance of prompt engineering in the healthcare domain is becoming increasingly prominent. The aim of this article is to provide useful resources and bridges for healthcare natural language processing researchers to better explore the application of prompt engineering in this field. We hope that this review can provide new ideas and inspire for research and application in medical natural language processing."
                },
                {
                    "title": "Differentiating ChatGPT-Generated and Human-Written Medical Texts: Quantitative Study",
                    "abstract": "Background Large language models, such as ChatGPT, are capable of generating grammatically perfect and human-like text content, and a large number of ChatGPT-generated texts have appeared on the internet. However, medical texts, such as clinical notes and diagnoses, require rigorous validation, and erroneous medical content generated by ChatGPT could potentially lead to disinformation that poses significant harm to health care and the general public. Objective This study is among the first on responsible artificial intelligence–generated content in medicine. We focus on analyzing the differences between medical texts written by human experts and those generated by ChatGPT and designing machine learning workflows to effectively detect and differentiate medical texts generated by ChatGPT. Methods We first constructed a suite of data sets containing medical texts written by human experts and generated by ChatGPT. We analyzed the linguistic features of these 2 types of content and uncovered differences in vocabulary, parts-of-speech, dependency, sentiment, perplexity, and other aspects. Finally, we designed and implemented machine learning methods to detect medical text generated by ChatGPT. The data and code used in this paper are published on GitHub. Results Medical texts written by humans were more concrete, more diverse, and typically contained more useful information, while medical texts generated by ChatGPT paid more attention to fluency and logic and usually expressed general terminologies rather than effective information specific to the context of the problem. A bidirectional encoder representations from transformers–based model effectively detected medical texts generated by ChatGPT, and the F1 score exceeded 95%. Conclusions Although text generated by ChatGPT is grammatically perfect and human-like, the linguistic characteristics of generated medical texts were different from those written by human experts. Medical text generated by ChatGPT could be effectively detected by the proposed machine learning algorithms. This study provides a pathway toward trustworthy and accountable use of large language models in medicine."
                },
                {
                    "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
                    "abstract": "While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations. Augmenting LLMs with domain-specific tools such as database utilities can facilitate easier and more precise access to specialized knowledge. In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions. Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs by in-context learning and an augmented decoding algorithm that can detect and execute API calls. Experimental results show that GeneGPT achieves state-of-the-art performance on eight tasks in the GeneTuring benchmark with an average score of 0.83, largely surpassing retrieval-augmented LLMs such as the new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1) API demonstrations have good cross-task generalizability and are more useful than documentations for in-context learning; (2) GeneGPT can generalize to longer chains of API calls and answer multi-hop questions in GeneHop, a novel dataset introduced in this work; (3) Different types of errors are enriched in different tasks, providing valuable insights for future improvements."
                },
                {
                    "title": "An Iterative Optimizing Framework for Radiology Report Summarization With ChatGPT",
                    "abstract": "The “Impression” section of a radiology report is a critical basis for communication between radiologists and other physicians. Typically written by radiologists, this part is derived from the “Findings” section, which can be laborious and error-prone. Although deep-learning-based models, such as bidirectional encoder representation from transformers (BERT), have achieved promising results in automatic impression generation (AIG), such models often require substantial amounts of medical data and have poor generalization performance. Recently, large language models (LLMs) like Chat Generative Pre-trained Transformer (ChatGPT) have shown strong generalization capabilities and performance, but their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, leveraging the contextual learning capabilities of LLMs through our dynamic prompt and iterative optimization algorithm to accomplish the AIG task. ImpressionGPT initially employs a small amount of domain-specific data to create a dynamic prompt, extracting contextual semantic information closely related to the test data. Subsequently, the iterative optimization algorithm automatically evaluates the output of LLMs and provides optimization suggestions, continuously refining the output results. The proposed ImpressionGPT model achieves superior performance of AIG task on both the Medical Information Mart for Intensive Care - Chest X-ray database (MIMIC-CXR) and Open Access Biomedical Image Search Engine (OpenI) datasets without requiring additional training data or fine-tuning the LLMs. This work presents a paradigm for localizing LLMs that can be applied in a wide range of similar application scenarios, bridging the gap between general-purpose LLMs and the specific language processing needs of various domains."
                },
                {
                    "title": "Biological Factor Regulatory Neural Network",
                    "abstract": "Genes are fundamental for analyzing biological systems and many recent works proposed to utilize gene expression for various biological tasks by deep learning models. Despite their promising performance, it is hard for deep neural networks to provide biological insights for humans due to their black-box nature. Recently, some works integrated biological knowledge with neural networks to improve the transparency and performance of their models. However, these methods can only incorporate partial biological knowledge, leading to suboptimal performance. In this paper, we propose the Biological Factor Regulatory Neural Network (BFReg-NN), a generic framework to model relations among biological factors in cell systems. BFReg-NN starts from gene expression data and is capable of merging most existing biological knowledge into the model, including the regulatory relations among genes or proteins (e.g., gene regulatory networks (GRN), protein-protein interaction networks (PPI)) and the hierarchical relations among genes, proteins and pathways (e.g., several genes/proteins are contained in a pathway). Moreover, BFReg-NN also has the ability to provide new biologically meaningful insights because of its white-box characteristics. Experimental results on different gene expression-based tasks verify the superiority of BFReg-NN compared with baselines. Our case studies also show that the key insights found by BFReg-NN are consistent with the biological literature."
                },
                {
                    "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
                    "abstract": "Purpose We present the first study to investigate Large Language Models (LLMs) in answering radiation oncology physics questions. Because popular exams like AP Physics, LSAT, and GRE have large test-taker populations and ample test preparation resources in circulation, they may not allow for accurately assessing the true potential of LLMs. This paper proposes evaluating LLMs on a highly-specialized topic, radiation oncology physics, which may be more pertinent to scientific and medical communities in addition to being a valuable benchmark of LLMs. Methods We developed an exam consisting of 100 radiation oncology physics questions based on our expertise. Four LLMs, ChatGPT (GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against medical physicists and non-experts. The performance of ChatGPT (GPT-4) was further explored by being asked to explain first, then answer. The deductive reasoning capability of ChatGPT (GPT-4) was evaluated using a novel approach (substituting the correct answer with “None of the above choices is the correct answer.”). A majority vote analysis was used to approximate how well each group could score when working together. Results ChatGPT GPT-4 outperformed all other LLMs and medical physicists, on average, with improved accuracy when prompted to explain before answering. ChatGPT (GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices across a number of trials, whether correct or incorrect, a characteristic that was not observed in the human test groups or Bard (LaMDA). In evaluating deductive reasoning ability, ChatGPT (GPT-4) demonstrated surprising accuracy, suggesting the potential presence of an emergent ability. Finally, although ChatGPT (GPT-4) performed well overall, its intrinsic properties did not allow for further improvement when scoring based on a majority vote across trials. In contrast, a team of medical physicists were able to greatly outperform ChatGPT (GPT-4) using a majority vote. Conclusion This study suggests a great potential for LLMs to work alongside radiation oncology experts as highly knowledgeable assistants."
                },
                {
                    "title": "When Brain-inspired AI Meets AGI",
                    "abstract": "Artificial General Intelligence (AGI) has been a long-standing goal of humanity, with the aim of creating machines capable of performing any intellectual task that humans can do. To achieve this, AGI researchers draw inspiration from the human brain and seek to replicate its principles in intelligent machines. Brain-inspired artificial intelligence is a field that has emerged from this endeavor, combining insights from neuroscience, psychology, and computer science to develop more efficient and powerful AI systems. In this article, we provide a comprehensive overview of brain-inspired AI from the perspective of AGI. We begin with the current progress in brain-inspired AI and its extensive connection with AGI. We then cover the important characteristics for both human intelligence and AGI (e.g., scaling, multimodality, and reasoning). We discuss important technologies toward achieving AGI in current AI systems, such as in-context learning and prompt tuning. We also investigate the evolution of AGI systems from both algorithmic and infrastructural perspectives. Finally, we explore the limitations and future of AGI."
                },
                {
                    "title": "GPT-4 Technical Report",
                    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
                },
                {
                    "title": "GeneTuring tests GPT models in genomics",
                    "abstract": "Generative Pre-trained Transformers (GPT) are powerful language models that have great potential to transform biomedical research. However, they are known to suffer from artificial hallucinations and provide false answers that are seemingly correct in some situations. We developed GeneTuring, a comprehensive QA database with 600 questions in genomics, and manually scored 10,800 answers returned by six GPT models, including GPT-3, ChatGPT, and New Bing. New Bing has the best overall performance and significantly reduces the level of AI hallucination compared to other models, thanks to its ability to recognize its incapacity in answering questions. We argue that improving incapacity awareness is equally important as improving model accuracy to address AI hallucination."
                },
                {
                    "title": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts",
                    "abstract": "Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original representation power. On downstream tasks, ProtST enables both supervised learning and zero-shot prediction. We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification, and ProtST also enables functional protein retrieval from a large-scale database without any function annotation."
                },
                {
                    "title": "Context Matters: A Strategy to Pre-train Language Model for Science Education",
                    "abstract": "This study aims at improving the performance of scoring student responses in science education automatically. BERT-based language models have shown significant superiority over traditional NLP models in various language-related tasks. However, science writing of students, including argumentation and explanation, is domain-specific. In addition, the language used by students is different from the language in journals and Wikipedia, which are training sources of BERT and its existing variants. All these suggest that a domain-specific model pre-trained using science education data may improve model performance. However, the ideal type of data to contextualize pre-trained language model and improve the performance in automatically scoring student written responses remains unclear. Therefore, we employ different data in this study to contextualize both BERT and SciBERT models and compare their performance on automatic scoring of assessment tasks for scientific argumentation. We use three datasets to pre-train the model: 1) journal articles in science education, 2) a large dataset of students' written responses (sample size over 50,000), and 3) a small dataset of students' written responses of scientific argumentation tasks. Our experimental results show that in-domain training corpora constructed from science questions and responses improve language model performance on a wide variety of downstream tasks. Our study confirms the effectiveness of continual pre-training on domain-specific data in the education domain and demonstrates a generalizable strategy for automating science education tasks with high accuracy. We plan to release our data and SciEdBERT models for public use and community engagement."
                },
                {
                    "title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
                    "abstract": "We seek to transform how new and emergent variants of pandemic-causing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pre-training on over 110 million prokaryotic gene sequences and fine-tuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole-genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data."
                },
                {
                    "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
                    "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License."
                },
                {
                    "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
                    "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms."
                },
                {
                    "title": "Annotation of biologically relevant ligands in UniProtKB using ChEBI",
                    "abstract": "Motivation To provide high quality, computationally tractable annotation of binding sites for biologically relevant (cognate) ligands in UniProtKB using the chemical ontology ChEBI (Chemical Entities of Biological Interest), to better support efforts to study and predict functionally relevant interactions between proteins and small molecule ligands. Results We structured the data model for cognate ligand binding site annotations in UniProtKB and performed a complete reannotation of all cognate ligand binding sites using stable unique identifiers from ChEBI, which we now use as the reference vocabulary for all such annotations. We developed improved search and query facilities for cognate ligands in the UniProt website, REST API and SPARQL endpoint that leverage the chemical structure data, nomenclature, and classification that ChEBI provides. Availability Binding site annotations for cognate ligands described using ChEBI are available for UniProtKB protein sequence records in several formats (text, XML, and RDF), and are freely available to query and download through the UniProt website (www.uniprot.org), REST API (www.uniprot.org/help/api), SPARQL endpoint (sparql.uniprot.org/), and FTP site (https://ftp.uniprot.org/pub/databases/uniprot/). Contact alan.bridge@sib.swiss Supplementary information Supplementary Table 1."
                },
                {
                    "title": "BioRED: a rich biomedical relation extraction dataset",
                    "abstract": "Abstract Automated relation extraction (RE) from biomedical literature is critical for many downstream text mining applications in both research and real-world settings. However, most existing benchmarking datasets for biomedical RE only focus on relations of a single type (e.g. protein–protein interactions) at the sentence level, greatly limiting the development of RE systems in biomedicine. In this work, we first review commonly used named entity recognition (NER) and RE datasets. Then, we present a first-of-its-kind biomedical relation extraction dataset (BioRED) with multiple entity types (e.g. gene/protein, disease, chemical) and relation pairs (e.g. gene–disease; chemical–chemical) at the document level, on a set of 600 PubMed abstracts. Furthermore, we label each relation as describing either a novel finding or previously known background knowledge, enabling automated algorithms to differentiate between novel and background information. We assess the utility of BioRED by benchmarking several existing state-of-the-art methods, including Bidirectional Encoder Representations from Transformers (BERT)-based models, on the NER and RE tasks. Our results show that while existing approaches can reach high performance on the NER task (F-score of 89.3%), there is much room for improvement for the RE task, especially when extracting novel relations (F-score of 47.7%). Our experiments also demonstrate that such a rich dataset can successfully facilitate the development of more accurate, efficient and robust RE systems for biomedicine. Availability: The BioRED dataset and annotation guidelines are freely available at https://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/."
                },
                {
                    "title": "Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions",
                    "abstract": "Non-coding RNA structure and function are essential to understanding various biological processes, such as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems in the RNA field. With the rapid growth of sequencing technology, we have accumulated a massive amount of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance. To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without using any labels. Furthermore, we demonstrate RNA-FM’s effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the proposed method improves the RNA structural and functional modelling results significantly and consistently. Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field."
                },
                {
                    "title": "TBGA: a large-scale Gene-Disease Association dataset for Biomedical Relation Extraction",
                    "abstract": null
                },
                {
                    "title": "Training language models to follow instructions with human feedback",
                    "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
                },
                {
                    "title": "Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning",
                    "abstract": "Abstract Effective embedding is actively conducted by applying deep learning to biomolecular information. Obtaining better embeddings enhances the quality of downstream analyses, such as DNA sequence motif detection and protein function prediction. In this study, we adopt a pre-training algorithm for the effective embedding of RNA bases to acquire semantically rich representations and apply this algorithm to two fundamental RNA sequence problems: structural alignment and clustering. By using the pre-training algorithm to embed the four bases of RNA in a position-dependent manner using a large number of RNA sequences from various RNA families, a context-sensitive embedding representation is obtained. As a result, not only base information but also secondary structure and context information of RNA sequences are embedded for each base. We call this ‘informative base embedding’ and use it to achieve accuracies superior to those of existing state-of-the-art methods on RNA structural alignment and RNA family clustering tasks. Furthermore, upon performing RNA sequence alignment by combining this informative base embedding with a simple Needleman–Wunsch alignment algorithm, we succeed in calculating structural alignments with a time complexity of O(n2) instead of the O(n6) time complexity of the naive implementation of Sankoff-style algorithm for input RNA sequence of length n."
                },
                {
                    "title": "Interpreting Language Models Through Knowledge Graph Extraction",
                    "abstract": "Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze\"fill-in-the-blank\"statements at various stages of RoBERTa's early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models' behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance."
                },
                {
                    "title": "HumanNet v3: an improved database of human gene networks for disease research",
                    "abstract": "Abstract Network medicine has proven useful for dissecting genetic organization of complex human diseases. We have previously published HumanNet, an integrated network of human genes for disease studies. Since the release of the last version of HumanNet, many large-scale protein–protein interaction datasets have accumulated in public depositories. Additionally, the numbers of research papers and functional annotations for gene–phenotype associations have increased significantly. Therefore, updating HumanNet is a timely task for further improvement of network-based research into diseases. Here, we present HumanNet v3 (https://www.inetbio.org/humannet/, covering 99.8% of human protein coding genes) constructed by means of the expanded data with improved network inference algorithms. HumanNet v3 supports a three-tier model: HumanNet-PI (a protein–protein physical interaction network), HumanNet-FN (a functional gene network), and HumanNet-XC (a functional network extended by co-citation). Users can select a suitable tier of HumanNet for their study purpose. We showed that on disease gene predictions, HumanNet v3 outperforms both the previous HumanNet version and other integrated human gene networks. Furthermore, we demonstrated that HumanNet provides a feasible approach for selecting host genes likely to be associated with COVID-19."
                },
                {
                    "title": "Improved prediction of protein-protein interactions using AlphaFold2",
                    "abstract": null
                },
                {
                    "title": "Biologically informed deep neural network for prostate cancer discovery",
                    "abstract": null
                },
                {
                    "title": "LoRA: Low-Rank Adaptation of Large Language Models",
                    "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
                },
                {
                    "title": "HUMANNET—A Two-Tiered Deep Neural Network Architecture for Self-Occluding Humanoid Pose Reconstruction",
                    "abstract": "Majority of current research focuses on a single static object reconstruction from a given pointcloud. However, the existing approaches are not applicable to real world applications such as dynamic and morphing scene reconstruction. To solve this, we propose a novel two-tiered deep neural network architecture, which is capable of reconstructing self-obstructed human-like morphing shapes from a depth frame in conjunction with cameras intrinsic parameters. The tests were performed using on custom dataset generated using a combination of AMASS and MoVi datasets. The proposed network achieved Jaccards’ Index of 0.7907 for the first tier, which is used to extract region of interest from the point cloud. The second tier of the network has achieved Earth Mover’s distance of 0.0256 and Chamfer distance of 0.276, indicating good experimental results. Further, subjective reconstruction results inspection shows strong predictive capabilities of the network, with the solution being able to reconstruct limb positions from very few object details."
                },
                {
                    "title": "SciFive: a text-to-text transformer model for biomedical literature",
                    "abstract": "In this report, we introduce SciFive, a domain-specific T5 model that has been pre-trained on large biomedical corpora. Our model outperforms the current SOTA methods (i.e. BERT, BioBERT, Base T5) on tasks in named entity relation, relation extraction, natural language inference, and question-answering. We show that text-generation methods have significant potential in a broad array of biomedical NLP tasks, particularly those requiring longer, more complex outputs. Our results support the exploration of more difficult text generation tasks and the development of new methods in this area"
                },
                {
                    "title": "Effective gene expression prediction from sequence by integrating long-range interactions",
                    "abstract": null
                },
                {
                    "title": "Learning Transferable Visual Models From Natural Language Supervision",
                    "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
                },
                {
                    "title": "Making Pre-trained Language Models Better Few-shot Learners",
                    "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning."
                },
                {
                    "title": "The complex relationship between genotype, pathology and phenotype in familial dementia",
                    "abstract": null
                },
                {
                    "title": "Recent Advances in Network-based Methods for Disease Gene Prediction",
                    "abstract": "Disease-gene association through genome-wide association study (GWAS) is an arduous task for researchers. Investigating single nucleotide polymorphisms that correlate with specific diseases needs statistical analysis of associations. Considering the huge number of possible mutations, in addition to its high cost, another important drawback of GWAS analysis is the large number of false positives. Thus, researchers search for more evidence to cross-check their results through different sources. To provide the researchers with alternative and complementary low-cost disease-gene association evidence, computational approaches come into play. Since molecular networks are able to capture complex interplay among molecules in diseases, they become one of the most extensively used data for disease-gene association prediction. In this survey, we aim to provide a comprehensive and up-to-date review of network-based methods for disease gene prediction. We also conduct an empirical analysis on 14 state-of-the-art methods. To summarize, we first elucidate the task definition for disease gene prediction. Secondly, we categorize existing network-based efforts into network diffusion methods, traditional machine learning methods with handcrafted graph features and graph representation learning methods. Thirdly, an empirical analysis is conducted to evaluate the performance of the selected methods across seven diseases. We also provide distinguishing findings about the discussed methods based on our empirical analysis. Finally, we highlight potential research directions for future studies on disease gene prediction."
                },
                {
                    "title": "Language Models are Few-Shot Learners",
                    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                {
                    "title": "The DisGeNET knowledge platform for disease genomics: 2019 update",
                    "abstract": "Abstract One of the most pressing challenges in genomic medicine is to understand the role played by genetic variation in health and disease. Thanks to the exploration of genomic variants at large scale, hundreds of thousands of disease-associated loci have been uncovered. However, the identification of variants of clinical relevance is a significant challenge that requires comprehensive interrogation of previous knowledge and linkage to new experimental results. To assist in this complex task, we created DisGeNET (http://www.disgenet.org/), a knowledge management platform integrating and standardizing data about disease associated genes and variants from multiple sources, including the scientific literature. DisGeNET covers the full spectrum of human diseases as well as normal and abnormal traits. The current release covers more than 24 000 diseases and traits, 17 000 genes and 117 000 genomic variants. The latest developments of DisGeNET include new sources of data, novel data attributes and prioritization metrics, a redesigned web interface and recently launched APIs. Thanks to the data standardization, the combination of expert curated information with data automatically mined from the scientific literature, and a suite of tools for accessing its publicly available data, DisGeNET is an interoperable resource supporting a variety of applications in genomic medicine and drug R&D."
                },
                {
                    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                {
                    "title": "A global overview of pleiotropy and genetic architecture in complex traits",
                    "abstract": null
                },
                {
                    "title": "Exploring genetic interaction manifolds constructed from rich single-cell phenotypes",
                    "abstract": "Manifold destiny Mapping of genetic interactions (GIs) is usually based on cell fitness as the phenotypic readout, which obscures the mechanistic origin of interactions. Norman et al. developed a framework for mapping and understanding GIs. This approach leverages high-dimensional single-cell RNA sequencing data gathered from CRISPR-mediated, pooled perturbation screens. Diverse transcriptomic phenotypes construct a “manifold” representing all possible states of the cell. Each perturbation and GI projects the cell state to a particular position on this manifold, enabling unbiased ordering of genes in pathways and systematic classifications of GIs. Science, this issue p. 786 Rich phenotyping with single-cell RNA sequencing reveals principles and mechanisms of genetic interactions in mammalian cells. How cellular and organismal complexity emerges from combinatorial expression of genes is a central question in biology. High-content phenotyping approaches such as Perturb-seq (single-cell RNA-sequencing pooled CRISPR screens) present an opportunity for exploring such genetic interactions (GIs) at scale. Here, we present an analytical framework for interpreting high-dimensional landscapes of cell states (manifolds) constructed from transcriptional phenotypes. We applied this approach to Perturb-seq profiling of strong GIs mined from a growth-based, gain-of-function GI map. Exploration of this manifold enabled ordering of regulatory pathways, principled classification of GIs (e.g., identifying suppressors), and mechanistic elucidation of synergistic interactions, including an unexpected synergy between CBL and CNN1 driving erythroid differentiation. Finally, we applied recommender system machine learning to predict interactions, facilitating exploration of vastly larger GI manifolds."
                },
                {
                    "title": "Publicly Available Clinical BERT Embeddings",
                    "abstract": "Contextual word embedding models such as ELMo and BERT have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on 3/5 clinical NLP tasks, establishing a new state-of-the-art on the MedNLI dataset. We find that these domain-specific models are not as performant on 2 clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text."
                },
                {
                    "title": "SciBERT: A Pretrained Language Model for Scientific Text",
                    "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/."
                },
                {
                    "title": "Genome-wide analysis of insomnia in 1,331,010 individuals identifies new risk loci and functional pathways",
                    "abstract": null
                },
                {
                    "title": "Parameter-Efficient Transfer Learning for NLP",
                    "abstract": "Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."
                },
                {
                    "title": "HumanNet v2: human gene networks for disease research",
                    "abstract": "Abstract Human gene networks have proven useful in many aspects of disease research, with numerous network-based strategies developed for generating hypotheses about gene-disease-drug associations. The ability to predict and organize genes most relevant to a specific disease has proven especially important. We previously developed a human functional gene network, HumanNet, by integrating diverse types of omics data using Bayesian statistics framework and demonstrated its ability to retrieve disease genes. Here, we present HumanNet v2 (http://www.inetbio.org/humannet), a database of human gene networks, which was updated by incorporating new data types, extending data sources and improving network inference algorithms. HumanNet now comprises a hierarchy of human gene networks, allowing for more flexible incorporation of network information into studies. HumanNet performs well in ranking disease-linked gene sets with minimal literature-dependent biases. We observe that incorporating model organisms’ protein–protein interactions does not markedly improve disease gene predictions, suggesting that many of the disease gene associations are now captured directly in human-derived datasets. With an improved interactive user interface for disease network analysis, we expect HumanNet will be a useful resource for network medicine."
                },
                {
                    "title": "Using deep learning to model the hierarchical structure and function of a cell",
                    "abstract": null
                },
                {
                    "title": "NetSig: network-based discovery from cancer genomes",
                    "abstract": null
                },
                {
                    "title": "Neuropathology of genetic synucleinopathies with parkinsonism: Review of the literature",
                    "abstract": "Clinical–pathological studies remain the gold‐standard for the diagnosis of Parkinson's disease (PD). However, mounting data from genetic PD autopsies challenge the diagnosis of PD based on Lewy body pathology. Most of the confirmed genetic risks for PD show heterogenous neuropathology, even within kindreds, which may or may not include Lewy body pathology. We review the literature of genetic PD autopsies from cases with molecularly confirmed PD or parkinsonism and summarize main findings on SNCA (n = 25), Parkin (n = 20, 17 bi‐allelic and 3 heterozygotes), PINK1 (n = 5, 1 bi‐allelic and 4 heterozygotes), DJ‐1 (n = 1), LRRK2 (n = 55), GBA (n = 10 Gaucher disease patients with parkinsonism), DNAJC13, GCH1, ATP13A2, PLA2G6 (n = 8 patients, 2 with PD), MPAN (n = 2), FBXO7, RAB39B, and ATXN2 (SCA2), as well as on 22q deletion syndrome (n = 3). Findings from autopsies of heterozygous mutation carriers of genes that are traditionally considered recessively inherited are also discussed. Lewy bodies may be present in syndromes clinically distinctive from PD (eg, MPAN‐related neurodegeneration) and absent in patients with clinical PD syndrome (eg, LRRK2‐PD or Parkin‐PD). Therefore, the authors can conclude that the presence of Lewy bodies are not specific to the diagnosis of PD and that PD can be diagnosed even in the absence of Lewy body pathology. Interventions that reduce alpha‐synuclein load may be more justified in SNCA‐PD or GBA‐PD than in other genetic forms of PD. The number of reported genetic PD autopsies remains small, and there are limited genotype‐clinical‐pathological‐phenotype studies. Therefore, larger series of autopsies from genetic PD patients are required. © 2017 International Parkinson and Movement Disorder Society"
                },
                {
                    "title": "eDGAR: a database of Disease-Gene Associations with annotated Relationships among genes",
                    "abstract": null
                },
                {
                    "title": "10 Years of GWAS Discovery: Biology, Function, and Translation.",
                    "abstract": null
                },
                {
                    "title": "Attention is All you Need",
                    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                {
                    "title": "GWAB: a web server for the network-based boosting of human genome-wide association data",
                    "abstract": "Abstract During the last decade, genome-wide association studies (GWAS) have represented a major approach to dissect complex human genetic diseases. Due in part to limited statistical power, most studies identify only small numbers of candidate genes that pass the conventional significance thresholds (e.g. P ≤ 5 × 10−8). This limitation can be partly overcome by increasing the sample size, but this comes at a higher cost. Alternatively, weak association signals can be boosted by incorporating independent data. Previously, we demonstrated the feasibility of boosting GWAS disease associations using gene networks. Here, we present a web server, GWAB (www.inetbio.org/gwab), for the network-based boosting of human GWAS data. Using GWAS summary statistics (P-values) for SNPs along with reference genes for a disease of interest, GWAB reprioritizes candidate disease genes by integrating the GWAS and network data. We found that GWAB could more effectively retrieve disease-associated reference genes than GWAS could alone. As an example, we describe GWAB-boosted candidate genes for coronary artery disease and supporting data in the literature. These results highlight the inherent value in sub-threshold GWAS associations, which are often not publicly released. GWAB offers a feasible general approach to boost such associations for human disease genetics."
                },
                {
                    "title": "The new NHGRI-EBI Catalog of published genome-wide association studies (GWAS Catalog)",
                    "abstract": "The NHGRI-EBI GWAS Catalog has provided data from published genome-wide association studies since 2008. In 2015, the database was redesigned and relocated to EMBL-EBI. The new infrastructure includes a new graphical user interface (www.ebi.ac.uk/gwas/), ontology supported search functionality and an improved curation interface. These developments have improved the data release frequency by increasing automation of curation and providing scaling improvements. The range of available Catalog data has also been extended with structured ancestry and recruitment information added for all studies. The infrastructure improvements also support scaling for larger arrays, exome and sequencing studies, allowing the Catalog to adapt to the needs of evolving study design, genotyping technologies and user needs in the future."
                },
                {
                    "title": "MUFFINN: cancer gene discovery via network analysis of somatic mutation data",
                    "abstract": null
                },
                {
                    "title": "Translation of Genotype to Phenotype by a Hierarchy of Cell Subsystems",
                    "abstract": null
                },
                {
                    "title": "Genetic studies of quantitative MCI and AD phenotypes in ADNI: Progress, opportunities, and plans",
                    "abstract": null
                },
                {
                    "title": "Understanding multicellular function and disease with human tissue-specific networks",
                    "abstract": null
                },
                {
                    "title": "OMIM.org: Online Mendelian Inheritance in Man (OMIM®), an online catalog of human genes and genetic disorders",
                    "abstract": "Online Mendelian Inheritance in Man, OMIM®, is a comprehensive, authoritative and timely research resource of curated descriptions of human genes and phenotypes and the relationships between them. The new official website for OMIM, OMIM.org (http://omim.org), was launched in January 2011. OMIM is based on the published peer-reviewed biomedical literature and is used by overlapping and diverse communities of clinicians, molecular biologists and genome scientists, as well as by students and teachers of these disciplines. Genes and phenotypes are described in separate entries and are given unique, stable six-digit identifiers (MIM numbers). OMIM entries have a structured free-text format that provides the flexibility necessary to describe the complex and nuanced relationships between genes and genetic phenotypes in an efficient manner. OMIM also has a derivative table of genes and genetic phenotypes, the Morbid Map. OMIM.org has enhanced search capabilities such as genome coordinate searching and thesaurus-enhanced search term options. Phenotypic series have been created to facilitate viewing genetic heterogeneity of phenotypes. Clinical synopsis features are enhanced with UMLS, Human Phenotype Ontology and Elements of Morphology terms and image links. All OMIM data are available for FTP download and through an API. MIMmatch is a novel outreach feature to disseminate updates and encourage collaboration."
                },
                {
                    "title": "FASTKD2 is associated with memory and hippocampal structure in older adults",
                    "abstract": null
                },
                {
                    "title": "Autosomal-dominant Alzheimer's disease: a review and proposal for the prevention of Alzheimer's disease",
                    "abstract": null
                },
                {
                    "title": "Imaging genetics—days of future past",
                    "abstract": null
                },
                {
                    "title": "Overview of the protein-protein interaction annotation extraction task of BioCreative II",
                    "abstract": null
                },
                {
                    "title": "The NCBI dbGaP database of genotypes and phenotypes",
                    "abstract": null
                },
                {
                    "title": "BioInfer: a corpus for information extraction in the biomedical domain",
                    "abstract": null
                },
                {
                    "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                    "abstract": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST."
                },
                {
                    "title": "Positional effects of presenilin-1 mutations on tau phosphorylation in cortical plaques",
                    "abstract": null
                },
                {
                    "title": "From gene networks to gene function.",
                    "abstract": "We propose a novel method to identify functionally related genes based on comparisons of neighborhoods in gene networks. This method does not rely on gene sequence or protein structure homologies, and it can be applied to any organism and a wide variety of experimental data sets. The character of the predicted gene relationships depends on the underlying networks;they concern biological processes rather than the molecular function. We used the method to analyze gene networks derived from genome-wide chromatin immunoprecipitation experiments, a large-scale gene deletion study, and from the genomic positions of consensus binding sites for transcription factors of the yeast Saccharomyces cerevisiae. We identified 816 functional relationships between 159 genes and show that these relationships correspond to protein-protein interactions, co-occurrence in the same protein complexes, and/or co-occurrence in abstracts of scientific articles. Our results suggest functions for seven previously uncharacterized yeast genes: KIN3 and YMR269W may be involved in biological processes related to cell growth and/or maintenance, whereas IES6, YEL008W, YEL033W, YHL029C, YMR010W, and YMR031W-A are likely to have metabolic functions."
                },
                {
                    "title": "GENIA corpus - a semantically annotated corpus for bio-textmining",
                    "abstract": "MOTIVATION\nNatural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack of an extensively annotated corpus of this literature, however, causes a major bottleneck for applying NLP techniques. GENIA corpus is being developed to provide reference materials to let NLP techniques work for bio-textmining.\n\n\nRESULTS\nGENIA corpus version 3.0 consisting of 2000 MEDLINE abstracts has been released with more than 400,000 words and almost 100,000 annotations for biological terms."
                }
            ]
        },
        "author_data": {
            "f8e16eb4-54ad-4c4e-9bd4-7e8932032bcd": {
                "pk": "f8e16eb4-54ad-4c4e-9bd4-7e8932032bcd",
                "project_name": null,
                "name": "Yanjun Lyu",
                "bio": "I am a researcher dedicated to understanding the complexities of brain function and structure through advanced machine learning techniques. My work primarily focuses on the intersection of cognitive neuroscience and artificial intelligence, particularly in the context of mild cognitive impairment (MCI) and its progression to Alzheimer's disease (AD). I have developed innovative frameworks, such as the Twin-Transformers, to simultaneously infer common and individual functional brain networks, revealing critical insights into brain organizational architecture.\n\nMy research also explores the unique roles of gyri and sulci in brain function, utilizing a novel Twin-Transformer approach to elucidate their interactions and contributions to cognitive processes. I am particularly interested in leveraging principles from biological neural networks to enhance artificial neural network architectures, as demonstrated in my Core-Periphery Vision Transformer (CP-ViT) model, which integrates biological insights to improve performance and interpretability.\n\nAdditionally, I am committed to enhancing model interpretability in deep learning by proactively instilling knowledge into the training process, rather than relying solely on post-hoc analysis. My work aims to bridge the gap between neuroscience and AI, providing a deeper understanding of brain mechanisms while advancing the field of machine learning. Through my research, I strive to contribute to the development of more efficient, reliable, and explainable AI systems that can ultimately benefit our understanding of human cognition and neurological disorders.",
                "collaborators": [
                    "Lu Zhang",
                    "Xiaowei Yu",
                    "Dajiang Zhu",
                    "Lin Zhao",
                    "Tianming Liu",
                    "Haixing Dai",
                    "Zihao Wu",
                    "Li Wang",
                    "David Liu",
                    "Zhengwang Wu"
                ],
                "pub_titles": [
                    "Representative Functional Connectivity Learning for Multiple Clinical groups in Alzheimer's Disease",
                    "Disentangling Spatial-Temporal Functional Brain Networks via Twin-Transformers",
                    "Representing Brain Anatomical Regularity and Variability by Few-Shot Embedding",
                    "Gyri vs. Sulci: Disentangling Brain Core-Periphery Functional Networks via Twin-Transformer",
                    "Core-Periphery Principle Guided Redesign of Self-Attention in Transformers",
                    "Hierarchical Semantic Tree Concept Whitening for Interpretable Image Classification"
                ],
                "pub_abstracts": [
                    "Mild cognitive impairment (MCI) is a high-risk dementia condition which progresses to probable Alzheimer's disease (AD) at approximately 10% to 15% per year. Characterization of group-level differences between two subtypes of MCI - stable MCI (sMCI) and progressive MCI (pMCI) is the key step to understand the mechanisms of MCI progression and enable possible delay of transition from MCI to AD. Functional connectivity (FC) is considered as a promising way to study MCI progression since which may show alterations even in preclinical stages and provide substrates for AD progression. However, the representative FC patterns during AD development for different clinical groups, especially for sMCI and pMCI, have been understudied. In this work, we integrated autoencoder and multi-class classification into a single deep model and successfully learned a set of clinical group related feature vectors. Specifically, we trained two non-linear mappings which realized the mutual transformations between original FC space and the feature space. By mapping the learned clinical group related feature vectors to the original FC space, representative FCs were constructed for each group. Moreover, based on these feature vectors, our model achieves a high classification accuracy - 68% for multi-class classification (NC vs SMC vs sMCI vs pMCI vs AD).",
                    "How to identify and characterize functional brain networks (BN) is fundamental to gain system-level insights into the mechanisms of brain organizational architecture. Current functional magnetic resonance (fMRI) analysis highly relies on prior knowledge of specific patterns in either spatial (e.g., resting-state network) or temporal (e.g., task stimulus) domain. In addition, most approaches aim to find group-wise common functional networks, individual-specific functional networks have been rarely studied. In this work, we propose a novel Twin-Transformers framework to simultaneously infer common and individual functional networks in both spatial and temporal space, in a self-supervised manner. The first transformer takes space-divided information as input and generates spatial features, while the second transformer takes time-related information as input and outputs temporal features. The spatial and temporal features are further separated into common and individual ones via interactions (weights sharing) and constraints between the two transformers. We applied our TwinTransformers to Human Connectome Project (HCP) motor task-fMRI dataset and identified multiple common brain networks, including both task-related and resting-state networks (e.g., default mode network). Interestingly, we also successfully recovered a set of individual-specific networks that are not related to task stimulus and only exist at the individual level.",
                    "Effective representation of brain anatomical architecture is fundamental in understanding brain regularity and variability. Despite numerous efforts, it is still difficult to infer reliable anatomical correspondence at finer scale, given the tremendous individual variability in cortical folding patterns. It is even more challenging to disentangle common and individual patterns when comparing brains at different neuro-developmental stages. In this work, we developed a novel learning-based few-shot embedding framework to encode the cortical folding patterns into a latent space represented by a group of anatomically meaningful embedding vectors. Specifically, we adopted 3-hinge (3HG) network as the substrate and designed an autoencoder-based embedding framework to learn a common embedding vector for each 3HG's multi-hop feature: each 3HG can be represented as a combination of these feature embeddings via a set of individual specific coefficients to characterize individualized anatomical information. That is, the regularity of folding patterns is encoded into the embeddings, while the individual variations are preserved by the multi=hop combination coefficients. To effectively learn the embeddings for the population with very limited samples, few-shot learning was adopted. We applied our method on adult HCP and pediatric datasets with 1,000+ brains (from 34 gestational weeks to young adult). Our experimental results show that: 1) the learned embedding vectors can quantitatively encode the commonality and individuality of cortical folding patterns; 2) with the embeddings we can robustly infer the complicated many-to-many anatomical correspondences among different brains and 3) our model can be successfully transferred to new populations with very limited training samples.",
                    "The human cerebral cortex is highly convoluted into convex gyri and concave sulci. It has been demonstrated that gyri and sulci are significantly different in their anatomy, connectivity, and function, besides exhibiting opposite shape patterns, long-distance axonal fibers connected to gyri are much denser than those connected to sulci, and neural signals on gyri are more complex in low-frequency while sulci are more complex in high-frequency. Although accumulating evidence shows significant differences between gyri and sulci, their primary roles in brain function have not been elucidated yet. To solve this fundamental problem, we design a novel Twin-Transformer framework to unveil the unique functional roles of gyri and sulci as well as their relationship in the whole brain function. Our Twin-Transformer framework adopts two structure-identical (twin) Transformers to disentangle spatial-temporal patterns of gyri and sulci, one focuses on the information of gyri and the other is on sulci. The Gyro-Sulcal interactions, along with the tremendous but widely existing variability across subjects, are characterized in the loss design. We validated our Twin-Transformer on the HCP task-fMRI dataset, for the first time, to elucidate the different roles of gyri and sulci in brain function. Our results suggest that gyri and sulci could work together in a core-periphery network manner, that is, gyri could serve as core networks for information gathering and distributing, while sulci could serve as periphery networks for specific local information processing. These findings have shed new light on our fundamental understanding of the brain's basic structural and functional mechanisms.",
                    "Designing more efficient, reliable, and explainable neural network architectures is critical to studies that are based on artificial intelligence (AI) techniques. Previous studies, by post-hoc analysis, have found that the best-performing ANNs surprisingly resemble biological neural networks (BNN), which indicates that ANNs and BNNs may share some common principles to achieve optimal performance in either machine learning or cognitive/behavior tasks. Inspired by this phenomenon, we proactively instill organizational principles of BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP) organization, which is widely found in human brain networks, to guide the information communication mechanism in the self-attention of vision transformer (ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention operation between nodes is defined by a sparse graph with a Core-Periphery structure (CP graph), where the core nodes are redesigned and reorganized to play an integrative role and serve as a center for other periphery nodes to exchange information. We evaluated the proposed CP-ViT on multiple public datasets, including medical image datasets (INbreast) and natural image datasets. Interestingly, by incorporating the BNN-derived principle (CP structure) into the redesign of ViT, our CP-ViT outperforms other state-of-the-art ANNs. In general, our work advances the state of the art in three aspects: 1) This work provides novel insights for brain-inspired AI: we can utilize the principles found in BNNs to guide and improve our ANN architecture design; 2) We show that there exist sweet spots of CP graphs that lead to CP-ViTs with significantly improved performance; and 3) The core nodes in CP-ViT correspond to task-related meaningful and important image patches, which can significantly enhance the interpretability of the trained deep model.",
                    "With the popularity of deep neural networks (DNNs), model interpretability is becoming a critical concern. Many approaches have been developed to tackle the problem through post-hoc analysis, such as explaining how predictions are made or understanding the meaning of neurons in middle layers. Nevertheless, these methods can only discover the patterns or rules that naturally exist in models. In this work, rather than relying on post-hoc schemes, we proactively instill knowledge to alter the representation of human-understandable concepts in hidden layers. Specifically, we use a hierarchical tree of semantic concepts to store the knowledge, which is leveraged to regularize the representations of image data instances while training deep models. The axes of the latent space are aligned with the semantic concepts, where the hierarchical relations between concepts are also preserved. Experiments on real-world image datasets show that our method improves model interpretability, showing better disentanglement of semantic concepts, without negatively affecting model classification performance."
                ],
                "domain": [
                    "Neuroimaging",
                    "Deep Learning",
                    "Machine Learning",
                    "Brain Networks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "4da654e6-1447-461a-a89e-05782f8bbc4a": {
                "pk": "4da654e6-1447-461a-a89e-05782f8bbc4a",
                "project_name": null,
                "name": "Zihao Wu",
                "bio": "I am a researcher with a diverse background in theoretical physics, machine learning, and robotics, focusing on the intersection of these fields to address complex challenges. My recent work explores open string pair production between D3 branes, drawing parallels to Schwinger pair production, and I have developed methods to enhance Feynman integral symmetry relations, which are crucial for high-precision calculations in quantum field theory.\n\nIn addition to my theoretical pursuits, I am passionate about applying advanced technologies to real-world problems. I have contributed to the field of robotic crop phenotyping by creating a novel Digital-Twin MARS-CycleGAN model that significantly improves crop object detection in challenging environments. This work aims to enhance agricultural productivity and sustainability in the face of climate change.\n\nI am also deeply engaged in the implications of Generative Artificial Intelligence (GenAI) in education. My research analyzes how universities are adapting to the rise of GenAI, providing insights and recommendations for educators to integrate these technologies effectively while maintaining academic integrity.\n\nFurthermore, I have investigated the dynamics of continual and multi-task learning, demonstrating that continual learning can outperform multi-task learning in certain scenarios, particularly when faced with adversarial tasks. My work aims to push the boundaries of machine learning methodologies, contributing to both theoretical understanding and practical applications across various domains.",
                "collaborators": [
                    "Yang Zhang",
                    "Ming-Ming Long",
                    "Fabian Lange",
                    "Johann Usovitsch",
                    "David Liu",
                    "Zhengkun Li",
                    "Changying Li",
                    "Hui Wang",
                    "Anh Dang",
                    "Son Mac"
                ],
                "pub_titles": [
                    "On Schwinger pair production between D3 branes",
                    "A new method for finding more symmetry relations of Feynman integrals",
                    "Evaluating master integrals in non-factorizable corrections to $t$-channel single-top production at NNLO QCD",
                    "Towards the next Kira release",
                    "DT/MARS-CycleGAN: Improved Object Detection for MARS Phenotyping Robot",
                    "Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines",
                    "Is Multi-Task Learning an Upper Bound for Continual Learning?"
                ],
                "pub_abstracts": [
                    "We study the open string pair production between two D3 branes, which will give rise to similar effect as Schwinger pair production for observers on one of the D3 branes. The D3 branes are placed parallel at a distance, and they are carrying world-volume electromagnetic fluxes that takes general form. We derive the pair production rate by computing the interaction amplitude between the D3 branes. We discussed how to maximize the pair production rate in this general case. We also mentioned that the general result can be used to describe other system such as D3-D1, where the pair production is ultra large compared to original Schwinger pair production, making it hopeful to observe pair production in experiments.",
                    "We introduce a new method for deriving Feynman integral symmetry relation. By solving the ansatz of momentum transformation in the field of rational functions rather than constants, the method can sometimes find more symmetry relations, comparing with some state-of-art software. The new method may help to further decrease the number of master integrals in an integral family. Well-chosen gauge conditions are implemented in this method, for the efficient symmetry searching.",
                    "We studied the two-loop non-factorizable Feynman diagrams for the $t$-channel single-top production process in quantum chromodynamics. We present a systematic computation of master integrals of the two-loop Feynman diagrams with one internal massive propagator in which a complete uniform transcendental basis can be built. The master integrals are derived by means of canonical differential equations and uniform transcendental integrals. The results are expressed in the form of Goncharov polylogarithm functions, whose variables are the scalar products of external momenta, as well as the masses of the top quark and the $W$ boson. We also gave a discussion on the diagrams with potential elliptic sectors.",
                    "The reduction of Feynman integrals to a basis of master integrals plays a crucial role for many high-precision calculations and Kira is one of the leading tools for this task. In these proceedings we discuss some of the new features and improvements currently being developed for the next release.",
                    "Robotic crop phenotyping has emerged as a key technology to assess crops' morphological and physiological traits at scale. These phenotypical measurements are essential for developing new crop varieties with the aim of increasing productivity and dealing with environmental challenges such as climate change. However, developing and deploying crop phenotyping robots face many challenges such as complex and variable crop shapes that complicate robotic object detection, dynamic and unstructured environments that baffle robotic control, and real-time computing and managing big data that challenge robotic hardware/software. This work specifically tackles the first challenge by proposing a novel Digital-Twin(DT)MARS-CycleGAN model for image augmentation to improve our Modular Agricultural Robotic System (MARS)'s crop object detection from complex and variable backgrounds. Our core idea is that in addition to the cycle consistency losses in the CycleGAN model, we designed and enforced a new DT-MARS loss in the deep learning model to penalize the inconsistency between real crop images captured by MARS and synthesized images sensed by DT MARS. Therefore, the generated synthesized crop images closely mimic real images in terms of realism, and they are employed to fine-tune object detectors such as YOLOv8. Extensive experiments demonstrated that our new DT/MARS-CycleGAN framework significantly boosts our MARS' crop object/row detector's performance, contributing to the field of robotic crop phenotyping.",
                    "The advancements in Generative Artificial Intelligence (GenAI) provide opportunities to enrich educational experiences, but also raise concerns about academic integrity. Many educators have expressed anxiety and hesitation in integrating GenAI in their teaching practices, and are in needs of recommendations and guidance from their institutions that can support them to incorporate GenAI in their classrooms effectively. In order to respond to higher educators' needs, this study aims to explore how universities and educators respond and adapt to the development of GenAI in their academic contexts by analyzing academic policies and guidelines established by top-ranked U.S. universities regarding the use of GenAI, especially ChatGPT. Data sources include academic policies, statements, guidelines, and relevant resources provided by the top 100 universities in the U.S. Results show that the majority of these universities adopt an open but cautious approach towards GenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most universities actively respond and provide diverse types of resources, such as syllabus templates, workshops, shared articles, and one-on-one consultations focusing on a range of topics: general technical introduction, ethical concerns, pedagogical applications, preventive strategies, data privacy, limitations, and detective tools. The findings provide four practical pedagogical implications for educators in teaching practices: accept its presence, align its use with learning objectives, evolve curriculum to prevent misuse, and adopt multifaceted evaluation strategies rather than relying on AI detectors. Two recommendations are suggested for educators in policy making: establish discipline-specific policies and guidelines, and manage sensitive information carefully.",
                    "Continual and multi-task learning are common machine learning approaches to learning from multiple tasks. The existing works in the literature often assume multi-task learning as a sensible performance upper bound for various continual learning algorithms. While this assumption is empirically verified for different continual learning benchmarks, it is not rigorously justified. Moreover, it is imaginable that when learning from multiple tasks, a small subset of these tasks could behave as adversarial tasks reducing the overall learning performance in a multi-task setting. In contrast, continual learning approaches can avoid the performance drop caused by such adversarial tasks to preserve their performance on the rest of the tasks, leading to better performance than a multi-task learner. This paper proposes a novel continual self-supervised learning setting, where each task corresponds to learning an invariant representation for a specific class of data augmentations. In this setting, we show that continual learning often beats multi-task learning on various benchmark datasets, including MNIST, CIFAR-10, and CIFAR-100."
                ],
                "domain": [
                    "Quantum Field Theory",
                    "Machine Learning",
                    "Robotics",
                    "Generative AI"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "04876890-1f97-4cef-8509-7cde8124b93c": {
                "pk": "04876890-1f97-4cef-8509-7cde8124b93c",
                "project_name": null,
                "name": "Lu Zhang",
                "bio": "I am a researcher with a strong focus on mathematical analysis, particularly in the realms of random matrix theory, potential theory, and numerical methods for partial differential equations. My recent work has delved into the deformed complex Ginibre ensemble, where I explored the bulk statistics of eigenvalues and their implications for local eigenvalue distributions. This builds on my previous studies of edge statistics in random matrices, showcasing my commitment to understanding the intricate behaviors of these mathematical structures.\n\nIn addition to my work in random matrices, I have made significant contributions to potential theory by establishing the Hadamard variational formula for Riesz capacity, which has led to important symmetry results in overdetermined problems. My research also extends to computational methods, where I have analyzed the conjugate gradient algorithm's applications in Bayesian analysis, particularly in sparse regression and spatial analysis.\n\nI am passionate about developing efficient numerical schemes, as demonstrated in my work on the energy-based discontinuous Galerkin method for wave equations and the ultra-weak local discontinuous Galerkin scheme for nonlinear biharmonic Schrödinger equations. These contributions not only enhance computational efficiency but also ensure stability and optimal convergence.\n\nMoreover, I have ventured into high-dimensional graphical models, proposing the StarTrek filter for selecting hub nodes in large-scale networks. This work addresses the challenges of variable selection in complex networks and provides robust statistical methodologies to control false discovery rates.\n\nOverall, my research is characterized by a blend of theoretical rigor and practical application, aiming to advance our understanding of complex mathematical phenomena while providing tools for real-world problems.",
                "collaborators": [
                    "Junwei Lu"
                ],
                "pub_titles": [
                    "Bulk universality for deformed GinUEs",
                    "The Hadamard variational formula for Riesz capacity and its applications",
                    "Applications of Conjugate Gradient in Bayesian computation",
                    "Mean eigenvector self-overlap in deformed complex Ginibre ensemble",
                    "A local energy-based discontinuous Galerkin method for fourth order semilinear wave equations",
                    "A discontinuous Galerkin method for nonlinear biharmonic Schrödinger equations",
                    "StarTrek: Combinatorial Variable Selection with False Discovery Rate Control"
                ],
                "pub_abstracts": [
                    "For the deformed complex Ginibre ensemble with a mean normal matrix, under certain assumptions on the mean matrix we prove that the same bulk statistics holds as in the complex Ginibre matrix bulk. This is the continuation of the previous joint papers ``Critical edge statistics for deformed GinUEs Preprint arXiv: 2311.13227v1'' and ``Repeated erfc statistics for deformed GinUEs Preprint arXiv: 2402.14362'', which deal with local eigenvalue statistics at the edge.",
                    "In this paper, our focus lies on a fundamental geometric invariant known as Riesz capacity, which holds an essential position in potential theory. We establish the Hadamard variational formula for Riesz capacity of convex bodies. As a meaningful application, we derive a Serrin-type symmetry result for an overdetermined problem.",
                    "Conjugate gradient is an efficient algorithm for solving large sparse linear systems. It has been utilized to accelerate the computation in Bayesian analysis for many large-scale problems. This article discusses the applications of conjugate gradient in Bayesian computation, with a focus on sparse regression and spatial analysis. A self-contained introduction of conjugate gradient is provided to facilitate potential applications in a broader range of problems.",
                    "Consider a random matrix of size $N$ as an additive deformation of the complex Ginibre ensemble under a deterministic matrix $X_0$ with a finite rank, independent of $N$. We prove that microscopic statistics for the mean diagonal overlap, near the edge point, are characterized by the iterative erfc integrals, which only depend on the geometric multiplicity of certain eigenvalue of $X_0$. We also investigate the microscopic statistics for the mean diagonal overlap of the outlier eigenvalues. Further we get a phenomenon of the phase transition for the mean diagonal overlap, with respect to the modulus of the eigenvalues of $X_0$.",
                    "This paper generalizes the earlier work on the energy-based discontinuous Galerkin method for second-order wave equations to fourth-order semilinear wave equations. We first rewrite the problem into a system with a second-order spatial derivative, then apply the energy-based discontinuous Galerkin method to the system. The proposed scheme, on the one hand, is more computationally efficient compared with the local discontinuous Galerkin method because of fewer auxiliary variables. On the other hand, it is unconditionally stable without adding any penalty terms, and admits optimal convergence in the $L^2$ norm for both solution and auxiliary variables. In addition, the energy-dissipating or energy-conserving property of the scheme follows from simple, mesh-independent choices of the interelement fluxes. We also present a stability and convergence analysis along with numerical experiments to demonstrate optimal convergence for certain choices of the interelement fluxes.",
                    "This paper proposes and analyzes a fully discrete scheme that discretizes space with an ultra-weak local discontinuous Galerkin scheme and time with the Crank--Nicolson method for the nonlinear biharmonic Schr\\\"odinger equation. We first rewrite the problem into a system with a second-order spatial derivative and then apply the ultra-weak discontinuous Galerkin method to the system. The proposed scheme is more computationally efficient compared with the local discontinuous Galerkin method because of fewer auxiliary variables, and unconditionally stable without any penalty terms; it also preserves the mass and Hamiltonian conservation that are important properties of the nonlinear biharmonic Schr\\\"odinger equation. We also derive optimal L2-error estimates of the semi-discrete scheme that measure both the solution and the auxiliary variable with general nonlinear terms. Several numerical studies demonstrate and support our theoretical findings.",
                    "Variable selection on the large-scale networks has been extensively studied in the literature. While most of the existing methods are limited to the local functionals especially the graph edges, this paper focuses on selecting the discrete hub structures of the networks. Specifically, we propose an inferential method, called StarTrek filter, to select the hub nodes with degrees larger than a certain thresholding level in the high dimensional graphical models and control the false discovery rate (FDR). Discovering hub nodes in the networks is challenging: there is no straightforward statistic for testing the degree of a node due to the combinatorial structures; complicated dependence in the multiple testing problem is hard to characterize and control. In methodology, the StarTrek filter overcomes this by constructing p-values based on the maximum test statistics via the Gaussian multiplier bootstrap. In theory, we show that the StarTrek filter can control the FDR by providing accurate bounds on the approximation errors of the quantile estimation and addressing the dependence structures among the maximal statistics. To this end, we establish novel Cram\\'er-type comparison bounds for the high dimensional Gaussian random vectors. Comparing to the Gaussian comparison bound via the Kolmogorov distance established by \\citet{chernozhukov2014anti}, our Cram\\'er-type comparison bounds establish the relative difference between the distribution functions of two high dimensional Gaussian random vectors. We illustrate the validity of the StarTrek filter in a series of numerical experiments and apply it to the genotype-tissue expression dataset to discover central regulator genes."
                ],
                "domain": [
                    "Random Matrix Theory",
                    "Bayesian Computation",
                    "Numerical Analysis",
                    "Graphical Models"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "2399ade3-0b03-40ff-979d-a8df5303839a": {
                "pk": "2399ade3-0b03-40ff-979d-a8df5303839a",
                "project_name": null,
                "name": "Jing Zhang",
                "bio": "I am a researcher with a strong focus on the intersection of algebraic geometry, combinatorial set theory, and quantum information theory. My recent work has delved into various aspects of algebraic varieties, particularly exploring criteria for affineness and the vanishing of cohomology groups. I have made significant contributions to understanding the structure of quasi-projective varieties and their relationship with affine properties, addressing longstanding conjectures in the field.\n\nIn addition to my work in algebraic geometry, I have investigated combinatorial principles such as the Ramsey property in forcing extensions, providing insights into the behavior of colorings in infinite sets. My research also extends to the realm of continuous-variable quantum states, where I have introduced new classes of multipartite entangled states and explored their entanglement properties and applications in quantum communication.\n\nI am particularly interested in the implications of reflection principles and uniformization properties in set theory, as well as the consistency results related to Rado's Conjecture. My work aims to bridge theoretical concepts with practical applications, contributing to a deeper understanding of both mathematical structures and their implications in quantum mechanics.\n\nThrough my research, I strive to uncover new connections between seemingly disparate areas of mathematics, fostering a comprehensive understanding of the underlying principles that govern them.",
                "collaborators": [],
                "pub_titles": [
                    "Monochromatic Sumset Without the use of large cardinals",
                    "Reflection principles, GCH and the uniformization properties",
                    "Hodge Cohomology Criteria For Affine Varieties",
                    "Cluster States for Continuous-Variable Multipartite Entanglement",
                    "Local complementation rule for continuous-variable four-mode unweighted graph states",
                    "Graphical rule of transforming continuous-variable graph states by local homodyne detection",
                    "Continuous-variable multipartite unlockable bound entangled Gaussian states",
                    "A tail cone version of the Halpern-Läuchli theorem at a large cardinal",
                    "Threefolds with Vanishing Hodge Cohomology",
                    "There Exist Nontrivial Threefolds with Vanishing Hodge Cohomology",
                    "On the $D$-dimension of a certain type of threefolds",
                    "On threefolds without nonconstant regular functions",
                    "Algebraic Stein Varieties",
                    "Affine Algebraic Varieties",
                    "Graphical description of local Gaussian operations for continuous-variable weighted graph states",
                    "Bertini Type Theorems",
                    "A Literature Survey of Cooperative Caching in Content Distribution Networks",
                    "Rado's conjecture and its Baire version",
                    "Some remarks on uncountable rainbow Ramsey theory",
                    "Almost global solutions to two classes of 1-d Hamiltonian Derivative Nonlinear Schrödinger equations"
                ],
                "pub_abstracts": [
                    "We show in this note that in the forcing extension by $Add(\\omega,\\beth_{\\omega})$, the following Ramsey property holds: for any $r\\in \\omega$ and any $f: \\mathbb{R}\\to r$, there exists an infinite $X\\subset \\mathbb{R}$ such that $X+X$ is monochromatic under $f$. We also show the Ramsey statement above is true in $\\mathrm{ZFC}$ when $r=2$. This answers two questions by Komj\\'ath, Leader, Russell, Shelah, Soukup and Vidny\\'anszky.",
                    "Reflection principles (or dually speaking, compactness principles) often give rise to combinatorial guessing principles. Uniformization properties, on the other hand, are examples of anti-guessing principles. We discuss the tension and the compatibility between reflection principles and uniformization properties at the level of the second uncountable cardinal.",
                    "We give several new criteria for a quasi-projective variety to be affine. In particular, we prove that an algebraic manifold $Y$ with dimension $n$ is affine if and only if $H^i(Y, \\Omega^j_Y)=0$ for all $j\\geq 0$, $i>0$ and $\\kappa(D, X)=n$, i.e., there are $n$ algebraically independent nonconstant regular functions on $Y$, where $X$ is the smooth completion of $Y$, $D$ is the effective boundary divisor with support $X-Y$ and $\\Omega^j_Y$ is the sheaf of regular $j$-forms on $Y$. This proves Mohan Kumar's affineness conjecture for algebraic manifolds and gives a partial answer to J.-P. Serre's Steinness question \\cite{36} in algebraic case since the associated analytic space of an affine variety is Stein [15, Chapter VI, Proposition 3.1].",
                    "We introduce a new class of continuous-variable (CV) multipartite entangled states, the CV cluster states, which might be generated from squeezing and kerr-like interaction. The entanglement properties of these states are studied in terms of classical communication and local operations. The quantum teleportation network with cluster states is investigated. The graph states as the general forms of cluster states are presented, which may be used to generate CV Greenberger-Horne-Zeilinger states by simply local measurements and classical communication. A chain for one-dimensional example of cluster states can be readily experimentally produced only with squeezed light and beamsplitters.",
                    "The local complementation rule is applied for continuous-variable (CV) graph states in the paper, which is an elementary graph transformation rule and successive application of which generates the orbit of any graph states. The corresponding local Gaussian transformations of local complementation for four-mode unweighted graph states were found, which do not mirror the form of the local Clifford unitary of qubit exactly. This work is an important step to characterize the local Gaussian equivalence classes of CV graph states.",
                    "Graphical rule, describing that any single-mode homodyne detection turns a given continuous-variable (CV) graph state into a new one, is presented. Employing two simple graphical rules: local complement operation and vertex deletion (single quadrature-amplitude $\\hat{x}$ measurement), the graphical rule for any single-mode quadrature component measurement can be obtained. The shape of CV weighted graph state may be designed and constructed easily from a given larger graph state by applying this graphical rule.",
                    "Continuous-variable (CV) multipartite unlockable bound-entangled states is investigated in this paper. Comparing with the qubit multipartite unlockable bound-entangled states, CV multipartite unlockable bound-entangled states present the new and different properties. CV multipartite unlockable bound-entangled states may serve as a useful quantum resource for new multiparty communication schemes. The experimental protocol for generating CV unlockable bound-entangled states is proposed with a setup that is at present accessible.",
                    "The classical Halpern-L\\\"auchli theorem states that for any finite coloring of a finite product of finitely branching perfect trees of height $\\omega$, there exist strong subtrees sharing the same level set such that tuples consisting of elements lying on the same level get the same color. Relative to large cardinals, we establish the consistency of a tail cone version of the Halpern-L\\\"auchli theorem at large cardinal, which, roughly speaking, deals with many colorings simultaneously and diagonally. Among other applications, we generalize a polarized partition relation on rational numbers due to Laver and Galvin to one on linear orders of larger saturation.",
                    "We consider algebraic manifolds $Y$ of dimension 3 over $\\Bbb{C}$ with $H^i(Y, \\Omega^j_Y)=0$ for all $j\\geq 0$ and $i>0$. Let $X$ be a smooth completion of $Y$ with $D=X-Y$, an effective divisor on $X$ with normal crossings. If the $D$-dimension of $X$ is not zero, then $Y$ is a fibre space over a smooth affine curve $C$ (i.e., we have a surjective morphism from $Y$ to $C$ such that general fibre is smooth and irreducible) such that every fibre satisfies the same vanishing condition. If an irreducible smooth fibre is not affine, then the Kodaira dimension of $X$ is $-\\infty$ and the $D$-dimension of X is 1. We also discuss sufficient conditions from the behavior of fibres or higher direct images to guarantee the global vanishing of Hodge cohomology and the affineness of $Y$.",
                    "We analyze the structure of the algebraic manifolds $Y$ of dimension 3 with $H^i(Y, \\Omega^j_Y)=0$ for all $j\\geq 0$, $i>0$ and $h^0(Y, {\\mathcal{O}}_Y) > 1$, by showing the deformation invariant of some open surfaces. Secondly, we show when a smooth threefold with nonconstant regular functions satisfies the vanishing Hodge cohomology. As an application, we prove the existence of nonaffine and nonproduct threefolds $Y$ with this property by constructing a family of a certain type of open surfaces parametrized by the affine curve $\\C-\\{0\\}$ such that the corresponding smooth completion $X$ has Kodaira dimension $-\\infty$ and $D$-dimension 1, where $D$ is the effective boundary divisor with support $X-Y$.",
                    "Let $Y$ be an algebraic manifold of dimension 3 with $H^i(Y, \\Omega^j_Y)=0$ for all $j\\geq 0$, $i>0$ and $h^0(Y, {\\mathcal{O}}_Y) > 1$. Let $X$ be a smooth completion of $Y$ such that the boundary $X-Y$ is the support of an effective divisor $D$ on $X$ with simple normal crossings. We prove that the $D$-dimension of $X$ cannot be 2, i.e., either any two nonconstant regular functions are algebraically dependent or there are three algebraically independent nonconstant regular functions on $Y$. Secondly, if the $D$-dimension of $X$ is greater than 1, then the associated scheme of $Y$ is isomorphic to Spec$\\Gamma(Y, {\\mathcal{O}}_Y)$. Furthermore, we prove that an algebraic manifold $Y$ of any dimension $d\\geq 1$ is affine if and only if $H^i(Y, \\Omega^j_Y)=0$ for all $j\\geq 0$, $i>0$ and it is regularly separable, i.e., for any two distinct points $y_1$, $y_2$ on $Y$, there is a regular function $f$ on $Y$ such that $f(y_1)\\neq f(y_2)$.",
                    "We consider smooth threefolds $Y$ defined over $\\Bbb{C}$ with $H^i(Y, \\Omega^j_Y)=0$ for all $j\\geq 0$, $i>0$. Let $X$ be a smooth projective threefold containing $Y$ and $D$ be the boundary divisor with support $X-Y$.   We are interested in the following question: What geometry information of $X$ can be obtained from the regular function information on $Y$?   Suppose that the boundary $X-Y$ is a smooth projective surface. In this paper, we analyse two different cases, i.e., there are no nonconstant regular functions on $Y$ or there are lots of regular functions on $Y$. More precisely, if $H^0(Y, {\\mathcal{O}}_Y)=\\Bbb{C}$, we prove that ${1/2}(c_1^2+c_2)\\cdot D=\\chi({\\mathcal{O}}_D)\\geq 0$. In particular, if the line bundle ${\\mathcal{O}}_D(D)$ is not torsion, then $q=h^1(X, {\\mathcal{O}}_X)=0$, ${1/2}(c_1^2+c_2)\\cdot D=\\chi({\\mathcal{O}}_D)=0$,   $\\chi({\\mathcal{O}}_X) >0$ and $K_X$ is not nef. If there is a positive constant $c$ such that $h^0(X, {\\mathcal{O}}_X(nD))\\geq c n^3$ for all sufficiently large $n$ (we say that $D$ is big or the $D$-dimension of $X$ is 3) and $D$ has no exceptional curves, then $|nD|$ is base point free for $n\\gg 0$. Therefore $Y$ is affine if $D$ is big.",
                    "It is well-known that the associated analytic space of an affine variety defined over $\\mathbb{C}$ is Stein but the converse is not true, that is, an algebraic Stein variety is not necessarily affine. In this paper, we give sufficient and necessary conditions for an algebraic Stein variety to be affine. One of our results is that an irreducible quasi-projective variety $Y$ defined over $\\mathbb{C}$ with dimension $d$ ($d\\geq 1$) is affine if and only if $Y$ is Stein, $H^i(Y, {\\mathcal{O}}_Y)=0$ for all $i>0$ and $\\kappa(D, X)= d$ (i.e., $D$ is a big divisor), where $X$ is a projective variety containing $Y$ and $D$ is an effective divisor with support $X-Y$. If $Y$ is algebraic Stein but not affine, we also discuss the possible transcendental degree of the nonconstant regular functions on $Y$. We prove that $Y$ cannot have $d-1$ algebraically independent nonconstant regular functions. The interesting phenomenon is that the transcendental degree can be even if the dimension of $Y$ is even and the degree can be odd if the dimension of $Y$ is odd.",
                    "In this paper, we give new criteria for affineness of a variety defined over $\\Bbb{C}$. Our main result is that an irreducible algebraic variety $Y$ (may be singular) of dimension $d$ ($d\\geq 1$) defined over $\\Bbb{C}$ is an affine variety if and only if $Y$ contains no complete curves, $H^i(Y, {\\mathcal{O}}_Y)=0$ for all $i>0$ and the boundary $X-Y$ is support of a big divisor, where $X$ is a projective variety containing $Y$. We construct three examples to show that a variety is not affine if it only satisfies two conditions among these three conditions. We also give examples to demonstrate the difference between the behavior of the boundary divisor $D$ and the affineness of $Y$. If $Y$ is an affine variety, then the ring $\\Gamma (Y, {\\mathcal{O}}_Y)$ is noetherian. However, to prove that $Y$ is an affine variety, we do not start from this ring. We explain why we do not need to check the noetherian property of the ring $\\Gamma (Y, {\\mathcal{O}}_Y)$ directly but use the techniques of sheaf and cohomology.",
                    "The form of a local Clifford (LC, also called local Gaussian (LG)) operation for the continuous-variable (CV) weighted graph states is presented in this paper, which is the counterpart of the LC operation of local complementation for qubit graph states. The novel property of the CV weighted graph states is shown, which can be expressed by the stabilizer formalism. It is distinctively different from the qubit weighted graph states, which can not be expressed by the stabilizer formalism. The corresponding graph rule, stated in purely graph theoretical terms, is described, which completely characterizes the evolution of CV weighted graph states under this LC operation. This LC operation may be applied repeatedly on a CV weighted graph state, which can generate the infinite LC equivalent graph states of this graph state. This work is an important step to characterize the LC equivalence class of CV weighted graph states.",
                    "Let $X$ be a smooth irreducible projective variety of dimension at least 2 over an algebraically closed field of characteristic 0 in the projective space ${\\mathbb{P}}^n$.   Bertini's Theorem states that a general hyperplane $H$ intersects $X$ with an irreducible smooth subvariety of $X$. However, the precise location of the smooth hyperplane section is not known. We show that for any $q\\leq n+1$ closed points in general position and any degree $a>1$, a general hypersurface $H$ of degree $a$ passing through these $q$ points intersects $X$ with an irreducible smooth codimension 1 subvariety on $X$. We also consider linear system of ample divisors and give precise location of smooth elements in the system. Similar result can be obtained for compact complex manifolds with holomorphic maps into projective spaces.",
                    "Content distribution networks (CDNs) which serve to deliver web objects (e.g., documents, applications, music and video, etc.) have seen tremendous growth since its emergence. To minimize the retrieving delay experienced by a user with a request for a web object, caching strategies are often applied - contents are replicated at edges of the network which is closer to the user such that the network distance between the user and the object is reduced. In this literature survey, evolution of caching is studied. A recent research paper [15] in the field of large-scale caching for CDN was chosen to be the anchor paper which serves as a guide to the topic. Research studies after and relevant to the anchor paper are also analyzed to better evaluate the statements and results of the anchor paper and more importantly, to obtain an unbiased view of the large scale collaborate caching systems as a whole.",
                    "Rado's Conjecture is a compactness/reflection principle that says any nonspecial tree of height $\\omega_1$ has a nonspecial subtree of size $\\leq \\aleph_1$. Though incompatible with Martin's Axiom, Rado's Conjecture turns out to have many interesting consequences that are consequences of forcing axioms. In this paper, we obtain consistency results concerning Rado's Conjecture and its Baire version. In particular, we show a fragment of PFA, that is the forcing axiom for \\emph{Baire Indestructibly proper forcings}, is compatible with the Baire Rado's Conjecture. As a corollary, Baire Rado's Conjecture does not imply Rado's Conjecture. Then we discuss the strength and limitations of the Baire Rado's Conjecture regarding its interaction with simultaneous stationary reflection and some families of weak square principles. Finally we investigate the influence of the Rado's Conjecture on some polarized partition relations.",
                    "We discuss the rainbow Ramsey theorems at limit cardinals and successors of singular cardinals, addressing some questions in \\cite{MR2354904} and \\cite{MR2902230}. In particular, we show for inaccessible $\\kappa$, $\\kappa\\to^{poly}(\\kappa)^2_{2-bdd}$ does not characterize weak compactness and for singular $\\kappa$, $\\mathrm{GCH}+\\square_\\kappa$ implies $\\kappa^+\\not\\to^{poly} (\\eta)^2_{<\\kappa-bdd}$ for any $\\eta\\geq cf(\\kappa)^+$ and $\\kappa^+\\to^{poly} (\\nu)^2_{<\\kappa-bdd}$ for any $\\nu<cf(\\kappa)^+$. We also provide a simplified construction of a model for $\\omega_2\\not\\to^{poly} (\\omega_1)^2_{2-bdd}$ originally constructed in \\cite{MR2902230} and show the witnessing coloring is indestructible under strongly proper forcings but destructible under some c.c.c forcing. Finally, we conclude with some remarks and questions on possible generalizations to rainbow partition relations for triples.",
                    "Consider two kinds of 1-d Hamiltonian Derivative Nonlinear Schr\\\"odinger (DNLS) equations with respect to different symplectic forms under periodic boundary conditions. The nonlinearities of these equations depend not only on $(x,\\psi,\\bar{\\psi})$ but also on $(\\psi_x,\\bar{\\psi}_x)$, which means the nonlinearities of these equations are unbounded. Suppose that the nonlinearities depend on the space-variable $x$ periodically.   Under some assumptions, for most potentials of these two kinds of Hamiltonian DNLS equations, if the initial value is smaller than $\\varepsilon\\ll1$ in $p$-Sobolev norm, then the corresponding solution to these equations is also smaller than $2\\varepsilon$ during a time interval $(-c\\varepsilon^{-r_*},c\\varepsilon^{-r_*})$(for any given positive $r_*$). The main methods are constructing Birkhoff normal forms to two kinds of Hamiltonian systems which have unbounded nonlinearities and using the special symmetry of the unbounded nonlinearities of Hamiltonian functions to obtain a long time estimate of the solution in $p$-Sobolev norm."
                ],
                "domain": [
                    "Algebraic Geometry",
                    "Combinatorial Set Theory",
                    "Quantum Information",
                    "Nonlinear Dynamics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "4ac8cc7a-2eaf-4d68-be9a-a7deaad447fb": {
                "pk": "4ac8cc7a-2eaf-4d68-be9a-a7deaad447fb",
                "project_name": null,
                "name": "Yiwei Li",
                "bio": "I am a researcher with a diverse background in machine learning, computational biology, and systems optimization. My work spans several domains, including the development of innovative algorithms for k-means clustering, where I generalized the center initialization process to enhance performance. I have also explored the critical role of activation functions in neural networks, introducing variational neural networks that optimize these functions through gradient descent.\n\nIn the realm of computational biology, I developed SPRINT, a groundbreaking tool for predicting protein-protein interactions, which stands out for its speed and accuracy in mapping the entire human interactome. My research extends to dialogue generation, where I tackled the generic response problem by proposing a novel negative distillation method that significantly improves model performance.\n\nAdditionally, I have contributed to the field of hybrid memory systems with Trimma, a design that optimizes metadata management to enhance performance in large-scale architectures. My recent work on decentralized optimization in multi-agent networks led to the development of a robust compressed push-pull algorithm, demonstrating linear convergence in complex communication environments.\n\nThrough my research, I aim to bridge theoretical insights with practical applications, enhancing our understanding of complex systems and improving computational efficiency across various domains. I am passionate about leveraging my findings to address real-world challenges and contribute to advancements in technology and science.",
                "collaborators": [
                    "Shaoxiong Feng",
                    "Bin Sun",
                    "Kan Li",
                    "Enzhi Li",
                    "Lucian Ilie",
                    "Boyu Tian",
                    "Mingyu Gao",
                    "Yiwei Liao",
                    "Zhuorui Li",
                    "Shi Pu"
                ],
                "pub_titles": [
                    "Generalization of k-means Related Algorithms",
                    "Variational Neural Networks: Every Layer and Neuron Can Be Unique",
                    "SPRINT: Ultrafast protein-protein interaction prediction of the entire human interactome",
                    "Diversifying Neural Dialogue Generation via Negative Distillation",
                    "Heterogeneous-Branch Collaborative Learning for Dialogue Generation",
                    "Stop Filtering: Multi-View Attribute-Enhanced Dialogue Learning",
                    "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                    "A Linearly Convergent Robust Compressed Push-Pull Method for Decentralized Optimization",
                    "Stochastic lattice model of synaptic membrane protein domains"
                ],
                "pub_abstracts": [
                    "This article briefly introduced Arthur and Vassilvitshii's work on \\textbf{k-means++} algorithm and further generalized the center initialization process. It is found that choosing the most distant sample point from the nearest center as new center can mostly have the same effect as the center initialization process in the \\textbf{k-means++} algorithm.",
                    "The choice of activation function can significantly influence the performance of neural networks. The lack of guiding principles for the selection of activation function is lamentable. We try to address this issue by introducing our variational neural networks, where the activation function is represented as a linear combination of possible candidate functions, and an optimal activation is obtained via minimization of a loss function using gradient descent method. The gradient formulae for the loss function with respect to these expansion coefficients are central for the implementation of gradient descent algorithm, and here we derive these gradient formulae.",
                    "Proteins perform their functions usually by interacting with other proteins. Predicting which proteins interact is a fundamental problem. Experimental methods are slow, expensive, and have a high rate of error. Many computational methods have been proposed among which sequence-based ones are very promising. However, so far no such method is able to predict effectively the entire human interactome: they require too much time or memory. We present SPRINT (Scoring PRotein INTeractions), a new sequence-based algorithm and tool for predicting protein-protein interactions. We comprehensively compare SPRINT with state-of-the-art programs on seven most reliable human PPI datasets and show that it is more accurate while running orders of magnitude faster and using very little memory. SPRINT is the only program that can predict the entire human interactome. Our goal is to transform the very challenging problem of predicting the entire human interactome into a routine task. The source code of SPRINT is freely available from github.com/lucian-ilie/SPRINT/ and the datasets and predicted PPIs from www.csd.uwo.ca/faculty/ilie/SPRINT/.",
                    "Generative dialogue models suffer badly from the generic response problem, limiting their applications to a few toy scenarios. Recently, an interesting approach, namely negative training, has been proposed to alleviate this problem by reminding the model not to generate high-frequency responses during training. However, its performance is hindered by two issues, ignoring low-frequency but generic responses and bringing low-frequency but meaningless responses. In this paper, we propose a novel negative training paradigm, called negative distillation, to keep the model away from the undesirable generic responses while avoiding the above problems. First, we introduce a negative teacher model that can produce query-wise generic responses, and then the student model is required to maximize the distance with multi-level negative knowledge. Empirical results show that our method outperforms previous negative training methods significantly.",
                    "With the development of deep learning, advanced dialogue generation methods usually require a greater amount of computational resources. One promising approach to obtaining a high-performance and lightweight model is knowledge distillation, which relies heavily on the pre-trained powerful teacher. Collaborative learning, also known as online knowledge distillation, is an effective way to conduct one-stage group distillation in the absence of a well-trained large teacher model. However, previous work has a severe branch homogeneity problem due to the same training objective and the independent identical training sets. To alleviate this problem, we consider the dialogue attributes in the training of network branches. Each branch learns the attribute-related features based on the selected subset. Furthermore, we propose a dual group-based knowledge distillation method, consisting of positive distillation and negative distillation, to further diversify the features of different branches in a steadily and interpretable way. The proposed approach significantly improves branch heterogeneity and outperforms state-of-the-art collaborative learning methods on two widely used open-domain dialogue datasets.",
                    "There is a growing interest in improving the conversational ability of models by filtering the raw dialogue corpora. Previous filtering strategies usually rely on a scoring method to assess and discard samples from one perspective, enabling the model to enhance the corresponding dialogue attributes (e.g., consistency) more easily. However, the discarded samples may obtain high scores in other perspectives and can provide regularization effects on the model learning, which causes the performance improvement to be sensitive to the filtering ratio. In this work, we propose a multi-view attribute-enhanced dialogue learning framework that strengthens the attribute-related features more robustly and comprehensively. Instead of filtering the raw dataset to train the model, our framework first pre-trains the model on the raw dataset and then fine-tunes it through adapters on the selected sub-sets, which also enhances certain attributes of responses but without suffering from the problems mentioned above. Considering the variety of the dialogue attribute, we further design a multi-view enhancement mechanism, including multi-view selection and inter-view fusion. It groups the high-quality samples from multiple perspectives, respectively, and enhances different attributes of responses with the corresponding sample sets and adapters, keeping knowledge independent and allowing flexible integration. Empirical results and analysis show that our framework can improve the performance significantly in terms of enhancing dialogue attributes and fusing view-specific knowledge.",
                    "Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity address mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative hybrid memory system with HBM3 and DDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup benefits, compared to state-of-the-art hybrid memory designs. These results show that Trimma effectively addresses metadata management overheads, especially for future scalable large-scale hybrid memory architectures.",
                    "In the modern paradigm of multi-agent networks, communication has become one of the main bottlenecks for decentralized optimization, where a large number of agents are involved in minimizing the average of the local cost functions. In this paper, we propose a robust compressed push-pull algorithm (RCPP) that combines gradient tracking with communication compression. In particular, RCPP is compatible with a much more general class of compression operators that allow both relative and absolute compression errors. We show that RCPP achieves linear convergence rate for smooth objective functions satisfying the Polyak-{\\L}ojasiewicz condition over general directed networks. Numerical examples verify the theoretical findings and demonstrate the efficiency, flexibility, and robustness of the proposed algorithm.",
                    "Neurotransmitter receptor molecules, concentrated in synaptic membrane domains along with scaffolds and other kinds of proteins, are crucial for signal transmission across chemical synapses. In common with other membrane protein domains, synaptic domains are characterized by low protein copy numbers and protein crowding, with rapid stochastic turnover of individual molecules. We study here in detail a stochastic lattice model of the receptor-scaffold reaction-diffusion dynamics at synaptic domains that was found previously to capture, at the mean-field level, the self-assembly, stability, and characteristic size of synaptic domains observed in experiments. We show that our stochastic lattice model yields quantitative agreement with mean-field models of nonlinear diffusion in crowded membranes. Through a combination of analytic and numerical solutions of the master equation governing the reaction dynamics at synaptic domains, together with kinetic Monte Carlo simulations, we find substantial discrepancies between mean-field and stochastic models for the reaction dynamics at synaptic domains. Based on the reaction and diffusion properties of synaptic receptors and scaffolds suggested by previous experiments and mean-field calculations, we show that the stochastic reaction-diffusion dynamics of synaptic receptors and scaffolds provide a simple physical mechanism for collective fluctuations in synaptic domains, the molecular turnover observed at synaptic domains, key features of the observed single-molecule trajectories, and spatial heterogeneity in the effective rates at which receptors and scaffolds are recycled at the cell membrane. Our work sheds light on the physical mechanisms and principles linking the collective properties of membrane protein domains to the stochastic dynamics that rule their molecular~components."
                ],
                "domain": [
                    "Machine Learning",
                    "Protein-Protein Interaction",
                    "Knowledge Distillation",
                    "Multi-Agent Systems"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "6860e96b-0c56-46da-a347-74c1953a477a": {
                "pk": "6860e96b-0c56-46da-a347-74c1953a477a",
                "project_name": null,
                "name": "Wei Ruan",
                "bio": "As a researcher deeply engaged in the exploration of high-temperature superconductivity (HTSC), I am driven by the quest to unravel the complex mechanisms behind Cooper pairs and the myriad of associated phenomena. My work seeks to address the ongoing debates surrounding critical concepts such as the pseudogap state, the peculiar Homes law, and the intricate interplay of electron-phonon interactions. \n\nIn my recent research, I propose a novel perspective grounded in low-dimensional Bose-Einstein Condensation (LDBEC). This framework aims to unify the various mysteries of HTSC into a coherent narrative, potentially shedding light on their interconnections. I believe that by re-examining these phenomena through the lens of LDBEC, we can gain deeper insights and foster a more comprehensive understanding of high-temperature superconductivity. My goal is to contribute to the scientific dialogue in this field, offering fresh perspectives that may help resolve longstanding controversies and advance our knowledge of these fascinating materials.",
                "collaborators": [],
                "pub_titles": [
                    "High temperature superconductivity: Cooper pairs in trap"
                ],
                "pub_abstracts": [
                    "The tremendous efforts to unveil high temperature superconductivity (HTSC) have been devoted to the search of the mechanism underlying Cooper pairs which, however, remains a mysterious subject of vigorous debate, let alone many other mysteries like the pseudogap state, the peculiar Homes law, the unnegligible electron-phonon interaction, the stripe, the universal nodal Fermi velocity, etc. Most of subsequent works either bring in more controversies or swell the list of mysteries. Here I tentatively propose a whole new perspective on the basis of low-dimensional Bose-Einstein Condensation (LDBEC), which possibly makes lots of those mysteries correlated and understood in single picture."
                ],
                "domain": [
                    "High Temperature Superconductivity",
                    "Bose-Einstein Condensation",
                    "Quantum Physics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b723606f-c5bc-494c-a78e-db5251bfbe59": {
                "pk": "b723606f-c5bc-494c-a78e-db5251bfbe59",
                "project_name": null,
                "name": "Zhengliang Liu",
                "bio": "I am a researcher deeply engaged in the intersection of artificial intelligence, data visualization, and healthcare applications. My recent work has focused on leveraging large language models (LLMs) to enhance various domains, including political discourse analysis, medical text classification, and radiology. I have developed frameworks that utilize LLMs for tasks such as evaluating presidential debate performances and improving the automatic scoring of student responses in science education. \n\nOne of my notable contributions is the MGH Radiology Llama, a specialized LLM designed to assist radiologists by generating accurate and clinically relevant reports from a vast dataset of medical records. I also explore the potential of LLMs in cohort recruitment from unstructured medical texts, employing knowledge graphs to enhance their predictive capabilities. \n\nIn addition to my work with LLMs, I have investigated the dynamics of functional brain networks using innovative deep learning models, proposing a Spatial and Channel-wise Attention Autoencoder to capture complex interactions in fMRI data. My research aims to improve model interpretability and performance in specialized domains, ultimately contributing to advancements in healthcare and education. \n\nI am committed to making my findings accessible to the community, as evidenced by my plans to release datasets and models that can facilitate further research and application in these critical areas.",
                "collaborators": [
                    "Tianming Liu",
                    "Zihao Wu",
                    "Xiang Li",
                    "Quanzheng Li",
                    "Dajiang Zhu",
                    "Wei Liu",
                    "Alvitta Ottley",
                    "Yiwei Li",
                    "Peng Shu",
                    "Ninghao Liu"
                ],
                "pub_titles": [
                    "Tropical analogues of a Dempe-Franke bilevel optimization problem",
                    "Let's Gamble: Uncovering the Impact of Visualization on Risk Perception and Decision-Making",
                    "Survey on Individual Differences in Visualization",
                    "Let's Gamble: How a Poor Visualization Can Elicit Risky Behavior",
                    "Context Matters: A Strategy to Pre-train Language Model for Science Education",
                    "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models",
                    "MGH Radiology Llama: A Llama 3 70B Model for Radiology",
                    "MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical Question Answering",
                    "Discovering Dynamic Functional Brain Networks via Spatial and Channel-wise Attention",
                    "Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis",
                    "ALDM-Grasping: Diffusion-aided Zero-Shot Sim-to-Real Transfer for Robot Grasping",
                    "CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study",
                    "Hierarchical Semantic Tree Concept Whitening for Interpretable Image Classification",
                    "Artificial General Intelligence for Medical Imaging",
                    "Evaluating Large Language Models in Ophthalmology",
                    "The Radiation Oncology NLP Database",
                    "Segment Anything Model (SAM) for Radiation Oncology"
                ],
                "pub_abstracts": [
                    "We consider the tropical analogues of a particular bilevel optimization problem studied by Dempe and Franke and suggest some methods of solving these new tropical bilevel optimization problems. In particular, it is found that the algorithm developed by Dempe and Franke can be formulated and its validity can be proved in a more general setting, which includes the tropical bilevel optimization problems in question. We also show how the feasible set can be decomposed into a finite number of tropical polyhedra, to which the tropical linear programming solvers can be applied.",
                    "Data visualizations are standard tools for assessing and communicating risks. However, it is not always clear which designs are optimal or how encoding choices might influence risk perception and decision-making. In this paper, we report the findings of a large-scale gambling game that immersed participants in an environment where their actions impacted their bonuses. Participants chose to either enter a draw or receive guaranteed monetary gains based on five common visualization designs. By measuring risk perception and observing decision-making, we showed that icon arrays tended to elicit economically sound behavior. We also found that people were more likely to gamble when presented area proportioned triangle and circle designs. Using our results, we model risk perception and decisions for each visualization and provide a ranking to improve visualization selection.",
                    "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one-size-fits-all visualization interfaces, as well as the significance of individual differences in the use of data visualization systems. Unfortunately, the absence of comprehensive surveys of the existing literature impedes the development of this research. In this paper, we review the research perspectives, as well as the personality traits and cognitive abilities, visualizations, tasks, and measures investigated in the existing literature. We aim to provide a detailed summary of existing scholarship, produce evidence-based reviews, and spur future inquiry.",
                    "Data visualizations are standard tools for assessing and communicating risks. However, it is not always clear which designs are optimal or how encoding choices might influence risk perception and decision-making. In this paper, we report the findings of a large-scale gambling game that immersed participants in an environment where their actions impacted their bonuses. Participants chose to either enter a lottery or receive guaranteed monetary gains based on five common visualization designs. By measuring risk perception and observing decision-making, we showed that icon arrays tended to elicit economically sound behavior. We also found that people were more likely to gamble when presented area proportioned triangle and circle designs. Using our results, we model risk perception and discuss how our findings can improve visualization selection.",
                    "This study aims at improving the performance of scoring student responses in science education automatically. BERT-based language models have shown significant superiority over traditional NLP models in various language-related tasks. However, science writing of students, including argumentation and explanation, is domain-specific. In addition, the language used by students is different from the language in journals and Wikipedia, which are training sources of BERT and its existing variants. All these suggest that a domain-specific model pre-trained using science education data may improve model performance. However, the ideal type of data to contextualize pre-trained language model and improve the performance in automatically scoring student written responses remains unclear. Therefore, we employ different data in this study to contextualize both BERT and SciBERT models and compare their performance on automatic scoring of assessment tasks for scientific argumentation. We use three datasets to pre-train the model: 1) journal articles in science education, 2) a large dataset of students' written responses (sample size over 50,000), and 3) a small dataset of students' written responses of scientific argumentation tasks. Our experimental results show that in-domain training corpora constructed from science questions and responses improve language model performance on a wide variety of downstream tasks. Our study confirms the effectiveness of continual pre-training on domain-specific data in the education domain and demonstrates a generalizable strategy for automating science education tasks with high accuracy. We plan to release our data and SciEdBERT models for public use and community engagement.",
                    "Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored. This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes. We propose a framework that analyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they resonate with the \"Interests, Ideologies, and Identity\" (3I) of four key audience groups: voters, businesses, donors, and politicians. Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances. Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts. In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation.",
                    "In recent years, the field of radiology has increasingly harnessed the power of artificial intelligence (AI) to enhance diagnostic accuracy, streamline workflows, and improve patient care. Large language models (LLMs) have emerged as particularly promising tools, offering significant potential in assisting radiologists with report generation, clinical decision support, and patient communication. This paper presents an advanced radiology-focused large language model: MGH Radiology Llama. It is developed using the Llama 3 70B model, building upon previous domain-specific models like Radiology-GPT and Radiology-Llama2. Leveraging a unique and comprehensive dataset from Massachusetts General Hospital, comprising over 6.5 million de-identified medical reports across various imaging modalities, the model demonstrates significant improvements in generating accurate and clinically relevant radiology impressions given the corresponding findings. Our evaluation, incorporating both traditional metrics and a GPT-4-based assessment, highlights the enhanced performance of this work over general-purpose LLMs.",
                    "Large Language Models (LLMs), although powerful in general domains, often perform poorly on domain-specific tasks such as medical question answering (QA). In addition, LLMs tend to function as \"black-boxes\", making it challenging to modify their behavior. To address the problem, our work employs a transparent process of retrieval augmented generation (RAG), aiming to improve LLM responses without the need for fine-tuning or retraining. Specifically, we propose a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then inject them into the LLM's query prompt. Focusing on medical QA, we evaluate the impact of different retrieval models and the number of facts on LLM performance using the MedQA-SMILE dataset. Notably, our retrieval-augmented Vicuna-7B model exhibited an accuracy improvement from 44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM performance, offering a practical approach to mitigate the challenges posed by black-box LLMs.",
                    "Using deep learning models to recognize functional brain networks (FBNs) in functional magnetic resonance imaging (fMRI) has been attracting increasing interest recently. However, most existing work focuses on detecting static FBNs from entire fMRI signals, such as correlation-based functional connectivity. Sliding-window is a widely used strategy to capture the dynamics of FBNs, but it is still limited in representing intrinsic functional interactive dynamics at each time step. And the number of FBNs usually need to be set manually. More over, due to the complexity of dynamic interactions in brain, traditional linear and shallow models are insufficient in identifying complex and spatially overlapped FBNs across each time step. In this paper, we propose a novel Spatial and Channel-wise Attention Autoencoder (SCAAE) for discovering FBNs dynamically. The core idea of SCAAE is to apply attention mechanism to FBNs construction. Specifically, we designed two attention modules: 1) spatial-wise attention (SA) module to discover FBNs in the spatial domain and 2) a channel-wise attention (CA) module to weigh the channels for selecting the FBNs automatically. We evaluated our approach on ADHD200 dataset and our results indicate that the proposed SCAAE method can effectively recover the dynamic changes of the FBNs at each fMRI time step, without using sliding windows. More importantly, our proposed hybrid attention modules (SA and CA) do not enforce assumptions of linearity and independence as previous methods, and thus provide a novel approach to better understanding dynamic functional brain networks.",
                    "In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs such as GPT-4 are employed for zero-shot text identification and label generation for radiology reports, where the labels are then used as measurements for text similarity. By testing the proposed framework on the MIMIC data, we find that GPT-4 generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the text data using semi-quantitative reasoning results by the LLMs for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains as well.",
                    "To tackle the \"reality gap\" encountered in Sim-to-Real transfer, this study proposes a diffusion-based framework that minimizes inconsistencies in grasping actions between the simulation settings and realistic environments. The process begins by training an adversarial supervision layout-to-image diffusion model(ALDM). Then, leverage the ALDM approach to enhance the simulation environment, rendering it with photorealistic fidelity, thereby optimizing robotic grasp task training. Experimental results indicate this framework outperforms existing models in both success rates and adaptability to new environments through improvements in the accuracy and reliability of visual grasping actions under a variety of conditions. Specifically, it achieves a 75\\% success rate in grasping tasks under plain backgrounds and maintains a 65\\% success rate in more complex scenarios. This performance demonstrates this framework excels at generating controlled image content based on text descriptions, identifying object grasp points, and demonstrating zero-shot learning in complex, unseen scenarios.",
                    "Participant recruitment based on unstructured medical texts such as clinical notes and radiology reports has been a challenging yet important task for the cohort establishment in clinical research. Recently, Large Language Models (LLMs) such as ChatGPT have achieved tremendous success in various downstream tasks thanks to their promising performance in language understanding, inference, and generation. It is then natural to test their feasibility in solving the cohort recruitment task, which involves the classification of a given paragraph of medical text into disease label(s). However, when applied to knowledge-intensive problem settings such as medical text classification, where the LLMs are expected to understand the decision made by human experts and accurately identify the implied disease labels, the LLMs show a mediocre performance. A possible explanation is that, by only using the medical text, the LLMs neglect to use the rich context of additional information that languages afford. To this end, we propose to use a knowledge graph as auxiliary information to guide the LLMs in making predictions. Moreover, to further boost the LLMs adapt to the problem setting, we apply a chain-of-thought (CoT) sample selection strategy enhanced by reinforcement learning, which selects a set of CoT samples given each individual medical report. Experimental results and various ablation studies show that our few-shot learning method achieves satisfactory performance compared with fine-tuning strategies and gains superb advantages when the available data is limited. The code and sample dataset of the proposed CohortGPT model is available at: https://anonymous.4open.science/r/CohortGPT-4872/",
                    "With the popularity of deep neural networks (DNNs), model interpretability is becoming a critical concern. Many approaches have been developed to tackle the problem through post-hoc analysis, such as explaining how predictions are made or understanding the meaning of neurons in middle layers. Nevertheless, these methods can only discover the patterns or rules that naturally exist in models. In this work, rather than relying on post-hoc schemes, we proactively instill knowledge to alter the representation of human-understandable concepts in hidden layers. Specifically, we use a hierarchical tree of semantic concepts to store the knowledge, which is leveraged to regularize the representations of image data instances while training deep models. The axes of the latent space are aligned with the semantic concepts, where the hierarchical relations between concepts are also preserved. Experiments on real-world image datasets show that our method improves model interpretability, showing better disentanglement of semantic concepts, without negatively affecting model classification performance.",
                    "In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond.",
                    "Purpose: The performance of three different large language models (LLMS) (GPT-3.5, GPT-4, and PaLM2) in answering ophthalmology professional questions was evaluated and compared with that of three different professional populations (medical undergraduates, medical masters, and attending physicians). Methods: A 100-item ophthalmology single-choice test was administered to three different LLMs (GPT-3.5, GPT-4, and PaLM2) and three different professional levels (medical undergraduates, medical masters, and attending physicians), respectively. The performance of LLM was comprehensively evaluated and compared with the human group in terms of average score, stability, and confidence. Results: Each LLM outperformed undergraduates in general, with GPT-3.5 and PaLM2 being slightly below the master's level, while GPT-4 showed a level comparable to that of attending physicians. In addition, GPT-4 showed significantly higher answer stability and confidence than GPT-3.5 and PaLM2. Conclusion: Our study shows that LLM represented by GPT-4 performs better in the field of ophthalmology. With further improvements, LLM will bring unexpected benefits in medical education and clinical decision making in the near future.",
                    "We present the Radiation Oncology NLP Database (ROND), the first dedicated Natural Language Processing (NLP) dataset for radiation oncology, an important medical specialty that has received limited attention from the NLP community in the past. With the advent of Artificial General Intelligence (AGI), there is an increasing need for specialized datasets and benchmarks to facilitate research and development. ROND is specifically designed to address this gap in the domain of radiation oncology, a field that offers many opportunities for NLP exploration. It encompasses various NLP tasks including Logic Reasoning, Text Classification, Named Entity Recognition (NER), Question Answering (QA), Text Summarization, and Patient-Clinician Conversations, each with a distinct focus on radiation oncology concepts and application cases. In addition, we have developed an instruction-tuning dataset consisting of over 20k instruction pairs (based on ROND) and trained a large language model, CancerChat. This serves to demonstrate the potential of instruction-tuning large language models within a highly-specialized medical domain. The evaluation results in this study could serve as baseline results for future research. ROND aims to stimulate advancements in radiation oncology and clinical NLP by offering a platform for testing and improving algorithms and models in a domain-specific context. The ROND dataset is a joint effort of multiple U.S. health institutions. The data is available at https://github.com/zl-liu/Radiation-Oncology-NLP-Database.",
                    "In this study, we evaluate the performance of the Segment Anything Model (SAM) in clinical radiotherapy. Our results indicate that SAM's 'segment anything' mode can achieve clinically acceptable segmentation results in most organs-at-risk (OARs) with Dice scores higher than 0.7. SAM's 'box prompt' mode further improves the Dice scores by 0.1 to 0.5. Considering the size of the organ and the clarity of its boundary, SAM displays better performance for large organs with clear boundaries but performs worse for smaller organs with unclear boundaries. Given that SAM, a model pre-trained purely on natural images, can handle the delineation of OARs from medical images with clinically acceptable accuracy, these results highlight SAM's robust generalization capabilities with consistent accuracy in automatic segmentation for radiotherapy. In other words, SAM can achieve delineation of different OARs at different sites using a generic automatic segmentation model. SAM's generalization capabilities across different disease sites suggest that it is technically feasible to develop a generic model for automatic segmentation in radiotherapy."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Data Visualization",
                    "Machine Learning",
                    "Medical Imaging"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "09323948-8cac-4472-b6b3-18f8346eaf14": {
                "pk": "09323948-8cac-4472-b6b3-18f8346eaf14",
                "project_name": null,
                "name": "Xiaowei Yu",
                "bio": "I am a researcher with a strong focus on graph theory and its applications, particularly in the areas of antimagic labeling, Ramsey theory, and graph coloring. My work has contributed to the understanding of antimagic orientations in various graph structures, supporting the conjecture that every connected undirected graph can be oriented to admit an antimagic labeling. I have also explored the Ramsey numbers of fan graphs, providing improved bounds that enhance our understanding of edge-coloring problems.\n\nIn addition to my work in pure graph theory, I have delved into the realm of graph coloring, specifically DP-coloring and star chromatic indices, where I have established significant results for planar graphs and generalized Halin graphs. My research emphasizes the interplay between structural properties of graphs and their coloring capabilities, leading to sharp upper bounds that advance the field.\n\nRecently, I have expanded my research into the application of graph-based methods in machine learning, particularly in unsupervised domain adaptation (UDA). I developed a novel brain-inspired robust core-periphery constrained transformer (RCCT) that leverages the core-periphery principle to enhance performance in UDA tasks. This interdisciplinary approach has yielded state-of-the-art results across multiple datasets, demonstrating the potential of integrating graph theory with machine learning techniques.\n\nOverall, my research is driven by a passion for uncovering the underlying principles of graph structures and their applications, and I am committed to advancing both theoretical and practical aspects of this vibrant field.",
                "collaborators": [
                    "Songling Shan",
                    "Guantao Chen",
                    "Yi Zhao",
                    "Seog-Jin Kim",
                    "Zhengke Miao",
                    "Yimin Song",
                    "Tao Wang",
                    "Dajiang Zhu",
                    "Tianming Liu"
                ],
                "pub_titles": [
                    "Antimagic orientation of biregular bipartite graphs",
                    "Antimagic Orientation of Forests",
                    "Improved bounds on the Ramsey number of fans",
                    "Planar graphs without 4-cycles adjacent to triangles are DP-4-colorable",
                    "List star edge coloring of generalized Halin graphs",
                    "Robust Core-Periphery Constrained Transformer for Domain Adaptation"
                ],
                "pub_abstracts": [
                    "An antimagic labeling of a directed graph $D$ with $n$ vertices and $m$ arcs is a bijection from the set of arcs of $D$ to the integers $\\{1, \\cdots, m\\}$ such that all $n$ oriented vertex sums are pairwise distinct, where an oriented vertex sum is the sum of labels of all arcs entering that vertex minus the sum of labels of all arcs leaving it. An undirected graph $G$ is said to have an antimagic orientation if $G$ has an orientation which admits an antimagic labeling. Hefetz, M{\\\"{u}}tze, and Schwartz conjectured that every connected undirected graph admits an antimagic orientation. In this paper, we support this conjecture by proving that every biregular bipartite graph admits an antimagic orientation.",
                    "An antimagic labeling of a digraph $D$ with $n$ vertices and $m$ arcs is a bijection from the set of arcs of $D$ to $\\{1,2,\\cdots,m\\}$ such that all $n$ oriented vertex-sums are pairwise distinct, where the oriented vertex-sum of a vertex is the sum of labels of all arcs entering that vertex minus the sum of labels of all arcs leaving it. A graph $G$ admits an antimagic orientation if $G$ has an orientation $D$ such that $D$ has an antimagic labeling. Hefetz, M{\\\"{u}}tze and Schwartz conjectured every connected graph admits an antimagic orientation. In this paper, we support this conjecture by proving that any forest obtained from a given forest with at most one isolated vertex by subdividing each edge at least once admits an antimagic orientation.",
                    "For a given graph $H$, the Ramsey number $r(H)$ is the minimum $N$ such that any 2-edge-coloring of the complete graph $K_N$ yields a monochromatic copy of $H$. Given a positive integer $n$, a \\emph{fan }$F_n$ is a graph formed by $n$ triangles that share one common vertex. We show that ${9n}/{2}-5\\le r(F_n)\\le {11n}/{2} + 6$ for any $n$. This improves previous best bounds $r(F_n) \\le 6n$ of Lin and Li and $r(F_n) \\ge 4n+2$ of Zhang, Broersma and Chen.",
                    "DP-coloring (also known as correspondence coloring) of a simple graph is a generalization of list coloring. It is known that planar graphs without 4-cycles adjacent to triangles are 4-choosable, and planar graphs without 4-cycles are DP-4-colorable. In this paper, we show that planar graphs without 4-cycles adjacent to triangles are DP-4-colorable, which is an extension of the two results above.",
                    "A star $k$-edge coloring is a proper edge coloring such that there are no bichromatic paths or cycles of length four. The smallest integer $k$ such that $G$ admits a star $k$-edge coloring is the star chromatic index of $G$. Deng \\etal \\cite{MR2933839}, and Bezegov{\\'a} \\etal \\cite{MR3431294} independently proved that the star chromatic index of a tree is at most $\\lfloor \\frac{3\\Delta}{2} \\rfloor$, and the bound is sharp. Han \\etal \\cite{MR3924408} strengthened the result to list version of star chromatic index, and proved that $\\lfloor \\frac{3\\Delta}{2} \\rfloor$ is also the sharp upper bound for the list star chromatic index of trees. A generalized Halin graph is a plane graph that consists of a plane embedding of a tree $T$ with $\\Delta(T) \\geq 3$, and a cycle $C$ connecting all the leaves of the tree such that $C$ is the boundary of the exterior face. In this paper, we prove that if $H := T \\cup C$ is a generalized Halin graph with $|C| \\neq 5$, then its list star chromatic index is at most \\[ \\max\\left\\{\\left\\lfloor\\frac{\\theta(T) + \\Delta(T)}{2}\\right\\rfloor, 2 \\left\\lfloor\\frac{\\Delta(T)}{2}\\right\\rfloor + 7\\right\\}, \\] where $\\theta(T) = \\max_{xy \\in E(T)}\\{d_{T}(x) + d_{T}(y)\\}$. As a consequence, if $H$ is a (generalized) Halin graph with maximum degree $\\Delta \\geq 13$, then the list star chromatic index is at most $\\lfloor \\frac{3\\Delta}{2} \\rfloor$. Moreover, the upper bound for the list star chromatic index is sharp.",
                    "Unsupervised domain adaptation (UDA) aims to learn transferable representation across domains. Recently a few UDA works have successfully applied Transformer-based methods and achieved state-of-the-art (SOTA) results. However, it remains challenging when there exists a large domain gap between the source and target domain. Inspired by humans' exceptional transferability abilities to adapt knowledge from familiar to uncharted domains, we try to apply the universally existing organizational structure in the human functional brain networks, i.e., the core-periphery principle to design the Transformer and improve its UDA performance. In this paper, we propose a novel brain-inspired robust core-periphery constrained transformer (RCCT) for unsupervised domain adaptation, which brings a large margin of performance improvement on various datasets. Specifically, in RCCT, the self-attention operation across image patches is rescheduled by an adaptively learned weighted graph with the Core-Periphery structure (CP graph), where the information communication and exchange between images patches are manipulated and controlled by the connection strength, i.e., edge weight of the learned weighted CP graph. Besides, since the data in domain adaptation tasks can be noisy, to improve the model robustness, we intentionally add perturbations to the patches in the latent space to ensure generating robust learned weighted core-periphery graphs. Extensive evaluations are conducted on several widely tested UDA benchmarks. Our proposed RCCT consistently performs best compared to existing works, including 88.3\\% on Office-Home, 95.0\\% on Office-31, 90.7\\% on VisDA-2017, and 46.0\\% on DomainNet."
                ],
                "domain": [
                    "Graph Theory",
                    "Combinatorics",
                    "Machine Learning",
                    "Domain Adaptation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "c2630a35-e729-4dc3-84a5-3d6993767049": {
                "pk": "c2630a35-e729-4dc3-84a5-3d6993767049",
                "project_name": null,
                "name": "Chao Cao",
                "bio": "I am a researcher specializing in the electronic structure and magnetic properties of iron-based superconductors and related materials, utilizing first-principles calculations to explore their complex behaviors. My recent work has focused on various compounds, such as KFe$_2$Se$_2$ and TlFe$_{2-x}$Se$_2$, where I have investigated their magnetic ordering, electronic states, and the implications of Fe-vacancy structures on their properties. \n\nI have developed insights into the interplay between magnetism and superconductivity, particularly in systems like EuRbFe$_4$As$_4$ and KCr$_3$As$_3$H, where I have identified key features such as unique crystal field splittings and ferromagnetic spin fluctuations. My research also extends to the study of quasi-one-dimensional superconductors and the effects of external pressure on magnetic ground states, revealing unconventional phase transitions and the role of hybridization in these materials.\n\nAdditionally, I have explored the electronic transport properties of graphene nanoribbons and the impact of metal terminations on their magnetic states, contributing to the understanding of spintronic applications. My work aims to bridge theoretical predictions with experimental findings, providing a deeper understanding of the fundamental mechanisms governing the behavior of these intriguing materials. Through my research, I strive to uncover new pathways for the development of advanced superconductors and spintronic devices.",
                "collaborators": [
                    "Jianhui Dai",
                    "Hai-Ping Cheng",
                    "Yan Wang",
                    "Fuchun Zhang",
                    "Hao Jiang",
                    "Guanghan Cao",
                    "Minghu Fang",
                    "Chenchao Xu",
                    "Qijin Chen",
                    "Jian-Xin Zhu"
                ],
                "pub_titles": [
                    "Electronic Structure of KFe$_2$Se$_2$ from First Principles Calculations",
                    "Electronic Structure and Mott Localization in Iron Deficient TlFe$_{1.5}$Se$_2$ with Superstructures",
                    "Block Spin Ground State and 3-Dimensionality of (K,Tl)Fe$_{1.6}$Se$_2$",
                    "Electronic structure of vacancy-ordered iron-selenide K$_{0.5}$Fe$_{1.75}$Se$_2$",
                    "Electronic structure of quasi-one-dimensional superconductor K$_2$Cr$_3$As$_3$ from first-principles calculations",
                    "Block Spin Magnetic Phase Transition of A$_y$Fe$_{1.6}$Se$_2$ Under High Pressure",
                    "Electronic structure of Ni-doped EuRbFe$_4$As$_4$: Unique crystal field splitting and multiband RKKY interactions",
                    "Pressure Dependent Electronic Structure in CeRh$_6$Ge$_4$",
                    "Large Extra Dimensions and Holography",
                    "Perfect Spin-filtering and Giant Magnetoresistance with Fe-terminated Graphene Nanoribbon",
                    "Lifshitz Transition and Nontrivial H-Doping Effect in Cr-based Superconductor KCr$_3$As$_3$H$_x$",
                    "Strongly Correlated Electrons in the $[Ni(hmp)(ROH)X]_4$ Single Molecule Magnet: A DFT+U Study",
                    "Metal-terminated Graphene Nanoribbons",
                    "Block spin magnetism and metal-insulator transition in a two-dimensional Hubbard model with perfect vacancy superstructure"
                ],
                "pub_abstracts": [
                    "Electronic structure and magnetic properties for iron-selenide KFe$_2$Se$_2$ are studied by first-principles calculations. The ground state is stripe-like antiferromagnetic with calculated 2.26 $\\mu_B$ magnetic moment on Fe atoms; and the $J_1$, $J_2$ coupling strengths are calculated to be 0.038 eV and 0.029 eV. The states around $E_F$ are dominated by the Fe-3d orbitals which hybridize noticeably to the Se-4p orbitals. While the band structure of KFe$_2$Se$_2$ is similar to a heavily electron-doped BaFe$_2$As$_2$ or FeSe system, the Fermi surface of KFe$_2$Se$_2$ is much closer to \\fs11 system since the electron sheets around $M$ is symmetric with respect to $x$-$y$ exchange. These features, as well as the absence of Fermi surface nesting, suggest that the parental KFe$_2$Se$_2$ could be regarded as an electron over-doped 11 system with possible local moment magnetism.",
                    "Electronic structure and magnetic properties for iron deficient TlFe$_{2-x}$Se$_2$ compounds are studied by first-principles calculations. We find that for the case of $x=0.5$ with a Fe-vacancy ordered orthorhombic superstructure, the ground state exhibits a stripe-like antiferromagnetic ordering and opens a sizable band gap if the short-ranged Coulomb interaction of Fe-3d electrons is moderately strong, manifesting a possible Mott insulating state. While increasing Fe-vacancies from the $x=0$ side, where the band structure is similar to that of a heavily electron-doped FeSe system, the Mott localization can be driven by kinetic energy reduction as evidenced by the band narrowing effect. Implications of this scenario in the recent experiments on TlFe$_{2-x}$Se$_2$ are discussed.",
                    "The magnetic properties and electronic structure of (K,Tl)y Fe1.6 Se2 is studied using first-principles calculations. The ground state is checkerboard antiferromagnetically coupled blocks of the minimal Fe4 squares, with a large block spin moment ~11.2{\\mu}B . The magnetic interactions could be modelled with a simple spin model involving both the inter- and intra-block, as well as the n.n. and n.n.n. couplings. The calculations also suggest a metallic ground state except for y = 0.8 where a band gap ~400 - 550 meV opens, showing an antiferromagnetic insulator ground state for (K,Tl)0.8 Fe1.6 Se2 . The electronic structure of the metallic (K,Tl)y Fe1.6 Se2 is highly 3-dimensional with unique Fermi surface structure and topology. These features indicate that the Fe-vacancy ordering is crucial to the physical properties of (K,Tl)y Fe2-x Se2 .",
                    "The electronic structure of the vacancy-ordered K$_{0.5}$Fe$_{1.75}$Se$_2$ iron-selenide compound (278 phase) is studied using the first-principles density functional method. The ground state of the 278 phase is stripe-like antiferromagnetic, and its bare electron susceptibility shows a large peak around $(\\pi, \\pi)$ in the folded Brillouin zone. Near Fermi level, the density of states are dominated by the Fe-3d orbitals, and both electron-like and hole-like Fermi surfaces appear in the Brillouin zone. Unfolded band structure shows limited similarities to a hole doped 122 phase. With 0.1e electron doping, the susceptibility peak is quickly suppressed and broadened; while the two-dimensionality of the electron-like Fermi surfaces are greatly enhanced, resulting in a better nesting behavior. Our study should be relevant to the recently reported superconducting phase K$_{0.5+x}$Fe$_{1.75+y}$Se$_2$ with both $x$ and $y$ very tiny.",
                    "The electronic structure of quasi-one-dimensional superconductor K$_2$Cr$_3$As$_3$ is studied through systematic first-principles calculations. The ground state of K$_2$Cr$_3$As$_3$ is paramagnetic but very close to a ferromagnetic instability. Close to the Fermi level, the Cr-3d$_{z^2}$, d$_{xy}$, and d$_{x^2-y^2}$ orbitals dominate the electronic states, and three bands cross $E_F$ to form one 3D Fermi surface sheet and two quasi-1D sheets. The electron DOS at $E_F$ is less than 1/3 of the experimental value, indicating an intermediate electron renormalization factor around $E_F$. Despite of the relatively small atomic numbers, the antisymmetric spin-orbit coupling splitting is sizable ($\\approx$ 60 meV) on the 3D Fermi surface sheet as well as on one of the quasi-1D sheets. Finally, the imaginary part of bare electron susceptibility shows large peaks at $\\Gamma$, suggesting the existence of large ferromagnetic spin fluctuation in the compound.",
                    "We predict an unconventional magnetic ground state in A$_y$Fe$_{1.6}$Se$_2$ with $\\sqrt{5}\\times\\sqrt{5}$ Fe-vacancy superstructure under hydraulic external pressure based on first-principles simulations. While the Fe-vacancy ordering persists up to at least $\\sim $ 12GPa, the magnetic ground state goes at $\\sim$10GPa from the BS-AFM phase to a N{\\'e}el-FM phase, a ferromagnetic arrangement of a \"{\\it{N{\\'e}el cluster}}\". The new magnetic phase is metallic, and the BS-AFM to N{\\'e}el-FM phase transition is accompanied by a sizable structural change. The two distinct magnetic phases can be understood within the extended $J_1$-$J_2$ Heisenberg model by assuming a pressure-tuned competition between the intrablock and interblock nearest-neighbor couplings of iron moments.",
                    "The relationship between magnetism and superconductivity has been one of the most discussed topics in iron-based superconductors. Using the first-principles calculations, we have studied the electronic structure of 1144-type iron-based superconductor EuRbFe$_4$As$_4$. We find the crystal field splitting of EuRbFe$_4$As$_4$ is unique, such that the $d_{z^2}$ orbitals are closer to the Fermi level $\\epsilon_F$ than the $d_{xy}$ orbitals. The RKKY interaction strength is estimated to be approximately 0.12 meV in prestine EuRbFe$_4$As$_4$. Upon Ni-doping on the Fe site, the RKKY interaction strength is barely changed upon Ni doping due to the highly anisotropic FSs and multiband effect, despite of the drastically reduced $d_{zx(y)}$ DOS at $\\epsilon_F$. Finally, in both pristine and doped compounds, the RKKY interaction is primarily mediated through bands due to Fe-$d_{z^2}$ orbitals.Our calculations suggest the RKKY interaction mediated by $d_{z^2}$ orbital is probably responsible for the magnetism in EuRbFe$_4$As$_4$ and doesn't change upon Ni-doping.",
                    "Using the state-of-art dynamical mean-field theory combined with density functional theory method, we have performed systematic study on the temperature and pressure dependent electronic structure of ferromagnetic quantum critical material candidate CeRh$_6$Ge$_4$. At -3.9 GPa and -8.3 GPa, the Ce-4$f$ occupation variation, the local magnetic susceptibility, and the low-frequency electronic self-energy behaviors suggest the Ce-4$f$ electrons are in the localized state; whereas at 6.5 GPa and 13.1 GPa, these quantities indicate the Ce-4$f$ electrons are in the itinerant state. The characteristic temperatures associated with the coherent Kondo screening is gradually suppressed to 0 around 0.8 GPa upon releasing external pressure, indicative of a local quantum critical point. Interestingly, the momentum-resolved spectrum function shows that even at the localized state side, highly anisotropic $\\mathbf{k}$-dependent hybridization between Ce-4$f$ and conduction electrons is still present along $\\Gamma$-A, causing hybridization gap in between. The calculations predict 8 Fermi surface sheets at the local-moment side and 6 sheets at the Kondo coherent state. Finally, the self-energy at 0.8 GPa can be well fitted by marginal Fermi-liquid form, giving rise to a linearly temperature dependent resistivity.",
                    "The holographic principle asserts that the entropy of a system cannot exceed its boundary area in Planck units. However, conventional quantum field theory fails to describe such systems. In this Letter, we assume the existence of large $n$ extra dimensions and propose a relationship between UV and IR cutoffs in this case. We find that if $n=2$, this effective field theory could be a good description of holographic systems. If these extra dimensions are detected in future experiments, it will help to prove the validity of the holographic principle. We also discuss implications for the cosmological constant problem.",
                    "Spin-dependent electronic transport properties of Fe-terminated zig-zag graphene nanoribbons (zGNR) have been studied using first-principles transport simulations. The spin configuration of proposed zGNR junction can be controlled with external magnetic field, and the tunneling junction show MR>1000 at small bias and is a perfect spin-filter by applying uniform external magnetic filed at small bias.",
                    "We report the first-principles study on the H-intercalated Cr-based superconductor KCr$_3$As$_3$H$_x$. Our results show a paramagnetic ground state for KCr$_3$As$_3$H. The electronic structure consists of two quasi-one-dimensional (Q1D) Fermi-surfaces and one 3D Fermi-surface which are mainly contributed by Cr-d$_{z^2}$, d$_{x^2-y^2}$ and d$_{xy}$ orbitals. The bare electron susceptibility shows a $\\Gamma$-centered imaginary peak, indicating possible ferromagnetic spin fluctuations. Upon moderate hole doping, the system undergoes a Lifshitz transition, which may enhance the Q1D feature of the system. The Bader charge analysis and electron localization functions reveal a strong bonding nature of hydrogen in KCr$_3$As$_3$H, which results in a nontrivial electron doping in KCr$_3$As$_3$H.",
                    "The single-molecule magnet $\\mathrm{[Ni(hmp)(MeOH)Cl]_4}$ is studied using both density functional theory (DFT) and the DFT+U method, and the results are compared. By incorporating a Hubbard-U like term for both the nickel and oxygen atoms, the experimentally determined ground state is successfully obtained, and the exchange coupling constants derived from the DFT+U calculation agree with experiment very well. The results show that the nickel 3d and oxygen 2p electrons in this molecule are strongly correlated, and thus the inclusion of on-site Coulomb energies is crucial to obtaining the correct results.",
                    "We have investigated structure, electronic, and magnetic properties of metal-terminated zigzag graphene nanoribbons (M-ZGNRs) by first-principles calculations. Two families of metal terminations are studied: (1) 3d-transition metals (TMs) Fe, Co, and Ni and (2) noble metals (NMs) Cu, Ag, and Au. All systems have spin-polarized edge states with antiferromagnetic (AFM) ordering between two edges, except Co-ZGNRs and Ni-ZGNRs which exhibit negligibly small energy differences between AFM and ferromagnetic states with the given ribbon width. In the AFM state the TM terminations transform semiconducting ZGNRs into metallic ones while the band gap remains in ZGNR with NM terminations. Ferromagnetic states of M-ZGNRs with TM terminations show a high degree of spin polarization at the Fermi energy. We predict a large magnetoresistance in Fe-ZGNR junctions with a low, uniform magnetic switching field.",
                    "We study the phase diagram of a square lattice Hubbard model with a perfect vacancy superstructure. The model can be also defined on a new bipartite lattice with each building blocks consisting of a minimal square. The non-interacting model is exactly solved and a mid-band gap opens at the Fermi energy in the weak inter-block hopping regime. Increasing the Coulomb interaction will develop the N\\'eel antiferromagnetic order with varying block spin moments. The metal-insulator transition with $U_{\\rm MI}$ smaller than the one without vacancies occurs above the magnetic instability $U_{\\rm M}$. The emergent intermediate magnetic metal phase develops substantially in the moderate inter-block hopping regime. Drastic increases in the ordered moment and gap magnitude on the verge of non-interacting band insulator signal a possible distinction between the magnetic semi-conductor and the Mott-insulator. The implications of these results for the recent discovered (A,Tl)$_{y}$Fe$_{2-x}$Se$_2$ compounds are discussed."
                ],
                "domain": [
                    "Condensed Matter Physics",
                    "Magnetic Materials",
                    "First-Principles Calculations",
                    "Superconductivity"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "a5636017-2b44-4f35-902e-c8c3149d0489": {
                "pk": "a5636017-2b44-4f35-902e-c8c3149d0489",
                "project_name": null,
                "name": "Tong Chen",
                "bio": "I am a researcher with a strong focus on statistical methodologies and machine learning applications in health and genomics. My work primarily revolves around optimizing two-phase sampling designs to enhance the efficiency of regression parameter estimates. I have developed multi-wave sampling strategies that leverage informative priors to improve sampling allocations, demonstrating significant gains in precision for design-based estimators.\n\nIn the realm of cancer genomics, I have integrated neural ordinary differential equations with Cox regression to model gene expression distributions over time, achieving substantial improvements in predictive performance across various cancer types. My research also extends to the robustness of deep neural network-based image compression, where I investigate vulnerabilities to adversarial attacks and propose effective defense strategies.\n\nAdditionally, I explore dataset condensation methods, focusing on their adversarial robustness and efficiency. My novel approach, based on Minimal Finite Covering, aims to enhance both dataset compression and model robustness, providing a promising avenue for future research.\n\nOverall, my work seeks to bridge the gap between theoretical statistical designs and practical applications in health research, contributing to more reliable and efficient methodologies in data analysis and machine learning.",
                "collaborators": [
                    "Thomas Lumley",
                    "Sheng Wang",
                    "Zhan Ma",
                    "Raghavendra Selvan"
                ],
                "pub_titles": [
                    "Optimal multi-wave sampling for regression modelling in two-phase designs",
                    "Optimal sampling for design-based estimators of regression models",
                    "SurvODE: Extrapolating Gene Expression Distribution for Early Cancer Identification",
                    "Towards Robust Neural Image Compression: Adversarial Attack and Model Finetuning",
                    "Choosing good subsamples for regression modelling",
                    "Is Adversarial Training with Compressed Datasets Effective?"
                ],
                "pub_abstracts": [
                    "Two-phase designs involve measuring extra variables on a subset of the cohort where some variables are already measured. The goal of two-phase designs is to choose a subsample of individuals from the cohort and analyse that subsample efficiently. It is of interest to obtain an optimal design that gives the most efficient estimates of regression parameters. In this paper, we propose a multi-wave sampling design to approximate the optimal design for design-based estimators. Influences functions are used to compute the optimal sampling allocations. We propose to use informative priors on regression parameters to derive the wave-1 sampling probabilities because any pre-specified sampling probabilities may be far from optimal and decrease efficiency. Generalised raking is used in statistical analysis. We show that a two-wave sampling with reasonable informative priors will end up with higher precision for the parameter of interest and be close to the underlying optimal design.",
                    "Two-phase designs measure variables of interest on a subcohort where the outcome and covariates are readily available or cheap to collect on all individuals in the cohort. Given limited resource availability, it is of interest to find an optimal design that includes more informative individuals in the final sample. We explore the optimal designs and efficiencies for analysis by design-based estimators. Generalized raking is an efficient design-based estimator that improves on the inverse-probability weighted (IPW) estimator by adjusting weights based on the auxiliary information. We derive a closed-form solution of the optimal design for estimating regression coefficients from generalized raking estimators. We compare it with the optimal design for analysis via the IPW estimator and other two-phase designs in measurement-error settings. We consider general two-phase designs where the outcome variable and variables of interest can be continuous or discrete. Our results show that the optimal designs for analysis by the two design-based estimators can be very different. The optimal design for IPW estimation is optimal for analysis via the IPW estimator and typically gives near-optimal efficiency for generalized raking, though we show there is potential improvement in some settings.",
                    "With the increasingly available large-scale cancer genomics datasets, machine learning approaches have played an important role in revealing novel insights into cancer development. Existing methods have shown encouraging performance in identifying genes that are predictive for cancer survival, but are still limited in modeling the distribution over genes. Here, we proposed a novel method that can simulate the gene expression distribution at any given time point, including those that are out of the range of the observed time points. In order to model the irregular time series where each patient is one observation, we integrated a neural ordinary differential equation (neural ODE) with cox regression into our framework. We evaluated our method on eight cancer types on TCGA and observed a substantial improvement over existing approaches. Our visualization results and further analysis indicate how our method can be used to simulate expression at the early cancer stage, offering the possibility for early cancer identification.",
                    "Deep neural network-based image compression has been extensively studied. However, the model robustness which is crucial to practical application is largely overlooked. We propose to examine the robustness of prevailing learned image compression models by injecting negligible adversarial perturbation into the original source image. Severe distortion in decoded reconstruction reveals the general vulnerability in existing methods regardless of their settings (e.g., network architecture, loss function, quality scale). A variety of defense strategies including geometric self-ensemble based pre-processing, and adversarial training, are investigated against the adversarial attack to improve the model's robustness. Later the defense efficiency is further exemplified in real-life image recompression case studies. Overall, our methodology is simple, effective, and generalizable, making it attractive for developing robust learned image compression solutions. All materials are made publicly accessible at https://njuvision.github.io/RobustNIC for reproducible research.",
                    "A common problem in health research is that we have a large database with many variables measured on a large number of individuals. We are interested in measuring additional variables on a subsample; these measurements may be newly available, or expensive, or simply not considered when the data were first collected. The intended use for the new measurements is to fit a regression model generalisable to the whole cohort (and to its source population). This is a two-phase sampling problem; it differs from some other two-phase sampling problems in the richness of the phase I data and in the goal of regression modelling. In particular, an important special case is measurement-error models, where a variable strongly correlated with the phase II measurements is available at phase I. We will explain how influence functions have been useful as a unifying concept for extending classical results to this setting, and describe the steps from designing for a simple weighted estimator at known parameter values through adaptive multiwave designs and the use of prior information. We will conclude with some comments on the information gap between design-based and model-based estimators in this setting.",
                    "Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed method is (1) obtained by one-time computation and is applicable for any model, (2) more effective than DC methods when applying adversarial training over MFC, (3) provably robust by minimizing the generalized adversarial loss. Additionally, empirical evaluation on three datasets shows that the proposed method is able to achieve better robustness and performance trade-off compared to DC methods such as distribution matching."
                ],
                "domain": [
                    "Statistical Design",
                    "Machine Learning",
                    "Image Compression",
                    "Health Research"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "e7123aec-c269-4306-87c7-569740c74fb5": {
                "pk": "e7123aec-c269-4306-87c7-569740c74fb5",
                "project_name": null,
                "name": "Minheng Chen",
                "bio": "I am a researcher specializing in 2D/3D medical image registration, with a focus on optimizing algorithms for surgical applications. My recent work has centered around enhancing the robustness and efficiency of registration frameworks, particularly in orthopedic surgery. I have developed a learning rate adaptation method for the covariance matrix adaptive evolution strategy (CMA-ES), which allows for effective 2D/3D registration with a smaller population size, significantly reducing computational costs while improving accuracy.\n\nIn addition to CMA-ES, I have introduced a coarse-to-fine registration framework that has shown promising results in real clinical settings, particularly in spine surgery. My exploration of deep learning has led to the creation of the Embedded Feature Similarity Optimization with Specific Parameter Initialization (SOPI) framework, which effectively addresses the challenges of dimensional mismatch and heavy computational loads in medical image registration.\n\nI am also passionate about advancing vertebrae identification techniques in CT imaging. My three-stage method leverages anatomical prior information and employs innovative clustering and supervised contrastive learning strategies to achieve state-of-the-art results in vertebrae identification tasks. My research not only aims to push the boundaries of existing methodologies but also strives to provide practical solutions that enhance surgical navigation systems and improve patient outcomes.",
                "collaborators": [
                    "Zhirun Zhang",
                    "Youyong Kong",
                    "Tonglong Li",
                    "Shuheng Gu",
                    "Zhangyang Ge",
                    "Sheng Zhang",
                    "Junxian Wu",
                    "Ziyue Zhang",
                    "Cheng Xue"
                ],
                "pub_titles": [
                    "Introducing Learning Rate Adaptation CMA-ES into Rigid 2D/3D Registration for Robotic Navigation in Spine Surgery",
                    "An Optimization-based Baseline for Rigid 2D/3D Registration Applied to Spine Surgical Navigation Using CMA-ES",
                    "Embedded Feature Similarity Optimization with Specific Parameter Initialization for 2D/3D Medical Image Registration",
                    "Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to CT Image Fusion",
                    "SpineCLUE: Automatic Vertebrae Identification Using Contrastive Learning and Uncertainty Estimation"
                ],
                "pub_abstracts": [
                    "The covariance matrix adaptive evolution strategy (CMA-ES) has been widely used in the field of 2D/3D registration in recent years. This optimization method exhibits exceptional robustness and usability for complex surgical scenarios. However, due to the inherent ill-posed nature of the 2D/3D registration task and the presence of numerous local minima in the landscape of similarity measures. Evolution strategies often require a larger population size in each generation in each generation to ensure the stability of registration and the globality and effectiveness of search, which makes the entire process computationally expensive. In this paper, we build a 2D/3D registration framework based on a learning rate adaptation CMA-ES manner. The framework employs a fixed and small population size, leading to minimized runtime and optimal utilization of computing resources. We conduct experimental comparisons between the proposed framework and other intensity-based baselines using a substantial volume of synthetic data. The results suggests that our method demonstrates superiority in both registration accuracy and running time. Code is available at github.com/m1nhengChen/CMAES-reg.",
                    "A robust and efficient optimization-based 2D/3D registration framework is crucial for the navigation system of orthopedic surgical robots. It can provide precise position information of surgical instruments and implants during surgery. While artificial intelligence technology has advanced rapidly in recent years, traditional optimization-based registration methods remain indispensable in the field of 2D/3D registration.he exceptional precision of this method enables it to be considered as a post-processing step of the learning-based methods, thereby offering a reliable assurance for registration. In this paper, we present a coarse-to-fine registration framework based on the CMA-ES algorithm. We conducted intensive testing of our method using data from different parts of the spine. The results shows the effectiveness of the proposed framework on real orthopedic spine surgery clinical data. This work can be viewed as an additional extension that complements the optimization-based methods employed in our previous studies.",
                    "We present a novel deep learning-based framework: Embedded Feature Similarity Optimization with Specific Parameter Initialization (SOPI) for 2D/3D medical image registration which is a most challenging problem due to the difficulty such as dimensional mismatch, heavy computation load and lack of golden evaluation standard. The framework we design includes a parameter specification module to efficiently choose initialization pose parameter and a fine-registration module to align images. The proposed framework takes extracting multi-scale features into consideration using a novel composite connection encoder with special training techniques. We compare the method with both learning-based methods and optimization-based methods on a in-house CT/X-ray dataset as well as simulated data to further evaluate performance. Our experiments demonstrate that the method in this paper has improved the registration performance, and thereby outperforms the existing methods in terms of accuracy and running time. We also show the potential of the proposed method as an initial pose estimator. The code is available at https://github.com/m1nhengChen/SOPI",
                    "Image-based rigid 2D/3D registration is a critical technique for fluoroscopic guided surgical interventions. In recent years, some learning-based fully differentiable methods have produced beneficial outcomes while the process of feature extraction and gradient flow transmission still lack controllability and interpretability. To alleviate these problems, in this work, we propose a novel fully differentiable correlation-driven network using a dual-branch CNN-transformer encoder which enables the network to extract and separate low-frequency global features from high-frequency local features. A correlation-driven loss is further proposed for low-frequency feature and high-frequency feature decomposition based on embedded information. Besides, a training strategy that learns to approximate a convex-shape similarity function is applied in our work. We test our approach on a in-house datasetand show that it outperforms both existing fully differentiable learning-based registration approaches and the conventional optimization-based baseline.",
                    "Vertebrae identification in arbitrary fields-of-view plays a crucial role in diagnosing spine disease. Most spine CT contain only local regions, such as the neck, chest, and abdomen. Therefore, identification should not depend on specific vertebrae or a particular number of vertebrae being visible. Existing methods at the spine-level are unable to meet this challenge. In this paper, we propose a three-stage method to address the challenges in 3D CT vertebrae identification at vertebrae-level. By sequentially performing the tasks of vertebrae localization, segmentation, and identification, the anatomical prior information of the vertebrae is effectively utilized throughout the process. Specifically, we introduce a dual-factor density clustering algorithm to acquire localization information for individual vertebra, thereby facilitating subsequent segmentation and identification processes. In addition, to tackle the issue of interclass similarity and intra-class variability, we pre-train our identification network by using a supervised contrastive learning method. To further optimize the identification results, we estimated the uncertainty of the classification network and utilized the message fusion module to combine the uncertainty scores, while aggregating global information about the spine. Our method achieves state-of-the-art results on the VerSe19 and VerSe20 challenge benchmarks. Additionally, our approach demonstrates outstanding generalization performance on an collected dataset containing a wide range of abnormal cases."
                ],
                "domain": [
                    "Medical Image Registration",
                    "Deep Learning",
                    "Optimization",
                    "Computer Vision"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ec6b799f-7649-4aae-93f2-c00703f8ceaa": {
                "pk": "ec6b799f-7649-4aae-93f2-c00703f8ceaa",
                "project_name": null,
                "name": "Yan Zhuang",
                "bio": "I am a researcher with a deep interest in combinatorial enumeration and the intricate relationships between permutation statistics and algebraic structures. My work began with a focus on generalizing Ira Gessel's reciprocity formula for noncommutative symmetric functions, which allowed me to explore a broader range of restrictions on increasing run lengths in permutations. This foundational research has led to significant advancements in enumerating permutations with parity restrictions on peaks and valleys, as well as generating functions for various permutation statistics.\n\nI have extensively utilized the Goulden-Jackson cluster method, adapting it for monoid networks and permutations, which has enabled me to derive closed-form and continued fraction formulas for counting Motzkin paths and other combinatorial structures. My research also delves into the connections between descent statistics and Eulerian polynomials, where I have established several identities and generating functions that enhance our understanding of these relationships.\n\nIn addition to my theoretical work, I have developed a framework for shuffle-compatible permutation statistics, which bridges combinatorial and algebraic perspectives. This has allowed me to unify previous results and provide new insights into the distribution of permutation statistics over shuffles. My recent explorations into cyclic permutations and their associated statistics have further enriched the field, revealing deep connections between combinatorial objects and algebraic structures.\n\nOverall, my research aims to uncover the underlying patterns and structures in combinatorial enumeration, contributing to both theoretical advancements and practical applications in the field.",
                "collaborators": [
                    "Justin M. Troyka",
                    "Ira M. Gessel",
                    "Yuxuan Zhang",
                    "Nadia Lafrenière",
                    "Jun Hu",
                    "T. Kyle Petersen",
                    "Jordan O. Tirrell",
                    "Jinting Liang",
                    "Bruce E. Sagan",
                    "M. Crossan Cooper"
                ],
                "pub_titles": [
                    "Counting permutations by runs",
                    "A generalized Goulden-Jackson cluster method and lattice path enumeration",
                    "Eulerian polynomials and descent statistics",
                    "A lifting of the Goulden-Jackson cluster method to the Malvenuto-Reutenauer algebra",
                    "A subfamily of skew Dyck paths related to $k$-ary trees",
                    "On the $\\operatorname{rix}$ statistic and valley-hopping",
                    "A Broad and General Sequential Sampling Scheme",
                    "Zig-zag Eulerian polynomials",
                    "Hopping from Chebyshev polynomials to permutation statistics",
                    "Fibonacci numbers, consecutive patterns, and inverse peaks",
                    "Plethystic formulas for permutation enumeration",
                    "Cyclic shuffle-compatibility via cyclic shuffle algebras",
                    "On the joint distribution of cyclic valleys and excedances over conjugacy classes of $\\mathfrak{S}_{n}$",
                    "Shuffle-compatible permutation statistics",
                    "Statistics on clusters and $r$-Stirling permutations"
                ],
                "pub_abstracts": [
                    "In his Ph.D. thesis, Ira Gessel proved a reciprocity formula for noncommutative symmetric functions which enables one to count words and permutations with restrictions on the lengths of their increasing runs. We generalize Gessel's theorem to allow for a much wider variety of restrictions on increasing run lengths, and use it to complete the enumeration of permutations with parity restrictions on peaks and valleys, and to give a systematic method for obtaining generating functions for permutation statistics that are expressible in terms of increasing runs. Our methods can also be used to obtain analogous results for alternating runs in permutations.",
                    "The Goulden-Jackson cluster method is a powerful tool for obtaining generating functions for counting words in a free monoid by occurrences of a set of subwords. We introduce a generalization of the cluster method for monoid networks, which generalize the combinatorial framework of free monoids. As a sample application of the generalized cluster method, we compute bivariate and multivariate generating functions counting Motzkin paths---both with height bounded and unbounded---by statistics corresponding to the number of occurrences of various subwords, yielding both closed-form and continued fraction formulae.",
                    "We prove several identities expressing polynomials counting permutations by various descent statistics in terms of Eulerian polynomials, extending results of Stembridge, Petersen, and Br\\\"and\\'en. Additionally, we find $q$-exponential generating functions for $q$-analogues of these descent statistic polynomials that also keep track of the inversion number or inverse major index. We also present identities relating several of these descent statistic polynomials to refinements of type B Eulerian polynomials and flag descent polynomials by the number of negative letters of a signed permutation. Our methods include permutation enumeration techniques involving noncommutative symmetric functions, Br\\\"and\\'en's modified Foata-Strehl action, and a group action of Petersen on signed permutations. Notably, the modified Foata-Strehl action yields an analogous relation between Narayana polynomials and the joint distribution of the peak number and descent number over 231-avoiding permutations, which we also interpret in terms of binary trees and Dyck paths.",
                    "The Goulden-Jackson cluster method is a powerful tool for counting words by occurrences of prescribed subwords, and was adapted by Elizalde and Noy for counting permutations by occurrences of prescribed consecutive patterns. In this paper, we lift the cluster method for permutations to the Malvenuto-Reutenauer algebra. Upon applying standard homomorphisms, our result specializes to both the cluster method for permutations as well as a q-analogue which keeps track of the inversion number statistic. We construct additional homomorphisms using the theory of shuffle-compatibility, leading to further specializations which keep track of various \"inverse statistics\", including the inverse descent number, inverse peak number, and inverse left peak number. This approach is then used to derive formulas for counting permutations by occurrences of two families of consecutive patterns -- monotone patterns and transpositional patterns -- refined by these statistics.",
                    "We introduce a subfamily of skew Dyck paths called box paths and show that they are in bijection with pairs of ternary trees, confirming an observation stated previously on the On-Line Encyclopedia of Integer Sequences. More generally, we define $k$-box paths, which are in bijection with $(k+1)$-tuples of $(k+2)$-ary trees. A bijection is given between $k$-box paths and a subfamily of $k_{t}$-Dyck paths, as well as a bijection with a subfamily of $(k,\\ell)$-threshold sequences. We also study the refined enumeration of $k$-box paths by the number of returns and the number of long ascents. Notably, the distribution of long ascents over $k$-box paths generalizes the Narayana distribution on Dyck paths, and we find that $(k-3)$-box paths with exactly two long ascents provide a combinatorial model for the second $k$-gonal numbers.",
                    "This paper studies the relationship between the modified Foata$\\unicode{x2013}$Strehl action (a.k.a. valley-hopping)$\\unicode{x2014}$a group action on permutations used to demonstrate the $\\gamma$-positivity of the Eulerian polynomials$\\unicode{x2014}$and the number of rixed points $\\operatorname{rix}$$\\unicode{x2014}$a recursively-defined permutation statistic introduced by Lin in the context of an equidistribution problem. We give a linear-time iterative algorithm for computing the set of rixed points, and prove that the $\\operatorname{rix}$ statistic is homomesic under valley-hopping. We also demonstrate that a bijection $\\Phi$ introduced by Lin and Zeng in the study of the $\\operatorname{rix}$ statistic sends orbits of the valley-hopping action to orbits of a cyclic version of valley-hopping, which implies that the number of fixed points $\\operatorname{fix}$ is homomesic under cyclic valley-hopping.",
                    "In this paper, we propose a broad and general sequential sampling scheme, which incorporates four different types of sampling procedures: i) the classic Anscombe-Chow-Robbins purely sequential sampling procedure; ii) the ordinary accelerated sequential sampling procedure; iii) the relatively new k-at-a-time purely sequential sampling procedure; iv) the new k-at-a-time improved accelerated sequential sampling procedure. The first-order and second-order properties of this general sequential sampling scheme are fully investigated with two illustrations on minimum risk point estimation for the mean of a normal distribution and on bounded variance point estimation for the location parameter of a negative exponential distribution, respectively. We also provide extensive computational simulation studies and real data analyses for each illustration.",
                    "For any finite partially ordered set $P$, the $P$-Eulerian polynomial is the generating function for the descent number over the set of linear extensions of $P$, and is closely related to the order polynomial of $P$ arising in the theory of $P$-partitions. Here we study the $P$-Eulerian polynomial where $P$ is a naturally labeled zig-zag poset; we call these zig-zag Eulerian polynomials. A result of Br\\\"and\\'en implies that these polynomials are gamma-nonnegative, and hence their coefficients are symmetric and unimodal. The zig-zag Eulerian polynomials and the associated order polynomials have appeared fleetingly in the literature in a wide variety of contexts$\\unicode{x2014}$e.g., in the study of polytopes, magic labelings of graphs, and Kekul\\'e structures$\\unicode{x2014}$but they do not appear to have been studied systematically.   In this paper, we use a \"relaxed\" version of $P$-partitions to both survey and unify results. Our technique shows that the zig-zag Eulerian polynomials also capture the distribution of \"big returns\" over the set of (up-down) alternating permutations, as first observed by Coons and Sullivant. We develop recurrences for refined versions of the relevant generating functions, which evoke similarities to recurrences for the classical Eulerian polynomials. We conclude with a literature survey and open questions.",
                    "We prove various formulas which express exponential generating functions counting permutations by the peak number, valley number, double ascent number, and double descent number statistics in terms of the exponential generating function for Chebyshev polynomials, as well as cyclic analogues of these formulas for derangements. We give several applications of these results, including formulas for the $(-1)$-evaluation of some of these distributions. Our proofs are combinatorial and involve the use of monomino-domino tilings, the modified Foata-Strehl action (a.k.a. valley-hopping), and a cyclic analogue of this action due to Sun and Wang.",
                    "We give multiple proofs of two formulas concerning the enumeration of permutations avoiding a monotone consecutive pattern with a certain value for the inverse peak number or inverse left peak number statistic. The enumeration in both cases is given by a sequence related to Fibonacci numbers. We also show that there is exactly one permutation whose inverse peak number is zero among all permutations with any fixed descent composition, and we give a few elementary consequences of this fact. Our proofs involve generating functions, symmetric functions, regular expressions, and monomino-domino tilings.",
                    "We prove several general formulas for the distributions of various permutation statistics over any set of permutations whose quasisymmetric generating function is a symmetric function. Our formulas involve certain kinds of plethystic substitutions on quasisymmetric generating functions, and the permutation statistics we consider include the descent number, peak number, left peak number, and the number of up-down runs. We apply these results to cyclic permutations, involutions, and derangements, and more generally, to derive formulas for counting all permutations by the above statistics jointly with the number of fixed points and jointly with cycle type. A number of known formulas are recovered as special cases of our results, including formulas of D\\'esarm\\'enien-Foata, Gessel-Reutenauer, Stembridge, Fulman, Petersen, Diaconis-Fulman-Holmes, Zhuang, and Athanasiadis.",
                    "A permutation statistic $\\operatorname{st}$ is said to be shuffle-compatible if the distribution of $\\operatorname{st}$ over the set of shuffles of two disjoint permutations $\\pi$ and $\\sigma$ depends only on $\\operatorname{st}\\pi$, $\\operatorname{st}\\sigma$, and the lengths of $\\pi$ and $\\sigma$. Shuffle-compatibility is implicit in Stanley's early work on $P$-partitions, and was first explicitly studied by Gessel and Zhuang, who developed an algebraic framework for shuffle-compatibility centered around their notion of the shuffle algebra of a shuffle-compatible statistic. For a family of statistics called descent statistics, these shuffle algebras are isomorphic to quotients of the algebra of quasisymmetric functions.   Recently, Domagalski, Liang, Minnich, Sagan, Schmidt, and Sietsema defined a version of shuffle-compatibility for statistics on cyclic permutations, and studied cyclic shuffle-compatibility through purely combinatorial means. In this paper, we define the cyclic shuffle algebra of a cyclic shuffle-compatible statistic, and develop an algebraic framework for cyclic shuffle-compatibility in which the role of quasisymmetric functions is replaced by the cyclic quasisymmetric functions recently introduced by Adin, Gessel, Reiner, and Roichman. We use our theory to provide explicit descriptions for the cyclic shuffle algebras of various cyclic permutation statistics, which in turn gives algebraic proofs for their cyclic shuffle-compatibility.",
                    "We derive a formula expressing the joint distribution of the cyclic valley number and excedance number statistics over a fixed conjugacy class of the symmetric group in terms of Eulerian polynomials. Our proof uses a slight extension of Sun and Wang's cyclic valley-hopping action as well as a formula of Brenti. Along the way, we give a new proof for the $\\gamma$-positivity of the excedance number distribution over any fixed conjugacy class along with a combinatorial interpretation of the $\\gamma$-coefficients.",
                    "Since the early work of Richard Stanley, it has been observed that several permutation statistics have a remarkable property with respect to shuffles of permutations. We formalize this notion of a shuffle-compatible permutation statistic and introduce the shuffle algebra of a shuffle-compatible permutation statistic, which encodes the distribution of the statistic over shuffles of permutations. This paper develops a theory of shuffle-compatibility for descent statistics (statistics that depend only on the descent set and length) which has close connections to the theory of $P$-partitions, quasisymmetric functions, and noncommutative symmetric functions. We use our framework to prove that many descent statistics are shuffle-compatible and to give explicit descriptions of their shuffle algebras, thus unifying past results of Stanley, Gessel, Stembridge, Aguiar-Bergeron-Nyman, and Petersen.",
                    "The Goulden$\\unicode{x2013}$Jackson cluster method, adapted to permutations by Elizalde and Noy, reduces the problem of counting permutations by occurrences of a prescribed consecutive pattern to that of counting clusters, which are special permutations with a lot of structure. Recently, Zhuang found a generalization of the cluster method which specializes to refinements by additional permutation statistics, namely the inverse descent number $\\operatorname{ides}$, the inverse peak number $\\operatorname{ipk}$, and the inverse left peak number $\\operatorname{ilpk}$. Continuing this line of work, we study the enumeration of $2134\\cdots m$-clusters by $\\operatorname{ides}$, $\\operatorname{ipk}$, and $\\operatorname{ilpk}$, which allows us to derive formulas for counting permutations by occurrences of the consecutive pattern $2134\\cdots m$ jointly with each of these statistics. Analogous results for the pattern $12\\cdots (m-2)m(m-1)$ are obtained via symmetry arguments. Along the way, we discover that $2134\\cdots (r+1)$-clusters are equinumerous with $r$-Stirling permutations introduced by Gessel and Stanley, and we establish some joint equidistributions between these two families of permutations."
                ],
                "domain": [
                    "Combinatorics",
                    "Permutation Statistics",
                    "Generating Functions",
                    "Algebraic Combinatorics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b2508e8d-5878-4125-a204-9bff08cec5e8": {
                "pk": "b2508e8d-5878-4125-a204-9bff08cec5e8",
                "project_name": null,
                "name": "Xiang Li",
                "bio": "I am a researcher with a diverse background in complex networks, dynamical systems, and computational methods. My recent work has focused on understanding collective behaviors in random complex networks, particularly in the context of limit-cycle oscillators and their synchronization properties. I have explored the criticality of collective synchronization, revealing that it remains consistent across various network topologies.\n\nIn addition to my work on synchronization, I have delved into the intricacies of cloud computing, addressing the pressing issue of trust in cloud service selection. I developed the FASTCloud framework, which employs a novel QoS-based trust assessment model to help potential cloud consumers identify trustworthy cloud services amidst a dynamic environment.\n\nMy research also extends to mathematical modeling, where I have investigated the existence and asymptotic behavior of solutions to the Hessian equation, as well as the control of disease propagation in small-world networks. I am particularly interested in the interplay between network structure and dynamic processes, which has led me to study the bifurcation behaviors in nonlinear spreading models.\n\nFurthermore, I have contributed to the field of computational linguistics by proposing a new method for bragging classification on social media, addressing data imbalance issues through disentangled representation and domain-aware strategies. My work is driven by a commitment to advancing our understanding of complex systems and their applications across various domains.",
                "collaborators": [
                    "Jiguang Bao",
                    "Guanrong Chen",
                    "Yucheng Zhou",
                    "Yang Liu"
                ],
                "pub_titles": [
                    "Uniform synchronous criticality of diversely random complex networks",
                    "Comment on \"Comparison of six simulation codes for positive streamers in air\"(Plasma Sources Sci. Technol. 27 (2018) 095002)",
                    "Sync in Complex Dynamical Networks: Stability, Evolution, Control, and Application",
                    "The stable property of Newton slopes for general Witt towers",
                    "FASTCloud: A novel framework of assessment and selection for trustworthy cloud service",
                    "Existence and asymptotic behavior of entire large solutions for Hessian equations",
                    "Controlling the spreading in small-world networks",
                    "Disentangled and Robust Representation Learning for Bragging Classification in Social Media",
                    "Simulation of adaptive feedforward control for magnetic alloy cavity"
                ],
                "pub_abstracts": [
                    "We investigate collective synchronous behaviors in random complex networks of limit-cycle oscillators with the non-identical asymmetric coupling scheme, and find a uniform coupling criticality of collective synchronization which is independent of complexity of network topologies. Numerically simulations on categories of random complex networks have verified this conclusion.",
                    "Recently, a comparison of six codes for streamer discharge simulations were performed in [1]. In this comment, we discuss about the big differences between the results obtained by the different codes using the same deterministic model, and raise questions on the convergence of the codes and the minimum spatial resolution that are required for a converged results.",
                    "In the past few years, the discoveries of small-world and scale-free properties of many natural and artificial complex networks have stimulated significant advances in better understanding the relationship between the topology and the collective dynamics of complex networks. This paper reports recent progresses in the literature of synchronization of complex dynamical networks including stability criteria, network synchronizability and uniform synchronous criticality in different topologies, and the connection between control and synchronization of complex networks as well. The economic-cycle synchronous phenomenon in the World Trade Web, a scale-free type of social economic networks, is used to illustrate an application of the network synchronization mechanism.",
                    "Any polynomial $f(x)\\in\\mathbb{Z}_q[x]$ defines a Witt vector $[f]\\in W(\\mathbb{F}_q[x])$. Consider the Artin-Schreier-Witt tower $y^F-y=[f]$. This is a tower of curves over $\\mathbb{F}_q$, with total Galois group $\\mathbb{Z}_p$. We want to study the Newton slopes of zeta functions of this tower. We reduce it to the Newton polygons of L-functions associated with characters on the Galois groups. We prove that, when the conductors are large enough, these Newton slopes are unions of arithmetic progressions which are changing proportionally as the conductor increases. This is a generalization of the result of \\cite{Da}, where they get the same result in the case the non-zero coefficients of $f(x)$ are roots of unity. To overcome the new difficulty in our process, we apply some $(p^{\\theta},T)$-topology.",
                    "By virtue of technology and benefit advantages, cloud computing has increasingly attracted a large number of potential cloud consumers (PCC) plan to migrate the traditional business to the cloud service. However, trust has become one of the most challenging issues that prevent the PCC from adopting cloud services, especially in trustworthy cloud service selection. Besides, due to the diversity and dynamic of quality of service (QoS) in the cloud environment, the existing trust assessment methods based on the single constant value of QoS attribute and the subjective weight assignment are not good enough to provide an effective solution for PCCs to identify and select a trustworthy cloud service among a wide range of functionally-equivalent cloud service providers (CSPs). To address the challenge, a novel assessment and selection framework for trustworthy cloud service, FASTCloud, is proposed in this study. This framework facilitates PCCs to select a trustworthy cloud service based on their actual QoS requirements. In order to accurately and efficiently assess the trust level of cloud services, a QoS-based trust assessment model is proposed. This model represents a trust level assessment method based on the interval multiple attributes with an objective weight assignment method based on the deviation maximization to adaptively determine the trust level of different cloud services provisioned by candidate CSPs. The advantage of the proposed trust level assessment method in time complexity is demonstrated by the performance analysis and comparison. The experimental result of a case study with an open-source dataset shows that the trust model is efficient in cloud service trust assessment and the FASTCloud can effectively help PCCs select a trustworthy cloud service.",
                    "In this paper, we give some existence and nonexistence results for nonradial entire large solutions of the Hessian equation $S_k\\left(D^2 u\\right)=b(x) u^\\gamma$ in the sublinear case $0<\\gamma<k$. The exact asymptotic behavior of large solutions at infinity is also studied when $b(x)$ is the oscillation of a radial function $|x|^{-l}$ at infinity for $l\\leq k-1$.",
                    "The spreading (propagation) of diseases, viruses, and disasters such as power blackout through a huge-scale and complex network is one of the most concerned issues today. In this paper, we study the control of such spreading in a nonlinear spreading model of small-world networks. We found that the short-cut adding probability $p$ in the N-W model \\cite{N-W:1999} of small-world networks determines the Hopf bifurcation and other bifurcating behaviors in the proposed model. We further show a control technique that stabilize a periodic spreading behavior onto a stable equilibrium over the proposed model of small-world networks.",
                    "Researching bragging behavior on social media arouses interest of computational (socio) linguists. However, existing bragging classification datasets suffer from a serious data imbalance issue. Because labeling a data-balance dataset is expensive, most methods introduce external knowledge to improve model learning. Nevertheless, such methods inevitably introduce noise and non-relevance information from external knowledge. To overcome the drawback, we propose a novel bragging classification method with disentangle-based representation augmentation and domain-aware adversarial strategy. Specifically, model learns to disentangle and reconstruct representation and generate augmented features via disentangle-based representation augmentation. Moreover, domain-aware adversarial strategy aims to constrain domain of augmented features to improve their robustness. Experimental results demonstrate that our method achieves state-of-the-art performance compared to other methods.",
                    "The upgrade plan of the China Spallation Neutron Source aims to enhance the beam power from 100 kW to 500 kW. To achieve this, the plan involves incorporating three new magnetic alloy cavities while maintaining the existing system to enable double harmonic acceleration. As a consequence of the increased current intensity, the beam loading effect will be significantly amplified, presenting a considerable challenge for the low-level RF control system of the magnetic alloy cavity. To address this challenge, an adaptive feedforward algorithm has been developed to enable optimal control. In addition, comprehensive simulations of the algorithm have been successfully conducted to validate its."
                ],
                "domain": [
                    "Complex Networks",
                    "Synchronization",
                    "Cloud Computing",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "4c021860-f9d4-4753-841d-fbe3e9a2e1c7": {
                "pk": "4c021860-f9d4-4753-841d-fbe3e9a2e1c7",
                "project_name": null,
                "name": "Rongjie Liu",
                "bio": "I am a researcher dedicated to advancing the fields of image compression and brain connectomics. My recent work focuses on developing innovative methods that enhance the efficiency and effectiveness of multi-dimensional image compression. One of my key contributions is the Compression via Adaptive Recursive Partitioning (CARP) method, which leverages a Bayesian probabilistic model to achieve high-quality compression across various image types and dimensions. CARP not only excels in preserving local details but also offers computational scalability and ease of tuning, outperforming traditional methods like JPEG and HEVC in extensive numerical experiments.\n\nIn addition to my work in image processing, I am passionate about understanding the brain's structural connectome and its relationship to human traits. I have introduced a novel tractography-based representation that clusters fiber endpoints to create a data-adaptive parcellation, known as Principal Parcellation Analysis (PPA). This approach reduces subjectivity in the analysis of brain connectivity and enhances the power of statistical predictions regarding human traits. By utilizing data from the Human Connectome Project, I have demonstrated that PPA significantly improves upon classical connectome methods while maintaining interpretability.\n\nI am committed to making my research accessible, as evidenced by the public availability of my PPA package on GitHub, enabling others to implement these techniques in their own diffusion image data analyses. Through my work, I aim to bridge the gap between complex data representation and practical applications in neuroscience and imaging technology.",
                "collaborators": [
                    "Meng Li",
                    "Li Ma",
                    "David B. Dunson"
                ],
                "pub_titles": [
                    "Efficient in-situ image and video compression through probabilistic image representation",
                    "PPA: Principal Parcellation Analysis for Brain Connectomes and Multiple Traits"
                ],
                "pub_abstracts": [
                    "Fast and effective image compression for multi-dimensional images has become increasingly important for efficient storage and transfer of massive amounts of high-resolution images and videos. Desirable properties in compression methods include (1) high reconstruction quality at a wide range of compression rates while preserving key local details, (2) computational scalability, (3) applicability to a variety of different image/video types and of different dimensions, (4) progressive transmission, and (5) ease of tuning. We present such a method for multi-dimensional image compression called Compression via Adaptive Recursive Partitioning (CARP). CARP uses an optimal permutation of the image pixels inferred from a Bayesian probabilistic model on recursive partitions of the image to reduce its effective dimensionality, achieving a parsimonious representation that preserves information. CARP uses a multi-layer Bayesian hierarchical model to achieve in-situ compression along with self-tuning and regularization, with just one single parameter to be specified by the user to achieve the desired compression rate. Extensive numerical experiments using a variety of datasets including 2D still images, real-life YouTube videos, and surveillance videos show that CARP dominates the state-of-the-art image/video compression approaches---including JPEG, JPEG2000, BPG, MPEG4, HEVC and a neural network-based method---for all of these different image types and on nearly all of the individual images and videos over some methods.",
                    "Our understanding of the structure of the brain and its relationships with human traits is largely determined by how we represent the structural connectome. Standard practice divides the brain into regions of interest (ROIs) and represents the connectome as an adjacency matrix having cells measuring connectivity between pairs of ROIs. Statistical analyses are then heavily driven by the (largely arbitrary) choice of ROIs. In this article, we propose a novel tractography-based representation of brain connectomes, which clusters fiber endpoints to define a data adaptive parcellation targeted to explain variation among individuals and predict human traits. This representation leads to Principal Parcellation Analysis (PPA), representing individual brain connectomes by compositional vectors building on a basis system of fiber bundles that captures the connectivity at the population level. PPA reduces subjectivity and facilitates statistical analyses. We illustrate the proposed approach through applications to data from the Human Connectome Project (HCP) and show that PPA connectomes improve power in predicting human traits over state-of-the-art methods based on classical connectomes, while dramatically improving parsimony and maintaining interpretability. Our PPA package is publicly available on GitHub, and can be implemented routinely for diffusion image data."
                ],
                "domain": [
                    "Image Compression",
                    "Brain Connectomics",
                    "Bayesian Modeling",
                    "Data Analysis"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "6b305388-fb95-45fb-b0e9-11de16e7d442": {
                "pk": "6b305388-fb95-45fb-b0e9-11de16e7d442",
                "project_name": null,
                "name": "Chao Huang",
                "bio": "I am a researcher specializing in matching theory, combinatorial optimization, and recommender systems. My work primarily focuses on two-sided many-to-one matching, where I have developed integer programming approaches to explore stable matchings in various market settings. I have investigated the existence of stable matchings under conditions such as total unimodularity and balancedness, and I have provided classes of preference profiles that satisfy these conditions, demonstrating the compatibility of stable matching with diverse firm preferences.\n\nIn addition to theoretical advancements, I have delved into practical applications, such as the design of recommender systems that leverage heterogeneous relational learning. My survey on this topic highlights the evolution of recommendation frameworks, emphasizing the importance of mapping complex user-item relationships into latent representation spaces. I explore various learning approaches, including matrix factorization and graph neural networks, to distill contextual information effectively.\n\nMy research not only contributes to the theoretical foundations of matching and optimization but also addresses real-world challenges in recommendation systems, aiming to enhance user experiences in an increasingly data-driven world. I am passionate about bridging the gap between theory and application, and I continuously seek innovative solutions to complex problems in my field.",
                "collaborators": [],
                "pub_titles": [
                    "Stable matching: an integer programming approach",
                    "Two-sided matching with firms' complementary preferences",
                    "Firm-worker hypergraphs",
                    "Concave many-to-one matching",
                    "Unidirectional substitutes and complements",
                    "Multilateral matching with scale economies",
                    "Balanced paring of $\\{1,2,\\ldots,(p-1)/2\\}$ for $p\\equiv 1 \\pmod{4}$",
                    "Legendre Symbol of $\\prod f(i,j) $ over $ 0<i<j<p/2, \\ p\\nmid f(i,j) $",
                    "Recent Advances in Heterogeneous Relation Learning for Recommendation"
                ],
                "pub_abstracts": [
                    "This paper develops an integer programming approach to two-sided many-to-one matching by investigating stable integral matchings of a fictitious market where each worker is divisible. We show that stable matchings exist in a discrete matching market when firms' preference profile satisfies a total unimodularity condition that is compatible with various forms of complementarities. We provide a class of firms' preference profiles that satisfy this condition.",
                    "This paper studies two-sided many-to-one matching in which firms have complementary preferences. We show that stable matchings exist under a balancedness condition that rules out a specific type of odd-length cycles formed by firms' acceptable sets. We also provide a class of preference profiles that satisfy this condition. Our results indicate that stable matching is compatible with a wide range of firms' complementary preferences.",
                    "A firm-worker hypergraph consists of edges in which each edge joins a firm and its possible employees. We show that a stable matching exists in both many-to-one matching with transferable utilities and discrete many-to-one matching when the firm-worker hypergraph has no nontrivial odd-length cycle. Firms' preferences satisfying this condition arise in a problem of matching specialized firms with specialists.",
                    "We propose a notion of concavity in two-sided many-to-one matching, which is an analogue to the balancedness condition in cooperative games. A stable matching exists when the market is concave. We provide a class of concave markets. In the proof of the existence theorem, we use Scarf's algorithm to find a stable schedule matching, which is of independent interest.",
                    "In discrete matching markets, substitutes and complements can be unidirectional between two groups of workers when members of one group are more important or competent than those of the other group for firms. We show that a stable matching exists and can be found by a two-stage Deferred Acceptance mechanism when firms' preferences satisfy a unidirectional substitutes and complements condition. This result applies to both firm-worker matching and controlled school choice. Under the framework of matching with continuous monetary transfers and quasi-linear utilities, we show that substitutes and complements are bidirectional for a pair of workers.",
                    "This paper studies multilateral matching in which any set of agents can negotiate contracts. We assume scale economies in the sense that an agent substitutes some contracts with some new contracts only if the newly signed contracts involve a weakly larger set of partners. We show that a weakly setwise stable outcome exists in a market with scale economies and a setwise stable outcome exists under a stronger scale economies condition. Our conditions apply to environments in which more partners bring advantages, and allow agents to bargain over contracts signed by them.",
                    "Let $p\\equiv 1 \\pmod{4}$ be a prime. Write $t = \\prod_{x=1}^{(p-1)/2}x$. Since $t ^2\\equiv -1 \\pmod{p}$ , we can divide $\\{1,2,\\ldots,(p-1)/2\\}$ into $(p-1)/4$ ordered pairs so that each pair, say $<a,\\tilde{a}>$ , satisfies that $t a \\equiv \\pm \\tilde{a} \\pmod{p}.$ For any two such pairs, assume $a<\\tilde{a}, b<\\tilde{b}, a<b $, then there are three possibilities for their relative order : $a<\\tilde{a} < b< \\tilde{b}$ , $a< b < \\tilde{a} < \\tilde{b}$ , $a< b < \\tilde{b}< \\tilde{a}$. We show this paring is balanced in the sense that the three cases occur with equal frequencies. Utilizing properties of this paring we solve one problem raised by Zhi-Wei Sun concerning the sign of permutation related to quadratic residues.",
                    "Let $p>3$ be a prime. We investigate Legendre symbol of $\\displaystyle \\prod_{0<i<j<p/2 \\atop p\\nmid f(i,j) } f(i,j) \\ $, where $i,j\\in \\Bbb Z, f(i,j)$ is a linear or quadratic form with integer coefficients. When $f=ai^2+bij+cj^2$ and $p\\nmid c(a+b+c)$ , we prove that to evaluate the product is equivalent to determine $ \\displaystyle \\sum_{y=1}^{p-1} \\bigg(\\frac{y(y+1)(y+k)}{p}\\bigg) \\pmod{16}$ , where $4c(a+b+c)k \\equiv (4ac-b^2)\\pmod{p}.$ Parallel results are given for $\\displaystyle \\prod_{i,j=1 \\atop p\\nmid f(i,j) }^{(p-1)/2} \\bigg(\\frac{ f(i,j) }{p}\\bigg).$ Then we show that $ \\displaystyle \\sum_{y=1}^{p-1} \\bigg(\\frac{y(y+1)(y+k)}{p}\\bigg) \\pmod{16}$ can be evaluated explicitly when k=2,4,5,9,10 or k is a square. And for several classes of f(i,j) these two kinds of products can be evaluated explicitly. Finally when f is a linear form we give unified identities for these products. Thus we prove these kind of problems raised in Sun's paper.",
                    "Recommender systems have played a critical role in many web applications to meet user's personalized interests and alleviate the information overload. In this survey, we review the development of recommendation frameworks with the focus on heterogeneous relational learning, which consists of different types of dependencies among users and items. The objective of this task is to map heterogeneous relational data into latent representation space, such that the structural and relational properties from both user and item domain can be well preserved. To address this problem, recent research developments can fall into three major lines: social recommendation, knowledge graph-enhanced recommender system, and multi-behavior recommendation. We discuss the learning approaches in each category, such as matrix factorization, attention mechanism and graph neural networks, for effectively distilling heterogeneous contextual information. Finally, we present an exploratory outlook to highlight several promising directions and opportunities in heterogeneous relational learning frameworks for recommendation."
                ],
                "domain": [
                    "Matching Theory",
                    "Graph Theory",
                    "Recommender Systems",
                    "Heterogeneous Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b22107c8-133b-4a8c-913f-a27c6a6ab7b6": {
                "pk": "b22107c8-133b-4a8c-913f-a27c6a6ab7b6",
                "project_name": null,
                "name": "Wentao Li",
                "bio": "I am a researcher specializing in statistical methods and Bayesian inference, with a particular focus on approximate Bayesian computation (ABC) and its applications in complex models. My work has explored the asymptotic properties of regression-adjusted ABC, demonstrating how appropriate bandwidth choices can lead to accurate posterior quantification of uncertainty. I have also investigated the asymptotic variance of estimators in large-data limits, proving the importance of dimensionally appropriate summary statistics.\n\nIn addition to theoretical advancements, I have developed practical methodologies such as dMEGA, a federated approach for generalized linear mixed model-based association testing that addresses privacy concerns while allowing for flexible modeling of genetic data. My research also delves into composite likelihood methods, where I have shown how ABC can enhance efficiency and accuracy in inference when dealing with intractable likelihoods.\n\nRecently, I have extended my work to include the topological properties of rotating black holes in general relativity, revealing significant insights into their behavior under various conditions. My latest contributions include reframing ABC within a frequentist context, leading to the development of approximate confidence distribution computing, which broadens the applicability of ABC while maintaining inferential integrity.\n\nOverall, my research aims to bridge theoretical advancements with practical applications, enhancing the robustness and efficiency of statistical inference in complex and high-dimensional settings.",
                "collaborators": [
                    "Paul Fearnhead",
                    "Han Chen",
                    "Xiaoqian Jiang",
                    "Arif Harmanci",
                    "Rosabeth White",
                    "Dennis Prangle",
                    "Wentao Liu",
                    "Li Zhang",
                    "Di Wu",
                    "Jieci Wang"
                ],
                "pub_titles": [
                    "Convergence of Regression Adjusted Approximate Bayesian Computation",
                    "On the Asymptotic Efficiency of Approximate Bayesian Computation Estimators",
                    "Federated Generalized Linear Mixed Models for Collaborative Genome-wide Association Studies",
                    "Optimal combination of composite likelihoods using approximate Bayesian computation with application to state-space models",
                    "Thermodynamic topological classes of the rotating, accelerating black holes",
                    "An effective likelihood-free approximate computing method with statistical inferential guarantees"
                ],
                "pub_abstracts": [
                    "We present asymptotic results for the regression-adjusted version of approximate Bayesian computation introduced by Beaumont(2002). We show that for an appropriate choice of the bandwidth, regression adjustment will lead to a posterior that, asymptotically, correctly quantifies uncertainty. Furthermore, for such a choice of bandwidth we can implement an importance sampling algorithm to sample from the posterior whose acceptance probability tends to unity as the data sample size increases. This compares favourably to results for standard approximate Bayesian computation, where the only way to obtain a posterior that correctly quantifies uncertainty is to choose a much smaller bandwidth; one for which the acceptance probability tends to zero and hence for which Monte Carlo error will dominate.",
                    "Many statistical applications involve models for which it is difficult to evaluate the likelihood, but from which it is relatively easy to sample. Approximate Bayesian computation is a likelihood-free method for implementing Bayesian inference in such cases. We present results on the asymptotic variance of estimators obtained using approximate Bayesian computation in a large-data limit. Our key assumption is that the data are summarized by a fixed-dimensional summary statistic that obeys a central limit theorem. We prove asymptotic normality of the mean of the approximate Bayesian computation posterior. This result also shows that, in terms of asymptotic variance, we should use a summary statistic that is the same dimension as the parameter vector, p; and that any summary statistic of higher dimension can be reduced, through a linear transformation, to dimension p in a way that can only reduce the asymptotic variance of the posterior mean. We look at how the Monte Carlo error of an importance sampling algorithm that samples from the approximate Bayesian computation posterior affects the accuracy of estimators. We give conditions on the importance sampling proposal distribution such that the variance of the estimator will be the same order as that of the maximum likelihood estimator based on the summary statistics used. This suggests an iterative importance sampling algorithm, which we evaluate empirically on a stochastic volatility model.",
                    "As the sequencing costs are decreasing, there is great incentive to perform large scale association studies to increase power of detecting new variants. Federated association testing among different institutions is a viable solution for increasing sample sizes by sharing the intermediate testing statistics that are aggregated by a central server. There are, however, standing challenges to performing federated association testing. Association tests are known to be confounded by numerous factors such as population stratification, which can be especially important in multiancestral studies and in admixed populations among different sites. Furthermore, disease etiology should be considered via flexible models to avoid biases in the significance of the genetic effect. A rising challenge for performing large scale association studies is the privacy of participants and related ethical concerns of stigmatization and marginalization. Here, we present dMEGA, a flexible and efficient method for performing federated generalized linear mixed model based association testing among multiple sites while underlying genotype and phenotype data are not explicitly shared. dMEGA first utilizes a reference projection to estimate population-based covariates without sharing genotype dataset among sites. Next, dMEGA uses Laplacian approximation for the parameter likelihoods and decomposes parameter estimation into efficient local-gradient updates among sites. We use simulated and real datasets to demonstrate the accuracy and efficiency of dMEGA. Overall, dMEGA's formulation is flexible to integrate fixed and random effects in a federated setting.",
                    "Composite likelihood provides approximate inference when the full likelihood is intractable and sub-likelihood functions of marginal events can be evaluated relatively easily. It has been successfully applied for many complex models. However, its wider application is limited by two issues. First, weight selection of marginal likelihood can have a significant impact on the information efficiency and is currently an open question. Second, calibrated Bayesian inference with composite likelihood requires curvature adjustment which is difficult for dependent data. This work shows that approximate Bayesian computation (ABC) can properly address these two issues by using multiple composite score functions as summary statistics. First, the summary-based posterior distribution gives the optimal Godambe information among a wide class of estimators defined by linear combinations of estimating functions. Second, to make ABC computationally feasible for models where marginal likelihoods have no closed form, a novel approach is proposed to estimate all simulated marginal scores using a Monte Carlo sample with size N. Sufficient conditions are given for the additional noise to be negligible with N fixed as the data size n goes to infinity, and the computational cost is O(n). Third, asymptotic properties of ABC with summary statistics having heterogeneous convergence rates is derived, and an adaptive scheme to choose the component composite scores is proposed. Numerical studies show that the new method significantly outperforms the existing Bayesian composite likelihood methods, and the efficiency of adaptively combined composite scores well approximates the efficiency of particle MCMC using the full likelihood.",
                    "In this paper, we extend our previous work [D. Wu, Phys. Rev. D 108, 084041 (2023)] to more general cases by including a rotation parameter. We investigate the topological numbers for the rotating, accelerating neutral black hole and its AdS extension, as well as the rotating, accelerating charged black hole and its AdS extension. We find that the topological number of an asymptotically flat accelerating black hole consistently differs by one from that of its non-accelerating counterpart. Furthermore, we show that for an asymptotically AdS accelerating black hole, the topological number is reduced by one compared to its non-accelerating AdS counterpart. In addition, we demonstrate that within the framework of general relativity, the acceleration parameter and the negative cosmological constant each independently add one to the topological number. However, when both factors are present, their effects neutralize each other, resulting in no overall change to the topological number.",
                    "Approximate Bayesian computing is a powerful likelihood-free method that has grown increasingly popular since early applications in population genetics. However, complications arise in the theoretical justification for Bayesian inference conducted from this method with a non-sufficient summary statistic. In this paper, we seek to re-frame approximate Bayesian computing within a frequentist context and justify its performance by standards set on the frequency coverage rate. In doing so, we develop a new computational technique called approximate confidence distribution computing, yielding theoretical support for the use of non-sufficient summary statistics in likelihood-free methods. Furthermore, we demonstrate that approximate confidence distribution computing extends the scope of approximate Bayesian computing to include data-dependent priors without damaging the inferential integrity. This data-dependent prior can be viewed as an initial `distribution estimate' of the target parameter which is updated with the results of the approximate confidence distribution computing method. A general strategy for constructing an appropriate data-dependent prior is also discussed and is shown to often increase the computing speed while maintaining statistical inferential guarantees. We supplement the theory with simulation studies illustrating the benefits of the proposed method, namely the potential for broader applications and the increased computing speed compared to the standard approximate Bayesian computing methods."
                ],
                "domain": [
                    "Bayesian Inference",
                    "Approximate Bayesian Computation",
                    "Statistical Modeling",
                    "Federated Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "04c55e10-0969-4d03-a691-36416d9ae8ed": {
                "pk": "04c55e10-0969-4d03-a691-36416d9ae8ed",
                "project_name": null,
                "name": "Tianming Liu",
                "bio": "I am a researcher dedicated to advancing the fields of medical imaging, machine learning, and education technology. My recent work has focused on developing innovative frameworks and models to address complex challenges, particularly in the context of Alzheimer's Disease (AD) progression, unsupervised domain adaptation, and automatic scoring of student responses in science education.\n\nOne of my notable contributions is the Disease2Vec framework, which encodes the continuous progression of AD through a disease embedding tree, allowing for accurate predictions across multiple clinical stages. I have also explored the application of brain-inspired principles in designing robust models, such as the core-periphery constrained transformer for unsupervised domain adaptation, which significantly enhances performance across various datasets.\n\nIn the realm of education, I have developed a zero-shot approach for scoring student responses, demonstrating that leveraging existing language models can yield effective results without extensive training. My work emphasizes the importance of domain-specific data in improving model performance, particularly in science education.\n\nAdditionally, I have investigated the dynamics of functional brain networks using advanced imaging techniques and deep learning, contributing to a better understanding of human brain function. My research also extends to mobile security, where I have conducted systematic reviews and empirical studies on third-party libraries and malware detection, highlighting the need for rigorous evaluation methods.\n\nOverall, my interdisciplinary approach combines insights from neuroscience, machine learning, and education to create impactful solutions that address real-world challenges. I am committed to sharing my findings with the community to foster collaboration and innovation in these critical areas.",
                "collaborators": [
                    "Dajiang Zhu",
                    "Li Li",
                    "Zhengliang Liu",
                    "Xinyu He",
                    "Xiaoming Zhai",
                    "Xian Zhan",
                    "Yang Liu",
                    "Haoyu Wang",
                    "Xiapu Luo",
                    "Lu Zhang"
                ],
                "pub_titles": [
                    "Disease2Vec: Representing Alzheimer's Progression via Disease Embedding Tree",
                    "Robust Core-Periphery Constrained Transformer for Domain Adaptation",
                    "Context Matters: A Strategy to Pre-train Language Model for Science Education",
                    "Spatial-Temporal Convolutional Attention for Mapping Functional Brain Networks",
                    "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models",
                    "Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education",
                    "Follow My Eye: Using Gaze to Supervise Computer-Aided Diagnosis",
                    "CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network",
                    "A Systematic Assessment on Android Third-party Library Detection Tools",
                    "Dataset Bias in Android Malware Detection",
                    "Research on Third-Party Libraries in AndroidApps: A Taxonomy and Systematic LiteratureReview",
                    "Morphological Analusis Of The Left Ventricular Eendocardial Surface Using A Bag-Of-Features Descriptor"
                ],
                "pub_abstracts": [
                    "For decades, a variety of predictive approaches have been proposed and evaluated in terms of their prediction capability for Alzheimer's Disease (AD) and its precursor - mild cognitive impairment (MCI). Most of them focused on prediction or identification of statistical differences among different clinical groups or phases (e.g., longitudinal studies). The continuous nature of AD development and transition states between successive AD related stages have been overlooked, especially in binary or multi-class classification. Though a few progression models of AD have been studied recently, they were mainly designed to determine and compare the order of specific biomarkers. How to effectively predict the individual patient's status within a wide spectrum of continuous AD progression has been largely overlooked. In this work, we developed a novel learning-based embedding framework to encode the intrinsic relations among AD related clinical stages by a set of meaningful embedding vectors in the latent space (Disease2Vec). We named this process as disease embedding. By disease em-bedding, the framework generates a disease embedding tree (DETree) which effectively represents different clinical stages as a tree trajectory reflecting AD progression and thus can be used to predict clinical status by projecting individuals onto this continuous trajectory. Through this model, DETree can not only perform efficient and accurate prediction for patients at any stages of AD development (across five clinical groups instead of typical two groups), but also provide richer status information by examining the projecting locations within a wide and continuous AD progression process.",
                    "Unsupervised domain adaptation (UDA) aims to learn transferable representation across domains. Recently a few UDA works have successfully applied Transformer-based methods and achieved state-of-the-art (SOTA) results. However, it remains challenging when there exists a large domain gap between the source and target domain. Inspired by humans' exceptional transferability abilities to adapt knowledge from familiar to uncharted domains, we try to apply the universally existing organizational structure in the human functional brain networks, i.e., the core-periphery principle to design the Transformer and improve its UDA performance. In this paper, we propose a novel brain-inspired robust core-periphery constrained transformer (RCCT) for unsupervised domain adaptation, which brings a large margin of performance improvement on various datasets. Specifically, in RCCT, the self-attention operation across image patches is rescheduled by an adaptively learned weighted graph with the Core-Periphery structure (CP graph), where the information communication and exchange between images patches are manipulated and controlled by the connection strength, i.e., edge weight of the learned weighted CP graph. Besides, since the data in domain adaptation tasks can be noisy, to improve the model robustness, we intentionally add perturbations to the patches in the latent space to ensure generating robust learned weighted core-periphery graphs. Extensive evaluations are conducted on several widely tested UDA benchmarks. Our proposed RCCT consistently performs best compared to existing works, including 88.3\\% on Office-Home, 95.0\\% on Office-31, 90.7\\% on VisDA-2017, and 46.0\\% on DomainNet.",
                    "This study aims at improving the performance of scoring student responses in science education automatically. BERT-based language models have shown significant superiority over traditional NLP models in various language-related tasks. However, science writing of students, including argumentation and explanation, is domain-specific. In addition, the language used by students is different from the language in journals and Wikipedia, which are training sources of BERT and its existing variants. All these suggest that a domain-specific model pre-trained using science education data may improve model performance. However, the ideal type of data to contextualize pre-trained language model and improve the performance in automatically scoring student written responses remains unclear. Therefore, we employ different data in this study to contextualize both BERT and SciBERT models and compare their performance on automatic scoring of assessment tasks for scientific argumentation. We use three datasets to pre-train the model: 1) journal articles in science education, 2) a large dataset of students' written responses (sample size over 50,000), and 3) a small dataset of students' written responses of scientific argumentation tasks. Our experimental results show that in-domain training corpora constructed from science questions and responses improve language model performance on a wide variety of downstream tasks. Our study confirms the effectiveness of continual pre-training on domain-specific data in the education domain and demonstrates a generalizable strategy for automating science education tasks with high accuracy. We plan to release our data and SciEdBERT models for public use and community engagement.",
                    "Using functional magnetic resonance imaging (fMRI) and deep learning to explore functional brain networks (FBNs) has attracted many researchers. However, most of these studies are still based on the temporal correlation between the sources and voxel signals, and lack of researches on the dynamics of brain function. Due to the widespread local correlations in the volumes, FBNs can be generated directly in the spatial domain in a self-supervised manner by using spatial-wise attention (SA), and the resulting FBNs has a higher spatial similarity with templates compared to the classical method. Therefore, we proposed a novel Spatial-Temporal Convolutional Attention (STCA) model to discover the dynamic FBNs by using the sliding windows. To validate the performance of the proposed method, we evaluate the approach on HCP-rest dataset. The results indicate that STCA can be used to discover FBNs in a dynamic way which provide a novel approach to better understand human brain.",
                    "Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored. This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes. We propose a framework that analyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they resonate with the \"Interests, Ideologies, and Identity\" (3I) of four key audience groups: voters, businesses, donors, and politicians. Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances. Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts. In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation.",
                    "Developing models to automatically score students' written responses to science problems is critical for science education. However, collecting and labeling sufficient student responses for training models is time and cost-consuming. Recent studies suggest that pre-trained language models can be adapted to downstream tasks without fine-tuning with prompts. However, no research has employed such a prompt approach in science education. As student responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching Exemplars as Next Sentence Prediction (MeNSP). This approach employs no training samples. We first apply MeNSP in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and F1 score ranges from 0.54 to 0.81. To improve the performance, we extend our research to the few-shots setting, either randomly selecting labeled student responses or manually constructing responses to fine-tune the models. We find that one task's performance is improved with more samples, Cohen's Kappa from 0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two others, scoring performance is not improved. We also find that randomly selected few-shots perform better than the human expert-crafted approach. This study suggests that MeNSP can yield referable automatic scoring for student responses while significantly reducing the cost of model training. This method can benefit low-stakes classroom assessment practices in science education. Future research should further explore the applicability of the MeNSP in different types of assessment tasks in science education and improve the model performance.",
                    "When deep neural network (DNN) was first introduced to the medical image analysis community, researchers were impressed by its performance. However, it is evident now that a large number of manually labeled data is often a must to train a properly functioning DNN. This demand for supervision data and labels is a major bottleneck in current medical image analysis, since collecting a large number of annotations from experienced experts can be time-consuming and expensive. In this paper, we demonstrate that the eye movement of radiologists reading medical images can be a new form of supervision to train the DNN-based computer-aided diagnosis (CAD) system. Particularly, we record the tracks of the radiologists' gaze when they are reading images. The gaze information is processed and then used to supervise the DNN's attention via an Attention Consistency module. To the best of our knowledge, the above pipeline is among the earliest efforts to leverage expert eye movement for deep-learning-based CAD. We have conducted extensive experiments on knee X-ray images for osteoarthritis assessment. The results show that our method can achieve considerable improvement in diagnosis performance, with the help of gaze supervision.",
                    "The evolution of convolutional neural networks (CNNs) can be largely attributed to the design of its architecture, i.e., the network wiring pattern. Neural architecture search (NAS) advances this by automating the search for the optimal network architecture, but the resulting network instance may not generalize well in different tasks. To overcome this, exploring network design principles that are generalizable across tasks is a more practical solution. In this study, We explore a novel brain-inspired design principle based on the core-periphery property of the human brain network to guide the design of CNNs. Our work draws inspiration from recent studies suggesting that artificial and biological neural networks may have common principles in optimizing network architecture. We implement the core-periphery principle in the design of network wiring patterns and the sparsification of the convolution operation. The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated on three different datasets. The experiments demonstrate the effectiveness and superiority compared to CNNs and ViT-based methods. Overall, our work contributes to the growing field of brain-inspired AI by incorporating insights from the human brain into the design of neural networks.",
                    "Third-party libraries (TPLs) have become a significant part of the Android ecosystem. Developers can employ various TPLs to facilitate their app development. Unfortunately, the popularity of TPLs also brings new security issues. For example, TPLs may carry malicious or vulnerable code, which can infect popular apps to pose threats to mobile users. Furthermore, TPL detection is essential for downstream tasks, such as vulnerabilities and malware detection. Thus, various tools have been developed to identify TPLs. However, no existing work has studied these TPL detection tools in detail, and different tools focus on different applications and techniques with performance differences. A comprehensive understanding of these tools will help us make better use of them. To this end, we conduct a comprehensive empirical study to fill the gap by evaluating and comparing all publicly available TPL detection tools based on six criteria: accuracy of TPL construction, effectiveness, efficiency, accuracy of version identification, resiliency to code obfuscation, and ease of use. Besides, we enhance these open-source tools by fixing their limitations, to improve their detection ability. Finally, we build an extensible framework that integrates all existing available TPL detection tools, providing an online service for the research community. We release the evaluation dataset and enhanced tools. According to our study, we also present the essential findings and discuss promising implications to the community. We believe our work provides a clear picture of existing TPL detection techniques and also gives a roadmap for future research.",
                    "Researchers have proposed kinds of malware detection methods to solve the explosive mobile security threats. We argue that the experiment results are inflated due to the research bias introduced by the variability of malware dataset. We explore the impact of bias in Android malware detection in three aspects, the method used to flag the ground truth, the distribution of malware families in the dataset, and the methods to use the dataset. We implement a set of experiments of different VT thresholds and find that the methods used to flag the malware data affect the malware detection performance directly. We further compare the impact of malware family types and composition on malware detection in detail. The superiority of each approach is different under various combinations of malware families. Through our extensive experiments, we showed that the methods to use the dataset can have a misleading impact on evaluation, and the performance difference can be up to over 40%. We argue that these research biases observed in this paper should be carefully controlled/eliminated to enforce a fair comparison of malware detection techniques. Providing reasonable and explainable results is better than only reporting a high detection accuracy with vague dataset and experimental settings.",
                    "Third-party libraries (TPLs) have been widely used in mobile apps, which play an essential part in the entire Android ecosystem. However, TPL is a double-edged sword. On the one hand, it can ease the development of mobile apps. On the other hand, it also brings security risks such as privacy leaks or increased attack surfaces (e.g., by introducing over-privileged permissions) to mobile apps. Although there are already many studies for characterizing third-party libraries, including automated detection, security and privacy analysis of TPLs, TPL attributes analysis, etc., what strikes us odd is that there is no systematic study to summarize those studies' endeavors. To this end, we conduct the first systematic literature review on Android TPL-related research. Following a well-defined systematic literature review protocol, we collected 74 primary research papers closely related to the Android third-party library from 2012 to 2020. After carefully examining these studies, we designed a taxonomy of TPL-related research studies and conducted a systematic study to summarize current solutions, limitations, challenges and possible implications of new research directions related to third-party library analysis. We hope that these contributions can give readers a clear overview of existing TPL-related studies and inspire them to go beyond the current status quo by advancing the discipline with innovative approaches.",
                    "The limitations of conventional imaging techniques have hitherto precluded a thorough and formal investigation of the complex morphology of the left ventricular (LV) endocardial surface and its relation to the severity of Coronary Artery Disease (CAD). Recent developments in high-resolution Multirow-Detector Computed Tomography (MDCT) scanner technology have enabled the imaging of LV endocardial surface morphology in a single heart beat. Analysis of high-resolution Computed Tomography (CT) images from a 320-MDCT scanner allows the study of the relationship between percent Diameter Stenosis (DS) of the major coronary arteries and localization of the cardiac segments affected by coronary arterial stenosis. In this paper a novel approach for the analysis using a combination of rigid transformation-invariant shape descriptors and a more generalized isometry-invariant Bag-of-Features (BoF) descriptor, is proposed and implemented. The proposed approach is shown to be successful in identifying, localizing and quantifying the incidence and extent of CAD and thus, is seen to have a potentially significant clinical impact. Specifically, the association between the incidence and extent of CAD, determined via the percent DS measurements of the major coronary arteries, and the alterations in the endocardial surface morphology is formally quantified. A multivariate regression test performed on a strict leave-one-out basis are shown to exhibit a distinct pattern in terms of the correlation coefficient within the cardiac segments where the incidence of coronary arterial stenosis is localized."
                ],
                "domain": [
                    "Machine Learning",
                    "Medical Imaging",
                    "Natural Language Processing",
                    "Domain Adaptation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "c61b74d4-d3ab-41cf-92aa-2fc14bd70557": {
                "pk": "c61b74d4-d3ab-41cf-92aa-2fc14bd70557",
                "project_name": null,
                "name": "Dajiang Zhu",
                "bio": "I am a researcher dedicated to advancing our understanding of brain function and its implications for artificial intelligence and healthcare. My work spans a variety of domains, including Alzheimer's Disease (AD) prediction, unsupervised domain adaptation, and the design of neural networks inspired by the human brain's architecture. \n\nIn my recent research, I developed Disease2Vec, a novel framework that encodes the continuous progression of AD, allowing for more accurate predictions across various clinical stages. I also introduced the robust core-periphery constrained transformer (RCCT) to enhance unsupervised domain adaptation, leveraging the organizational structure of human brain networks to improve model performance. My exploration of the core-periphery principle has led to the design of core-periphery principle guided CNNs (CP-CNNs) and vision transformers (CP-ViT), which demonstrate superior performance across multiple datasets.\n\nI am particularly interested in the intersection of neuroscience and AI, as evidenced by my work on brain-inspired models that enhance interpretability and performance. My research on functional brain networks and cortical folding patterns has provided insights into the complex relationships between brain structure and function, contributing to our understanding of neurodevelopmental disorders.\n\nThrough my work, I aim to bridge the gap between neuroscience and artificial intelligence, ultimately contributing to the development of more efficient, reliable, and interpretable AI systems that can benefit healthcare and beyond.",
                "collaborators": [
                    "Tianming Liu",
                    "Lu Zhang",
                    "Xiaowei Yu",
                    "Lin Zhao",
                    "Haixing Dai",
                    "Zihao Wu",
                    "Yanjun Lyu",
                    "Li Wang",
                    "Xintao Hu",
                    "Zhengliang Liu"
                ],
                "pub_titles": [
                    "Disease2Vec: Representing Alzheimer's Progression via Disease Embedding Tree",
                    "Robust Core-Periphery Constrained Transformer for Domain Adaptation",
                    "CP-CNN: Core-Periphery Principle Guided Convolutional Neural Network",
                    "InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning",
                    "Representative Functional Connectivity Learning for Multiple Clinical groups in Alzheimer's Disease",
                    "Disentangling Spatial-Temporal Functional Brain Networks via Twin-Transformers",
                    "Brain Cortical Functional Gradients Predict Cortical Folding Patterns via Attention Mesh Convolution",
                    "BI AVAN: Brain inspired Adversarial Visual Attention Network",
                    "NoisyNN: Exploring the Influence of Information Entropy Change in Learning Systems",
                    "Representing Brain Anatomical Regularity and Variability by Few-Shot Embedding",
                    "Gyri vs. Sulci: Disentangling Brain Core-Periphery Functional Networks via Twin-Transformer",
                    "Brain Dialogue Interface (BDI): A User-Friendly fMRI Model for Interactive Brain Decoding",
                    "Discovering Dynamic Functional Brain Networks via Spatial and Channel-wise Attention",
                    "Core-Periphery Principle Guided Redesign of Self-Attention in Transformers",
                    "Role of Data-driven Regional Growth Model in Shaping Brain Folding Patterns",
                    "When Brain-inspired AI Meets AGI",
                    "Artificial General Intelligence for Medical Imaging",
                    "Identification of Causal Relationship between Amyloid-beta Accumulation and Alzheimer's Disease Progression via Counterfactual Inference",
                    "Hierarchical Semantic Tree Concept Whitening for Interpretable Image Classification"
                ],
                "pub_abstracts": [
                    "For decades, a variety of predictive approaches have been proposed and evaluated in terms of their prediction capability for Alzheimer's Disease (AD) and its precursor - mild cognitive impairment (MCI). Most of them focused on prediction or identification of statistical differences among different clinical groups or phases (e.g., longitudinal studies). The continuous nature of AD development and transition states between successive AD related stages have been overlooked, especially in binary or multi-class classification. Though a few progression models of AD have been studied recently, they were mainly designed to determine and compare the order of specific biomarkers. How to effectively predict the individual patient's status within a wide spectrum of continuous AD progression has been largely overlooked. In this work, we developed a novel learning-based embedding framework to encode the intrinsic relations among AD related clinical stages by a set of meaningful embedding vectors in the latent space (Disease2Vec). We named this process as disease embedding. By disease em-bedding, the framework generates a disease embedding tree (DETree) which effectively represents different clinical stages as a tree trajectory reflecting AD progression and thus can be used to predict clinical status by projecting individuals onto this continuous trajectory. Through this model, DETree can not only perform efficient and accurate prediction for patients at any stages of AD development (across five clinical groups instead of typical two groups), but also provide richer status information by examining the projecting locations within a wide and continuous AD progression process.",
                    "Unsupervised domain adaptation (UDA) aims to learn transferable representation across domains. Recently a few UDA works have successfully applied Transformer-based methods and achieved state-of-the-art (SOTA) results. However, it remains challenging when there exists a large domain gap between the source and target domain. Inspired by humans' exceptional transferability abilities to adapt knowledge from familiar to uncharted domains, we try to apply the universally existing organizational structure in the human functional brain networks, i.e., the core-periphery principle to design the Transformer and improve its UDA performance. In this paper, we propose a novel brain-inspired robust core-periphery constrained transformer (RCCT) for unsupervised domain adaptation, which brings a large margin of performance improvement on various datasets. Specifically, in RCCT, the self-attention operation across image patches is rescheduled by an adaptively learned weighted graph with the Core-Periphery structure (CP graph), where the information communication and exchange between images patches are manipulated and controlled by the connection strength, i.e., edge weight of the learned weighted CP graph. Besides, since the data in domain adaptation tasks can be noisy, to improve the model robustness, we intentionally add perturbations to the patches in the latent space to ensure generating robust learned weighted core-periphery graphs. Extensive evaluations are conducted on several widely tested UDA benchmarks. Our proposed RCCT consistently performs best compared to existing works, including 88.3\\% on Office-Home, 95.0\\% on Office-31, 90.7\\% on VisDA-2017, and 46.0\\% on DomainNet.",
                    "The evolution of convolutional neural networks (CNNs) can be largely attributed to the design of its architecture, i.e., the network wiring pattern. Neural architecture search (NAS) advances this by automating the search for the optimal network architecture, but the resulting network instance may not generalize well in different tasks. To overcome this, exploring network design principles that are generalizable across tasks is a more practical solution. In this study, We explore a novel brain-inspired design principle based on the core-periphery property of the human brain network to guide the design of CNNs. Our work draws inspiration from recent studies suggesting that artificial and biological neural networks may have common principles in optimizing network architecture. We implement the core-periphery principle in the design of network wiring patterns and the sparsification of the convolution operation. The resulting core-periphery principle guided CNNs (CP-CNNs) are evaluated on three different datasets. The experiments demonstrate the effectiveness and superiority compared to CNNs and ViT-based methods. Overall, our work contributes to the growing field of brain-inspired AI by incorporating insights from the human brain into the design of neural networks.",
                    "Semi-supervised learning (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a supervised classification objective with a regularization term derived solely from unlabeled data. This formulation neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve representation learning. The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model's predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a medical SSL task with an uncurated unlabeled set show clear benefits to our approach. On the STL-10 dataset with only 40 labels, InterLUDE achieves 3.2% error rate, while the best previous method reports 14.9%.",
                    "Mild cognitive impairment (MCI) is a high-risk dementia condition which progresses to probable Alzheimer's disease (AD) at approximately 10% to 15% per year. Characterization of group-level differences between two subtypes of MCI - stable MCI (sMCI) and progressive MCI (pMCI) is the key step to understand the mechanisms of MCI progression and enable possible delay of transition from MCI to AD. Functional connectivity (FC) is considered as a promising way to study MCI progression since which may show alterations even in preclinical stages and provide substrates for AD progression. However, the representative FC patterns during AD development for different clinical groups, especially for sMCI and pMCI, have been understudied. In this work, we integrated autoencoder and multi-class classification into a single deep model and successfully learned a set of clinical group related feature vectors. Specifically, we trained two non-linear mappings which realized the mutual transformations between original FC space and the feature space. By mapping the learned clinical group related feature vectors to the original FC space, representative FCs were constructed for each group. Moreover, based on these feature vectors, our model achieves a high classification accuracy - 68% for multi-class classification (NC vs SMC vs sMCI vs pMCI vs AD).",
                    "How to identify and characterize functional brain networks (BN) is fundamental to gain system-level insights into the mechanisms of brain organizational architecture. Current functional magnetic resonance (fMRI) analysis highly relies on prior knowledge of specific patterns in either spatial (e.g., resting-state network) or temporal (e.g., task stimulus) domain. In addition, most approaches aim to find group-wise common functional networks, individual-specific functional networks have been rarely studied. In this work, we propose a novel Twin-Transformers framework to simultaneously infer common and individual functional networks in both spatial and temporal space, in a self-supervised manner. The first transformer takes space-divided information as input and generates spatial features, while the second transformer takes time-related information as input and outputs temporal features. The spatial and temporal features are further separated into common and individual ones via interactions (weights sharing) and constraints between the two transformers. We applied our TwinTransformers to Human Connectome Project (HCP) motor task-fMRI dataset and identified multiple common brain networks, including both task-related and resting-state networks (e.g., default mode network). Interestingly, we also successfully recovered a set of individual-specific networks that are not related to task stimulus and only exist at the individual level.",
                    "Since gyri and sulci, two basic anatomical building blocks of cortical folding patterns, were suggested to bear different functional roles, a precise mapping from brain function to gyro-sulcal patterns can provide profound insights into both biological and artificial neural networks. However, there lacks a generic theory and effective computational model so far, due to the highly nonlinear relation between them, huge inter-individual variabilities and a sophisticated description of brain function regions/networks distribution as mosaics, such that spatial patterning of them has not been considered. we adopted brain functional gradients derived from resting-state fMRI to embed the \"gradual\" change of functional connectivity patterns, and developed a novel attention mesh convolution model to predict cortical gyro-sulcal segmentation maps on individual brains. The convolution on mesh considers the spatial organization of functional gradients and folding patterns on a cortical sheet and the newly designed channel attention block enhances the interpretability of the contribution of different functional gradients to cortical folding prediction. Experiments show that the prediction performance via our model outperforms other state-of-the-art models. In addition, we found that the dominant functional gradients contribute less to folding prediction. On the activation maps of the last layer, some well-studied cortical landmarks are found on the borders of, rather than within, the highly activated regions. These results and findings suggest that a specifically designed artificial neural network can improve the precision of the mapping between brain functions and cortical folding patterns, and can provide valuable insight of brain anatomy-function relation for neuroscience.",
                    "Visual attention is a fundamental mechanism in the human brain, and it inspires the design of attention mechanisms in deep neural networks. However, most of the visual attention studies adopted eye-tracking data rather than the direct measurement of brain activity to characterize human visual attention. In addition, the adversarial relationship between the attention-related objects and attention-neglected background in the human visual system was not fully exploited. To bridge these gaps, we propose a novel brain-inspired adversarial visual attention network (BI-AVAN) to characterize human visual attention directly from functional brain activity. Our BI-AVAN model imitates the biased competition process between attention-related/neglected objects to identify and locate the visual objects in a movie frame the human brain focuses on in an unsupervised manner. We use independent eye-tracking data as ground truth for validation and experimental results show that our model achieves robust and promising results when inferring meaningful human visual attention and mapping the relationship between brain activities and visual stimuli. Our BI-AVAN model contributes to the emerging field of leveraging the brain's functional architecture to inspire and guide the model design in artificial intelligence (AI), e.g., deep neural networks.",
                    "We explore the impact of entropy change in deep learning systems via noise injection at different levels, i.e., the latent space and input image. The series of models that employ our methodology are collectively known as Noisy Neural Networks (NoisyNN), with examples such as NoisyViT and NoisyCNN. Noise is conventionally viewed as a harmful perturbation in various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers (ViTs), as well as different learning tasks like image classification and transfer learning. However, this work shows noise can be an effective way to change the entropy of the learning system. We demonstrate that specific noise can boost the performance of various deep architectures under certain conditions. We theoretically prove the enhancement gained from positive noise by reducing the task complexity defined by information entropy and experimentally show the significant performance gain in large image datasets, such as the ImageNet. Herein, we use the information entropy to define the complexity of the task. We categorize the noise into two types, positive noise (PN) and harmful noise (HN), based on whether the noise can help reduce the complexity of the task. Extensive experiments of CNNs and ViTs have shown performance improvements by proactively injecting positive noise, where we achieved an unprecedented top 1 accuracy of over 95$\\%$ on ImageNet. Both theoretical analysis and empirical evidence have confirmed that the presence of positive noise, can benefit the learning process, while the traditionally perceived harmful noise indeed impairs deep learning models. The different roles of noise offer new explanations for deep models on specific tasks and provide a new paradigm for improving model performance. Moreover, it reminds us that we can influence the performance of learning systems via information entropy change.",
                    "Effective representation of brain anatomical architecture is fundamental in understanding brain regularity and variability. Despite numerous efforts, it is still difficult to infer reliable anatomical correspondence at finer scale, given the tremendous individual variability in cortical folding patterns. It is even more challenging to disentangle common and individual patterns when comparing brains at different neuro-developmental stages. In this work, we developed a novel learning-based few-shot embedding framework to encode the cortical folding patterns into a latent space represented by a group of anatomically meaningful embedding vectors. Specifically, we adopted 3-hinge (3HG) network as the substrate and designed an autoencoder-based embedding framework to learn a common embedding vector for each 3HG's multi-hop feature: each 3HG can be represented as a combination of these feature embeddings via a set of individual specific coefficients to characterize individualized anatomical information. That is, the regularity of folding patterns is encoded into the embeddings, while the individual variations are preserved by the multi=hop combination coefficients. To effectively learn the embeddings for the population with very limited samples, few-shot learning was adopted. We applied our method on adult HCP and pediatric datasets with 1,000+ brains (from 34 gestational weeks to young adult). Our experimental results show that: 1) the learned embedding vectors can quantitatively encode the commonality and individuality of cortical folding patterns; 2) with the embeddings we can robustly infer the complicated many-to-many anatomical correspondences among different brains and 3) our model can be successfully transferred to new populations with very limited training samples.",
                    "The human cerebral cortex is highly convoluted into convex gyri and concave sulci. It has been demonstrated that gyri and sulci are significantly different in their anatomy, connectivity, and function, besides exhibiting opposite shape patterns, long-distance axonal fibers connected to gyri are much denser than those connected to sulci, and neural signals on gyri are more complex in low-frequency while sulci are more complex in high-frequency. Although accumulating evidence shows significant differences between gyri and sulci, their primary roles in brain function have not been elucidated yet. To solve this fundamental problem, we design a novel Twin-Transformer framework to unveil the unique functional roles of gyri and sulci as well as their relationship in the whole brain function. Our Twin-Transformer framework adopts two structure-identical (twin) Transformers to disentangle spatial-temporal patterns of gyri and sulci, one focuses on the information of gyri and the other is on sulci. The Gyro-Sulcal interactions, along with the tremendous but widely existing variability across subjects, are characterized in the loss design. We validated our Twin-Transformer on the HCP task-fMRI dataset, for the first time, to elucidate the different roles of gyri and sulci in brain function. Our results suggest that gyri and sulci could work together in a core-periphery network manner, that is, gyri could serve as core networks for information gathering and distributing, while sulci could serve as periphery networks for specific local information processing. These findings have shed new light on our fundamental understanding of the brain's basic structural and functional mechanisms.",
                    "Brain decoding techniques are essential for understanding the neurocognitive system. Although numerous methods have been introduced in this field, accurately aligning complex external stimuli with brain activities remains a formidable challenge. To alleviate alignment difficulties, many studies have simplified their models by employing single-task paradigms and establishing direct links between brain/world through classification strategies. Despite improvements in decoding accuracy, this strategy frequently encounters issues with generality when adapting these models to various task paradigms. To address this issue, this study introduces a user-friendly decoding model that enables dynamic communication with the brain, as opposed to the static decoding approaches utilized by traditional studies. The model functions as a brain simulator, allowing for interactive engagement with the brain and enabling the decoding of a subject's experiences through dialogue-like queries. Uniquely, our model is trained in a completely unsupervised and task-free manner. Our experiments demonstrate the feasibility and versatility of our proposed method. Notably, our model demonstrates exceptional capabilities in signal compression, successfully representing the entire brain signal of approximately 185,751 voxels with just 32 signals. Furthermore, we show how our model can integrate seamlessly with multimodal models, thus enhancing the potential for controlling brain decoding through textual or image inputs.",
                    "Using deep learning models to recognize functional brain networks (FBNs) in functional magnetic resonance imaging (fMRI) has been attracting increasing interest recently. However, most existing work focuses on detecting static FBNs from entire fMRI signals, such as correlation-based functional connectivity. Sliding-window is a widely used strategy to capture the dynamics of FBNs, but it is still limited in representing intrinsic functional interactive dynamics at each time step. And the number of FBNs usually need to be set manually. More over, due to the complexity of dynamic interactions in brain, traditional linear and shallow models are insufficient in identifying complex and spatially overlapped FBNs across each time step. In this paper, we propose a novel Spatial and Channel-wise Attention Autoencoder (SCAAE) for discovering FBNs dynamically. The core idea of SCAAE is to apply attention mechanism to FBNs construction. Specifically, we designed two attention modules: 1) spatial-wise attention (SA) module to discover FBNs in the spatial domain and 2) a channel-wise attention (CA) module to weigh the channels for selecting the FBNs automatically. We evaluated our approach on ADHD200 dataset and our results indicate that the proposed SCAAE method can effectively recover the dynamic changes of the FBNs at each fMRI time step, without using sliding windows. More importantly, our proposed hybrid attention modules (SA and CA) do not enforce assumptions of linearity and independence as previous methods, and thus provide a novel approach to better understanding dynamic functional brain networks.",
                    "Designing more efficient, reliable, and explainable neural network architectures is critical to studies that are based on artificial intelligence (AI) techniques. Previous studies, by post-hoc analysis, have found that the best-performing ANNs surprisingly resemble biological neural networks (BNN), which indicates that ANNs and BNNs may share some common principles to achieve optimal performance in either machine learning or cognitive/behavior tasks. Inspired by this phenomenon, we proactively instill organizational principles of BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP) organization, which is widely found in human brain networks, to guide the information communication mechanism in the self-attention of vision transformer (ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention operation between nodes is defined by a sparse graph with a Core-Periphery structure (CP graph), where the core nodes are redesigned and reorganized to play an integrative role and serve as a center for other periphery nodes to exchange information. We evaluated the proposed CP-ViT on multiple public datasets, including medical image datasets (INbreast) and natural image datasets. Interestingly, by incorporating the BNN-derived principle (CP structure) into the redesign of ViT, our CP-ViT outperforms other state-of-the-art ANNs. In general, our work advances the state of the art in three aspects: 1) This work provides novel insights for brain-inspired AI: we can utilize the principles found in BNNs to guide and improve our ANN architecture design; 2) We show that there exist sweet spots of CP graphs that lead to CP-ViTs with significantly improved performance; and 3) The core nodes in CP-ViT correspond to task-related meaningful and important image patches, which can significantly enhance the interpretability of the trained deep model.",
                    "The surface morphology of the developing mammalian brain is crucial for understanding brain function and dysfunction. Computational modeling offers valuable insights into the underlying mechanisms for early brain folding. Recent findings indicate significant regional variations in brain tissue growth, while the role of these variations in cortical development remains unclear. In this study, we unprecedently explored how regional cortical growth affects brain folding patterns using computational simulation. We first developed growth models for typical cortical regions using machine learning (ML)-assisted symbolic regression, based on longitudinal real surface expansion and cortical thickness data from prenatal and infant brains derived from over 1,000 MRI scans of 735 pediatric subjects with ages ranging from 29 post-menstrual weeks to 24 months. These models were subsequently integrated into computational software to simulate cortical development with anatomically realistic geometric models. We comprehensively quantified the resulting folding patterns using multiple metrics such as mean curvature, sulcal depth, and gyrification index. Our results demonstrate that regional growth models generate complex brain folding patterns that more closely match actual brains structures, both quantitatively and qualitatively, compared to conventional uniform growth models. Growth magnitude plays a dominant role in shaping folding patterns, while growth trajectory has a minor influence. Moreover, multi-region models better capture the intricacies of brain folding than single-region models. Our results underscore the necessity and importance of incorporating regional growth heterogeneity into brain folding simulations, which could enhance early diagnosis and treatment of cortical malformations and neurodevelopmental disorders such as cerebral palsy and autism.",
                    "Artificial General Intelligence (AGI) has been a long-standing goal of humanity, with the aim of creating machines capable of performing any intellectual task that humans can do. To achieve this, AGI researchers draw inspiration from the human brain and seek to replicate its principles in intelligent machines. Brain-inspired artificial intelligence is a field that has emerged from this endeavor, combining insights from neuroscience, psychology, and computer science to develop more efficient and powerful AI systems. In this article, we provide a comprehensive overview of brain-inspired AI from the perspective of AGI. We begin with the current progress in brain-inspired AI and its extensive connection with AGI. We then cover the important characteristics for both human intelligence and AGI (e.g., scaling, multimodality, and reasoning). We discuss important technologies toward achieving AGI in current AI systems, such as in-context learning and prompt tuning. We also investigate the evolution of AGI systems from both algorithmic and infrastructural perspectives. Finally, we explore the limitations and future of AGI.",
                    "In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond.",
                    "Alzheimer's disease (AD) is a neurodegenerative disorder that is beginning with amyloidosis, followed by neuronal loss and deterioration in structure, function, and cognition. The accumulation of amyloid-beta in the brain, measured through 18F-florbetapir (AV45) positron emission tomography (PET) imaging, has been widely used for early diagnosis of AD. However, the relationship between amyloid-beta accumulation and AD pathophysiology remains unclear, and causal inference approaches are needed to uncover how amyloid-beta levels can impact AD development. In this paper, we propose a graph varying coefficient neural network (GVCNet) for estimating the individual treatment effect with continuous treatment levels using a graph convolutional neural network. We highlight the potential of causal inference approaches, including GVCNet, for measuring the regional causal connections between amyloid-beta accumulation and AD pathophysiology, which may serve as a robust tool for early diagnosis and tailored care.",
                    "With the popularity of deep neural networks (DNNs), model interpretability is becoming a critical concern. Many approaches have been developed to tackle the problem through post-hoc analysis, such as explaining how predictions are made or understanding the meaning of neurons in middle layers. Nevertheless, these methods can only discover the patterns or rules that naturally exist in models. In this work, rather than relying on post-hoc schemes, we proactively instill knowledge to alter the representation of human-understandable concepts in hidden layers. Specifically, we use a hierarchical tree of semantic concepts to store the knowledge, which is leveraged to regularize the representations of image data instances while training deep models. The axes of the latent space are aligned with the semantic concepts, where the hierarchical relations between concepts are also preserved. Experiments on real-world image datasets show that our method improves model interpretability, showing better disentanglement of semantic concepts, without negatively affecting model classification performance."
                ],
                "domain": [
                    "Neuroscience",
                    "Deep Learning",
                    "Machine Learning",
                    "Alzheimer's Disease"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively represent and map the complex relationships between genes, phenotypes, and diseases using an AI-based system that integrates both structured and unstructured genomic knowledge?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for advancing our understanding of the intricate interplay between genetic factors and phenotypic outcomes, which has significant implications for the research community. By developing a comprehensive model that captures these relationships, we can enhance knowledge discovery in genomics, facilitate hypothesis generation, and improve the identification of gene-disease associations. This could lead to practical applications in personalized medicine, drug discovery, and the development of targeted therapies, ultimately benefiting patient care and public health.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in addressing this problem stem from the complexity of biological systems, where genes, proteins, and phenotypes interact in multifaceted ways. Naive approaches may fail because they often focus on isolated components rather than the holistic system. Technical obstacles include the need to process vast amounts of unstructured data, integrate diverse data types, and accurately model the relationships among various biological entities. Theoretical challenges involve developing robust frameworks that can capture the nuances of these interactions while maintaining interpretability and accuracy.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on single or dual-molecule analyses, leading to a fragmented understanding of gene-phenotype relationships. Limitations in existing solutions include a lack of comprehensive models that consider the entire genomic system and the underutilization of valuable experimental knowledge and bio-computational evidence. Our approach differs by leveraging large language models to integrate and analyze both structured and unstructured data, providing a more holistic view of the genetic landscape and addressing the gaps left by prior studies.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves developing the GP-GPT model, which integrates data from multiple sources, including OMIM, DisGeNET, and dbGaP. We will convert this data into bio-text to identify and categorize gene entities, gene functions, protein entities, protein functions, phenotype entities, and genotype-phenotype associations. The model's performance will be evaluated using metrics such as accuracy, precision, and recall in identifying gene-disease associations. We expect the outcomes to reveal new insights into gene-phenotype relationships, enhance the understanding of complex biological mechanisms, and facilitate"
    },
    "2403.14801": {
        "paper_data": {
            "title": "Assessing the Utility of Large Language Models for Phenotype-Driven Gene Prioritization in Rare Genetic Disorder Diagnosis",
            "url": "http://arxiv.org/abs/2403.14801v2",
            "arxiv_id": "2403.14801",
            "authors": [
                "Junyoung Kim",
                "Jingye Yang",
                "Kai Wang",
                "Chunhua Weng",
                "Cong Liu"
            ],
            "abstract": "Phenotype-driven gene prioritization is a critical process in the diagnosis of rare genetic disorders for identifying and ranking potential disease-causing genes based on observed physical traits or phenotypes. While traditional approaches rely on curated knowledge graphs with phenotype-gene relations, recent advancements in large language models have opened doors to the potential of AI predictions through extensive training on diverse corpora and complex models. This study conducted a comprehensive evaluation of five large language models, including two Generative Pre-trained Transformers series, and three Llama2 series, assessing their performance across three key metrics: task completeness, gene prediction accuracy, and adherence to required output structures. Various experiments explored combinations of models, prompts, input types, and task difficulty levels. Our findings reveal that even the best-performing LLM, GPT-4, achieved an accuracy of 16.0%, which still lags behind traditional bioinformatics tools. Prediction accuracy increased with the parameter/model size. A similar increasing trend was observed for the task completion rate, with complicated prompts more likely to increase task completeness in models smaller than GPT-4. However, complicated prompts are more likely to decrease the structure compliance rate, but no prompt effects on GPT-4. Compared to HPO term-based input, LLM was also able to achieve better than random prediction accuracy by taking free-text input, but slightly lower than with the HPO input. Bias analysis showed that certain genes, such as MECP2, CDKL5, and SCN1A, are more likely to be top-ranked, potentially explaining the variances observed across different datasets. This study provides valuable insights into the integration of LLMs within genomic analysis, contributing to the ongoing discussion on the utilization of advanced LLMs in clinical workflows.",
            "introduction": " Introduction   Phenotype -driven gene prioritization is a process that involves identifying and ranking  candidate disease -causing  genes by examining  an individual’s observed physical traits , known  as phenotypes, in contrast to genotypes . It plays a crucial role in rare disease diagnosis when   analyzing genomic data from high -throughput (e.g., Whole Genome/Exome Sequencing) methods for Mendelian  diseases. Briefings in Bioinformatics . 2022;23(2):bbac019.   40. Gravel J, D’Amours -Gravel M, Osmanlliu E. Learning to fake it: limited responses and fabricated results. In the  right panel, GPT fabricates prediction genes as “Gene 1 ”, “Gene 2 ”, and “Gene 3 ”.         26   Supplementary Figure 2. Examples of Two Instances of GPT Responses (Negative and  Positive Examples) Indicating the Correct or Incorrect Gene Prediction for an Individual  Whose Final Diagnosed Gene is 'ANK1.'  In the left panel, 'ANK1' is not present, indicating  a wrong prediction. In the right panel, GPT provided a gene list containing 'ANK1,' indicating  a correct prediction.          27   Supplementary Figure 3. Examples of Two Instances of GPT Responses That Do Not  Adhere to the Required Output Structure. In the left panel, GPT did not provide a gene  list nor 'Not Applicable' explicitly. In the right panel, although GPT generated a gene list,  the format did not align with the required comma -separated gene list. A compliant result  should consist of a gene list with the exact number of genes requested in the prompt, such  as [OPA1, GJB2, GJB6, …], o r generate ‘Not Applicable’.     true_gene gpt_version k_i o_i k m e_i odds_ratio BRCA1 llama-2-7b-chat 6 62406 870.216957605985037 27.655172413793103 SHH llama-2-7b-chat 6 42406 870.2169576059850374 18.436781609195403 TP53 llama-2-7b-chat 84 462406 873.0374064837905235 15.144499178981938 ASXL1 llama-2-7b-chat 6 32406 870.2169576059850374 13.827586206896552 EP300 llama-2-7b-chat 6 32406 870.2169576059850374 13.827586206896552 LMNA llama-2-7b-chat 6 32406 870.2169576059850374 13.827586206896552 NPC1 llama-2-7b-chat 6 32406 870.2169576059850374 13.827586206896552 SDHA llama-2-7b-chat 6 32406 870.2169576059850374 13.827586206896552 SCN1A llama-2-7b-chat 12 32406 870.4339152119700748 6.913793103448276 CACNA1C llama-2-7b-chat 6 12406 870.2169576059850374 4.609195402298851 PTEN llama-2-7b-chat 12 22406 870.4339152119700748 4.609195402298851 SYNGAP1 llama-2-7b-chat 18 22406 870.6508728179551122 3.0727969348659006 RAC1 llama-2-7b-chat 66 62406 872.3865336658354113 2.5141065830721003 BRAF llama-2-7b-chat 24 22406 870.8678304239401496 2.3045977011494254 ABCD1 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ACTG2 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ADA2 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ADSS1 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 AGL llama-2-7b-chat 6 02406 870.2169576059850374 0.00 AHDC1 llama-2-7b-chat 18 02406 870.6508728179551122 0.00 AIFM1 llama-2-7b-chat 12 02406 870.4339152119700748 0.00 AIRE llama-2-7b-chat 12 02406 870.4339152119700748 0.00 ALG13 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ALPK3 llama-2-7b-chat 12 02406 870.4339152119700748 0.00 ANK1 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ANKRD11 llama-2-7b-chat 24 02406 870.8678304239401496 0.00 APTX llama-2-7b-chat 6 02406 870.2169576059850374 0.00 AQP4 llama-2-7b-chat 12 02406 870.4339152119700748 0.00 ARID1B llama-2-7b-chat 24 02406 870.8678304239401496 0.00 ARID2 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ARX llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ASXL3 llama-2-7b-chat 18 02406 870.6508728179551122 0.00 ATN1 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 ATP1A3 llama-2-7b-chat 18 02406 870.6508728179551122 0.00 ATRX llama-2-7b-chat 6 02406 870.2169576059850374 0.00 AUTS2 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 B3GLCT llama-2-7b-chat 6 02406 870.2169576059850374 0.00BICD2 llama-2-7b-chat 24 02406 870.8678304239401496 0.00 BTD llama-2-7b-chat 6 02406 870.2169576059850374 0.00 BTK llama-2-7b-chat 6 02406 870.2169576059850374 0.00 C3AR1 llama-2-7b-chat 12 02406 870.4339152119700748 0.00 CAMK4 llama-2-7b-chat 12 02406 870.4339152119700748 0.00 CAPN5 llama-2-7b-chat 12 02406 870.4339152119700748 0.00 CCN6 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 CDK10 llama-2-7b-chat 96 02406 873.4713216957605986 0.00 CDK13 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 CDKL5 llama-2-7b-chat 24 02406 870.8678304239401496 0.00 CHAMP1 llama-2-7b-chat 6 02406 870.2169576059850374 0.00 CLUAP1 llama-2-7b-chat",
            "references": [
                {
                    "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
                    "abstract": "Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns."
                },
                {
                    "title": "Zero-Shot Information Extraction via Chatting with ChatGPT",
                    "abstract": "Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources."
                }
            ]
        },
        "author_data": {
            "49883c8a-79ab-43ca-a7b1-a59ef89f2965": {
                "pk": "49883c8a-79ab-43ca-a7b1-a59ef89f2965",
                "project_name": null,
                "name": "Junyoung Kim",
                "bio": "I am a researcher specializing in optimization, machine learning, and robotic mapping, with a focus on developing innovative frameworks that enhance performance in complex environments. My recent work includes tackling the chance-constrained binary knapsack problem (CKP) through a novel non-convex relaxation approach, which not only provides tight upper and lower bounds but can also be solved in polynomial time. This has significant implications for decision-making in uncertain scenarios.\n\nIn the realm of semantic mapping, I have pioneered an evidential semantic mapping framework that integrates Dempster-Shafer Theory of Evidence with Evidential Deep Learning. This approach effectively addresses the challenges of constructing reliable semantic maps in perceptually difficult environments, enhancing both accuracy and robustness in off-road scenarios.\n\nAdditionally, I have developed Amulet, an advanced compiler framework that optimizes matrix multiplication-like tasks, bridging the gap between user-friendly programming and high-performance computation. By leveraging both database-style and compiler optimization techniques, Amulet achieves impressive speedups, making complex computations more accessible.\n\nThrough my research, I aim to push the boundaries of how we understand and interact with data and environments, ensuring that my contributions not only advance theoretical knowledge but also have practical applications in real-world scenarios.",
                "collaborators": [
                    "Junwon Seo",
                    "Kyungsik Lee",
                    "Kenneth Ross",
                    "Eric Sedlar",
                    "Lukas Stadler",
                    "Jihong Min"
                ],
                "pub_titles": [
                    "Non-convex relaxation and 1/2-approximation algorithm for the chance-constrained binary knapsack problem",
                    "Uncertainty-aware Semantic Mapping in Off-road Environments with Dempster-Shafer Theory of Evidence",
                    "AMULET: Adaptive Matrix-Multiplication-Like Tasks",
                    "Evidential Semantic Mapping in Off-road Environments with Uncertainty-aware Bayesian Kernel Inference"
                ],
                "pub_abstracts": [
                    "We consider the chance-constrained binary knapsack problem (CKP), where the item weights are independent and normally distributed. We introduce a continuous relaxation for the CKP, represented as a non-convex optimization problem, which we call the non-convex relaxation. A comparative study shows that the non-convex relaxation provides an upper bound for the CKP, at least as tight as those obtained from other continuous relaxations for the CKP. Furthermore, the quality of the obtained upper bound is guaranteed to be at most twice the optimal objective value of the CKP. Despite its non-convex nature, we show that the non-convex relaxation can be solved in polynomial time. Subsequently, we proposed a polynomial-time 1/2-approximation algorithm for the CKP based on this relaxation, providing a lower bound for the CKP. Computational test results demonstrate that the non-convex relaxation and the proposed approximation algorithm yields tight lower and upper bounds for the CKP within a short computation time, ensuring the quality of the obtained bounds.",
                    "Semantic mapping with Bayesian Kernel Inference (BKI) has shown promise in providing a richer understanding of environments by effectively leveraging local spatial information. However, existing methods face challenges in constructing accurate semantic maps or reliable uncertainty maps in perceptually challenging environments due to unreliable semantic predictions. To address this issue, we propose an evidential semantic mapping framework, which integrates the evidential reasoning of Dempster-Shafer Theory of Evidence (DST) into the entire mapping pipeline by adopting Evidential Deep Learning (EDL) and Dempster's rule of combination. Additionally, the extended belief is devised to incorporate local spatial information based on their uncertainty during the mapping process. Comprehensive experiments across various off-road datasets demonstrate that our framework enhances the reliability of uncertainty maps, consistently outperforming existing methods in scenes with high perceptual uncertainties while showing semantic accuracy comparable to the best-performing semantic mapping techniques.",
                    "Many useful tasks in data science and machine learning applications can be written as simple variations of matrix multiplication. However, users have difficulty performing such tasks as existing matrix/vector libraries support only a limited class of computations hand-tuned for each unique hardware platform. Users can alternatively write the task as a simple nested loop but current compilers are not sophisticated enough to generate fast code for the task written in this way. To address these issues, we extend an open-source compiler to recognize and optimize these matrix multiplication-like tasks. Our framework, called Amulet, uses both database-style and compiler optimization techniques to generate fast code tailored to its execution environment. We show through experiments that Amulet achieves speedups on a variety of matrix multiplication-like tasks compared to existing compilers. For large matrices Amulet typically performs within 15% of hand-tuned matrix multiplication libraries, while handling a much broader class of computations.",
                    "Robotic mapping with Bayesian Kernel Inference (BKI) has shown promise in creating semantic maps by effectively leveraging local spatial information. However, existing semantic mapping methods face challenges in constructing reliable maps in unstructured outdoor scenarios due to unreliable semantic predictions. To address this issue, we propose an evidential semantic mapping, which can enhance reliability in perceptually challenging off-road environments. We integrate Evidential Deep Learning into the semantic segmentation network to obtain the uncertainty estimate of semantic prediction. Subsequently, this semantic uncertainty is incorporated into an uncertainty-aware BKI, tailored to prioritize more confident semantic predictions when accumulating semantic information. By adaptively handling semantic uncertainties, the proposed framework constructs robust representations of the surroundings even in previously unseen environments. Comprehensive experiments across various off-road datasets demonstrate that our framework enhances accuracy and robustness, consistently outperforming existing methods in scenes with high perceptual uncertainties."
                ],
                "domain": [
                    "Optimization",
                    "Machine Learning",
                    "Semantic Mapping",
                    "Robotics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "d123b5e2-8d8e-4cb9-90df-b9eec4d740a9": {
                "pk": "d123b5e2-8d8e-4cb9-90df-b9eec4d740a9",
                "project_name": null,
                "name": "Jingye Yang",
                "bio": "I am a researcher deeply engaged in the intersection of mathematics, machine learning, and biomedical applications. My work spans a diverse range of topics, from the topology of fibered manifolds to the application of large language models (LLMs) in clinical settings. Recently, I have explored the properties of great circle fibrations in spheres, revealing intriguing distinctions in contact structures as dimensions increase. My investigations into Seifert-fibered 3-manifolds have led to significant insights into the homotopy types of their fiberings, contributing to our understanding of non-aspherical manifolds.\n\nIn the realm of machine learning, I have focused on the capabilities of LLMs for tasks such as clinical phenotype detection, where I developed PhenoBCBERT and PhenoGPT to enhance the extraction of phenotype terms from biomedical literature. My research highlights the strengths of different model architectures, emphasizing the importance of feature engineering in classification tasks, particularly in the context of integer classification based on residues.\n\nAdditionally, I have pioneered FairSFS, an algorithm designed to ensure fairness in streaming feature selection, addressing biases that can arise in real-time data processing. My work on multimodal machine learning with GestaltMML aims to improve the diagnostic process for rare genetic disorders by integrating diverse data sources, ultimately facilitating more accurate and timely clinical diagnoses. Through these efforts, I strive to bridge theoretical insights with practical applications, enhancing our understanding of complex systems and improving healthcare outcomes.",
                "collaborators": [
                    "Da Wu",
                    "Kai Wang",
                    "Herman Gluck",
                    "Yi Wang",
                    "Cong Liu",
                    "Chunhua Weng",
                    "Mian Umair Ahsan",
                    "Wendy Deng",
                    "Yunyun Zhou",
                    "Dennis DeTurck"
                ],
                "pub_titles": [
                    "Great Circle Fibrations and Contact Structures on Odd-Dimensional Spheres",
                    "On the homotopy type of the space of fiberings of $S^1 \\times S^2$ by simple closed curves",
                    "Exploring the Reversal Curse and Other Deductive Logical Reasoning in BERT and GPT-Based Large Language Models",
                    "Classification of integers based on residue classes via modern deep learning algorithms",
                    "Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT",
                    "Deformation retraction of the group of strict contactomorphisms of the three-sphere to the unitary group",
                    "Fair Streaming Feature Selection",
                    "GestaltMML: Enhancing Rare Genetic Disease Diagnosis through Multimodal Machine Learning Combining Facial Images and Clinical Texts"
                ],
                "pub_abstracts": [
                    "It is known that for every smooth great circle fibration of the 3-sphere, the distribution of tangent 2-planes orthogonal to the fibres is a contact structure, in fact a tight one, but we show here that, beginning with the 5-sphere, there exist smooth great circle fibrations of all odd-dimensional spheres for which the hyperplane distribution orthogonal to the fibres is not a contact structure.",
                    "For most aspherical Seifert-fibered 3-manifolds $M$, the space of Seifert fiberings $SF(M)$ is known to have contractible components. It is also known that the space of Hopf fiberings of the three-sphere is noncontractible. We provide the second example of a non-aspherical 3-manifold $M$ such that $SF(M)$ has noncontractible components. In particular, we show that certain components of $SF(S^1 \\times S^2)$ are homotopy equivalent to a subspace homeomorphic to the identity-based loop space $\\Omega SO(3)$, and we exhibit second homology generators for both connected components of $SF(S^1 \\times S^2)$.",
                    "The term \"Reversal Curse\" refers to the scenario where auto-regressive decoder large language models (LLMs), such as ChatGPT, trained on \"A is B\" fail to learn \"B is A,\" assuming that B and A are distinct and can be uniquely identified from each other, demonstrating a basic failure of logical deduction. This raises a red flag in the use of GPT models for certain general tasks such as constructing knowledge graphs, considering their adherence to this symmetric principle. In our study, we examined a bidirectional LLM, BERT, and found that it is immune to the reversal curse. Driven by ongoing efforts to construct biomedical knowledge graphs with LLMs, we also embarked on evaluating more complex but essential deductive reasoning capabilities. This process included first training encoder and decoder language models to master the intersection and union operations on two sets and then moving on to assess their capability to infer different combinations of union and intersection operations on three newly created sets. The findings showed that while both encoder and decoder language models, trained for tasks involving two sets (union/intersection), were proficient in such scenarios, they encountered difficulties when dealing with operations that included three sets (various combinations of union and intersection). Our research highlights the distinct characteristics of encoder and decoder models in simple and complex logical reasoning. In practice, the choice between BERT and GPT should be guided by the specific requirements and nature of the task at hand, leveraging their respective strengths in bidirectional context comprehension and sequence prediction.",
                    "Judging whether an integer can be divided by prime numbers such as 2 or 3 may appear trivial to human beings, but can be less straightforward for computers. Here, we tested multiple deep learning architectures and feature engineering approaches on classifying integers based on their residues when divided by small prime numbers. We found that the ability of classification critically depends on the feature space. We also evaluated Automated Machine Learning (AutoML) platforms from Amazon, Google and Microsoft, and found that they failed on this task without appropriately engineered features. Furthermore, we introduced a method that utilizes linear regression on Fourier series basis vectors, and demonstrated its effectiveness. Finally, we evaluated Large Language Models (LLMs) such as GPT-4, GPT-J, LLaMA and Falcon, and demonstrated their failures. In conclusion, feature engineering remains an important task to improve performance and increase interpretability of machine-learning models, even in the era of AutoML and LLMs.",
                    "We hypothesize that large language models (LLMs) based on the transformer architecture can enable automated detection of clinical phenotype terms, including terms not documented in the HPO. In this study, we developed two types of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERT as its pre-trained model, and PhenoGPT, a GPT-based model that can be initialized from diverse GPT models, including open-source versions such as GPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 and GPT-3.5. We compared our methods with PhenoTagger, a recently developed HPO recognition tool that combines rule-based and deep learning methods. We found that our methods can extract more phenotype concepts, including novel ones not characterized by HPO. We also performed case studies on biomedical literature to illustrate how new phenotype information can be recognized and extracted. We compared current BERT-based versus GPT-based models for phenotype tagging, in multiple aspects including model architecture, memory usage, speed, accuracy, and privacy protection. We also discussed the addition of a negation step and an HPO normalization layer to the transformer models for improved HPO term tagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discovery of phenotype terms from clinical notes and biomedical literature, facilitating automated downstream tasks to derive new biological insights on human diseases.",
                    "We prove that the group of strict contactomorphisms of the standard tight contact structure on the three-sphere deformation retracts to its unitary subgroup U(2).",
                    "Streaming feature selection techniques have become essential in processing real-time data streams, as they facilitate the identification of the most relevant attributes from continuously updating information. Despite their performance, current algorithms to streaming feature selection frequently fall short in managing biases and avoiding discrimination that could be perpetuated by sensitive attributes, potentially leading to unfair outcomes in the resulting models. To address this issue, we propose FairSFS, a novel algorithm for Fair Streaming Feature Selection, to uphold fairness in the feature selection process without compromising the ability to handle data in an online manner. FairSFS adapts to incoming feature vectors by dynamically adjusting the feature set and discerns the correlations between classification attributes and sensitive attributes from this revised set, thereby forestalling the propagation of sensitive data. Empirical evaluations show that FairSFS not only maintains accuracy that is on par with leading streaming feature selection methods and existing fair feature techniques but also significantly improves fairness metrics.",
                    "Individuals with suspected rare genetic disorders often undergo multiple clinical evaluations, imaging studies, laboratory tests and genetic tests, to find a possible answer over a prolonged period of time. Addressing this \"diagnostic odyssey\" thus has substantial clinical, psychosocial, and economic benefits. Many rare genetic diseases have distinctive facial features, which can be used by artificial intelligence algorithms to facilitate clinical diagnosis, in prioritizing candidate diseases to be further examined by lab tests or genetic assays, or in helping the phenotype-driven reinterpretation of genome/exome sequencing data. Existing methods using frontal facial photos were built on conventional Convolutional Neural Networks (CNNs), rely exclusively on facial images, and cannot capture non-facial phenotypic traits and demographic information essential for guiding accurate diagnoses. Here we introduce GestaltMML, a multimodal machine learning (MML) approach solely based on the Transformer architecture. It integrates facial images, demographic information (age, sex, ethnicity), and clinical notes (optionally, a list of Human Phenotype Ontology terms) to improve prediction accuracy. Furthermore, we also evaluated GestaltMML on a diverse range of datasets, including 528 diseases from the GestaltMatcher Database, several in-house datasets of Beckwith-Wiedemann syndrome (BWS, over-growth syndrome with distinct facial features), Sotos syndrome (overgrowth syndrome with overlapping features with BWS), NAA10-related neurodevelopmental syndrome, Cornelia de Lange syndrome (multiple malformation syndrome), and KBG syndrome (multiple malformation syndrome). Our results suggest that GestaltMML effectively incorporates multiple modalities of data, greatly narrowing candidate genetic diagnoses of rare diseases and may facilitate the reinterpretation of genome/exome sequencing data."
                ],
                "domain": [
                    "Machine Learning",
                    "Contact Geometry",
                    "Biomedical Informatics",
                    "Feature Selection"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "c77568ea-1026-4b5f-abb1-7839f9ed04ce": {
                "pk": "c77568ea-1026-4b5f-abb1-7839f9ed04ce",
                "project_name": null,
                "name": "Kai Wang",
                "bio": "I am a researcher with a strong focus on graph theory, combinatorial algorithms, and their applications in both theoretical and practical contexts. My work has primarily revolved around developing efficient algorithms for testing graphical degree sequences and counting specific sets of degree sequences, such as zero-free, connected, and biconnected graphs. I take pride in having introduced novel dynamic programming techniques that not only run in polynomial time but also significantly improve upon previous methods, enabling the tabulation of degree sequences up to n=118.\n\nIn addition to my work in graph theory, I have explored the implications of discrete gauge symmetries in particle physics, particularly in the context of the Standard Model and its extensions. My research has provided insights into the axion solution to the strong CP problem and the mu-term problem in supersymmetry, showcasing how discrete symmetries can stabilize solutions and hint at new physics.\n\nI am also interested in the intersection of statistics and finance, where I have investigated heavy-tailed distributions and their implications for risk assessment. My work in M-estimation for high-dimensional linear regression models demonstrates my commitment to developing robust statistical methods.\n\nOverall, my research is characterized by a blend of theoretical rigor and practical application, aiming to address complex problems across various domains, from combinatorial optimization to fundamental questions in physics and statistics.",
                "collaborators": [
                    "Yanling Zhu",
                    "Harald Upmeier",
                    "Zongguo Si",
                    "Chengxiu Ling",
                    "Chen Li",
                    "Wenjiang Pei"
                ],
                "pub_titles": [
                    "An Efficient Algorithm to Test Potentially Bipartiteness of Graphical Degree Sequences",
                    "Efficient Counting of Degree Sequences",
                    "An efficient algorithm to test forcibly-biconnectedness of graphical degree sequences",
                    "An efficient algorithm to test forcibly-connectedness of graphical degree sequences",
                    "Hidden symmetries and their implications for Particle Physics",
                    "Stabilizing the axion and a natural solution to the mu problem of supersymmetry",
                    "Dixmier Trace for Toeplitz Operators on Symmetric Domains",
                    "GeV Majorana Neutrinos in Top-quark Decay at the LHC",
                    "Asymptotic properties of a stochastic Gilpin-Ayala model under regime switching",
                    "M-estimation in high-dimensional linear model",
                    "Tail Asymptotic of Heavy-Tail Risks with Elliptical Copula",
                    "Joint Projective Spectrum of $D_{\\infty h}$",
                    "Applied Symbolic Vector Dynamics of Coupled Map Lattice"
                ],
                "pub_abstracts": [
                    "As a partial answer to a question of Rao, a deterministic and customizable efficient algorithm is presented to test whether an arbitrary graphical degree sequence has a bipartite realization. The algorithm can be configured to run in polynomial time, at the expense of possibly producing an erroneous output on some \"yes\" instances but with very low error rate.",
                    "Novel dynamic programming algorithms to count the set $D(n)$ of zero-free degree sequences of length $n$, the set $D_c(n)$ of degree sequences of connected graphs on $n$ vertices and the set $D_b(n)$ of degree sequences of biconnected graphs on $n$ vertices exactly are presented. They are all based on a recurrence of Barnes and Savage and shown to run in polynomial time and are asymptotically much faster than the previous best known algorithms for these problems. These appear to be the first polynomial time algorithms to compute $|D(n)|$, $|D_c(n)|$ and $|D_b(n)|$ to the author's knowledge and have enabled us to tabulate them up to $n=118$, the majority of which were unknown. The available numerical results of $|D(n)|$ tend to give more supporting evidence of a conjecture of Gordon F. Royle about the limit of $|D(n)|/|D(n-1)|$. The OEIS entries that can be computed by algorithms in this paper are A004251, A007721, A007722 and A095268.",
                    "We present an algorithm to test whether a given graphical degree sequence is forcibly biconnected or not and prove its correctness. The worst case run time complexity of the algorithm is shown to be exponential but still much better than the previous basic algorithm presented in \\cite{Wang2018}. We show through experimental evaluations that the algorithm is efficient on average. We also adapt Ruskey et al's classic algorithm to enumerate zero-free graphical degree sequences of length $n$ and Barnes and Savage's classic algorithm to enumerate graphical partitions of an even integer $n$ by incorporating our testing algorithm into theirs and then obtain some enumerative results about forcibly biconnected graphical degree sequences of given length $n$ and forcibly biconnected graphical partitions of given even integer $n$. Based on these enumerative results we make some conjectures such as: when $n$ is large, (1) the proportion of forcibly biconnected graphical degree sequences of length $n$ among all zero-free graphical degree sequences of length $n$ is asymptotically a constant between 0 and 1; (2) the proportion of forcibly biconnected graphical partitions of even $n$ among all forcibly connected graphical partitions of $n$ is asymptotically 0.",
                    "We present an algorithm to test whether a given graphical degree sequence is forcibly connected or not and prove its correctness. We also outline the extensions of the algorithm to test whether a given graphical degree sequence is forcibly $k$-connected or not for every fixed $k\\ge 2$. We show through experimental evaluations that the algorithm is efficient on average, though its worst case run time is probably exponential. We also adapt Ruskey et al's classic algorithm to enumerate zero-free graphical degree sequences of length $n$ and Barnes and Savage's classic algorithm to enumerate graphical partitions of even integer $n$ by incorporating our testing algorithm into theirs and then obtain some enumerative results about forcibly connected graphical degree sequences of given length $n$ and forcibly connected graphical partitions of given even integer $n$. Based on these enumerative results we make some conjectures such as: when $n$ is large, (1) almost all zero-free graphical degree sequences of length $n$ are forcibly connected; (2) almost none of the graphical partitions of even $n$ are forcibly connected.",
                    "In this thesis, we study the hidden symmetries in the SM and also use discrete gauge symmetries as model building tools to solve various problems in the SM as well as the MSSM, such as R-parity, mu-term, stabilizing the axion solutions, etc. The flavor independent non-supersymmetric SM at the renormalizable level has a discrete Z_3 gauge symmetry known as baryon parity. It is anomaly free at the discrete level as a result of the existence of three generations. The symmetry can effectively act as the Baryon number up to the \\Delta B=3 mod 3 level which is also consistent with the prediction from non-perturbative processes corrections in the SM, such as electroweak instanton and sphaleron processes. This symmetry is not consistent with the simple GUTs since those theories explicitly break it. Thus this baryon parity provides a strong hint for new physics like GUTs. Quantum mechanically, we estimate the triple nucleon decay rate which is predicted by the existence of this symmetry. We find a simple U(1) realization with the presence of right-handed neutrinos, from which this baryon parity can naturally emerge. Gauged R-parity is studied in the following chapter. We also study various different approaches to the mu-term problem via a symmetry classification. One explicit example in terms of a Z_4 subgroup of the anomalous U(1) symmetry is given. Another solution arises from a SUSY version of QCD invisible axion. Discrete flavor gauge symmetries are discussed to explain the observed hierarchy fermion masses. D-term splitting problem can then be avoided. In the last part, we show how to use discrete gauge symmetries to stabilize the QCD invisible axion solutions, for both DFSZ and KSVZ invisible axion models.",
                    "The axion solution to the strong CP problem makes use of a global Peccei-Quinn (PQ) U(1) symmetry which is susceptible to violations from quantum gravitational effects. We show explicitly how discrete gauge symmetries can protect the axion from such violations. PQ symmetry emerges as an accidental global symmetry from discrete gauge symmetries which are subgroups of the anomalous U(1) of string origin. We also show how the Dine-Fischler-Srednicki-Zhitnitsky (DFSZ) axion model provides a natural solution to mu problem of supersymmetry as mu ~ M_{SUSY} ~ M^2_{PQ}/M_{Pl}.",
                    "For Toeplitz operators on bounded symmetric domains of arbitrary rank, we define a Hilbert quotient module corresponding to partitions of length $1$ and prove that it belongs to the Macaev class ${\\mathcal{L}}^{n,\\infty}$. We next obtain an explicit formula for the Dixmier trace of Toeplitz commutators in terms of the underlying boundary geometry.",
                    "We explore the \\Delta L=2 same-sign dilepton signal from top-quark decay via a Majorana neutrino at the LHC in the top anti-top pair production samples. The signature is same-sign dilepton plus multi-jets with no significant missing energy. The most optimistic region lies where the Majorana neutrino mass is between 15-65 GeV. For 300 fb^-1 integrated luminosity, it is possible to probe S_{ij}, the effective mixing parameter, to order of 10^-5.",
                    "In this paper, a stochastic Gilpin-Ayala population model with regime switching and white noise is considered. All parameters are influenced by stochastic perturbations. The existence of global positive solution, asymptotic stability in probability, $p$th moment exponential stability, extinction, weak persistence, stochastic permanence and stationary distribution of the model are investigated, which generalize some results in the literatures. Moreover, the conditions presented for the stochastic permanence and the existence of stationary distribution improve the previous results.",
                    "We mainly study the M-estimation method for the high-dimensional linear regression model, and discuss the properties of M-estimator when the penalty term is the local linear approximation. In fact, M-estimation method is a framework, which covers the methods of the least absolute deviation, the quantile regression, least squares regression and Huber regression. We show that the proposed estimator possesses the good properties by applying certain assumptions. In the part of numerical simulation, we select the appropriate algorithm to show the good robustness of this method",
                    "We consider a family of multivariate distributions with heavy-tailed margins and the type I elliptical dependence structure. This class of risks is common in finance, insurance, environmental and biostatistic applications. We obtain the asymptotic tail risk probabilities and characterize the multivariate regular variation property. The results demonstrate how the rate of decay of probabilities on tail sets varies in tail sets and the covariance matrix of the elliptical copula. The theoretical results are well illustrated by typical examples and numerical simulations. A real data application shows its advantages in a more flexible dependence structure to characterize joint insurance losses.",
                    "We compute the joint spectrum of $D_{\\infty h}$ with respect to the left regular representation, and finds two generators of the De Rham cohomology group of joint resolvent set which is induced by different central linear functionals. Through action of $D_{\\infty h}$ on 4-ary trees, we get a self-similar realization of the group $C^*$ algebra of $D_{\\infty h}$.",
                    "Symbolic dynamics, which partitions an infinite number of finite-length trajectories into a finite number of trajectory sets, describes the dynamics of a system in a simplified and coarse-grained way with a limited number of symbols. The study of symbolic dynamics in 1D chaotic map has been further developed and is named as the applied symbolic dynamics. In this paper, we will study the applied symbolic vector dynamics of CML. Based on the original contribution proposed in Refs.[6], we will study the ergodic property of CML. We will analyze the relation between admissibility condition and control parameters, and then give a coupling coefficient estimation method based on the ergodic property. Both theoretical and experimental results show that we provide a natural analytical technique for understanding turbulences in CML. Many of our findings could be expanded to a wider range of application."
                ],
                "domain": [
                    "Graph Theory",
                    "Algorithm Design",
                    "Statistical Modeling",
                    "Quantum Physics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "8d9cc8cb-a402-4ef1-8992-f32701079fb1": {
                "pk": "8d9cc8cb-a402-4ef1-8992-f32701079fb1",
                "project_name": null,
                "name": "Chunhua Weng",
                "bio": "I am a researcher dedicated to harnessing the power of large language models (LLMs) to enhance clinical data extraction and improve healthcare outcomes. My recent work includes the development of PhenoBCBERT and PhenoGPT, innovative models that automate the detection of clinical phenotype terms, even those not documented in the Human Phenotype Ontology (HPO). By leveraging transformer architectures, I have demonstrated that these models can extract novel phenotype concepts from clinical notes and biomedical literature, significantly outperforming existing tools like PhenoTagger.\n\nIn addition to phenotype extraction, I have created PICOX, a novel method for identifying overlapping PICO entities, which has shown superior performance across multiple datasets. My research also addresses the challenges of extracting diagnostic information from pathology reports, where I developed a transformer-based method that achieves high accuracy in classifying dysplasia and Barrett's esophagus diagnoses.\n\nRecognizing the complexities of diagnosing rare genetic disorders, I introduced GestaltMML, a multimodal machine learning approach that integrates facial images, demographic data, and clinical notes to enhance diagnostic accuracy. This work aims to alleviate the \"diagnostic odyssey\" faced by individuals with rare diseases.\n\nI am passionate about the intersection of AI and healthcare, particularly in developing accountable and inclusive models that can synthesize medical evidence effectively. My goal is to leverage generative AI to improve decision-making in evidence-based medicine, ultimately enhancing patient care and outcomes.",
                "collaborators": [
                    "Jingye Yang",
                    "Cong Liu",
                    "Da Wu",
                    "Kai Wang",
                    "Gongbo Zhang",
                    "Yifan Peng",
                    "Wendy Deng",
                    "Yunyun Zhou",
                    "Yiliang Zhou",
                    "Yan Hu"
                ],
                "pub_titles": [
                    "Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT",
                    "A Span-based Model for Extracting Overlapping PICO Entities from RCT Publications",
                    "Large Language Models for Granularized Barrett's Esophagus Diagnosis Classification",
                    "GestaltMML: Enhancing Rare Genetic Disease Diagnosis through Multimodal Machine Learning Combining Facial Images and Clinical Texts",
                    "Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness"
                ],
                "pub_abstracts": [
                    "We hypothesize that large language models (LLMs) based on the transformer architecture can enable automated detection of clinical phenotype terms, including terms not documented in the HPO. In this study, we developed two types of models: PhenoBCBERT, a BERT-based model, utilizing Bio+Clinical BERT as its pre-trained model, and PhenoGPT, a GPT-based model that can be initialized from diverse GPT models, including open-source versions such as GPT-J, Falcon, and LLaMA, as well as closed-source versions such as GPT-3 and GPT-3.5. We compared our methods with PhenoTagger, a recently developed HPO recognition tool that combines rule-based and deep learning methods. We found that our methods can extract more phenotype concepts, including novel ones not characterized by HPO. We also performed case studies on biomedical literature to illustrate how new phenotype information can be recognized and extracted. We compared current BERT-based versus GPT-based models for phenotype tagging, in multiple aspects including model architecture, memory usage, speed, accuracy, and privacy protection. We also discussed the addition of a negation step and an HPO normalization layer to the transformer models for improved HPO term tagging. In conclusion, PhenoBCBERT and PhenoGPT enable the automated discovery of phenotype terms from clinical notes and biomedical literature, facilitating automated downstream tasks to derive new biological insights on human diseases.",
                    "Objectives Extraction of PICO (Populations, Interventions, Comparison, and Outcomes) entities is fundamental to evidence retrieval. We present a novel method PICOX to extract overlapping PICO entities.   Materials and Methods PICOX first identifies entities by assessing whether a word marks the beginning or conclusion of an entity. Then it uses a multi-label classifier to assign one or more PICO labels to a span candidate. PICOX was evaluated using one of the best-performing baselines, EBM-NLP, and three more datasets, i.e., PICO-Corpus, and RCT publications on Alzheimer's Disease or COVID-19, using entity-level precision, recall, and F1 scores.   Results PICOX achieved superior precision, recall, and F1 scores across the board, with the micro F1 score improving from 45.05 to 50.87 (p << 0.01). On the PICO-Corpus, PICOX obtained higher recall and F1 scores than the baseline and improved the micro recall score from 56.66 to 67.33. On the COVID-19 dataset, PICOX also outperformed the baseline and improved the micro F1 score from 77.10 to 80.32. On the AD dataset, PICOX demonstrated comparable F1 scores with higher precision when compared to the baseline.   Conclusion PICOX excels in identifying overlapping entities and consistently surpasses a leading baseline across multiple datasets. Ablation studies reveal that its data augmentation strategy effectively minimizes false positives and improves precision.",
                    "Diagnostic codes for Barrett's esophagus (BE), a precursor to esophageal cancer, lack granularity and precision for many research or clinical use cases. Laborious manual chart review is required to extract key diagnostic phenotypes from BE pathology reports. We developed a generalizable transformer-based method to automate data extraction. Using pathology reports from Columbia University Irving Medical Center with gastroenterologist-annotated targets, we performed binary dysplasia classification as well as granularized multi-class BE-related diagnosis classification. We utilized two clinically pre-trained large language models, with best model performance comparable to a highly tailored rule-based system developed using the same data. Binary dysplasia extraction achieves 0.964 F1-score, while the multi-class model achieves 0.911 F1-score. Our method is generalizable and faster to implement as compared to a tailored rule-based approach.",
                    "Individuals with suspected rare genetic disorders often undergo multiple clinical evaluations, imaging studies, laboratory tests and genetic tests, to find a possible answer over a prolonged period of time. Addressing this \"diagnostic odyssey\" thus has substantial clinical, psychosocial, and economic benefits. Many rare genetic diseases have distinctive facial features, which can be used by artificial intelligence algorithms to facilitate clinical diagnosis, in prioritizing candidate diseases to be further examined by lab tests or genetic assays, or in helping the phenotype-driven reinterpretation of genome/exome sequencing data. Existing methods using frontal facial photos were built on conventional Convolutional Neural Networks (CNNs), rely exclusively on facial images, and cannot capture non-facial phenotypic traits and demographic information essential for guiding accurate diagnoses. Here we introduce GestaltMML, a multimodal machine learning (MML) approach solely based on the Transformer architecture. It integrates facial images, demographic information (age, sex, ethnicity), and clinical notes (optionally, a list of Human Phenotype Ontology terms) to improve prediction accuracy. Furthermore, we also evaluated GestaltMML on a diverse range of datasets, including 528 diseases from the GestaltMatcher Database, several in-house datasets of Beckwith-Wiedemann syndrome (BWS, over-growth syndrome with distinct facial features), Sotos syndrome (overgrowth syndrome with overlapping features with BWS), NAA10-related neurodevelopmental syndrome, Cornelia de Lange syndrome (multiple malformation syndrome), and KBG syndrome (multiple malformation syndrome). Our results suggest that GestaltMML effectively incorporates multiple modalities of data, greatly narrowing candidate genetic diagnoses of rare diseases and may facilitate the reinterpretation of genome/exome sequencing data.",
                    "Evidence-based medicine promises to improve the quality of healthcare by empowering medical decisions and practices with the best available evidence. The rapid growth of medical evidence, which can be obtained from various sources, poses a challenge in collecting, appraising, and synthesizing the evidential information. Recent advancements in generative AI, exemplified by large language models, hold promise in facilitating the arduous task. However, developing accountable, fair, and inclusive models remains a complicated undertaking. In this perspective, we discuss the trustworthiness of generative AI in the context of automated summarization of medical evidence."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Biomedical Informatics",
                    "Machine Learning",
                    "Clinical AI"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "25039859-0d09-44eb-ba44-b7ae5bb1bf4f": {
                "pk": "25039859-0d09-44eb-ba44-b7ae5bb1bf4f",
                "project_name": null,
                "name": "Cong Liu",
                "bio": "I am a researcher with a diverse background in both materials science and real-time systems, focusing on the transport properties of iron-hydrogen alloys and the scheduling of heterogeneous multiprocessor systems. My recent work investigates the thermal transport properties of iron doped with hydrogen, utilizing first-principles density functional theory molecular dynamics simulations. I have explored how varying hydrogen content affects electrical resistivity and thermal conductivity under extreme conditions, revealing critical insights into planetary magnetic field stability.\n\nIn addition to my work in materials science, I have made significant contributions to the field of real-time systems. I developed a GEDF-based scheduling algorithm for soft real-time heterogeneous multiprocessors, demonstrating that bounded response times can be achieved without utilization loss. My research also addresses the challenges posed by I/O suspension delays in embedded systems, presenting improved analysis techniques and scheduling strategies that enhance system utilization.\n\nFurthermore, I have delved into the scheduling and analysis of real-time directed acyclic graph (DAG) task systems, proposing novel techniques that significantly improve schedulability. Recently, I have ventured into the realm of computer vision, developing a Dynamic Group Attention mechanism that enhances the performance of vision transformers by allowing for more relevant dependencies without spatial constraints. My interdisciplinary approach combines theoretical insights with practical applications, driving advancements in both materials science and real-time computing.",
                "collaborators": [
                    "Guangmo Tong",
                    "Ronald Cohen",
                    "Zheng Dong",
                    "Kai Liu",
                    "Tianyi Wu",
                    "Guodong Guo"
                ],
                "pub_titles": [
                    "Electrical resistivity, thermal conductivity, and viscosity of Fe-H alloys at Earth's core conditions",
                    "Supporting Soft Real-Time Sporadic Task Systems on Heterogeneous Multiprocessors with No Utilization Loss",
                    "Supporting Read/Write Applications in Embedded Real-time Systems via Suspension-aware Analysis",
                    "New Analysis Techniques for Supporting Hard Real-Time Sporadic DAG Task Systems on Multiprocessors",
                    "Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention"
                ],
                "pub_abstracts": [
                    "The transport properties (electrical resistivity, thermal conductivity, and viscosity) of iron-hydrogen alloy are of great significance in the stability and evolution of planetary magnetic fields. Here, we investigate the thermal transport properties of iron doped with varying hydrogen content as functions of pressure (P) and temperature (T) for the top and bottom of Earth's outer core and beyond, corresponding to pressures of about 130 to 300 GPa and temperatures of 4000 to 7000 K. Using first-principles density functional theory molecular dynamic simulations (FPMD), we verify that crystalline FeH$_x$ is superionic with H diffusing freely. We find a low frequency viscosity of 10-11 mPa$\\cdot$s for liquid Fe-H alloys at Earth's outer core conditions by the linear response Green-Kubo formula. Using the KKR method within density functional theory (DFT) plus Dynamical mean-field Theory (DMFT), we find saturation of electrical resistivity with increasing temperatures in liquid iron at outer core conditions. The effect of H on electrical and thermal transport we find is small, so that the exact H content of the core is not needed. The primary effect of H is on the equation of state, decreasing the density at constant P and T. We find the Lorenz number is smaller than the ideal value, and obtain for X(H)= 0.20, or 0.45 wt% H , thermal conductivity $\\kappa$ of $\\sim$105 and $\\sim$190 $Wm^{-1}K^{-1}$, respectively, at conditions near the core-mantle and inner-outer core boundary.",
                    "Heterogeneous multicore architectures are becoming increasingly popular due to their potential of achieving high performance and energy efficiency compared to the homogeneous multicore architectures. In such systems, the real-time scheduling problem becomes more challenging in that processors have different speeds. A job executing on a processor with speed $x$ for $t$ time units completes $(x \\cdot t)$ units of execution. Prior research on heterogeneous multiprocessor real-time scheduling has focused on hard real-time systems, where, significant processing capacity may have to be sacrificed in the worst-case to ensure that all deadlines are met. As meeting hard deadlines is overkill for many soft real-time systems in practice, this paper shows that on soft real-time heterogeneous multiprocessors, bounded response times can be ensured for globally-scheduled sporadic task systems with no utilization loss. A GEDF-based scheduling algorithm, namely GEDF-H, is presented and response time bounds are established under both preemptive and non-preemptive GEDF-H scheduling. Extensive experiments show that the magnitude of the derived response time bound is reasonable, often smaller than three task periods. To the best of our knowledge, this paper is the first to show that soft real-time sporadic task systems can be supported on heterogeneous multiprocessors without utilization loss, and with reasonable predicted response time.",
                    "In many embedded real-time systems, applications often interact with I/O devices via read/write operations, which may incur considerable suspension delays. Unfortunately, prior analysis methods for validating timing correctness in embedded systems become quite pessimistic when suspension delays are present. In this paper, we consider the problem of supporting two common types of I/O applications in a multiprocessor system, that is, write-only applications and read-write applications. For the write-only application model, we present a much improved analysis technique that results in only O(m) suspension-related utilization loss, where m is the number of processors. For the second application model, we present a flexible I/O placement strategy and a corresponding new scheduling algorithm, which can completely circumvent the negative impact due to read- and write-induced suspension delays. We illustrate the feasibility of the proposed I/O-placement-based schedule via a case study implementation. Furthermore, experiments presented herein show that the improvement with respect to system utilization over prior methods is often significant.",
                    "The scheduling and schedulability analysis of real-time directed acyclic graph (DAG) task systems have received much recent attention. The DAG model can accurately represent intra-task parallelim and precedence constraints existing in many application domains. Existing techniques show that analyzing the DAG model is fundamentally more challenging compared to the ordinary sporadic task model, due to the complex intra-DAG precedence constraints which may cause rather pessimistic schedulability loss. However,such increased loss is counter-intuitive because the DAG structure shall better exploit the parallelism provided by the multiprocessor platform. Our observation is that the intra-DAG precedence constraints, if not carefully considered by the scheduling algorithm, may cause very unpredictable execution behaviors of subtasks in a DAG and further cause pessimistic analysis. In this paper, we present a set of novel scheduling and analysis techniques for better supporting hard real-time sporadic DAG tasks on multiprocessors, through smartly defining and analyzing the execution order of subtasks in each DAG. Evaluation demonstrates that our developed utilization-based schedulability test is highly efficient, which dramatically improves schedulability of existing utilization-based tests by over 60% on average. Interestingly, when each DAG in the system is an ordinary sporadic task, our test becomes identical to the classical density test designed for the sporadic task model.",
                    "Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by each query attending to all keys/values, various methods have constrained the range of attention within local regions, where each query only attends to keys/values within a hand-crafted window. However, these hand-crafted window partition mechanisms are data-agnostic and ignore their input content, so it is likely that one query maybe attends to irrelevant keys/values. To address this issue, we propose a Dynamic Group Attention (DG-Attention), which dynamically divides all queries into multiple groups and selects the most relevant keys/values for each group. Our DG-Attention can flexibly model more relevant dependencies without any spatial constraint that is used in hand-crafted window based attention. Built on the DG-Attention, we develop a general vision transformer backbone named Dynamic Group Transformer (DGT). Extensive experiments show that our models can outperform the state-of-the-art methods on multiple common vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation."
                ],
                "domain": [
                    "Computational Materials",
                    "Real-Time Systems",
                    "Vision Transformers",
                    "Scheduling Algorithms"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "### [Question 1] - What is the problem?\nHow can we improve phenotype-driven gene prioritization for rare disease diagnosis using machine learning techniques?\n\n### [Question 2] - Why is it interesting and important?\nSolving this problem is crucial for the research community as it can significantly enhance the accuracy and efficiency of identifying disease-causing genes, particularly in the context of rare Mendelian diseases. Improved gene prioritization can lead to faster diagnoses, better patient outcomes, and more targeted therapeutic strategies. Furthermore, advancements in this area could stimulate further research into the genetic basis of diseases, potentially uncovering new pathways and mechanisms that could be targeted for treatment. This could also pave the way for the development of more sophisticated machine learning models that integrate genomic and phenotypic data, thereby advancing the field of computational biology.\n\n### [Question 3] - Why is it hard?\nThe challenges in improving phenotype-driven gene prioritization stem from the complexity of biological systems and the vast amount of genomic data available. Naive approaches may fail due to the high dimensionality of the data, the presence of noise, and the intricate relationships between genes and phenotypes. Additionally, existing models may not adequately capture the nuances of phenotypic expression or the multifactorial nature of diseases. Technical obstacles include the need for robust feature selection methods, the integration of heterogeneous data types, and the development of models that can generalize well across different populations and phenotypic presentations.\n\n### [Question 4] - Why hasn't it been solved before?\nPrevious research has often been limited by the lack of comprehensive datasets that link phenotypes to genotypes, as well as the reliance on traditional statistical methods that may not fully leverage the power of machine learning. Barriers such as insufficient computational resources, inadequate model interpretability, and the challenge of integrating diverse data sources have also hindered progress. Our approach aims to address these limitations by employing advanced machine learning techniques, utilizing larger and more diverse datasets, and focusing on model interpretability to ensure that the results are clinically relevant and actionable.\n\n### [Question 5] - What are the key components of my approach and results?\nOur proposed methodology involves the use of advanced machine learning algorithms, such as deep learning and ensemble methods, to analyze genomic data in conjunction with phenotypic information. We will utilize a comprehensive dataset derived from whole genome/exome sequencing of individuals with rare diseases, focusing on accurately predicting candidate disease-causing genes. The performance of our models will be evaluated using metrics such as precision, recall, and F1-score to ensure robustness"
    },
    "2403.07144": {
        "paper_data": {
            "title": "Thought Graph: Generating Thought Process for Biological Reasoning",
            "url": "http://arxiv.org/abs/2403.07144v1",
            "arxiv_id": "2403.07144",
            "authors": [
                "Chi-Yang Hsu",
                "Kyle Cox",
                "Jiawei Xu",
                "Zhen Tan",
                "Tianhua Zhai",
                "Mengzhou Hu",
                "Dexter Pratt",
                "Tianlong Chen",
                "Ziniu Hu",
                "Ying Ding"
            ],
            "abstract": "We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.",
            "introduction": "   1. Introduction  The systematic study of human disease necessitates an in-depth understanding of the links between diseases, drugs, phenotypes, genes, and biological processes (Chandak et al., 2023). Analyzing gene sets that share common biological functions, locations, or regulatory mechanisms can reveal patterns in gene behavior across health and disease states, contributing to the advancement of precision medicine for cancer treatment (Subramanian et al., 2005). Yet, the task of identifying biological processes from gene sets is fraught with challenges. Individual genes often display weak signals, and when strong signals are present, they rarely converge on a singular biological theme (Subramanian et al., 2005). This complexity is compounded when different research groups studying the same biological systems arrive at vastly divergent conclusions.   In response to these challenges, our paper introduces the Thought Graph framework that aims to address two critical aspects: firstly, it adopted a Tree-of-Thought (ToT) (Yao et al., 2023) architecture to facilitate thought expansion with Large Language Model (LLM), ensuring inclusive yet precise coverage of biological processes across varying specificity levels. Thought expansion is strategically directed with the assistance of a voter LLM, which guides the decision-making for future steps. This design aims to mitigate the potential discrepancies in human annotations encountered by researchers, yet ensure the quality of the generated processes. Second, our framework prioritizes the integration of domain-specific external knowledge bases to understand the semantics of connections within the Thought Graph. Consequently, it creates semantic relationships like “is-a” and “part-of” among various thought steps. This strategy not only facilitates complex decision-making processes but also ensures a more nuanced and interconnected understanding of biological systems, facilitating data interoperability and knowledge integration. Our novel contributions can be summarized as follows:      (1)  We propose Thought Graph as a complex reasoning framework that generates diverse yet precise entities to tackle potential annotations discrepancies in biological processes.    (2)  Thought Graph can generate thought graphs with edge semantics by recalling external knowledge (e.g., Gene Ontology) to build rich semantics among thought steps.     (3)  We have successfully applied Thought Graph in biological process generation with significant improvement compared to SOTA methods, surpassing GSEA by 40.28% and LLM baselines by 5.38% in cosine similarity score, and identified the optimal steps of complex reasoning by balancing specificity and accuracy.         2. Related Work   2.1. LLM Reasoning  Prompt strategies attempt to decompose a complicated problem into a sequence of smaller sub-problems so that the problem becomes more manageable (Zhou et al., 2023). One popular line of study is the Chain-of-Thought (CoT) (Wei et al., 2023) series, structuring prompts to encourage the LLM to step through its reasoning process, such as Least-to-Most prompting (Zhou et al., 2023), and Self-Consistency with CoT (CoT-SC) (Wang et al., 2023). However, these prompting strategies only utilize linear reasoning paths and struggle in tasks that require exploration and strategic lookahead. Alternatively, Tree of Thoughts (ToT)  (Yao et al., 2023) and Graph of Thoughts (GoT)  (Besta et al., 2023) excel in these sorts of tasks. LLM-based prompting frameworks’ effectiveness is hindered by inherent limitations such as self-bias and hallucination. To address this, through in-context learning, our work introduces the semantics of edges within our Thought Graph, offering structural information.      2.2. Knowledge Graph for LLM Reasoning  LLMs exhibit limitations in integrating new knowledge and occasionally generate hallucinations. A survey  (Agrawal",
            "references": [
                {
                    "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
                    "abstract": "The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field."
                },
                {
                    "title": "Evaluation of large language models for discovery of gene set function",
                    "abstract": "Gene set analysis is a mainstay of functional genomics, but it relies on curated databases of gene functions that are incomplete. Here we evaluate five Large Language Models (LLMs) for their ability to discover the common biological functions represented by a gene set, substantiated by supporting rationale, citations and a confidence assessment. Benchmarking against canonical gene sets from the Gene Ontology, GPT-4 confidently recovered the curated name or a more general concept (73% of cases), while benchmarking against random gene sets correctly yielded zero confidence. Gemini-Pro and Mixtral-Instruct showed ability in naming but were falsely confident for random sets, whereas Llama2–70b had poor performance overall. In gene sets derived from ‘omics data, GPT-4 identified novel functions not reported by classical functional enrichment (32% of cases), which independent review indicated were largely verifiable and not hallucinations. The ability to rapidly synthesize common gene functions positions LLMs as valuable ‘omics assistants."
                },
                {
                    "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
                    "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks"
                },
                {
                    "title": "Building a knowledge graph to enable precision medicine",
                    "abstract": null
                },
                {
                    "title": "Self-Alignment Pretraining for Biomedical Entity Representations",
                    "abstract": "Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust."
                },
                {
                    "title": "From the Cover: Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles",
                    "abstract": "Although genomewide RNA expression analysis has become a routine tool in biomedical research, extracting biological insight from such information remains a major challenge. Here, we describe a powerful analytical method called Gene Set Enrichment Analysis (GSEA) for interpreting gene expression data. The method derives its power by focusing on gene sets, that is, groups of genes that share common biological function, chromosomal location, or regulation. We demonstrate how GSEA yields insights into several cancer-related data sets, including leukemia and lung cancer. Notably, where single-gene analysis finds little similarity between two independent studies of patient survival in lung cancer, GSEA reveals many biological pathways in common. The GSEA method is embodied in a freely available software package, together with an initial database of 1,325 biologically defined gene sets."
                },
                {
                    "title": "Gene Ontology: tool for the unification of biology",
                    "abstract": null
                },
                {
                    "title": "2023. MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
                    "abstract": null
                },
                {
                    "title": "2023. Self-ConsistencyImprovesChain of Thought Reasoning in Language Models",
                    "abstract": null
                },
                {
                    "title": "2023.Chain-of-ThoughtPromptingElicitsReasoninginLargeLanguageModels",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "bad5a2a5-cdc4-40d8-b0bf-37213df1e633": {
                "pk": "bad5a2a5-cdc4-40d8-b0bf-37213df1e633",
                "project_name": null,
                "name": "Chi-Yang Hsu",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I am passionate about understanding the broader design space of GNNs. My work on AutoTransfer and FALCON aims to streamline the process of finding optimal model designs by leveraging prior knowledge and efficiently navigating the design space. I believe that by systematically studying these dimensions, we can unlock new potentials in machine learning applications.\n\nOverall, my research is driven by a desire to push the boundaries of GNNs, making them more effective and applicable to real-world challenges while fostering a deeper understanding of their underlying principles.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "c766b587-0986-49ca-a342-f0f00906d814": {
                "pk": "c766b587-0986-49ca-a342-f0f00906d814",
                "project_name": null,
                "name": "Kyle Cox",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I am passionate about understanding the broader design space of GNNs. My work on AutoTransfer and FALCON aims to streamline the process of finding optimal model designs by leveraging prior knowledge and task similarities, ultimately making GNNs more accessible and effective for diverse applications.\n\nThrough my research, I strive to bridge the gap between theoretical insights and practical implementations, contributing to the evolving landscape of machine learning and graph-based data analysis.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "26068c7c-64bb-4cac-a4ef-ba12012d8188": {
                "pk": "26068c7c-64bb-4cac-a4ef-ba12012d8188",
                "project_name": null,
                "name": "Jiawei Xu",
                "bio": "I am a researcher dedicated to advancing the field of multi-rotor aerial vehicles (MAVs) and their applications in various domains. My work spans a range of topics, including the development of modular MAV systems that can adapt their capabilities through reconfiguration, enhancing their actuation abilities to meet diverse task requirements. I have explored the intricacies of physical interactions involving MAVs, providing a comprehensive survey that categorizes these interactions and highlights the strengths and limitations of various methodologies.\n\nIn addition to MAVs, I have delved into the realm of wireless communications, particularly focusing on Rate-Splitting Multiple Access (RSMA) techniques. My research has demonstrated the advantages of RSMA in uplink communications, optimizing power allocation and decoding strategies to improve performance under finite blocklength constraints. I have also investigated the potential of multimode fibers in optical communications, revealing the existence of curved principal modes that can withstand bending, which has significant implications for various applications.\n\nMy recent projects include the design and control of innovative robotic systems, such as the PogoDrone, which combines jumping and flying capabilities for applications like soil sampling in agriculture. I am also passionate about dexterous manipulation, employing model-free reinforcement learning to enable fine contact control in robotic systems using low-cost tactile sensors. Through my research, I aim to bridge the gap between theoretical advancements and practical applications, paving the way for more versatile and efficient robotic systems.",
                "collaborators": [
                    "David Saldaña",
                    "Bruno Clerckx",
                    "Diego S. D'Antonio",
                    "Yijie Mao",
                    "Xiaosheng Xiao",
                    "Onur Dizdar",
                    "Brian Zhu",
                    "Andrew Charway",
                    "Zongqing Lu",
                    "Qingmin Liao"
                ],
                "pub_titles": [
                    "Multi-rotor Aerial Vehicles in Physical Interactions: A Survey",
                    "Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite Blocklength",
                    "Finding Optimal Modular Robots for Aerial Tasks",
                    "Principal modes of multimode fibers resisting fiber bending",
                    "Max-Min Fairness and PHY-Layer Design of Uplink MIMO Rate-Splitting Multiple Access with Finite Blocklength",
                    "Rate-Splitting Multiple Access for Short-Packet Uplink Communications: A Finite Blocklength Error Probability Analysis",
                    "H-ModQuad: Modular Multi-Rotors with 4, 5, and 6 Controllable DOF",
                    "Modular Multi-Rotors: From Quadrotors to Fully-Actuated Aerial Vehicles",
                    "PogoDrone: Design, Model, and Control of a Jumping Quadrotor",
                    "LLA-FLOW: A Lightweight Local Aggregation on Cost Volume for Optical Flow Estimation",
                    "Toward Fine Contact Interactions: Learning to Control Normal Contact Force with Limited Information"
                ],
                "pub_abstracts": [
                    "Research on Multi-rotor Aerial Vehicles (MAVs) has experienced remarkable advancements over the past two decades, propelling the field forward at an accelerated pace. Through the implementation of motion control and the integration of specialized mechanisms, researchers have unlocked the potential of MAVs to perform a wide range of tasks in diverse scenarios. Notably, the literature has highlighted the distinctive attributes of MAVs that endow them with a competitive edge in physical interaction when compared to other robotic systems. In this survey, we present a categorization of the various types of physical interactions in which MAVs are involved, supported by comprehensive case studies. We examine the approaches employed by researchers to address different challenges using MAVs and their applications, including the development of different types of controllers to handle uncertainties inherent in these interactions. By conducting a thorough analysis of the strengths and limitations associated with different methodologies, as well as engaging in discussions about potential enhancements, this survey aims to illuminate the path for future research focusing on MAVs with high actuation capabilities.",
                    "In this letter, we investigate the performance of Max Minimum Fairness (MMF) for uplink Rate-Splitting Multiple Access (RSMA) in short-packet communications. Specifically, considering a Single-Input Single-Output (SISO) Multiple Access Channel (MAC), we optimize the transmit power allocation between the splitting user messages to maximize the minimum rate among users with Finite Blocklength (FBL) constraints. To tackle this problem, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Numerical results show that RSMA outperforms conventional transmission schemes such as Non-orthogonal Multiple Access (NOMA) in terms of MMF.",
                    "Traditional aerial vehicles have limitations in their capabilities due to actuator constraints, such as motor saturation. The hardware components and their arrangement are designed to satisfy specific requirements and are difficult to modify during operation. To address this problem, we introduce a versatile modular multi-rotor vehicle that can change its capabilities by reconfiguration. Our modular robot consists of homogeneous cuboid modules, propelled by quadrotors with tilted rotors. Depending on the number of modules and their configuration, the robot can expand its actuation capabilities. In this paper, we build a mathematical model for the actuation capability of a modular multi-rotor vehicle and develop methods to determine if a vehicle is capable of satisfying a task requirement. Based on this result, we find the optimal configurations for a given task. Our approach is validated in realistic 3D simulations, showing that our modular system can adapt to tasks with varying requirements.",
                    "Multimode fibers (MMFs) have found wide application across various fields, such as optical communications, mode-locked lasers, and endoscopy. However, the practical use of MMFs is limited by the challenges posed by fiber bending, which leads to mode coupling. In this study, we present evidence that MMFs possess principal modes, named curved principal modes, that can resist significant bending. These curved principal modes are identified by extending the Wigner-Smith operator to curved MMFs, and are demonstrated for arbitrary bending by numerical simulations. These findings have substantial implications for mode-divide-multiplexed optical fiber communications, MMF-based endoscopy, and other related applications.",
                    "Rate-Splitting Multiple Access (RSMA) has emerged as a potent and reliable multiple access and interference management technique in wireless communications. While downlink Multiple-Input Multiple-Ouput (MIMO) RSMA has been widely investigated, uplink MIMO RSMA has not been fully explored. In this paper, we investigate the performance of uplink RSMA in short-packet communications with perfect Channel State Information at Transmitter (CSIT) and Channel State Information at Receiver (CSIR). We propose an uplink MIMO RSMA framework and optimize both precoders and combiners with Max-Min Fairness (MMF) metric and Finite Blocklength (FBL) constraints. Due to the high coupling between precoders and combiners, we apply the Alternating Optimization (AO) to decompose the optimization problem into two subproblems. To tackle these subproblems, we propose a Successive Convex Approximation (SCA)-based approach. Additionally, we introduce a low-complexity scheme to design the decoding order at the receiver. Subsequently, the Physical (PHY)-layer of the uplink MIMO RSMA architecture is designed and evaluated using multi-user Link-Level Simulations (LLS), accounting for finite constellation modulation, finite length polar codes, message splitting, adaptive modulation and coding, and Successive Interference Cancellation (SIC) at the receiver. Numerical results demonstrate that applying RSMA in uplink MIMO with FBL constraints not only achieves MMF gains over conventional transmission schemes such as Space Division Multiple Access (SDMA) and Non-orthogonal Multiple Access (NOMA) but also exhibits robustness to network loads. The benefits of splitting messages from multiple users are also illustrated. LLS results confirm the improved max-min throughput benefits of RSMA over SDMA and NOMA.",
                    "In this letter, we investigate Rate-Splitting Multiple Access (RSMA) for an uplink communication system with finite blocklength. Considering a two-user Single-Input Single-Output (SISO) Multiple Access Channel (MAC), we study the impact of Signal-to-Noise Ratio (SNR), blocklength, power allocation and target rate on the error probability performance of RSMA where one user message is split. We demonstrate that RSMA can improve the error probability performance significantly compared to Non-Orthogonal Multiple Access (NOMA) and RSMA can have a larger rate region than NOMA.",
                    "Traditional aerial vehicles are usually custom-designed for specific tasks. Although they offer an efficient solution, they are not always able to adapt to changes in the task specification, e.g., increasing the payload. This applies to quadrotors, having a maximum payload and only four controllable degrees of freedom, limiting their adaptability to the task's variations. We propose a versatile modular robotic system that can increase its payload and degrees of freedom by assembling heterogeneous modules; we call it H-ModQuad. It consists of cuboid modules propelled by quadrotors with tilted propellers that can generate forces in different directions. By connecting different types of modules, an H-ModQuad can increase its controllable degrees of freedom from 4 to 5 and 6. We model the general structure and propose three controllers, one for each number of controllable degrees of freedom. We extend the concept of the actuation ellipsoid to find the best reference orientation that can maximize the performance of the structure. Our approach is validated with experiments using actual robots, showing the independence of the translation and orientation of a structure.",
                    "Traditional aerial vehicles have specific characteristics to perform specific tasks but designing a versatile vehicle that can adapt depending on the task is still a challenge. Based on modularity, we propose an aerial robotic system that can increase its payload capacity and actuated degrees of freedom by reconfiguring heterogeneous modules to adapt to different task specifications. The system consists of cuboid modules propelled by quadrotors with tilted rotors. We present two module designs with different actuation properties. By assembling different types of modules, H-ModQuad can increase its actuated degrees of freedom from 4 to 5 and 6 depending on its configuration. By extending the concept of actuation ellipsoids, we find the body frame of a vehicle with which the controller can maximize the maximum thrust. We use polytopes to represent the actuation capability of the vehicles and examine them against task requirements. We derive the modular vehicles' dynamics and propose a general control strategy that applies for all possible numbers of actuated degrees of freedom. The design is validated with simulations and experiments using actual robots, showing that the modular vehicles provide different actuation properties.",
                    "We present a design, model, and control for a novel jumping-flying robot that is called PogoDrone. The robot is composed of a quadrotor with a passive mechanism for jumping. The robot can continuously jump in place or fly like a normal quadrotor. Jumping in place allows the robot to quickly move and operate very close to the ground. For instance, in agricultural applications, the jumping mechanism allows the robot to take samples of soil. We propose a hybrid controller that switches from attitude to position control to allow the robot to fall horizontally and recover to the original position. We compare the jumping mode with the hovering mode to analyze the energy consumption. In simulations, we evaluate the effect of different factors on energy consumption. In real experiments, we show that our robot can repeatedly impact the ground, jump, and fly in a physical environment.",
                    "Lack of texture often causes ambiguity in matching, and handling this issue is an important challenge in optical flow estimation. Some methods insert stacked transformer modules that allow the network to use global information of cost volume for estimation. But the global information aggregation often incurs serious memory and time costs during training and inference, which hinders model deployment. We draw inspiration from the traditional local region constraint and design the local similarity aggregation (LSA) and the shifted local similarity aggregation (SLSA). The aggregation for cost volume is implemented with lightweight modules that act on the feature maps. Experiments on the final pass of Sintel show the lower cost required for our approach while maintaining competitive performance.",
                    "Dexterous manipulation of objects through fine control of physical contacts is essential for many important tasks of daily living. A fundamental ability underlying fine contact control is compliant control, \\textit{i.e.}, controlling the contact forces while moving. For robots, the most widely explored approaches heavily depend on models of manipulated objects and expensive sensors to gather contact location and force information needed for real-time control. The models are difficult to obtain, and the sensors are costly, hindering personal robots' adoption in our homes and businesses. This study performs model-free reinforcement learning of a normal contact force controller on a robotic manipulation system built with a low-cost, information-poor tactile sensor. Despite the limited sensing capability, our force controller can be combined with a motion controller to enable fine contact interactions during object manipulation. Promising results are demonstrated in non-prehensile, dexterous manipulation experiments."
                ],
                "domain": [
                    "Aerial Robotics",
                    "Multi-rotor Vehicles",
                    "Wireless Communications",
                    "Reinforcement Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "2003517d-3e78-428b-8ecd-abcc40058464": {
                "pk": "2003517d-3e78-428b-8ecd-abcc40058464",
                "project_name": null,
                "name": "Zhen Tan",
                "bio": "I am a researcher deeply engaged in the intersection of graph representation learning and natural language processing, with a particular focus on few-shot learning and model interpretability. My recent work has centered around addressing the challenges of few-shot node classification (FSNC) through innovative methods like Virtual Node Tuning (VNT), which leverages pretrained graph transformers and introduces virtual nodes to enhance performance even in scenarios with sparse labels.\n\nI am also passionate about improving the interpretability of large language models (LLMs). My framework, SparseCBM, integrates sparsity-guided techniques to provide a comprehensive understanding of LLM behaviors across multiple layers of interpretation. This work is complemented by my exploration of metacognitive approaches, such as CLEAR, which empower LLMs to self-identify and correct errors, enhancing their reliability in high-stakes applications.\n\nIn addition to these contributions, I have tackled the Graph Few-shot Class-incremental (Graph FCL) problem, developing a Hierarchical-Attention-based Graph Meta-learning framework (HAG-Meta) that balances the learning of new and old classes effectively. My research also delves into the evolving nature of disinformation generated by LLMs, leading to the development of DELD, a parameter-efficient approach that enhances detection capabilities.\n\nThrough my work, I aim to bridge the gap between theoretical advancements and practical applications, providing robust solutions that address real-world challenges in graph mining and natural language processing. I am committed to advancing the field by releasing my code and findings to foster collaboration and further research.",
                "collaborators": [
                    "Huan Liu",
                    "Ruocheng Guo",
                    "Tianlong Chen",
                    "Kaize Ding",
                    "Bohan Jiang",
                    "Song Wang",
                    "Jundong Li",
                    "Dawei Li",
                    "Zhenyu Zhang",
                    "Jie Peng"
                ],
                "pub_titles": [
                    "Virtual Node Tuning for Few-shot Node Classification",
                    "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention",
                    "Graph Few-shot Class-incremental Learning",
                    "Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach",
                    "Inductive Linear Probing for Few-shot Node Classification",
                    "Disinformation Detection: An Evolving Challenge in the Age of LLMs",
                    "Noise-Robust Fine-Tuning of Pretrained Language Models via External Guidance",
                    "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
                    "Catching Chameleons: Detecting Evolving Disinformation Generated using Large Language Models",
                    "Exploring Large Language Models for Feature Selection: A Data-centric Perspective",
                    "Contrastive Meta-Learning for Few-shot Node Classification"
                ],
                "pub_abstracts": [
                    "Few-shot Node Classification (FSNC) is a challenge in graph representation learning where only a few labeled nodes per class are available for training. To tackle this issue, meta-learning has been proposed to transfer structural knowledge from base classes with abundant labels to target novel classes. However, existing solutions become ineffective or inapplicable when base classes have no or limited labeled nodes. To address this challenge, we propose an innovative method dubbed Virtual Node Tuning (VNT). Our approach utilizes a pretrained graph transformer as the encoder and injects virtual nodes as soft prompts in the embedding space, which can be optimized with few-shot labels in novel classes to modulate node embeddings for each specific FSNC task. A unique feature of VNT is that, by incorporating a Graph-based Pseudo Prompt Evolution (GPPE) module, VNT-GPPE can handle scenarios with sparse labels in base classes. Experimental results on four datasets demonstrate the superiority of the proposed approach in addressing FSNC with unlabeled or sparsely labeled base classes, outperforming existing state-of-the-art methods and even fully supervised baselines.",
                    "Large Language Models (LLMs) have achieved unprecedented breakthroughs in various natural language processing domains. However, the enigmatic ``black-box'' nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications. While past approaches, such as attention visualization, pivotal subnetwork extraction, and concept-based analyses, offer some insight, they often focus on either local or global explanations within a single dimension, occasionally falling short in providing comprehensive clarity. In response, we propose a novel methodology anchored in sparsity-guided techniques, aiming to provide a holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively integrates sparsity to elucidate three intertwined layers of interpretation: input, subnetwork, and concept levels. In addition, the newly introduced dimension of interpretable inference-time intervention facilitates dynamic adjustments to the model during deployment. Through rigorous empirical evaluations on real-world datasets, we demonstrate that SparseCBM delivers a profound understanding of LLM behaviors, setting it apart in both interpreting and ameliorating model inaccuracies. Codes are provided in supplements.",
                    "The ability to incrementally learn new classes is vital to all real-world artificial intelligence systems. A large portion of high-impact applications like social media, recommendation systems, E-commerce platforms, etc. can be represented by graph models. In this paper, we investigate the challenging yet practical problem, Graph Few-shot Class-incremental (Graph FCL) problem, where the graph model is tasked to classify both newly encountered classes and previously learned classes. Towards that purpose, we put forward a Graph Pseudo Incremental Learning paradigm by sampling tasks recurrently from the base classes, so as to produce an arbitrary number of training episodes for our model to practice the incremental learning skill. Furthermore, we design a Hierarchical-Attention-based Graph Meta-learning framework, HAG-Meta. We present a task-sensitive regularizer calculated from task-level attention and node class prototypes to mitigate overfitting onto either novel or base classes. To employ the topological knowledge, we add a node-level attention module to adjust the prototype representation. Our model not only achieves greater stability of old knowledge consolidation, but also acquires advantageous adaptability to new knowledge with very limited data samples. Extensive experiments on three real-world datasets, including Amazon-clothing, Reddit, and DBLP, show that our framework demonstrates remarkable advantages in comparison with the baseline and other related state-of-the-art methods.",
                    "Large Language Models (LLMs) have catalyzed transformative advances across a spectrum of natural language processing tasks through few-shot or zero-shot prompting, bypassing the need for parameter tuning. While convenient, this modus operandi aggravates ``hallucination'' concerns, particularly given the enigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns are exacerbated in high-stakes applications (e.g., healthcare), where unaccountable decision errors can lead to devastating consequences. In contrast, human decision-making relies on nuanced cognitive processes, such as the ability to sense and adaptively correct misjudgments through conceptual understanding. Drawing inspiration from human cognition, we propose an innovative \\textit{metacognitive} approach, dubbed \\textbf{CLEAR}, to equip LLMs with capabilities for self-aware error identification and correction. Our framework facilitates the construction of concept-specific sparse subnetworks that illuminate transparent decision pathways. This provides a novel interface for model \\textit{intervention} after deployment. Our intervention offers compelling advantages: (\\textit{i})~at deployment or inference time, our metacognitive LLMs can self-consciously identify potential mispredictions with minimum human involvement, (\\textit{ii})~the model has the capability to self-correct its errors efficiently, obviating the need for additional tuning, and (\\textit{iii})~the rectification procedure is not only self-explanatory but also user-friendly, enhancing the interpretability and accessibility of the model. By integrating these metacognitive features, our approach pioneers a new path toward engendering greater trustworthiness and accountability in the deployment of LLMs.",
                    "Meta-learning has emerged as a powerful training strategy for few-shot node classification, demonstrating its effectiveness in the transductive setting. However, the existing literature predominantly focuses on transductive few-shot node classification, neglecting the widely studied inductive setting in the broader few-shot learning community. This oversight limits our comprehensive understanding of the performance of meta-learning based methods on graph data. In this work, we conduct an empirical study to highlight the limitations of current frameworks in the inductive few-shot node classification setting. Additionally, we propose a simple yet competitive baseline approach specifically tailored for inductive few-shot node classification tasks. We hope our work can provide a new path forward to better understand how the meta-learning paradigm works in the graph domain.",
                    "The advent of generative Large Language Models (LLMs) such as ChatGPT has catalyzed transformative advancements across multiple domains. However, alongside these advancements, they have also introduced potential threats. One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. This work aims to address this issue by answering three research questions: (1) To what extent can the current disinformation detection technique reliably detect LLM-generated disinformation? (2) If traditional techniques prove less effective, can LLMs themself be exploited to serve as a robust defense against advanced disinformation? and, (3) Should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? A holistic exploration for the formation and detection of disinformation is conducted to foster this line of research.",
                    "Adopting a two-stage paradigm of pretraining followed by fine-tuning, Pretrained Language Models (PLMs) have achieved substantial advancements in the field of natural language processing. However, in real-world scenarios, data labels are often noisy due to the complex annotation process, making it essential to develop strategies for fine-tuning PLMs with such noisy labels. To this end, we introduce an innovative approach for fine-tuning PLMs using noisy labels, which incorporates the guidance of Large Language Models (LLMs) like ChatGPT. This guidance assists in accurately distinguishing between clean and noisy samples and provides supplementary information beyond the noisy labels, thereby boosting the learning process during fine-tuning PLMs. Extensive experiments on synthetic and real-world noisy datasets further demonstrate the superior advantages of our framework over the state-of-the-art baselines.",
                    "While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into generating path selection, as well as the choosing of suitable distillation tasks. All the code and data in this work will be released at https://github.com/David-Li0406/Contextulization-Distillation",
                    "Despite recent advancements in detecting disinformation generated by large language models (LLMs), current efforts overlook the ever-evolving nature of this disinformation. In this work, we investigate a challenging yet practical research problem of detecting evolving LLM-generated disinformation. Disinformation evolves constantly through the rapid development of LLMs and their variants. As a consequence, the detection model faces significant challenges. First, it is inefficient to train separate models for each disinformation generator. Second, the performance decreases in scenarios when evolving LLM-generated disinformation is encountered in sequential order. To address this problem, we propose DELD (Detecting Evolving LLM-generated Disinformation), a parameter-efficient approach that jointly leverages the general fact-checking capabilities of pre-trained language models (PLM) and the independent disinformation generation characteristics of various LLMs. In particular, the learned characteristics are concatenated sequentially to facilitate knowledge accumulation and transformation. DELD addresses the issue of label scarcity by integrating the semantic embeddings of disinformation with trainable soft prompts to elicit model-specific knowledge. Our experiments show that \\textit{DELD} significantly outperforms state-of-the-art methods. Moreover, our method provides critical insights into the unique patterns of disinformation generation across different LLMs, offering valuable perspectives in this line of research.",
                    "The rapid advancement of Large Language Models (LLMs) has significantly influenced various domains, leveraging their exceptional few-shot and zero-shot learning capabilities. In this work, we aim to explore and understand the LLMs-based feature selection methods from a data-centric perspective. We begin by categorizing existing feature selection methods with LLMs into two groups: data-driven feature selection which requires samples values to do statistical inference and text-based feature selection which utilizes prior knowledge of LLMs to do semantical associations using descriptive context. We conduct extensive experiments in both classification and regression tasks with LLMs in various sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the effectiveness and robustness of text-based feature selection methods and showcase their potentials using a real-world medical application. We also discuss the challenges and future opportunities in employing LLMs for feature selection, offering insights for further research and development in this emerging field.",
                    "Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. Particularly, in this paper, we refer to the task of classifying nodes in classes with a few labeled nodes as the few-shot node classification problem. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and inter-class generalizability of the model. We create a novel contrastive meta-learning framework on graphs, named COSMIC, with two key designs. First, we propose to enhance the intra-class generalizability by involving a contrastive two-step optimization in each episode to explicitly align node embeddings in the same classes. Second, we strengthen the inter-class generalizability by generating hard node classes via a novel similarity-sensitive mix-up strategy. Extensive experiments on few-shot node classification datasets verify the superiority of our framework over state-of-the-art baselines. Our code is provided at https://github.com/SongW-SW/COSMIC."
                ],
                "domain": [
                    "Graph Representation Learning",
                    "Few-shot Learning",
                    "Large Language Models",
                    "Meta-learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ed40e207-49ae-4a5a-9b4b-f55ac5973c53": {
                "pk": "ed40e207-49ae-4a5a-9b4b-f55ac5973c53",
                "project_name": null,
                "name": "Tianhua Zhai",
                "bio": "I am a researcher dedicated to uncovering the intricate relationships between social determinants of health (SDoH) and Alzheimer's disease (AD). My recent work focuses on developing an innovative, automated framework that harnesses the power of large language models and natural language processing to extract and integrate SDoH knowledge from extensive literature. By combining this information with biological entities related to AD from the PrimeKG knowledge graph, I aim to shed light on the underlying mechanisms that connect these nonmedical factors to health outcomes.\n\nUtilizing graph neural networks, I have conducted link prediction tasks to evaluate the effectiveness of the SDoH-augmented knowledge graph, demonstrating its potential for enhancing knowledge discovery in the field of Alzheimer's research. My work not only contributes to a deeper understanding of how social factors influence health but also provides a versatile tool that can be applied to other areas of SDoH research. I am passionate about leveraging advanced computational techniques to drive insights that can ultimately improve health outcomes and inform public health strategies. You can find my code and further details about my research at: https://github.com/hwq0726/SDoHenPKG.",
                "collaborators": [
                    "Tianqi Shang",
                    "Shu Yang",
                    "Weiqing He",
                    "Dawei Li",
                    "Bojian Hou",
                    "Tianlong Chen",
                    "Jason H. Moore",
                    "Marylyn D. Ritchie",
                    "Li Shen"
                ],
                "pub_titles": [
                    "Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs"
                ],
                "pub_abstracts": [
                    "Growing evidence suggests that social determinants of health (SDoH), a set of nonmedical factors, affect individuals' risks of developing Alzheimer's disease (AD) and related dementias. Nevertheless, the etiological mechanisms underlying such relationships remain largely unclear, mainly due to difficulties in collecting relevant information. This study presents a novel, automated framework that leverages recent advancements of large language model (LLM) and natural language processing techniques to mine SDoH knowledge from extensive literature and integrate it with AD-related biological entities extracted from the general-purpose knowledge graph PrimeKG. Utilizing graph neural networks, we performed link prediction tasks to evaluate the resultant SDoH-augmented knowledge graph. Our framework shows promise for enhancing knowledge discovery in AD and can be generalized to other SDoH-related research areas, offering a new tool for exploring the impact of social determinants on health outcomes. Our code is available at: https://github.com/hwq0726/SDoHenPKG"
                ],
                "domain": [
                    "Natural Language Processing",
                    "Graph Neural Network",
                    "Social Determinants of Health",
                    "Alzheimer's Disease"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "eb15b089-2fe6-4924-aac2-50ea7a2ff09c": {
                "pk": "eb15b089-2fe6-4924-aac2-50ea7a2ff09c",
                "project_name": null,
                "name": "Mengzhou Hu",
                "bio": "As a researcher in functional genomics, I am deeply invested in enhancing gene set analysis, a critical component of understanding biological functions. My recent work evaluates the capabilities of Large Language Models (LLMs) in this domain, particularly focusing on their ability to uncover common biological functions within gene sets. In my latest study, I benchmarked five LLMs, including GPT-4, against established gene sets from the Gene Ontology. I found that GPT-4 demonstrated a remarkable ability to accurately identify gene functions, achieving a 73% success rate in recovering curated names or broader concepts.\n\nWhat excites me most is the potential of these models to reveal novel functions from 'omics data that traditional methods might overlook. In fact, my research showed that GPT-4 identified new functions in 32% of cases, many of which were independently verified. This work positions LLMs as promising tools for researchers in the field, serving as efficient 'omics assistants that can synthesize complex biological information rapidly. I am passionate about exploring how these advanced technologies can transform our understanding of genomics and contribute to the broader scientific community.",
                "collaborators": [
                    "Sahar Alkhairy",
                    "Ingoo Lee",
                    "Rudolf T. Pillich",
                    "Dylan Fong",
                    "Kevin Smith",
                    "Robin Bachelder",
                    "Trey Ideker",
                    "Dexter Pratt"
                ],
                "pub_titles": [
                    "Evaluation of large language models for discovery of gene set function"
                ],
                "pub_abstracts": [
                    "Gene set analysis is a mainstay of functional genomics, but it relies on curated databases of gene functions that are incomplete. Here we evaluate five Large Language Models (LLMs) for their ability to discover the common biological functions represented by a gene set, substantiated by supporting rationale, citations and a confidence assessment. Benchmarking against canonical gene sets from the Gene Ontology, GPT-4 confidently recovered the curated name or a more general concept (73% of cases), while benchmarking against random gene sets correctly yielded zero confidence. Gemini-Pro and Mixtral-Instruct showed ability in naming but were falsely confident for random sets, whereas Llama2-70b had poor performance overall. In gene sets derived from 'omics data, GPT-4 identified novel functions not reported by classical functional enrichment (32% of cases), which independent review indicated were largely verifiable and not hallucinations. The ability to rapidly synthesize common gene functions positions LLMs as valuable 'omics assistants."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Functional Genomics",
                    "Large Language Models"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "41a533cd-b4b0-4174-9da0-bf4c16c07aa6": {
                "pk": "41a533cd-b4b0-4174-9da0-bf4c16c07aa6",
                "project_name": null,
                "name": "Dexter Pratt",
                "bio": "As a researcher in functional genomics, I am deeply invested in enhancing gene set analysis, a critical component of understanding biological functions. My recent work evaluates the capabilities of Large Language Models (LLMs) in this domain, particularly focusing on their ability to uncover common biological functions within gene sets. In my latest study, I benchmarked five LLMs, including GPT-4, against established gene sets from the Gene Ontology. I found that GPT-4 demonstrated a remarkable ability to accurately identify gene functions, achieving a 73% success rate in recovering curated names or broader concepts.\n\nWhat excites me most is the potential of LLMs to reveal novel functions from 'omics data that traditional methods may overlook. In fact, my research showed that GPT-4 identified new functions in 32% of cases, many of which were independently verified. This positions LLMs as promising tools for researchers in the field, serving as efficient 'omics assistants that can synthesize complex biological information rapidly. I am passionate about exploring how these advanced models can transform our approach to genomics and contribute to a more comprehensive understanding of gene functions.",
                "collaborators": [
                    "Mengzhou Hu",
                    "Sahar Alkhairy",
                    "Ingoo Lee",
                    "Rudolf T. Pillich",
                    "Dylan Fong",
                    "Kevin Smith",
                    "Robin Bachelder",
                    "Trey Ideker"
                ],
                "pub_titles": [
                    "Evaluation of large language models for discovery of gene set function"
                ],
                "pub_abstracts": [
                    "Gene set analysis is a mainstay of functional genomics, but it relies on curated databases of gene functions that are incomplete. Here we evaluate five Large Language Models (LLMs) for their ability to discover the common biological functions represented by a gene set, substantiated by supporting rationale, citations and a confidence assessment. Benchmarking against canonical gene sets from the Gene Ontology, GPT-4 confidently recovered the curated name or a more general concept (73% of cases), while benchmarking against random gene sets correctly yielded zero confidence. Gemini-Pro and Mixtral-Instruct showed ability in naming but were falsely confident for random sets, whereas Llama2-70b had poor performance overall. In gene sets derived from 'omics data, GPT-4 identified novel functions not reported by classical functional enrichment (32% of cases), which independent review indicated were largely verifiable and not hallucinations. The ability to rapidly synthesize common gene functions positions LLMs as valuable 'omics assistants."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Functional Genomics",
                    "Large Language Models"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "11a1a488-e980-4227-9baa-7d932178adc7": {
                "pk": "11a1a488-e980-4227-9baa-7d932178adc7",
                "project_name": null,
                "name": "Tianlong Chen",
                "bio": "I am a researcher dedicated to advancing the field of deep learning, particularly in the areas of generative adversarial networks (GANs), self-supervised learning, and graph neural networks. My recent work has focused on optimizing GANs for resource-constrained applications, where I explored the existence of trainable matching subnetworks within GANs, demonstrating significant performance improvements over traditional compression methods. I have also investigated the lottery ticket hypothesis, proposing efficient strategies for finding winning tickets in large-scale models, which has implications for both model efficiency and intellectual property protection.\n\nIn addition to GANs, I have made strides in enhancing adversarial robustness through innovative self-supervised pre-training techniques. My work on Adversarial Contrastive Learning (ACL) has shown that models can achieve state-of-the-art robustness by learning representations that are consistent under various perturbations. I have also tackled the challenges of sparse adversarial attacks, developing a homotopy algorithm that effectively balances sparsity and perturbation constraints.\n\nMy research extends to the realm of graph-structured data, where I have pioneered methods for self-supervised learning in graph convolutional networks (GCNs). By integrating self-supervision into GCNs, I have demonstrated improved generalizability and robustness. Furthermore, I have developed a unified framework for automating data augmentation in graph contrastive learning, significantly enhancing the applicability of these methods across diverse datasets.\n\nOverall, my work aims to bridge the gap between theoretical advancements and practical applications in deep learning, contributing to more efficient, robust, and interpretable models.",
                "collaborators": [
                    "Zhangyang Wang",
                    "Zhenyu Zhang",
                    "Xuxi Chen",
                    "Ziyu Jiang",
                    "Yang Shen",
                    "Ting Chen",
                    "Yuning You",
                    "Zhen Tan",
                    "Huan Liu",
                    "Yongduo Sui"
                ],
                "pub_titles": [
                    "GANs Can Play Lottery Tickets Too",
                    "Efficient Lottery Ticket Finding: Less Data is More",
                    "You are caught stealing my winning lottery ticket! Making a lottery ticket claim its ownership",
                    "Robust Pre-Training by Adversarial Contrastive Learning",
                    "Sparse and Imperceptible Adversarial Attack via a Homotopy Algorithm",
                    "Improving Contrastive Learning on Imbalanced Seed Data via Open-World Sampling",
                    "Network community detection using modularity density measures",
                    "Learning to Optimize in Swarms",
                    "Focus Longer to See Better:Recursively Refined Attention for Fine-Grained Image Classification",
                    "When Does Self-Supervision Help Graph Convolutional Networks?",
                    "Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention",
                    "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
                    "Rethinking Pruning for Vision-Language Models: Strategies for Effective Sparsity and Performance Restoration",
                    "Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark",
                    "Self-Damaging Contrastive Learning",
                    "Graph Contrastive Learning Automated"
                ],
                "pub_abstracts": [
                    "Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at 67%-74% sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator play a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.",
                    "The lottery ticket hypothesis (LTH) reveals the existence of winning tickets (sparse but critical subnetworks) for dense networks, that can be trained in isolation from random initialization to match the latter's accuracies. However, finding winning tickets requires burdensome computations in the train-prune-retrain process, especially on large-scale datasets (e.g., ImageNet), restricting their practical benefits. This paper explores a new perspective on finding lottery tickets more efficiently, by doing so only with a specially selected subset of data, called Pruning-Aware Critical set (PrAC set), rather than using the full training set. The concept of PrAC set was inspired by the recent observation, that deep networks have samples that are either hard to memorize during training, or easy to forget during pruning. A PrAC set is thus hypothesized to capture those most challenging and informative examples for the dense model. We observe that a high-quality winning ticket can be found with training and pruning the dense network on the very compact PrAC set, which can substantially save training iterations for the ticket finding process. Extensive experiments validate our proposal across diverse datasets and network architectures. Specifically, on CIFAR-10, CIFAR-100, and Tiny ImageNet, we locate effective PrAC sets at 35.32%~78.19% of their training set sizes. On top of them, we can obtain the same competitive winning tickets for the corresponding dense networks, yet saving up to 82.85%~92.77%, 63.54%~74.92%, and 76.14%~86.56% training iterations, respectively. Crucially, we show that a PrAC set found is reusable across different network architectures, which can amortize the extra cost of finding PrAC sets, yielding a practical regime for efficient lottery ticket finding.",
                    "Despite tremendous success in many application scenarios, the training and inference costs of using deep learning are also rapidly increasing over time. The lottery ticket hypothesis (LTH) emerges as a promising framework to leverage a special sparse subnetwork (i.e., winning ticket) instead of a full model for both training and inference, that can lower both costs without sacrificing the performance. The main resource bottleneck of LTH is however the extraordinary cost to find the sparse mask of the winning ticket. That makes the found winning ticket become a valuable asset to the owners, highlighting the necessity of protecting its copyright. Our setting adds a new dimension to the recently soaring interest in protecting against the intellectual property (IP) infringement of deep models and verifying their ownerships, since they take owners' massive/unique resources to develop or train. While existing methods explored encrypted weights or predictions, we investigate a unique way to leverage sparse topological information to perform lottery verification, by developing several graph-based signatures that can be embedded as credentials. By further combining trigger set-based methods, our proposal can work in both white-box and black-box verification scenarios. Through extensive experiments, we demonstrate the effectiveness of lottery verification in diverse models (ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100. Specifically, our verification is shown to be robust to removal attacks such as model fine-tuning and pruning, as well as several ambiguity attacks. Our codes are available at https://github.com/VITA-Group/NO-stealing-LTH.",
                    "Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99% on robust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.",
                    "Sparse adversarial attacks can fool deep neural networks (DNNs) by only perturbing a few pixels (regularized by l_0 norm). Recent efforts combine it with another l_infty imperceptible on the perturbation magnitudes. The resultant sparse and imperceptible attacks are practically relevant, and indicate an even higher vulnerability of DNNs that we usually imagined. However, such attacks are more challenging to generate due to the optimization difficulty by coupling the l_0 regularizer and box constraints with a non-convex objective. In this paper, we address this challenge by proposing a homotopy algorithm, to jointly tackle the sparsity and the perturbation bound in one unified framework. Each iteration, the main step of our algorithm is to optimize an l_0-regularized adversarial loss, by leveraging the nonmonotone Accelerated Proximal Gradient Method (nmAPG) for nonconvex programming; it is followed by an l_0 change control step, and an optional post-attack step designed to escape bad local minima. We also extend the algorithm to handling the structural sparsity regularizer. We extensively examine the effectiveness of our proposed homotopy attack for both targeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets. Compared to state-of-the-art methods, our homotopy attack leads to significantly fewer perturbations, e.g., reducing 42.91% on CIFAR-10 and 75.03% on ImageNet (average case, targeted attack), at similar maximal perturbation magnitudes, when still achieving 100% attack success rates. Our codes are available at: https://github.com/VITA-Group/SparseADV_Homotopy.",
                    "Contrastive learning approaches have achieved great success in learning visual representations with few labels of the target classes. That implies a tantalizing possibility of scaling them up beyond a curated \"seed\" benchmark, to incorporating more unlabeled images from the internet-scale external sources to enhance its performance. However, in practice, larger amount of unlabeled data will require more computing resources due to the bigger model size and longer training needed. Moreover, open-world unlabeled data usually follows an implicit long-tail class or attribute distribution, many of which also do not belong to the target classes. Blindly leveraging all unlabeled data hence can lead to the data imbalance as well as distraction issues. This motivates us to seek a principled approach to strategically select unlabeled data from an external source, in order to learn generalizable, balanced and diverse representations for relevant classes. In this work, we present an open-world unlabeled data sampling framework called Model-Aware K-center (MAK), which follows three simple principles: (1) tailness, which encourages sampling of examples from tail classes, by sorting the empirical contrastive loss expectation (ECLE) of samples over random data augmentations; (2) proximity, which rejects the out-of-distribution outliers that may distract training; and (3) diversity, which ensures diversity in the set of sampled examples. Empirically, using ImageNet-100-LT (without labels) as the seed dataset and two \"noisy\" external data sources, we demonstrate that MAK can consistently improve both the overall representation quality and the class balancedness of the learned features, as evaluated via linear classifier evaluation on full-shot and few-shot settings. The code is available at: https://github.com/VITA-Group/MAK",
                    "Modularity, since its introduction, has remained one of the most widely used metrics to assess the quality of community structure in a complex network. However the resolution limit problem associated with modularity limits its applicability to networks with community sizes smaller than a certain scale. In the past various attempts have been made to solve this problem. More recently a new metric, modularity density, was introduced for the quality of community structure in networks in order to solve some of the known problems with modularity, particularly the resolution limit problem. Modularity density resolves some communities which are otherwise undetectable using modularity. However, we find that it does not solve the resolution limit problem completely by investigating some cases where it fails to detect expected community structures. To address this problem, we introduce a variant of this metric and show that it further reduces the resolution limit problem, effectively eliminating the problem in a wide range of networks.",
                    "Learning to optimize has emerged as a powerful framework for various optimization and machine learning tasks. Current such \"meta-optimizers\" often learn in the space of continuous optimization algorithms that are point-based and uncertainty-unaware. To overcome the limitations, we propose a meta-optimizer that learns in the algorithmic space of both point-based and population-based optimization algorithms. The meta-optimizer targets at a meta-loss function consisting of both cumulative regret and entropy. Specifically, we learn and interpret the update formula through a population of LSTMs embedded with sample- and feature-level attentions. Meanwhile, we estimate the posterior directly over the global optimum and use an uncertainty measure to help guide the learning process. Empirical results over non-convex test functions and the protein-docking application demonstrate that this new meta-optimizer outperforms existing competitors.",
                    "Deep Neural Network has shown great strides in the coarse-grained image classification task. It was in part due to its strong ability to extract discriminative feature representations from the images. However, the marginal visual difference between different classes in fine-grained images makes this very task harder. In this paper, we tried to focus on these marginal differences to extract more representative features. Similar to human vision, our network repetitively focuses on parts of images to spot small discriminative parts among the classes. Moreover, we show through interpretability techniques how our network focus changes from coarse to fine details. Through our experiments, we also show that a simple attention model can aggregate (weighted) these finer details to focus on the most dominant discriminative part of the image. Our network uses only image-level labels and does not need bounding box/part annotation information. Further, the simplicity of our network makes it an easy plug-n-play module. Apart from providing interpretability, our network boosts the performance (up to 2%) when compared to its baseline counterparts. Our codebase is available at https://github.com/TAMU-VITA/Focus-Longer-to-See-Better",
                    "Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate self-supervision into GCNs, analyze the limitations of pretraining & finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel self-supervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https://github.com/Shen-Lab/SS-GCNs.",
                    "Large Language Models (LLMs) have achieved unprecedented breakthroughs in various natural language processing domains. However, the enigmatic ``black-box'' nature of LLMs remains a significant challenge for interpretability, hampering transparent and accountable applications. While past approaches, such as attention visualization, pivotal subnetwork extraction, and concept-based analyses, offer some insight, they often focus on either local or global explanations within a single dimension, occasionally falling short in providing comprehensive clarity. In response, we propose a novel methodology anchored in sparsity-guided techniques, aiming to provide a holistic interpretation of LLMs. Our framework, termed SparseCBM, innovatively integrates sparsity to elucidate three intertwined layers of interpretation: input, subnetwork, and concept levels. In addition, the newly introduced dimension of interpretable inference-time intervention facilitates dynamic adjustments to the model during deployment. Through rigorous empirical evaluations on real-world datasets, we demonstrate that SparseCBM delivers a profound understanding of LLM behaviors, setting it apart in both interpreting and ameliorating model inaccuracies. Codes are provided in supplements.",
                    "While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into generating path selection, as well as the choosing of suitable distillation tasks. All the code and data in this work will be released at https://github.com/David-Li0406/Contextulization-Distillation",
                    "Vision-Language Models (VLMs) integrate information from multiple modalities and have shown remarkable success across various tasks. However, deploying large-scale VLMs in resource-constrained scenarios is challenging. Pruning followed by finetuning offers a potential solution but remains underexplored for VLMs. This study addresses two key questions: how to distribute sparsity across different modality-specific models, and how to restore the performance of pruned sparse VLMs. Our preliminary studies identified two effective pruning settings: applying the same sparsity to both vision and language models, and pruning only the language models. While LoRA finetuning aims to restore sparse models, it faces challenges due to incompatibility with sparse models, disrupting the pruned sparsity. To overcome these issues, we propose SparseLoRA, which applies sparsity directly to LoRA weights. Our experimental results demonstrate significant improvements, including an 11.3\\% boost under 2:4 sparsity and a 47.6\\% enhancement under unstructured 70\\% sparsity. Code is released at: \\url{https://github.com/Shwai-He/VLM-Compression}.",
                    "Large Language Models~(LLMs) have become foundational in the realm of natural language processing, demonstrating performance improvements as model sizes increase. The Mixture-of-Experts~(MoE) approach offers a promising way to scale LLMs more efficiently by using fewer computational FLOPs through sparse activation. However, it suffers from significant memory overheads, necessitating model compression techniques. Post-training quantization, a popular method for model compression, proves less effective when directly applied to MoE models due to MoE's overlooked inherent sparsity. This paper explores several MoE structure-aware quantization heuristics, ranging from coarse to fine granularity, from MoE block to individual linear weight. Our investigations reveal critical principles: different MoE structures (i.e., blocks, experts, linear layers) require varying numbers of weight bits for effective and efficient quantization. Conclusions are supported by extensive benchmarking across two representative MoE models and six tasks. We further introduce novel enhancements to more accurately identify the most critical weights in MoE quantization that necessitate higher bit allocations, including the linear weight outlier scorer and MoE block scorer. Additionally, subsequent experiments validate our findings in the context of both weight and activation quantization.",
                    "The recent breakthrough achieved by contrastive learning accelerates the pace for deploying unsupervised training on real-world data applications. However, unlabeled data in reality is commonly imbalanced and shows a long-tail distribution, and it is unclear how robustly the latest contrastive learning methods could perform in the practical scenario. This paper proposes to explicitly tackle this challenge, via a principled framework called Self-Damaging Contrastive Learning (SDCLR), to automatically balance the representation learning without knowing the classes. Our main inspiration is drawn from the recent finding that deep models have difficult-to-memorize samples, and those may be exposed through network pruning. It is further natural to hypothesize that long-tail samples are also tougher for the model to learn well due to insufficient examples. Hence, the key innovation in SDCLR is to create a dynamic self-competitor model to contrast with the target model, which is a pruned version of the latter. During training, contrasting the two models will lead to adaptive online mining of the most easily forgotten samples for the current target model, and implicitly emphasize them more in the contrastive loss. Extensive experiments across multiple datasets and imbalance settings show that SDCLR significantly improves not only overall accuracies but also balancedness, in terms of linear evaluation on the full-shot and few-shot settings. Our code is available at: https://github.com/VITA-Group/SDCLR.",
                    "Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous \"best practices\" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at https://github.com/Shen-Lab/GraphCL_Automated."
                ],
                "domain": [
                    "Deep Learning",
                    "Generative Adversarial Networks",
                    "Self-Supervised Learning",
                    "Graph Neural Networks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "56aa11f2-b832-4475-888d-b714b93d99c6": {
                "pk": "56aa11f2-b832-4475-888d-b714b93d99c6",
                "project_name": null,
                "name": "Ziniu Hu",
                "bio": "I am a researcher dedicated to enhancing the usability and functionality of mobile applications and advancing the field of graph neural networks (GNNs). My work spans a variety of topics, including inter-app navigation, reproducible testing for Android apps, and the development of novel algorithms for unbiased learning-to-rank systems. I have conducted extensive empirical studies, such as analyzing deep link adoption in Android apps and proposing the Aladdin framework to automate deep link generation, which significantly reduces the manual effort required by developers.\n\nIn the realm of GNNs, I have contributed to the design of the Heterogeneous Graph Transformer (HGT) for modeling complex graph structures and developed frameworks like GPT-GNN for generative pre-training of GNNs. My research also addresses challenges in commonsense reasoning and knowledge transfer in heterogeneous graphs, where I introduced the Knowledge Transfer Network (KTN) to improve performance on zero-labeled node types.\n\nI am particularly passionate about bridging the gap between theoretical advancements and practical applications, as evidenced by my work on stock trend prediction using hybrid attention networks and my exploration of large language models in strategic games. My goal is to create tools and methodologies that not only push the boundaries of research but also provide tangible benefits to developers and users alike. Through my research, I aim to foster a deeper understanding of user behavior and improve the overall experience in both mobile and graph-based applications.",
                "collaborators": [
                    "Yizhou Sun",
                    "Kai-Wei Chang",
                    "Yun Ma",
                    "Yuxiao Dong",
                    "Xuanzhe Liu",
                    "Kuansan Wang",
                    "Sheng Shen",
                    "Jonathan Light",
                    "Ting Chen",
                    "Qiaozhu Mei"
                ],
                "pub_titles": [
                    "Roaming across the Castle Tunnels: an Empirical Study of Inter-App Navigation Behaviors of Android Users",
                    "DroidWalker: Generating Reproducible Test Cases via Automatic Exploration of Android Apps",
                    "Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithm",
                    "Heterogeneous Graph Transformer",
                    "Relation-Guided Pre-Training for Open-Domain Question Answering",
                    "Towards Release Strategy Optimization for Apps in Google Play",
                    "Automating Release of Deep Link APIs for Android Applications",
                    "Fuzzy Logic Based Logical Query Answering on Knowledge Graphs",
                    "Dataset Distillation for Offline Reinforcement Learning",
                    "Motif-Driven Contrastive Learning of Graph Representations",
                    "Learning to Group Auxiliary Datasets for Molecule",
                    "AvalonBench: Evaluating LLMs Playing the Game of Avalon",
                    "Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction",
                    "Pre-Training Graph Neural Networks for Generic Structural Feature Extraction",
                    "Few-Shot Representation Learning for Out-Of-Vocabulary Words",
                    "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
                    "DroidLink: Automated Generation of Deep Links for Android Apps",
                    "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning",
                    "Zero-shot Transfer Learning within a Heterogeneous Graph via Knowledge Transfer Networks",
                    "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search"
                ],
                "pub_abstracts": [
                    "Mobile applications (a.k.a., apps), which facilitate a large variety of tasks on mobile devices, have become indispensable in our everyday lives. Accomplishing a task may require the user to navigate among various apps. Unlike Web pages that are inherently interconnected through hyperlinks, mobile apps are usually isolated building blocks, and the lack of direct links between apps has largely compromised the efficiency of task completion. In this paper, we present the first in-depth empirical study of inter-app navigation behaviors of smartphone users based on a comprehensive dataset collected through a sizable user study over three months. We propose a model to distinguish informational pages and transitional pages, based on which a large number of inter-app navigation are identified. We reveal that developing 'tunnels' between of isolated apps has a huge potential to reduce the cost of navigation. Our analysis provides various practical implications on how to improve app-navigation experiences from both the operating system's perspective and the developer's perspective.",
                    "Generating test cases through automatic app exploration is very useful for analyzing and testing Android apps. However, test cases generated by current app-exploration tools are not reproducible, i.e. when the generated test case is re-executed, the app cannot reach the same state as the explored one. As a result, app developers are not able to reproduce the failure or crash reported during the exploration, to conduct regression test after fixing the bug, or to execute the same test in different environments. In this paper, we present DroidWalker, a dynamic-analysis tool to generate reproducible test cases for Android apps. The key design of our tool is a dynamic-adaptive model that can abstract the app state in a proper granularity so every state in the model can be reached afterwards. Given an app under test, DroidWalker first explores the app to build the model. Then developers can select the state in the model to be reproduced. Finally, DroidWalker executes all the generated test cases and the app could reach exactly the same state as the explored one. We apply DroidWalker in three real usage scenarios to demonstrate its practical usage. The video of our tool is at https://youtu.be/ndUD8Gxs800.",
                    "Although click data is widely used in search systems in practice, so far the inherent bias, most notably position bias, has prevented it from being used in training of a ranker for search, i.e., learning-to-rank. Recently, a number of authors have proposed new techniques referred to as 'unbiased learning-to-rank', which can reduce position bias and train a relatively high-performance ranker using click data. Most of the algorithms, based on the inverse propensity weighting (IPW) principle, first estimate the click bias at each position, and then train an unbiased ranker with the estimated biases using a learning-to-rank algorithm. However, there has not been a method for pairwise learning-to-rank that can jointly conduct debiasing of click data and training of a ranker using a pairwise loss function. In this paper, we propose a novel algorithm, which can jointly estimate the biases at click positions and the biases at unclick positions, and learn an unbiased ranker. Experiments on benchmark data show that our algorithm can significantly outperform existing algorithms. In addition, an online A/B Testing at a commercial search engine shows that our algorithm can effectively conduct debiasing of click data and enhance relevance ranking.",
                    "Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%--21% on various downstream tasks.",
                    "Answering complex open-domain questions requires understanding the latent relations between involving entities. However, we found that the existing QA datasets are extremely imbalanced in some types of relations, which hurts the generalization performance over questions with long-tail relations. To remedy this problem, in this paper, we propose a Relation-Guided Pre-Training (RGPT-QA) framework. We first generate a relational QA dataset covering a wide range of relations from both the Wikidata triplets and Wikipedia hyperlinks. We then pre-train a QA model to infer the latent relations from the question, and then conduct extractive QA to get the target answer entity. We demonstrate that by pretraining with propoed RGPT-QA techique, the popular open-domain QA model, Dense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute improvement in Exact Match accuracy on Natural Questions, TriviaQA, and WebQuestions. Particularly, we show that RGPT-QA improves significantly on questions with long-tail relations",
                    "In the appstore-centric ecosystem, app developers have an urgent requirement to optimize their release strategy to maximize the success opportunity of their apps. To address this problem, we introduce an approach to assisting developers to select the proper release opportunity based on the purpose of the update and current condition of the app. Before that, we propose the interval of an update to its previous update to characterize release patterns, and find significance of the release opportunity through empirical analysis. We mined the update-history data of 17,820 apps from 33 categories in Google Play, over a period of 105 days. With 41,028 releases identified from these apps, we reveal important characteristics of update intervals and how these factors can influence update effects. We suggest developers to synthetically consider app ranking, rating trend, and what to update in addition to the opportunity before releasing an app version. We propose a Multinomial Naive Bayes model to help decide an optimal release opportunity to gain better user adoption.",
                    "Unlike the Web where each web page has a global URL to reach, a specific \"content page\" inside a mobile app cannot be opened unless the user explores the app with several operations from the landing page. Recently, deep links have been advocated by major companies to enable targeting and opening a specific page of an app externally with an accessible uniform resource identifier (URI). To empirically investigate the state of the practice on adopting deep links, in this article, we present the largest empirical study of deep links over 20,000 Android apps, and find that deep links do not get wide adoption among current Android apps, and non-trivial manual efforts are required for app developers to support deep links. To address such an issue, we propose the Aladdin approach and supporting tool to release deep links to access arbitrary location of existing apps. Aladdin instantiates our novel cooperative framework to synergically combine static analysis and dynamic analysis while minimally engaging developers to provide inputs to the framework for automation, without requiring any coding efforts or additional deployment efforts. We evaluate Aladdin with popular apps and demonstrate its effectiveness and performance.",
                    "Answering complex First-Order Logical (FOL) queries on large-scale incomplete knowledge graphs (KGs) is an important yet challenging task. Recent advances embed logical queries and KG entities in the same space and conduct query answering via dense similarity search. However, most logical operators designed in previous studies do not satisfy the axiomatic system of classical logic, limiting their performance. Moreover, these logical operators are parameterized and thus require many complex FOL queries as training data, which are often arduous to collect or even inaccessible in most real-world KGs. We thus present FuzzQE, a fuzzy logic based logical query embedding framework for answering FOL queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a principled and learning-free manner, where only entity and relation embeddings require learning. FuzzQE can further benefit from labeled complex logical queries for training. Extensive experiments on two benchmark datasets demonstrate that FuzzQE provides significantly better performance in answering FOL queries compared to state-of-the-art methods. In addition, FuzzQE trained with only KG link prediction can achieve comparable performance to those trained with extra complex query data.",
                    "Offline reinforcement learning often requires a quality dataset that we can train a policy on. However, in many situations, it is not possible to get such a dataset, nor is it easy to train a policy to perform well in the actual environment given the offline data. We propose using data distillation to train and distill a better dataset which can then be used for training a better policy model. We show that our method is able to synthesize a dataset where a model trained on it achieves similar performance to a model trained on the full dataset or a model trained using percentile behavioral cloning. Our project site is available at $\\href{https://datasetdistillation4rl.github.io}{\\text{here}}$. We also provide our implementation at $\\href{https://github.com/ggflow123/DDRL}{\\text{this GitHub repository}}$.",
                    "Pre-training Graph Neural Networks (GNN) via self-supervised contrastive learning has recently drawn lots of attention. However, most existing works focus on node-level contrastive learning, which cannot capture global graph structure. The key challenge to conducting subgraph-level contrastive learning is to sample informative subgraphs that are semantically meaningful. To solve it, we propose to learn graph motifs, which are frequently-occurring subgraph patterns (e.g. functional groups of molecules), for better subgraph sampling. Our framework MotIf-driven Contrastive leaRning Of Graph representations (MICRO-Graph) can: 1) use GNNs to extract motifs from large graph datasets; 2) leverage learned motifs to sample informative subgraphs for contrastive learning of GNN. We formulate motif learning as a differentiable clustering problem, and adopt EM-clustering to group similar and significant subgraphs into several motifs. Guided by these learned motifs, a sampler is trained to generate more informative subgraphs, and these subgraphs are used to train GNNs through graph-to-subgraph contrastive learning. By pre-training on the ogbg-molhiv dataset with MICRO-Graph, the pre-trained GNN achieves 2.04% ROC-AUC average performance enhancement on various downstream benchmark datasets, which is significantly higher than other state-of-the-art self-supervised learning baselines.",
                    "The limited availability of annotations in small molecule datasets presents a challenge to machine learning models. To address this, one common strategy is to collaborate with additional auxiliary datasets. However, having more data does not always guarantee improvements. Negative transfer can occur when the knowledge in the target dataset differs or contradicts that of the auxiliary molecule datasets. In light of this, identifying the auxiliary molecule datasets that can benefit the target dataset when jointly trained remains a critical and unresolved problem. Through an empirical analysis, we observe that combining graph structure similarity and task similarity can serve as a more reliable indicator for identifying high-affinity auxiliary datasets. Motivated by this insight, we propose MolGroup, which separates the dataset affinity into task and structure affinity to predict the potential benefits of each auxiliary molecule dataset. MolGroup achieves this by utilizing a routing mechanism optimized through a bi-level optimization framework. Empowered by the meta gradient, the routing mechanism is optimized toward maximizing the target dataset's performance and quantifies the affinity as the gating score. As a result, MolGroup is capable of predicting the optimal combination of auxiliary datasets for each target dataset. Our extensive experiments demonstrate the efficiency and effectiveness of MolGroup, showing an average improvement of 4.41%/3.47% for GIN/Graphormer trained with the group of molecule datasets selected by MolGroup on 11 target molecule datasets.",
                    "In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots playing evil, while good-role bot achieves 38.2% win rate in the same setting. We envision AvalonBench could be a good test-bed for developing more advanced LLMs (with self-playing) and agent frameworks that can effectively model the layered complexities of such game environments.",
                    "Stock trend prediction plays a critical role in seeking maximized profit from stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of stock market. Exploding information on Internet together with advancing development of natural language processing and text mining techniques have enable investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness and comprehensiveness of online content related to stock market varies drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our approach.",
                    "Graph neural networks (GNNs) are shown to be successful in modeling applications with graph structures. However, training an accurate GNN model requires a large collection of labeled data and expressive features, which might be inaccessible for some applications. To tackle this problem, we propose a pre-training framework that captures generic graph structural information that is transferable across tasks. Our framework can leverage the following three tasks: 1) denoising link reconstruction, 2) centrality score ranking, and 3) cluster preserving. The pre-training procedure can be conducted purely on the synthetic graphs, and the pre-trained GNN is then adapted for downstream applications. With the proposed pre-training procedure, the generic structural information is learned and preserved, thus the pre-trained GNN requires less amount of labeled data and fewer domain-specific features to achieve high performance on different downstream tasks. Comprehensive experiments demonstrate that our proposed framework can significantly enhance the performance of various tasks at the level of node, link, and graph.",
                    "Existing approaches for learning word embeddings often assume there are sufficient occurrences for each word in the corpus, such that the representation of words can be accurately estimated from their contexts. However, in real-world scenarios, out-of-vocabulary (a.k.a. OOV) words that do not appear in training corpus emerge frequently. It is challenging to learn accurate representations of these words with only a few observations. In this paper, we formulate the learning of OOV embeddings as a few-shot regression problem, and address it by training a representation function to predict the oracle embedding vector (defined as embedding trained with abundant observations) based on limited observations. Specifically, we propose a novel hierarchical attention-based architecture to serve as the neural regression function, with which the context information of a word is encoded and aggregated from K observations. Furthermore, our approach can leverage Model-Agnostic Meta-Learning (MAML) for adapting the learned model to the new corpus fast and robustly. Experiments show that the proposed approach significantly outperforms existing methods in constructing accurate embeddings for OOV words, and improves downstream tasks where these embeddings are utilized.",
                    "Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",
                    "The mobile application (app) has become the main entrance to access the Internet on handheld devices. Unlike the Web where each webpage has a global URL to reach directly, a specific \"content page\" of an app can be opened only by exploring the app with several operations from the landing page. The interoperability between apps is quite fixed and thus limits the value-added \"linked data\" between apps. Recently, deep link has been proposed to enable targeting and opening a specific page of an app externally with an accessible uniform resource identifier (URI). However, implementing deep link for mobile apps requires a lot of manual efforts by app developers, which can be very error-prone and time-consuming. In this paper, we propose DroidLink to automatically generating deep links for existing Android apps. We design a deep link model suitable for automatic generation. Then we explore the transition of pages and build a navigation graph based on static and dynamic analysis of Android apps. Next, we realize an updating mechanism that keeps on revisiting the target app and discover new pages, and thus generates deep links for every single page of the app. Finally, we repackage the app with deep link supports, but requires no additional deployment requirements. We generate deep links for some popular apps and demonstrate the feasibility of DroidLink.",
                    "Commonsense is defined as the knowledge that is shared by everyone. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenarios of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models' ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard multimodal commonsense benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR.",
                    "Data continuously emitted from industrial ecosystems such as social or e-commerce platforms are commonly represented as heterogeneous graphs (HG) composed of multiple node/edge types. State-of-the-art graph learning methods for HGs known as heterogeneous graph neural networks (HGNNs) are applied to learn deep context-informed node representations. However, many HG datasets from industrial applications suffer from label imbalance between node types. As there is no direct way to learn using labels rooted at different node types, HGNNs have been applied to only a few node types with abundant labels. We propose a zero-shot transfer learning module for HGNNs called a Knowledge Transfer Network (KTN) that transfers knowledge from label-abundant node types to zero-labeled node types through rich relational information given in the HG. KTN is derived from the theoretical relationship, which we introduce in this work, between distinct feature extractors for each node type given in an HGNN model. KTN improves performance of 6 different types of HGNN models by up to 960% for inference on zero-labeled node types and outperforms state-of-the-art transfer learning baselines by up to 73% across 18 different transfer learning tasks on HGs.",
                    "Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\\text{EM}$ and Self-Rewarding LM."
                ],
                "domain": [
                    "Mobile Computing",
                    "Graph Neural Networks",
                    "Natural Language Processing",
                    "Reinforcement Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "3b5e4f04-8be9-425d-80db-84538a16a065": {
                "pk": "3b5e4f04-8be9-425d-80db-84538a16a065",
                "project_name": null,
                "name": "Ying Ding",
                "bio": "I am a researcher with a strong focus on the intersection of bibliometrics, survival analysis, and machine learning. My work has primarily revolved around developing innovative methodologies to assess scholarly impact and treatment effects in various domains. Recently, I have explored the application of weighted PageRank algorithms to author citation networks, revealing significant correlations between citation metrics and scholarly prestige. This research has provided valuable insights into how we can better measure the influence of scholars in the field of Information Retrieval.\n\nIn addition to bibliometric analysis, I have delved into survival analysis, particularly in estimating heterogeneous treatment effects (HTE) for survival outcomes. My framework integrates multiple meta-learners to simultaneously estimate causal effects and identify relevant subgroups, which I successfully applied to a large clinical trial for age-related macular degeneration. This work underscores my commitment to precision healthcare and the importance of understanding treatment efficacy across diverse patient populations.\n\nMy research also extends to developing novel statistical models, such as copula-based semiparametric transformation models for bivariate interval-censored data, which have practical applications in genetic studies. I am passionate about advancing methodologies that not only enhance our understanding of complex data but also contribute to real-world applications in healthcare and information science. Through my work, I aim to bridge the gap between theoretical advancements and practical implementations, fostering a deeper understanding of the dynamics of knowledge transfer and scholarly impact.",
                "collaborators": [
                    "Erjia Yan",
                    "Bin Nan",
                    "Na Bo",
                    "Tao Sun",
                    "Arthur Frazho",
                    "James Caverlee",
                    "Xin Liu",
                    "Forrest Sheng Bao",
                    "Blaise Cronin",
                    "Loet Leydesdorff"
                ],
                "pub_titles": [
                    "Applying weighted PageRank to author citation networks",
                    "Estimating mean survival time: when is it possible?",
                    "A sieve M-theorem for bundled parameters in semiparametric models, with application to the efficient estimation in a linear model for censored data",
                    "Estimating Interpretable Heterogeneous Treatment Effect with Causal Subgroup Discovery in Survival Outcomes",
                    "Copula-based Semiparametric Regression Method for Bivariate Data under General Interval Censoring",
                    "Semantic Web: Who is who in the field - A bibliometric analysis",
                    "PageRank for ranking authors in co-citation networks",
                    "Discovering author impact: A PageRank perspective",
                    "Weighted citation: An indicator of an article's prestige",
                    "General Scaled Support Vector Machines",
                    "A bird's-eye view of scientific trading: Dependency relations among fields of science",
                    "Citation content analysis (cca): A framework for syntactic and semantic analysis of citation content"
                ],
                "pub_abstracts": [
                    "This paper aims to identify whether different weighted PageRank algorithms can be applied to author citation networks to measure the popularity and prestige of a scholar from a citation perspective. Information Retrieval (IR) was selected as a test field and data from 1956-2008 were collected from Web of Science (WOS). Weighted PageRank with citation and publication as weighted vectors were calculated on author citation networks. The results indicate that both popularity rank and prestige rank were highly correlated with the weighted PageRank. Principal Component Analysis (PCA) was conducted to detect relationships among these different measures. For capturing prize winners within the IR field, prestige rank outperformed all the other measures.",
                    "For right censored survival data, it is well known that the mean survival time can be consistently estimated when the support of the censoring time contains the support of the survival time. In practice, however, this condition can be easily violated because the follow-up of a study is usually within a finite window. In this article we show that the mean survival time is still estimable from a linear model when the support of some covariate(s) with nonzero coefficient(s) is unbounded regardless of the length of follow-up. This implies that the mean survival time can be well estimated when the covariate range is wide in practice. The theoretical finding is further verified for finite samples by simulation studies. Simulations also show that, when both models are correctly specified, the linear model yields reasonable mean square prediction errors and outperforms the Cox model, particularly with heavy censoring and short follow-up time.",
                    "In many semiparametric models that are parameterized by two types of parameters---a Euclidean parameter of interest and an infinite-dimensional nuisance parameter---the two parameters are bundled together, that is, the nuisance parameter is an unknown function that contains the parameter of interest as part of its argument. For example, in a linear regression model for censored survival data, the unspecified error distribution function involves the regression coefficients. Motivated by developing an efficient estimating method for the regression parameters, we propose a general sieve M-theorem for bundled parameters and apply the theorem to deriving the asymptotic theory for the sieve maximum likelihood estimation in the linear regression model for censored survival data. The numerical implementation of the proposed estimating method can be achieved through the conventional gradient-based search algorithms such as the Newton--Raphson algorithm. We show that the proposed estimator is consistent and asymptotically normal and achieves the semiparametric efficiency bound. Simulation studies demonstrate that the proposed method performs well in practical settings and yields more efficient estimates than existing estimating equation based methods. Illustration with a real data example is also provided.",
                    "Estimating heterogeneous treatment effect (HTE) for survival outcomes has gained increasing attention, as it captures the variation in treatment efficacy across patients or subgroups in delaying disease progression. However, most existing methods focus on post-hoc subgroup identification rather than simultaneously estimating HTE and selecting relevant subgroups. In this paper, we propose an interpretable HTE estimation framework that integrates three meta-learners that simultaneously estimate CATE for survival outcomes and identify predictive subgroups. We evaluated the performance of our method through comprehensive simulation studies across various randomized clinical trial (RCT) settings. Additionally, we demonstrated its application in a large RCT for age-related macular degeneration (AMD), a polygenic progressive eye disease, to estimate the HTE of an antioxidant and mineral supplement on time-to-AMD progression and to identify genetics-based subgroups with enhanced treatment effects. Our method offers a direct interpretation of the estimated HTE and provides evidence to support precision healthcare.",
                    "This research is motivated by discovering and underpinning genetic causes for the progression of a bilateral eye disease, Age-related Macular Degeneration (AMD), of which the primary outcomes, progression times to late-AMD, are bivariate and interval-censored due to intermittent assessment times. We propose a novel class of copula-based semiparametric transformation models for bivariate data under general interval censoring, which includes the case 1 interval censoring (current status data) and case 2 interval censoring. Specifically, the joint likelihood is modeled through a two-parameter Archimedean copula, which can flexibly characterize the dependence between the two margins in both tails. The marginal distributions are modeled through semiparametric transformation models using sieves, with the proportional hazards or odds model being a special case. We develop a computationally efficient sieve maximum likelihood estimation procedure for the unknown parameters, together with a generalized score test for the regression parameter(s). For the proposed sieve estimators of finite-dimensional parameters, we establish their asymptotic normality and efficiency. Extensive simulations are conducted to evaluate the performance of the proposed method in finite samples. Finally, we apply our method to a genome-wide analysis of AMD progression using the Age-Related Eye Disease Study (AREDS) data, to successfully identify novel risk variants associated with the disease progression. We also produce predicted joint and conditional progression-free probabilities, for patients with different genetic characteristics.",
                    "The Semantic Web is one of the main efforts aiming to enhance human and machine interaction by representing data in an understandable way for machines to mediate data and services. It is a fast-moving and multidisciplinary field. This study conducts a thorough bibliometric analysis of the field by collecting data from Web of Science (WOS) and Scopus for the period of 1960-2009. It utilizes a total of 44,157 papers with 651,673 citations from Scopus, and 22,951 papers with 571,911 citations from WOS. Based on these papers and citations, it evaluates the research performance of the Semantic Web (SW) by identifying the most productive players, major scholarly communication media, highly cited authors, influential papers and emerging stars.",
                    "Google's PageRank has created a new synergy to information retrieval for a better ranking of Web pages. It ranks documents depending on the topology of the graphs and the weights of the nodes. PageRank has significantly advanced the field of information retrieval and keeps Google ahead of competitors in the search engine market. It has been deployed in bibliometrics to evaluate research impact, yet few of these studies focus on the important impact of the damping factor (d) for ranking purposes. This paper studies how varied damping factors in the PageRank algorithm can provide additional insight into the ranking of authors in an author co-citation network. Furthermore, we propose weighted PageRank algorithms. We select 108 most highly cited authors in the information retrieval (IR) area from the 1970s to 2008 to form the author co-citation network. We calculate the ranks of these 108 authors based on PageRank with damping factor ranging from 0.05 to 0.95. In order to test the relationship between these different measures, we compare PageRank and weighted PageRank results with the citation ranking, h-index, and centrality measures. We found that in our author co-citation network, citation rank is highly correlated with PageRank's with different damping factors and also with different PageRank algorithms; citation rank and PageRank are not significantly correlated with centrality measures; and h-index is not significantly correlated with centrality measures.",
                    "This article provides an alternative perspective for measuring author impact by applying PageRank algorithm to a coauthorship network. A weighted PageRank algorithm considering citation and coauthorship network topology is proposed. We test this algorithm under different damping factors by evaluating author impact in the informetrics research community. In addition, we also compare this weighted PageRank with the h-index, citation, and program committee (PC) membership of the International Society for Scientometrics and Informetrics (ISSI) conferences. Findings show that this weighted PageRank algorithm provides reliable results in measuring author impact.",
                    "We propose using the technique of weighted citation to measure an article's prestige. The technique allocates a different weight to each reference by taking into account the impact of citing journals and citation time intervals. Weighted citation captures prestige, whereas citation counts capture popularity. We compare the value variances for popularity and prestige for articles published in the Journal of the American Society for Information Science and Technology from 1998 to 2007, and find that the majority have comparable status.",
                    "Support Vector Machines (SVMs) are popular tools for data mining tasks such as classification, regression, and density estimation. However, original SVM (C-SVM) only considers local information of data points on or over the margin. Therefore, C-SVM loses robustness. To solve this problem, one approach is to translate (i.e., to move without rotation or change of shape) the hyperplane according to the distribution of the entire data. But existing work can only be applied for 1-D case. In this paper, we propose a simple and efficient method called General Scaled SVM (GS-SVM) to extend the existing approach to multi-dimensional case. Our method translates the hyperplane according to the distribution of data projected on the normal vector of the hyperplane. Compared with C-SVM, GS-SVM has better performance on several data sets.",
                    "We use a trading metaphor to study knowledge transfer in the sciences as well as the social sciences. The metaphor comprises four dimensions: (a) Discipline Self-dependence, (b) Knowledge Exports/Imports, (c) Scientific Trading Dynamics, and (d) Scientific Trading Impact. This framework is applied to a dataset of 221 Web of Science subject categories. We find that: (i) the Scientific Trading Impact and Dynamics of Materials Science And Transportation Science have increased; (ii) Biomedical Disciplines, Physics, And Mathematics are significant knowledge exporters, as is Statistics & Probability; (iii) in the social sciences, Economics, Business, Psychology, Management, And Sociology are important knowledge exporters; (iv) Discipline Self-dependence is associated with specialized domains which have ties to professional practice (e.g., Law, Ophthalmology, Dentistry, Oral Surgery & Medicine, Psychology, Psychoanalysis, Veterinary Sciences, And Nursing).",
                    "This paper proposes a new framework for Citation Content Analysis (CCA), for syntactic and semantic analysis of citation content that can be used to better analyze the rich sociocultural context of research behavior. The framework could be considered the next generation of citation analysis. This paper briefly reviews the history and features of content analysis in traditional social sciences, and its previous application in Library and Information Science. Based on critical discussion of the theoretical necessity of a new method as well as the limits of citation analysis, the nature and purposes of CCA are discussed, and potential procedures to conduct CCA, including principles to identify the reference scope, a two-dimensional (citing and cited) and two-modular (syntactic and semantic modules) codebook, are provided and described. Future works and implications are also suggested."
                ],
                "domain": [
                    "Information Retrieval",
                    "Survival Analysis",
                    "Citation Analysis",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively identify and generate biological processes from gene sets while addressing discrepancies in human annotations and ensuring a nuanced understanding of complex biological systems?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for advancing precision medicine, particularly in cancer treatment, as it enhances our understanding of the intricate relationships between genes, diseases, and biological processes. By improving the accuracy and reliability of biological process identification, this research could lead to more effective therapeutic strategies and foster collaboration across research groups. Furthermore, it could pave the way for future studies that leverage the Thought Graph framework, ultimately contributing to a more integrated approach in biomedical research and data interoperability.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the inherent complexity of biological systems, where individual genes often exhibit weak signals, and strong signals rarely converge on a single biological theme. Naive approaches may fail due to the multifaceted nature of biological processes and the potential for divergent conclusions from different research groups. Additionally, technical obstacles include the need for sophisticated reasoning frameworks that can manage complex decision-making and integrate external knowledge bases effectively, while theoretical challenges involve ensuring the quality and accuracy of generated processes amidst these complexities.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often been limited by linear reasoning approaches, such as Chain-of-Thought prompting, which struggle with tasks requiring exploration and strategic lookahead. Additionally, existing methods have not adequately addressed the integration of domain-specific knowledge, leading to gaps in understanding the semantics of biological connections. Barriers such as self-bias and hallucination in LLMs have also hindered progress. Our approach differs by introducing the Thought Graph framework, which incorporates edge semantics and external knowledge bases, thereby enhancing the reasoning process and improving upon prior work.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves the Thought Graph framework, which utilizes a Tree-of-Thought architecture to facilitate thought expansion with Large Language Models (LLMs). We will employ datasets related to gene sets and biological processes, and evaluate our results using cosine similarity scores as the primary metric. The expected outcomes include a significant improvement in the generation of biological processes, as evidenced by surpassing state-of-the-art methods, specifically achieving a 40.28% improvement over GSEA and a 5.38% improvement over LLM baselines in cosine similarity"
    },
    "2401.04155": {
        "paper_data": {
            "title": "Large language models in bioinformatics: applications and perspectives",
            "url": "http://arxiv.org/abs/2401.04155v1",
            "arxiv_id": "2401.04155",
            "authors": [
                "Jiajia Liu",
                "Mengyuan Yang",
                "Yankai Yu",
                "Haixia Xu",
                "Kang Li",
                "Xiaobo Zhou"
            ],
            "abstract": "Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will present a summary of the prominent large language models used in natural language processing, such as BERT and GPT, and focus on exploring the applications of large language models at different omics levels in bioinformatics, mainly including applications of large language models in genomics, transcriptomics, proteomics, drug discovery and single cell analysis. Finally, this review summarizes the potential and prospects of large language models in solving bioinformatic problems.",
            "introduction": " Introduction   Significant progress has been made in the field of natural language processing with the advent of  large language models. Examples of these models include OpenAI’s GPT -X [1] and Google’s  BERT [2] models . These models are transformative because they can understand, generate, and  manipulate human language at an unprecedented scale. Vast Large language models are typically  trained on datasets that encompass a significant portion of the internet ’s text, enabling them to  learn the complexities of language and context.  These models are built upon a neural network  architecture called transformers  [3]. The transformer  architecture revolutionized NLP due to its  paralle lization, scalability, and ability to  capture long -range dependencies in text. Instead of  relying on recurrent or convolutional layers, transformers use self -attention mechanisms, as  previously described, which allow them to assess  the importance of every word in a sentence when  understanding context. This innovation is key to their remarkable performance.   The training regimen for large language models comprises two phases: pre -training and fine -tuning.  During pre -training, the model is trained on an extens ive corpus of text data to acquire proficiency  in grammar, factual knowledge, reasoning abilities, and word understanding.  Fine-tuning tailors  these models for specific tasks like translation, summarization, or question -answering. The  adaptability of large  language models is a major advantage; they can excel at  various NLP tasks  without task -specific architectures.  However, they have found applications in diverse fields beyond NLP, including biology, healthcare, education, finance, customer service, and more.  In  particular, there have been many successful applications of large language models in the field of  bioinformatics.  In this manuscript, we focus on the applications of large language models to  several bioinformatic tasks through five areas:  DNA leve l, RNA level, protein level , drug  discovery and  single cell analysis , respectively corresponding to sections: applications of large  language models in genomics, transcriptomics, proteomics, drug discovery  and single cell analysis .  Applications of LLMs in g enomics focus on LLMs using DNA sequence; applications of LLMs  in transcriptomics using RNA sequence; applications of LLMs in proteomics focus on LLMs using  protein sequence; applications of LLMs in drug discovery focus on LLMs using Molecular  SMILES  (seq)  and applications of LL Ms in single -cell analysis focus on LLMs using gene  expression data from scRNA -seq or scMulti -omics data  (Figure 1 ).    2. Large language model s in natural language processing   The emergence of large language models has brought milestone progress to natural language  processing . These large language models, particularly exemplified by BERT [2] and GPT  [1],  usually use the “pre-training + fine -tuning ” approach  (Figure 2). Pre-training is mainly to train a  general /foundation model with strong generalization ability on a large -scale text corpus. Fine- tuning relies on the pre -trained model, undergoing additional training for a specific task. This  process allows the model to adjust to the unique data and demands of the task at hand. ",
            "references": []
        },
        "author_data": {
            "688c0e50-905f-4368-95e0-5062bde7c1f3": {
                "pk": "688c0e50-905f-4368-95e0-5062bde7c1f3",
                "project_name": null,
                "name": "Jiajia Liu",
                "bio": "I am a researcher specializing in the intersection of security, automation, and machine learning, particularly within the context of the Internet of Things (IoT) and autonomous systems. My recent work has focused on addressing the security vulnerabilities inherent in Trigger-Action Programming (TAP) frameworks used in home automation systems. I developed TAPInspector, a system that detects rule interaction vulnerabilities, and TAPFixer, which automatically repairs these vulnerabilities, achieving an impressive 86.65% success rate in practical applications.\n\nIn addition to my work on TAP, I have explored the vulnerabilities of deep reinforcement learning (DRL) systems, particularly in autonomous driving contexts. I investigated Trojan attacks on DRL policies, demonstrating how adversaries can exploit spatio-temporal features to compromise system safety. My research also delves into covert communication strategies in noisy wireless networks, where I developed methods to enhance secure transmissions amidst interference.\n\nMy passion for understanding complex systems extends to astrophysics, where I have studied swirling motions in the solar atmosphere and their implications for energy transfer. Through the development of the Automated Swirl Detection Algorithm (ASDA), I have contributed to the analysis of solar phenomena, revealing insights into the dynamics of solar jets and their relationship with magnetic flux ropes.\n\nOverall, my work aims to bridge theoretical insights with practical applications, enhancing the security and efficiency of automated systems while contributing to our understanding of natural phenomena.",
                "collaborators": [
                    "Yinbo Yu",
                    "Robert Erdélyi",
                    "Zhihong Liu",
                    "Yong Zeng",
                    "Jianfeng Ma",
                    "David Jess",
                    "Mihalis Mathioudakis",
                    "Chris J. Nelson",
                    "Yuming Wang",
                    "Robertus Erdélyi"
                ],
                "pub_titles": [
                    "TAPInspector: Safety and Liveness Verification of Concurrent Trigger-Action IoT Systems",
                    "Don't Watch Me: A Spatio-Temporal Trojan Attack on Deep-Reinforcement-Learning-Augment Autonomous Driving",
                    "Hiding Communications in AWGN Channels and THz Band with Interference Uncertainty",
                    "On the 5-Minute Oscillations of Photospheric and Chromospheric Swirls",
                    "Automated Swirl Detection Algorithm (ASDA) and Its Application to Simulation and Observational Data",
                    "How Many Twists Do Solar Coronal Jets Release?",
                    "TAPFixer: Automatic Detection and Repair of Home Automation Vulnerabilities based on Negated-property Reasoning",
                    "A Spatiotemporal Stealthy Backdoor Attack against Cooperative Multi-Agent Deep Reinforcement Learning"
                ],
                "pub_abstracts": [
                    "Trigger-action programming (TAP) is a popular end-user programming framework that can simplify the Internet of Things (IoT) automation with simple trigger-action rules. However, it also introduces new security and safety threats. A lot of advanced techniques have been proposed to address this problem. Rigorously reasoning about the security of a TAP-based IoT system requires a well-defined model and verification method both against rule semantics and physical-world features, e.g., concurrency, rule latency, extended action, tardy attributes, and connection-based rule interactions, which has been missing until now. By analyzing these features, we find 9 new types of rule interaction vulnerabilities and validate them on two commercial IoT platforms. We then present TAPInspector, a novel system to detect these interaction vulnerabilities in concurrent TAP-based IoT systems. It automatically extracts TAP rules from IoT apps, translates them into a hybrid model by model slicing and state compression, and performs semantic analysis and model checking with various safety and liveness properties. Our experiments corroborate that TAPInspector is practical: it identifies 533 violations related to rule interaction from 1108 real-world market IoT apps and is at least 60000 times faster than the baseline without optimization.",
                    "Deep reinforcement learning (DRL) is one of the most popular algorithms to realize an autonomous driving (AD) system. The key success factor of DRL is that it embraces the perception capability of deep neural networks which, however, have been proven vulnerable to Trojan attacks. Trojan attacks have been widely explored in supervised learning (SL) tasks (e.g., image classification), but rarely in sequential decision-making tasks solved by DRL. Hence, in this paper, we explore Trojan attacks on DRL for AD tasks. First, we propose a spatio-temporal DRL algorithm based on the recurrent neural network and attention mechanism to prove that capturing spatio-temporal traffic features is the key factor to the effectiveness and safety of a DRL-augment AD system. We then design a spatial-temporal Trojan attack on DRL policies, where the trigger is hidden in a sequence of spatial and temporal traffic features, rather than a single instant state used in existing Trojan on SL and DRL tasks. With our Trojan, the adversary acts as a surrounding normal vehicle and can trigger attacks via specific spatial-temporal driving behaviors, rather than physical or wireless access. Through extensive experiments, we show that while capturing spatio-temporal traffic features can improve the performance of DRL for different AD tasks, they suffer from Trojan attacks since our designed Trojan shows high stealthy (various spatio-temporal trigger patterns), effective (less than 3.1\\% performance variance rate and more than 98.5\\% attack success rate), and sustainable to existing advanced defenses.",
                    "Covert communication can prevent an adversary from knowing that a wireless transmission has occurred. In additive white Gaussian noise (AWGN) channels, a square root law is found that Alice can reliably and covertly transmit $\\mathcal{O}(\\sqrt{n})$ bits to Bob in $n$ channel uses. In this paper, we consider covert communications in noisy wireless networks, where the receivers not only experience the background noise, but also the aggregate interference from other transmitters. Our results show that uncertainty in interference experienced by the adversary Willie is beneficial to Alice. In AWGN channels, when the distance between Alice and Willie $d_{a,w}=\\omega(n^{1/(2\\alpha)})$ ($\\alpha$ is the path loss exponent), Alice can reliably and covertly transmit $\\mathcal{O}(\\log_2\\sqrt{n})$ bits to Bob in $n$ channel uses. Although the covert throughput is lower than the square root law, the spatial throughput is higher. In THz (Terahertz) Band networks,covert communication is more difficult because Willie can simply place a receiver in the narrow beam between Alice and Bob to detect or block their LOS (Line-of-Sight) communications. We then present a covert communication scheme that utilizes the reflection or diffuse scattering from a rough surface to prevent being detected by Willie. From the network perspective, the communications are hidden in the interference of noisy wireless networks, and what Willie sees is merely a \"shadow\" wireless network.",
                    "Swirls are ubiquitous in the solar atmosphere. They are believed to be related to the excitation of different modes of magnetohydrodynamic waves and pulses, as well as spicules. However, statistical studies of their collective behaviour are rare. In this paper, we aim to study the collective, as well as the behaviour of individual photospheric and chromospheric swirls detected by the automated swirl detection algorithm (ASDA) from observations obtained by the Swedish 1-m Solar Telescope and the Hinode satellite. Detailed analysis of six different parameters of photospheric and chromospheric swirls is performed employing the wavelet analysis. Two clusters of periods with significant wavelet power, one from $3-8$ minutes and the other from $10-14$ minutes, have been found. The former coincides with the dominant period of the global $p$-mode spectrum. Wavelet and Fast Fourier Transform (FFT) analysis of example swirls also reveals similar periods. These results suggest that global $p$-modes might be important for triggering photospheric and thus chromospheric swirls. A novel scenario of global $p$-modes providing energy and mass fluxes to the upper solar atmosphere via generating swirls, Alfv\\'en pulses and spicules is then proposed.",
                    "Swirling motions in the solar atmosphere have been widely observed in recent years and suggested to play a key role in channeling energy from the photosphere into the corona. Here, we present a newly-developed Automated Swirl Detection Algorithm (ASDA) and discuss its applications. ASDA is found to be very proficient at detecting swirls in a variety of synthetic data with various levels of noise, implying our subsequent scientific results are astute. Applying ASDA to photospheric observations with a spatial resolution of 39.2 km sampled by the Solar Optical Telescope (SOT) on-board Hinode, suggests a total number of $1.62\\times10^5$ swirls in the photosphere, with an average radius and rotating speed of $\\sim290$ km and $< 1.0$ km s$^{-1}$, respectively. Comparisons between swirls detected in Bifrost numerical MHD simulations and both ground-based and space-borne observations, suggest that: 1) the spatial resolution of data plays a vital role in the total number and radii of swirls detected; and 2) noise introduced by seeing effects could decrease the detection rate of swirls, but has no significant influences in determining their inferred properties. All results have shown that there is no significant difference in the analysed properties between counter-clockwise or clockwise rotating swirls. About 70% of swirls are located in intergranular lanes. Most of the swirls have lifetimes less than twice of the cadences, meaning future research should aim to use data with much higher cadences than 6 s. In the conclusions, we propose some promising future research applications where ASDA may provide useful insights.",
                    "Highly twisted magnetic flux ropes, with finite length, are subject to kink instabilities, and could lead to a number of eruptive phenomena in the solar atmosphere, including flares, coronal mass ejections (CMEs) and coronal jets. The kink instability threshold, which is the maximum twist a kink-stable magnetic flux rope could contain, has been widely studied in analytical models and numerical simulations, but still needs to be examined by observations. In this article, we will study twists released by 30 off-limb rotational solar coronal jets, and compare the observational findings with theoretical kink instability thresholds. We have found that: 1) the number of events with more twist release becomes less; 2) each of the studied jets has released a twist number of at least 1.3 turns (a twist angle of 2.6$\\pi$); and 3) the size of a jet is highly related to its twist pitch instead of twist number. Our results suggest that the kink instability threshold in the solar atmosphere should not be a constant. The found lower limit of twist number of 1.3 turns should be merely a necessary but not a sufficient condition for a finite solar magnetic flux rope to become kink unstable.",
                    "Trigger-Action Programming (TAP) is a popular end-user programming framework in the home automation (HA) system, which eases users to customize home automation and control devices as expected. However, its simplified syntax also introduces new safety threats to HA systems through vulnerable rule interactions. Accurately fixing these vulnerabilities by logically and physically eliminating their root causes is essential before rules are deployed. However, it has not been well studied. In this paper, we present TAPFixer, a novel framework to automatically detect and repair rule interaction vulnerabilities in HA systems. It extracts TAP rules from HA profiles, translates them into an automaton model with physical and latency features, and performs model checking with various correctness properties. It then uses a novel negated-property reasoning algorithm to automatically infer a patch via model abstraction and refinement and model checking based on negated-properties. We evaluate TAPFixer on market HA apps (1177 TAP rules and 53 properties) and find that it can achieve an 86.65% success rate in repairing rule interaction vulnerabilities. We additionally recruit 23 HA users to conduct a user study that demonstrates the usefulness of TAPFixer for vulnerability repair in practical HA scenarios.",
                    "Recent studies have shown that cooperative multi-agent deep reinforcement learning (c-MADRL) is under the threat of backdoor attacks. Once a backdoor trigger is observed, it will perform abnormal actions leading to failures or malicious goals. However, existing proposed backdoors suffer from several issues, e.g., fixed visual trigger patterns lack stealthiness, the backdoor is trained or activated by an additional network, or all agents are backdoored. To this end, in this paper, we propose a novel backdoor attack against c-MADRL, which attacks the entire multi-agent team by embedding the backdoor only in a single agent. Firstly, we introduce adversary spatiotemporal behavior patterns as the backdoor trigger rather than manual-injected fixed visual patterns or instant status and control the attack duration. This method can guarantee the stealthiness and practicality of injected backdoors. Secondly, we hack the original reward function of the backdoored agent via reward reverse and unilateral guidance during training to ensure its adverse influence on the entire team. We evaluate our backdoor attacks on two classic c-MADRL algorithms VDN and QMIX, in a popular c-MADRL environment SMAC. The experimental results demonstrate that our backdoor attacks are able to reach a high attack success rate (91.6\\%) while maintaining a low clean performance variance rate (3.7\\%)."
                ],
                "domain": [
                    "Internet of Things",
                    "Deep Reinforcement Learning",
                    "Covert Communication",
                    "Solar Physics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "acc61e5f-04a2-4060-94c1-e60d5502d346": {
                "pk": "acc61e5f-04a2-4060-94c1-e60d5502d346",
                "project_name": null,
                "name": "Mengyuan Yang",
                "bio": "I am a mathematician specializing in geometric structures and graph theory, with a particular focus on fractal circle packings and their underlying symmetries. My recent work introduces a new class of fractal circle packings that extends the polyhedral packings previously defined by Kontorovich and Nakamura. Through the application of infinite versions of the Koebe-Andreev-Thurston theorem, I have established the existence and uniqueness of these packings, providing a comprehensive description of their symmetry groups.\n\nIn addition to my work on circle packings, I have investigated the planar Turán numbers of specific $\\Theta_6$-graphs, uncovering the maximum number of edges in planar graphs that avoid these structures as subgraphs. My research not only identifies these extremal constructions but also demonstrates their sharpness, contributing to a deeper understanding of graph properties and their combinatorial implications. I am passionate about exploring the intersections of number theory and group theory within these mathematical frameworks, and I strive to illuminate the rich structures that arise from these investigations.",
                "collaborators": [
                    "Philip Rehwinkel",
                    "Ian Whitehead",
                    "David Yang",
                    "David Guan",
                    "Ervin Győri",
                    "Diep Luong-Le",
                    "Felicia Wang"
                ],
                "pub_titles": [
                    "Circle Packings from Tilings of the Plane",
                    "The Planar Turán Number of $Θ_6$-graphs"
                ],
                "pub_abstracts": [
                    "We introduce a new class of fractal circle packings in the plane, generalizing the polyhedral packings defined by Kontorovich and Nakamura. The existence and uniqueness of these packings are guaranteed by infinite versions of the Koebe-Andreev-Thurston theorem. We prove structure theorems giving a complete description of the symmetry groups for these packings. And we give several examples to illustrate their number-theoretic and group-theoretic significance.",
                    "There are two particular $\\Theta_6$-graphs - the 6-cycle graphs with a diagonal. We find the planar Tur\\'an number of each of them, i.e. the maximum number of edges in a planar graph $G$ of $n$ vertices not containing the given $\\Theta_6$ as a subgraph and we find infinitely many extremal constructions showing the sharpness of these results - apart from a small additive constant error in one of the cases."
                ],
                "domain": [
                    "Graph Theory",
                    "Combinatorics",
                    "Geometry",
                    "Number Theory"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "7ff97bac-3003-4f4c-b517-9dbded1aefeb": {
                "pk": "7ff97bac-3003-4f4c-b517-9dbded1aefeb",
                "project_name": null,
                "name": "Yankai Yu",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance on tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge and enhancing search efficiency.\n\nThrough these contributions, I strive to push the boundaries of what GNNs can achieve, fostering a deeper understanding of their structure and performance while making them more accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ac89ebd0-bdaf-4557-a054-e5ff9d60698e": {
                "pk": "ac89ebd0-bdaf-4557-a054-e5ff9d60698e",
                "project_name": null,
                "name": "Haixia Xu",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge, thus reducing computational costs.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and making them more accessible and effective for real-world applications. I am excited about the future directions in this field and the potential for my work to contribute to the broader understanding of machine learning and graph-based data.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ca1dbb9f-16e2-4311-88d5-91f00dfdd2b1": {
                "pk": "ca1dbb9f-16e2-4311-88d5-91f00dfdd2b1",
                "project_name": null,
                "name": "Kang Li",
                "bio": "I am a researcher with a strong focus on the intersection of quantum mechanics, electromagnetic theory, and machine learning applications. My recent work has delved into the duality symmetries of electromagnetic fields, revealing how electric and magnetic charges are interdependent under specific conditions. I have also explored the potential of masked autoencoders (MAE) in ultrasound imaging, proposing a novel deblurring approach that significantly enhances classification performance.\n\nMy investigations extend into non-commutative quantum mechanics, where I have studied various effects such as the HMW and Aharonov-Casher effects, employing innovative methods like Bopp's shift to derive corrections in quantum phases. Additionally, I have contributed to the understanding of reduced C*-algebras associated with étale groupoids, establishing important connections between algebraic structures and topological properties.\n\nIn the realm of renewable energy, I have developed optimal scheduling strategies for microgrids that incorporate electric vehicle demand response, demonstrating how this integration can lead to more efficient energy management. My work is characterized by a blend of theoretical insights and practical applications, aiming to bridge gaps between fundamental physics and real-world challenges. I am passionate about advancing our understanding of complex systems and leveraging this knowledge to drive innovation in technology and energy solutions.",
                "collaborators": [
                    "Jianhua Wang",
                    "Sayipjamal Dulat",
                    "Qingbo Kang",
                    "Jun Gao",
                    "Qicheng Lao",
                    "Yang Li",
                    "Christian Bönicke",
                    "Xiaomin Yu",
                    "Carlos Naon"
                ],
                "pub_titles": [
                    "Comments on the dependence between electric charge and magnetic charge",
                    "Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition",
                    "The HMW effect in Noncommutative Quantum Mechanics",
                    "The topological AC effect on noncommutative phase space",
                    "Incorporating demand response of electric vehicles in scheduling of isolated microgrids with renewables using a bi-level programming approach",
                    "Ideal structure and pure infiniteness of ample groupoid $C^*$-algebras",
                    "Two-dimensional Noncommutative atom Gas with Anandan interaction",
                    "Equivalent bosonic theory for the massive Thirring model with non-local interaction",
                    "Commutator Anomaly in Noncommutative Quantum Mechanics",
                    "The Aharonov-Bohm Effect in Noncommutative Quantum Mechanics"
                ],
                "pub_abstracts": [
                    "By using the two 4-dimensional potential formulation of electromagnetic (EM) field theory introduced in [1], we found that the SO(2) duality symmetric EM field theory can be reduced to the magnetic source free case by a special choice of SO(2) parameter,this special case we called nature picture of the EM field theory, the reduction condition led to a result, i.e. the electric charge and magnetic charge are no more independent. Some comments to paper [SD] are also mentioned.",
                    "Masked autoencoder (MAE) has attracted unprecedented attention and achieves remarkable performance in many vision tasks. It reconstructs random masked image patches (known as proxy task) during pretraining and learns meaningful semantic representations that can be transferred to downstream tasks. However, MAE has not been thoroughly explored in ultrasound imaging. In this work, we investigate the potential of MAE for ultrasound image recognition. Motivated by the unique property of ultrasound imaging in high noise-to-signal ratio, we propose a novel deblurring MAE approach that incorporates deblurring into the proxy task during pretraining. The addition of deblurring facilitates the pretraining to better recover the subtle details presented in the ultrasound images, thus improving the performance of the downstream classification task. Our experimental results demonstrate the effectiveness of our deblurring MAE, achieving state-of-the-art performance in ultrasound image classification. Overall, our work highlights the potential of MAE for ultrasound image recognition and presents a novel approach that incorporates deblurring to further improve its effectiveness.",
                    "The HMW effect in non-commutative quantum mechanics is studied. By solving the Dirac equations on non-commutative (NC) space and non-commutative phase space, we obtain topological HMW phase on NC space and NC phase space respectively, where the additional terms related to the space-space and momentum-momentum non-commutativity are given explicitly.",
                    "The Aharonov-Casher (AC) effect in non-commutative(NC) quantum mechanics is studied. Instead of using the star product method, we use a generalization of Bopp's shift method. After solving the Dirac equations both on noncommutative space and noncommutative phase space by the new method, we obtain the corrections to AC phase on NC space and NC phase space respectively.",
                    "In this work, a novel optimal scheduling approach is proposed for isolated microgrids (MGs) with renewable generations by incorporating demand response of electric vehicles (EVs). First, a bi-level programming-based MG scheduling model is proposed under real-time pricing environments, where the upper- and lower- levels seek to minimize the MG net operating cost and the EV charging cost. Second, a hybrid solution algorithm called JAYA-interior point method is put forward to solve the model. And finally, the simulation results demonstrate that incorporating demand response of electric vehicles is able to guide EV users to actively participate in MG scheduling and achieve the peak load shaving, which offers a fundamental way to balance the interests between MG and EV users.",
                    "In this paper, we study the ideal structure of reduced $C^*$-algebras $C^*_r(G)$ associated to \\'etale groupoids $G$. In particular, we characterize when there is a one-to-one correspondence between the closed, two-sided ideals in $C_r^*(G)$ and the open invariant subsets of the unit space $G^{(0)}$ of $G$. As a consequence, we show that if $G$ is an inner exact, essentially principal, ample groupoid, then $C_r^*(G)$ is (strongly) purely infinite if and only if every non-zero projection in $C_0(G^{(0)})$ is properly infinite in $C_r^*(G)$. We also establish a sufficient condition on the ample groupoid $G$ that ensures pure infiniteness of $C_r^*(G)$ in terms of paradoxicality of compact open subsets of the unit space $G^{(0)}$.   Finally, we introduce the type semigroup for ample groupoids and also obtain a dichotomy result: Let $G$ be an ample groupoid with compact unit space which is minimal and topologically principal. If the type semigroup is almost unperforated, then $C_r^*(G)$ is a simple $C^*$-algebra which is either stably finite or strongly purely infinite.",
                    "Landau like quantization of the Anandan system in a special electromagnetic field is studied. Unlike the cases of the AC system and the HMW system, the torques of the system on the magnetic dipole and the electric dipole don't vanish. By constructing Heisenberg algebra, the Landau analog levels and eigenstates on commutative space, NC space and NC phase space are obtained respectively. By using the coherent state method, some statistical properties of such free atom gas are studied and the expressions of some thermodynamic quantities related to revolution direction are obtained. Two particular cases of temperature are discussed and the more simple expressions of the free energy on the three spaces are obtained. We give the relation between the value of $\\sigma$ and revolution direction clearly, and find Landau like levels of the Anandan system are invariant and the levels between the AC system and the HMW system are interchanged each other under Maxwell dual transformations on the three spaces. The two sets of eigenstates labelled by $\\sigma$ can be related by a supersymmetry transformation on commutative space, but the phenomenon don't occur on NC situation. We emphasize that some results relevant to Anandan interaction are suitable for the cases of AC interaction and HMW interaction under special conditions.",
                    "We study, through path-integral methods, an extension of the massive Thirring model in which the interaction between currents is non-local. By examining the mass-expansion of the partition function we show that this non-local massive Thirring model is equivalent to a certain non-local extension of the sine-Gordon theory. Thus, we establish a non-local generalization of the famous Coleman's equivalence. We also discuss some possible applications of this result in the context of one-dimensional strongly correlated systems and finite-size Quantum Field Theories.",
                    "In this letter, firstly, the Schr$\\ddot{o}$dinger equation on noncommutative phase space is given by using a generalized Bopp's shift. Then the anomaly term of commutator of arbitrary physical observable operators on noncommutative phase space is obtained. Finally, the basic uncertainty relations for space-space and space-momentum as well as momentum-momentum operators in noncommutative quantum mechanics (NCQM), and uncertainty relation for arbitrary physical observable operators in NCQM are discussed.",
                    "The Aharonov-Bohm (AB) effect in non-commutative quantum mechanics (NCQM) is studied. First, by introducing a shift for the magnetic vector potential we give the Schr$\\ddot{o}$dinger equations in the presence of a magnetic field on NC space and NC phase space, respectively. Then by solving the Schr$\\ddot{o}$dinger equations, we obtain the Aharonov-Bohm (AB) phase on NC space and NC phase space, respectively."
                ],
                "domain": [
                    "Quantum Mechanics",
                    "Non-Commutative Geometry",
                    "Machine Learning",
                    "Electromagnetic Theory"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "25559ad2-4e31-4c67-9bfd-76d27d87bd00": {
                "pk": "25559ad2-4e31-4c67-9bfd-76d27d87bd00",
                "project_name": null,
                "name": "Xiaobo Zhou",
                "bio": "I am a researcher with a diverse focus spanning coding theory, computer vision, and wireless communication networks. My recent work has delved into the construction of binary cyclic codes, where I developed several classes of codes that achieve parameters with square-root bounds, contributing to the understanding of their minimum distances.\n\nIn the realm of computer vision, I introduced 4D-Editor, an interactive framework for object-level editing in dynamic scenes represented by neural radiance fields (NeRF). This innovative approach allows users to edit multiple objects seamlessly while maintaining spatial-temporal consistency, showcasing my commitment to enhancing user interaction in dynamic environments.\n\nAdditionally, I have explored memory management in multi-tenant datacenter systems, addressing the challenges faced by latency-critical services under memory pressure. My work led to the development of Hermes, a fast memory allocation mechanism that significantly reduces latency and improves performance for critical applications.\n\nFurthermore, I have investigated the potential of intelligent reflecting surfaces (IRS) in wireless powered communication networks, optimizing resource allocation and phase shifts to enhance energy efficiency. My findings provide valuable insights into the practical implementation of IRS technology, demonstrating its promise in improving wireless communication systems.\n\nOverall, my research is driven by a passion for solving complex problems across various domains, leveraging innovative methodologies to push the boundaries of technology.",
                "collaborators": [
                    "Xianhong Xie",
                    "Yaxin Zhao",
                    "Zhonghua Sun",
                    "Dadong Jiang",
                    "Zhihui Ke",
                    "Xidong Shi",
                    "Aidi Pi",
                    "Junxian Zhao",
                    "Shaoqi Wang",
                    "Qingqing Wu"
                ],
                "pub_titles": [
                    "Binary $[n,(n\\pm1)/2]$ cyclic codes with good minimum distances from sequences",
                    "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via Semantic Distillation",
                    "Memory at Your Service: Fast Memory Allocation for Latency-critical Services",
                    "IRS-Assisted Wireless Powered NOMA: Do We Really Need Different Phase Shifts in DL and UL?"
                ],
                "pub_abstracts": [
                    "Recently, binary cyclic codes with parameters $[n,(n\\pm1)/2,\\geq \\sqrt{n}]$ have been a hot topic since their minimum distances have a square-root bound. In this paper, we construct four classes of binary cyclic codes $\\mathcal{C}_{\\mathcal{S},0}$, $\\mathcal{C}_{\\mathcal{S},1}$ and $\\mathcal{C}_{\\mathcal{D},0}$, $\\mathcal{C}_{\\mathcal{D},1}$ by using two families of sequences, and obtain some codes with parameters $[n,(n\\pm1)/2,\\geq \\sqrt{n}]$. For $m\\equiv2\\pmod4$, the code $\\mathcal{C}_{\\mathcal{S},0}$ has parameters $[2^m-1,2^{m-1},\\geq2^{\\frac{m}{2}}+2]$, and the code $\\mathcal{C}_{\\mathcal{D},0}$ has parameters $[2^m-1,2^{m-1},\\geq2^{\\frac{m}{2}}+2]$ if $h=1$ and $[2^m-1,2^{m-1},\\geq2^{\\frac{m}{2}}]$ if $h=2$.",
                    "This paper targets interactive object-level editing (e.g., deletion, recoloring, transformation, composition) in dynamic scenes. Recently, some methods aiming for flexible editing static scenes represented by neural radiance field (NeRF) have shown impressive synthesis quality, while similar capabilities in time-variant dynamic scenes remain limited. To solve this problem, we propose 4D-Editor, an interactive semantic-driven editing framework, allowing editing multiple objects in a dynamic NeRF with user strokes on a single frame. We propose an extension to the original dynamic NeRF by incorporating a hybrid semantic feature distillation to maintain spatial-temporal consistency after editing. In addition, we design Recursive Selection Refinement that significantly boosts object segmentation accuracy within a dynamic NeRF to aid the editing process. Moreover, we develop Multi-view Reprojection Inpainting to fill holes caused by incomplete scene capture after editing. Extensive experiments and editing examples on real-world demonstrate that 4D-Editor achieves photo-realistic editing on dynamic NeRFs. Project page: https://patrickddj.github.io/4D-Editor",
                    "Co-location and memory sharing between latency-critical services, such as key-value store and web search, and best-effort batch jobs is an appealing approach to improving memory utilization in multi-tenant datacenter systems. However, we find that the very diverse goals of job co-location and the GNU/Linux system stack can lead to severe performance degradation of latency-critical services under memory pressure in a multi-tenant system. We address memory pressure for latency-critical services via fast memory allocation and proactive reclamation. We find that memory allocation latency dominates the overall query latency, especially under memory pressure. We analyze the default memory management mechanism provided by GNU/Linux system stack and identify the reasons why it is inefficient for latency-critical services in a multi-tenant system. We present Hermes, a fast memory allocation mechanism in user space that adaptively reserves memory for latency-critical services. It advises Linux OS to proactively reclaim memory of batch jobs. We implement Hermes in GNU C Library. Experimental result shows that Hermes reduces the average and the $99^{th}$ percentile memory allocation latency by up to 54.4% and 62.4% for a micro benchmark, respectively. For two real-world latency-critical services, Hermes reduces both the average and the $99^{th}$ percentile tail query latency by up to 40.3%. Compared to the default Glibc, jemalloc and TCMalloc, Hermes reduces Service Level Objective violation by up to 84.3% under memory pressure.",
                    "Intelligent reflecting surface (IRS) is a promising technology to improve the performance of wireless powered communication networks (WPCNs) due to its capability to reconfigure signal propagation environments via smart reflection. In particular, the high passive beamforming gain promised by IRS can significantly enhance the efficiency of both downlink wireless power transfer (DL WPT) and uplink wireless information transmission (UL WIT) in WPCNs. Although adopting different IRS phase shifts for DL WPT and UL WIT, i.e., dynamic IRS beamforming, is in principle possible but incurs additional signaling overhead and computational complexity, it is an open problem whether it is actually beneficial. To answer this question, we consider an IRS-assisted WPCN where multiple devices employ a hybrid access point (HAP) to first harvest energy and then transmit information using non-orthogonal multiple access (NOMA). Specifically, we aim to maximize the sum throughput of all devices by jointly optimizing the IRS phase shifts and the resource allocation. To this end, we first prove that dynamic IRS beamforming is not needed for the considered system, which helps reduce the number of IRS phase shifts to be optimized. Then, we propose both joint and alternating optimization based algorithms to solve the resulting problem. Simulation results demonstrate the effectiveness of our proposed designs over benchmark schemes and also provide useful insights into the importance of IRS for realizing spectrally and energy efficient WPCNs."
                ],
                "domain": [
                    "Coding Theory",
                    "Neural Networks",
                    "Memory Management",
                    "Wireless Communication"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can large language models (LLMs) be effectively applied to various bioinformatics tasks, such as genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it can significantly enhance the capabilities of bioinformatics by leveraging the advanced understanding and generative abilities of LLMs. This integration could lead to breakthroughs in biological research, improve the accuracy of genomic analyses, and facilitate drug discovery processes. Furthermore, addressing this question could advance knowledge in both NLP and bioinformatics, leading to practical applications that improve healthcare outcomes, accelerate scientific discoveries, and foster interdisciplinary collaboration.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in applying LLMs to bioinformatics stem from the complexity of biological data, which often requires domain-specific knowledge and understanding of intricate biological processes. Naive approaches may fail due to the high dimensionality of biological datasets, the need for specialized preprocessing, and the potential for overfitting when adapting general models to specific tasks. Additionally, the interpretability of LLMs in a biological context poses a significant obstacle, as understanding the reasoning behind model predictions is critical for trust and validation in scientific applications.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has often focused on traditional machine learning methods or domain-specific models that do not leverage the full potential of LLMs. Limitations in computational resources, the lack of large, annotated biological datasets for training, and insufficient interdisciplinary collaboration between NLP and bioinformatics have hindered progress. Our approach differs by utilizing the pre-training and fine-tuning paradigm of LLMs specifically tailored for bioinformatics tasks, thereby addressing the unique challenges posed by biological data and enhancing model performance.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves the following key components: (1) Utilizing pre-trained LLMs, such as BERT or GPT, and fine-tuning them on specific bioinformatics datasets (e.g., DNA sequences for genomics, RNA sequences for transcriptomics, protein sequences for proteomics, Molecular SMILES for drug discovery, and scRNA-seq data for single-cell analysis). (2) Employing evaluation metrics such as accuracy, F1-score, and area under the ROC curve (AUC) to assess model performance across tasks."
    },
    "2409.15675": {
        "paper_data": {
            "title": "Northeast Materials Database (NEMAD): Enabling Discovery of High Transition Temperature Magnetic Compounds",
            "url": "http://arxiv.org/abs/2409.15675v1",
            "arxiv_id": "2409.15675",
            "authors": [
                "Suman Itani",
                "Yibo Zhang",
                "Jiadong Zang"
            ],
            "abstract": "The discovery of novel magnetic materials with greater operating temperature ranges and optimized performance is essential for advanced applications. Current data-driven approaches are challenging and limited due to the lack of accurate, comprehensive, and feature-rich databases. This study aims to address this challenge by introducing a new approach that uses Large Language Models (LLMs) to create a comprehensive, experiment-based, magnetic materials database named the Northeast Materials Database (NEMAD), which consists of 26,706 magnetic materials (www.nemad.org). The database incorporates chemical composition, magnetic phase transition temperatures, structural details, and magnetic properties. Enabled by NEMAD, machine learning models were developed to classify materials and predict transition temperatures. Our classification model achieved an accuracy of 90% in categorizing materials as ferromagnetic (FM), antiferromagnetic (AFM), and non-magnetic (NM). The regression models predict Curie (N\\'eel) temperature with a coefficient of determination (R2) of 0.86 (0.85) and a mean absolute error (MAE) of 62K (32K). These models identified 62 (19) FM (AFM) candidates with a predicted Curie (N\\'eel) temperature above 500K (100K) from the Materials Project. This work shows the feasibility of combining LLMs for automated data extraction and machine learning models in accelerating the discovery of magnetic materials.",
            "introduction": "  Introduction  For centuries, magnetic materials have been discovered and studied due to their broad applications in modern science and technology, including data storage devices, energy technologies, advanced medical equipment, quantum computing, and consumer electronics[1, 2, 3]. Searching for efficient magnetic materials is crucial to addressing global energy challenges and revolutionizing the technology industry[4]. For example, high-performance permanent magnets can increase efficiency in renewable energy, like wind power and hydroelectric power generators. At the same time, the consumption of fossil fuels and greenhouse gases is reduced. It can also give us high density data storage solutions. Despite of these wide applications, limits for existing magnetic materials are present. For example, most high-performance magnetic materials contain rare earth elements and have a limited operating temperature range [5, 6]. The discovery of novel magnetic materials with greater operating temperature ranges using more abundant elements is a fundamental challenge in material science, partly due to the vast combinatorial space of possible compositions and the limitations of conventional methods.   The conventional techniques for material discovery have been largely based on systematic exploration of compositional space and intuition-directed experimentation. Although successful, these approaches require a lot of time and resources, and they can take years to produce useful outcomes [7]. The development of computational methods has opened up exciting new avenue toward materials discovery and property prediction. The first principles calculation such as the density functional theory (DFT) has enabled the predictions of material properties[8, 9]. Exploration of vast compositional spaces has been possible using high-throughput computational screening [10, 11]. These computational breakthroughs have resulted in the construction of huge materials databases, such as the Materials Project [10], AFLOW [11], AFLOWLIB [12], and OQMD [13]. Unfortunately, applying DFT to predict the magnetic properties of magnetic materials generally leads to less reliable results. Using the standard exchange-correlation functionals could not accurately describe the strongly correlated electron system in magnetic materials especially itinerant magnets. To accurately describe such a system, one should impose the space and spin symmetry constraints as done in ab initio wave function theory. However, the DFT method could not properly incorporate these constraints[14]. The integration of electronic structure information and mean field models for determining exchange interaction and constructing an effective Hamiltonian is a conventional approach in calculating the Curie temperature[15, 16, 17, 18]. This method, however, receives good results only for a short list of materials[19]. Moreover, the requirement for prior knowledge of the crystal structure of the material also puts a limit on the applicability of the DFT method. Another drawback of DFT calculation is that it is mostly limited to the materials of small unit cells due to the high computational costs of large unit cells[20, 21]. Due to the above constraints, determining the magnetic properties and discovering novel magnetic materials remains a difficult task.   The data-driven approach is a new paradigm of research that has received great interest in recent years because of its capabilities in accurate predictions and discovering hidden patterns that otherwise may get missed by traditional methods[22, 23]. Machine learning predicts physical properties quickly, discovers new materials, and optimizes the existing ones by analyzing large",
            "references": [
                {
                    "title": "Enhancing magnetocaloric material discovery: A machine learning approach using an autogenerated database by large language models",
                    "abstract": "Magnetic cooling based on the magnetocaloric effect is a promising solid-state refrigeration technology for a wide range of applications in different temperature ranges. Previous studies have mostly focused on near room temperature (300 K) and cryogenic temperature (<10 K) ranges, while important applications such as hydrogen liquefaction call for efficient magnetic refrigerants for the intermediate temperature range of 10–100 K. For efficient use in this range, new magnetocaloric materials with matching Curie temperatures need to be discovered, while conventional experimental approaches are typically time-consuming and expensive. Here, we report a computational material discovery pipeline based on a materials database containing more than 6000 entries auto-generated by extracting reported material properties from the literature using a large language model. We then use this database to train a machine learning model that can efficiently predict the magnetocaloric properties of materials based on their chemical composition. We further verify the magnetocaloric properties of the predicted compounds using ab initio atomistic spin dynamics simulations to complete the computational material discovery. Using this approach, we identify 11 new promising magnetocaloric materials for the target temperature range. Our work demonstrates the potential of combining large language models, machine learning, and ab initio simulations to efficiently discover new functional materials."
                },
                {
                    "title": "Structured information extraction from scientific text with large language models",
                    "abstract": null
                },
                {
                    "title": "GPTArticleExtractor: An automated workflow for magnetic material database construction",
                    "abstract": null
                },
                {
                    "title": "Unleashing the Power of Artificial Intelligence in Materials Design",
                    "abstract": "The integration of artificial intelligence (AI) algorithms in materials design is revolutionizing the field of materials engineering thanks to their power to predict material properties, design de novo materials with enhanced features, and discover new mechanisms beyond intuition. In addition, they can be used to infer complex design principles and identify high-quality candidates more rapidly than trial-and-error experimentation. From this perspective, herein we describe how these tools can enable the acceleration and enrichment of each stage of the discovery cycle of novel materials with optimized properties. We begin by outlining the state-of-the-art AI models in materials design, including machine learning (ML), deep learning, and materials informatics tools. These methodologies enable the extraction of meaningful information from vast amounts of data, enabling researchers to uncover complex correlations and patterns within material properties, structures, and compositions. Next, a comprehensive overview of AI-driven materials design is provided and its potential future prospects are highlighted. By leveraging such AI algorithms, researchers can efficiently search and analyze databases containing a wide range of material properties, enabling the identification of promising candidates for specific applications. This capability has profound implications across various industries, from drug development to energy storage, where materials performance is crucial. Ultimately, AI-based approaches are poised to revolutionize our understanding and design of materials, ushering in a new era of accelerated innovation and advancement."
                },
                {
                    "title": "Physics-Informed Machine-Learning Prediction of Curie Temperatures and Its Promise for Guiding the Discovery of Functional Magnetic Materials",
                    "abstract": null
                },
                {
                    "title": "Machine learning predictions of high-Curie-temperature materials",
                    "abstract": "Technologies that function at room temperature often require magnets with a high Curie temperature, TC, and can be improved with better materials. Discovering magnetic materials with a substantial TC is challenging because of the large number of candidates and the cost of fabricating and testing them. Using the two largest known datasets of experimental Curie temperatures, we develop machine-learning models to make rapid TC predictions solely based on the chemical composition of a material. We train a random-forest model and a k-NN one and predict on an initial dataset of over 2500 materials and then validate the model on a new dataset containing over 3000 entries. The accuracy is compared for multiple compounds' representations (“descriptors”) and regression approaches. A random-forest model provides the most accurate predictions and is not improved by dimensionality reduction or by using more complex descriptors based on atomic properties. A random-forest model trained on a combination of both datasets shows that cobalt-rich and iron-rich materials have the highest Curie temperatures for all binary and ternary compounds. An analysis of the model reveals systematic error that causes the model to over-predict low-TC materials and under-predict high-TC materials. For exhaustive searches to find new high-TC materials, analysis of the learning rate suggests either that much more data is needed or that more efficient descriptors are necessary."
                },
                {
                    "title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
                    "abstract": null
                },
                {
                    "title": "Machine learning-based Curie temperature prediction for magnetic 14:2:1 phases",
                    "abstract": "The TM14RE2B-based phases (TM = transition metal, RE = rare earth metal; hereafter called 14:2:1) enable permanent magnets with outstanding magnetic properties. Novel chemical compositions that represent new 14:2:1 phases necessitate that they do not demagnetize at application-specific operating temperatures. Therefore, an accurate knowledge of the Curie temperature ( T c) is important. For magnetic 14:2:1 phases, we present a machine learning model that predicts T c by using merely chemical compositional features. Hyperparameter tuning on bagging and boosting models, as well as averaging predictions from individual models using the voting regressor, enables a low mean-absolute-error of 16 K on an unseen test set. The training set and a test set have been constructed by randomly splitting, in an 80:20 ratio, of a database that contains 449 phases (270 compositionally unique) mapped with their T c, taken from distinct publications. The model correctly identifies the relative importance of key substitutional elements that influence T c, especially in an Fe base such as Co, Mn, and Al. This paper is expected to serve as a basis for accurate Curie temperature predictions in the sought-after 14:2:1 permanent magnet family, particularly for transition metal substitution of within 20% in an Fe or Co base."
                },
                {
                    "title": "A rule-free workflow for the automated generation of databases from scientific literature",
                    "abstract": null
                },
                {
                    "title": "Machine Learning Study of the Magnetic Ordering in 2D Materials.",
                    "abstract": "Magnetic materials have been applied in a large variety of technologies, from data storage to quantum devices. The development of two-dimensional (2D) materials has opened new arenas for magnetic compounds, even when classical theories discourage their examination. Here we propose a machine-learning-based strategy to predict and understand magnetic ordering in 2D materials. This strategy couples the prediction of the existence of magnetism in 2D materials using a random forest and the Shapley additive explanations method with material maps defined by atomic features predicting the magnetic ordering (ferromagnetic or antiferromagnetic). While the random forest model predicts magnetism with an accuracy of 86%, the material maps obtained by the sure independence screening and sparsifying method have an accuracy of ∼90% in predicting the magnetic ordering. Our model indicates that 3d transition metals, halides, and structural clusters with regular transition-metal sublattices have a positive contribution in the total weight deciding the existence of magnetism in 2D compounds. This behavior is associated with the competition between crystal field and exchange splitting. The machine learning model also indicates that the atomic spin orbit coupling (SOC) is a determinant feature for the identification of the patterns separating ferro- from antiferromagnetic order. The proposed strategy is used to identify novel 2D magnetic compounds that, together with the fundamental trends in the chemical and structural space, pave novel routes for experimental exploration."
                },
                {
                    "title": "On-the-fly interpretable machine learning for rapid discovery of two-dimensional ferromagnets with high Curie temperature",
                    "abstract": null
                },
                {
                    "title": "Magnetic and superconducting phase diagrams and transition temperatures predicted using text mining and machine learning",
                    "abstract": null
                },
                {
                    "title": "Perspective and Prospects for Rare Earth Permanent Magnets",
                    "abstract": null
                },
                {
                    "title": "A regression-based model evaluation of the Curie temperature of transition-metal rare-earth compounds",
                    "abstract": "The Curie temperature (TC) of RT binary compounds consisting of 3d transition-metal (T ) and 4f rare-earth elements (R) is analyzed systematically by a developed machine learning technique called kernel regression-based model evaluation. Twenty-one descriptive variables were designed assuming completely obtained information of the TC. Multiple kernel regression analyses with different kernel types: cosine, linear, Gaussian, polynomial, and Laplacian kernels were implemented and examined. All possible descriptive variable combinations were generated to construct the corresponding prediction models. As a result, by appropriate combinations between descriptive variable sets and kernel formulations, we demonstrate that a number of kernel regression models can accurately reproduce the TC of the RT compounds. The relevance of descriptive variables for predicting TC are systematically investigated. The results indicate that the rare-earth concentration is the most relevant variable in the TC phenomenon. We demonstrate that the regression-based model selection technique can be applied to learn the relationship between the descriptive variables and the actuation mechanism of the corresponding physical phenomenon, i.e., TC in the present case."
                },
                {
                    "title": "An accelerating approach of designing ferromagnetic materials via machine learning modeling of magnetic ground state and Curie temperature",
                    "abstract": "ABSTRACT Magnetic materials have a plethora of applications from information technologies to energy harvesting. However, their functionalities are often limited by the magnetic ordering temperature. In this work, we performed random forest on the magnetic ground state and the Curie temperature (TC ) to classify ferromagnetic and antiferromagnetic compounds and to predict the TC of the ferromagnets. The resulting accuracy is about 87% for classification and 91% for regression. When the trained model is applied to magnetic intermetallic materials in Materials Project, the accuracy is comparable. Our work paves the way to accelerate the discovery of new magnetic compounds for technological applications. GRAPHICAL ABSTRACT"
                },
                {
                    "title": "Permanent Magnetism",
                    "abstract": null
                },
                {
                    "title": "Data‐Driven Materials Science: Status, Challenges, and Perspectives",
                    "abstract": "Data‐driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning—typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high‐throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data‐driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data‐driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field."
                },
                {
                    "title": "Predicting the Curie temperature of ferromagnets using machine learning",
                    "abstract": "The magnetic properties of a material are determined by a subtle balance between the various interactions at play, a fact that makes the design of new magnets a daunting task. High-throughput electronic structure theory may help to explore the vast chemical space available and offers a design tool to the experimental synthesis. This method efficiently predicts the elementary magnetic properties of a compound and its thermodynamical stability, but it is blind to information concerning the magnetic critical temperature. Here we introduce a range of machine-learning models to predict the Curie temperature, $T_\\mathrm{C}$, of ferromagnets. The models are constructed by using experimental data for about 2,500 known magnets and consider the chemical composition of a compound as the only feature determining $T_\\mathrm{C}$. Thus, we are able to establish a one-to-one relation between the chemical composition and the critical temperature. We show that the best model can predict $T_\\mathrm{C}$'s with an accuracy of about 50K. Most importantly our model is able to extrapolate the predictions to regions of the chemical space, where only a little fraction of the data was considered for training. This is demonstrated by tracing the $T_\\mathrm{C}$ of binary intermetallic alloys along their composition space and for the Al-Co-Fe ternary system."
                },
                {
                    "title": "Heavy rare earth free, free rare earth and rare earth free magnets - vision and reality.",
                    "abstract": null
                },
                {
                    "title": "Machine learning for molecular and materials science",
                    "abstract": null
                },
                {
                    "title": "Data-Driven Materials Investigations: The Next Frontier in Understanding and Predicting Fatigue Behavior",
                    "abstract": null
                },
                {
                    "title": "Billion-Scale Similarity Search with GPUs",
                    "abstract": "Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 × faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."
                },
                {
                    "title": "MAGNDATA: towards a database of magnetic structures. I. The commensurate case",
                    "abstract": "A free web page under the name MAGNDATA, which provides detailed quantitative information on more than 400 published magnetic structures, has been made available at the Bilbao Crystallographic Server (http://www.cryst.ehu.es). It includes both commensurate and incommensurate structures. In the first article in this series, the information available on commensurate magnetic structures was presented [Gallego, Perez-Mato, Elcoro, Tasci, Hanson, Momma, Aroyo & Madariaga (2016). J. Appl. Cryst. 49, 1750–1776]. In this second article, the subset of the database devoted to incommensurate magnetic structures is discussed. These structures are described using magnetic superspace groups, i.e. a direct extension of the non-magnetic superspace groups, which is the standard approach in the description of aperiodic crystals. The use of magnetic superspace symmetry ensures a robust and unambiguous description of both atomic positions and magnetic moments within a common unique formalism. The point-group symmetry of each structure is derived from its magnetic superspace group, and any macroscopic tensor property of interest governed by this point-group symmetry can be retrieved through direct links to other programs of the Bilbao Crystallographic Server. The fact that incommensurate magnetic structures are often reported with ambiguous or incomplete information has made it impossible to include in this collection a good number of the published structures which were initially considered. However, as a proof of concept, the published data of about 30 structures have been re-interpreted and transformed, and together with ten structures where the superspace formalism was directly employed, they form this section of MAGNDATA. The relevant symmetry of most of the structures could be identified with an epikernel or isotropy subgroup of one irreducible representation of the space group of the parent phase, but in some cases several irreducible representations are active. Any entry of the collection can be visualized using the online tools available on the Bilbao server or can be retrieved as a magCIF file, a file format under development by the International Union of Crystallography. These CIF-like files are supported by visualization programs like Jmol and by analysis programs like JANA and ISODISTORT."
                },
                {
                    "title": "A General-Purpose Machine Learning Framework for Predicting Properties of Inorganic Materials",
                    "abstract": null
                },
                {
                    "title": "Computational predictions of energy materials using density functional theory",
                    "abstract": null
                },
                {
                    "title": "Criteria for Predicting the Formation of Single-Phase High-Entropy Alloys",
                    "abstract": "High entropy alloys constitute a new class of materials whose very existence poses fundamental questions. Originally thought to be stabilized by the large entropy of mixing, these alloys have attracted attention due to their potential applications, yet no model capable of robustly predicting which combinations of elements will form a single-phase currently exists. Here we propose a model that, through the use of high-throughput computation of the enthalpies of formation of binary compounds, is able to confirm all known high-entropy alloys while rejecting similar alloys that are known to form multiple phases. Despite the increasing entropy, our model predicts that the number of potential single-phase multicomponent alloys decreases with an increasing number of components: out of more than two million possible 7-component alloys considered, fewer than twenty single-phase alloys are likely."
                },
                {
                    "title": "Materials Design and Discovery with High-Throughput Density Functional Theory: The Open Quantum Materials Database (OQMD)",
                    "abstract": null
                },
                {
                    "title": "Commentary: The Materials Project: A materials genome approach to accelerating materials innovation",
                    "abstract": "Accelerating the discovery of advanced materials is essential for human welfare and sustainable, clean energy. In this paper, we introduce the Materials Project (www.materialsproject.org), a core program of the Materials Genome Initiative that uses high-throughput computing to uncover the properties of all known inorganic materials. This open dataset can be accessed through multiple channels for both interactive exploration and data mining. The Materials Project also seeks to create open-source platforms for developing robust, sophisticated materials analyses. Future efforts will enable users to perform ‘‘rapid-prototyping’’ of new materials in silico, and provide researchers with new avenues for cost-effective, data-driven materials design. © 2013 Author(s). All article content, except where otherwise noted, is licensed under a Creative Commons Attribution 3.0 Unported License."
                },
                {
                    "title": "Python Materials Genomics (pymatgen): A robust, open-source python library for materials analysis",
                    "abstract": null
                },
                {
                    "title": "AFLOW: An automatic framework for high-throughput materials discovery",
                    "abstract": null
                },
                {
                    "title": "AFLOWLIB.ORG: A distributed materials properties repository from high-throughput ab initio calculations",
                    "abstract": null
                },
                {
                    "title": "Inorganic Materials Database for Exploring the Nature of Material",
                    "abstract": "An inorganic materials database system, AtomWork, has been developed and released on the Internet. It includes the phase diagram, crystal structure, X-ray powder diffraction, and property data of more than 80,000 inorganic materials extracted from scientific literature. The feature of this database is that the information of the synthesis, identification, and property of materials is organically linked, which enables the data reported in different papers to be grouped and compared at four different levels: chemical system, compound, substance, and material. The database can provide users with a comprehensive overview of substances and necessary information to understand the relationships among chemical component, structure, and property."
                },
                {
                    "title": "Scikit-learn: Machine Learning in Python",
                    "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."
                },
                {
                    "title": "A systematic analysis of performance measures for classification tasks",
                    "abstract": null
                },
                {
                    "title": "Insights into Current Limitations of Density Functional Theory",
                    "abstract": "Density functional theory of electronic structure is widely and successfully applied in simulations throughout engineering and sciences. However, for many predicted properties, there are spectacular failures that can be traced to the delocalization error and static correlation error of commonly used approximations. These errors can be characterized and understood through the perspective of fractional charges and fractional spins introduced recently. Reducing these errors will open new frontiers for applications of density functional theory."
                },
                {
                    "title": "Computational complexity of interacting electrons and fundamental limitations of density functional theory",
                    "abstract": null
                },
                {
                    "title": "Exchange interactions, spin waves, and transition temperatures in itinerant magnets",
                    "abstract": "This contribution reviews an ab initio two-step procedure to determine exchange interactions, spin-wave spectra, and thermodynamic properties of itinerant magnets. In the first step, the self-consistent electronic structure of a system is calculated for a collinear spin structure at zero temperature. In the second step, parameters of an effective classical Heisenberg Hamiltonian are determined using the magnetic force theorem and the one-electron Green functions. The Heisenberg Hamiltonian and methods of statistical physics are employed in subsequent evaluation of magnon dispersion laws, spin-wave stiffness constants, and Curie/Néel temperatures. The applicability of the developed scheme is illustrated by selected properties of various systems such as transition and rare-earth metals, disordered alloys including diluted magnetic semiconductors, ultrathin films, and surfaces. A comparison to other ab initio approaches is presented as well."
                },
                {
                    "title": "Spin Symmetry Requirements in Density Functional Theory: The Proper Way to Predict Magnetic Coupling Constants in Molecules and Solids",
                    "abstract": null
                },
                {
                    "title": "Electronic structure and volume magnetostriction of rare-earth metals and compounds",
                    "abstract": null
                },
                {
                    "title": "Handbook of Magnetic Materials",
                    "abstract": null
                },
                {
                    "title": "Magnetic Interactions in Transition Metal Oxides with Orbital Degrees of Freedom",
                    "abstract": "We review the frustrated magnetic interactions in spin‐orbital models which describe superexchange in transition metal oxides with orbital degeneracy, and analyze the reasons for the symmetry breaking in cubic perovskites. The superexchange in eg systems is dominated by orbital interactions responsible for the orbital ordering, and the A‐type antiferromagnetic ordering follows at lower temperatures. Instead, a generic tendency towards dimerization, found already in the degenerate Hubbard model, occurs in t2g systems. In this case the quantum orbital fluctuations may stabilize orbital liquid states along one directions even in some undoped t2g systems, leading to the C‐type antiferromagnetic order. The orbital liquid in manganites is triggered by doping. The present understanding of the spectroscopic parameters provides reliable information on the magnetic interactions, as shown on the example of magnons in ferromagnetic cubic and bilayer manganites."
                },
                {
                    "title": "Molecular field analysis for melt-spun amorphous Fe100−xGdx alloys (18≦X≦60)",
                    "abstract": null
                },
                {
                    "title": "Mean field analysis of the magnetic properties of amorphous transition-metal--rare-earth alloys",
                    "abstract": "A generalized form of mean field theory and procedures for applying it to analyze experimental data on the temperature dependence of the saturation magnetization or the effective gyromagnetic factor of n‐component amorphous transition‐metal–rare‐earth alloys are outlined. This analysis yields the spin values of each component and the effective exchange interaction energies between them, from which other magnetic properties such as the subnetwork magnetizations and the macroscopic exchange stiffness can be calculated. Examples of application of this mean field procedure are given."
                },
                {
                    "title": "Magnetic properties of amorphous alloy films of Fe with Gd, Tb, Dy, Ho, or Er",
                    "abstract": "Amorphous rare‐earth RE(Gd, Tb, Dy, Ho, Er) ‐Fe films prepared by cosputtering were studied. The compositional analysis was obtained from known deposition profiles, x‐ray microanalysis, and the stripe‐width measurements. The structural variation with composition change was investigated by electron diffraction and dark‐field microscopy. The Curie temperature Tc, the compensation temperature Tcomp, the coercive force Hc, the uniaxial anisotropy energy Ku and the static domain properties such as the stripe width Ws, the wall energy σw, and the exchange stiffness constant A were investigated. The systematic variation of Tc and Tcomp associated with a variation of composition and RE species could be described by the Heiman et al. model. The static domain properties could be interpreted in terms of the wall energy model and the mean field approximation of the exchange stiffness constant A."
                },
                {
                    "title": "Bibliography of Magnetic Materials and Tabulation of Magnetic Transition Temperatures",
                    "abstract": null
                },
                {
                    "title": "Self-Consistent Equations Including Exchange and Correlation Effects",
                    "abstract": "From a theory of Hohenberg and Kohn, approximation methods for treating an inhomogeneous system of interacting electrons are developed. These methods are exact for systems of slowly varying or high density. For the ground state, they lead to self-consistent equations analogous to the Hartree and Hartree-Fock equations, respectively. In these equations the exchange and correlation portions of the chemical potential of a uniform electron gas appear as additional effective potentials. (The exchange portion of our effective potential differs from that due to Slater by a factor of $\\frac{2}{3}$.) Electronic systems at finite temperatures and in magnetic fields are also treated by similar methods. An appendix deals with a further correction for systems with short-wavelength density oscillations."
                },
                {
                    "title": "Magnetic Oxides and Other Compounds",
                    "abstract": null
                },
                {
                    "title": "Descriptor : Auto-generated materials database of Curie and Néel temperatures via semi-supervised relationship extraction",
                    "abstract": "Large auto-generated databases of magnetic materials properties have the potential for great utility in materials science research. This article presents an auto-generated database of 39,822 records containing chemical compounds and their associated Curie and Néel magnetic phase transition temperatures. The database was produced using natural language processing and semi-supervised quaternary relationship extraction, applied to a corpus of 68,078 chemistry and physics articles. Evaluation of the database shows an estimated overall precision of 73%. Therein, records processed with the text-mining toolkit, ChemDataExtractor, were assisted by a modified Snowball algorithm, whose original binary relationship extraction capabilities were extended to quaternary relationship extraction. Consequently, its machine learning component can now train with ≤ 500 seeds, rather than the 4,000 originally used. Data processed with the modified Snowball algorithm affords 82% precision. Database records are available in MongoDB, CSV and JSON formats which can easily be read using Python, R, Java and MatLab. This makes the database easy to query for tackling big-data materials science initiatives and provides a basis for magnetic materials discovery."
                },
                {
                    "title": "Machine learning in materials informatics: recent applications and prospects",
                    "abstract": null
                },
                {
                    "title": "Magnetism in Condensed Matter",
                    "abstract": "1. Introduction 2. Isolated magnetic moments 3. Environments 4. Interactions 5. Order and magnetic structures 6. Order and broken symmetry 7. Magnetism in metals 8. Competing interactions and low dimensionality Appendix A: Units in electromagnetism Appendix B: Electromagnetism Appendix C: Quantum and atomic physics Appendix D: Energy in magnetism and demagnetism Appendix E: Statistical mechanics Appendix F: List of symbols Index"
                }
            ]
        },
        "author_data": {
            "c634dca8-0c17-4e5f-94c3-59bee339d067": {
                "pk": "c634dca8-0c17-4e5f-94c3-59bee339d067",
                "project_name": null,
                "name": "Suman Itani",
                "bio": "I am a researcher dedicated to the exploration and characterization of magnetic materials, with a particular focus on leveraging advanced technologies to enhance our understanding of their properties. My recent work centers around the development of a comprehensive database of magnetic materials, which I created by harnessing the power of large language models to extract critical information from a vast corpus of scientific literature. \n\nBy analyzing over 22,120 articles from the Journal of Magnetism and Magnetic Materials, I successfully compiled a database that includes 2,035 distinct magnetic materials, predominantly ferromagnetic in nature. Each entry in this database is meticulously curated, containing essential details such as chemical compositions, structural characteristics, and key magnetic temperatures like Curie and N'eel points. \n\nI take great pride in the accuracy of this database, having rigorously cross-verified each entry against the original literature to ensure its reliability. My goal is to provide a valuable resource for researchers in the field, facilitating the discovery of new magnetic materials and advancing our collective knowledge in this exciting area of study.",
                "collaborators": [
                    "Yibo Zhang",
                    "Kamal Khanal",
                    "Emmanuel Okyere",
                    "Gavin Smith",
                    "Koichiro Takahashi",
                    "Jiadong Zang"
                ],
                "pub_titles": [
                    "GPTArticleExtractor: An Automated Workflow for Magnetic Material Database Construction"
                ],
                "pub_abstracts": [
                    "A comprehensive database of magnetic materials is valuable for researching the properties of magnetic materials and discovering new ones. This article introduces a novel workflow that leverages large language models for extracting key information from scientific literature. From 22,120 articles in the Journal of Magnetism and Magnetic Materials, a database containing 2,035 magnetic materials was automatically generated, with ferromagnetic materials constituting 76% of the total. Each entry in the database includes the material's chemical compounds, as well as related structures (space group, crystal structure) and magnetic temperatures (Curie, N'eel, and other transitional temperatures). To ensure data accuracy, we meticulously compared each entry in the database against the original literature, verifying the precision and reliability of each entry."
                ],
                "domain": [
                    "Materials Science",
                    "Data Mining",
                    "Natural Language Processing",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "2621e8c6-b53a-455c-8a6d-9f57dc9577d8": {
                "pk": "2621e8c6-b53a-455c-8a6d-9f57dc9577d8",
                "project_name": null,
                "name": "Yibo Zhang",
                "bio": "I am a researcher with a keen interest in the interplay between topology, algebra, and optimization. My recent work has focused on torus fibrations over the 2-sphere, where I explored the Hurwitz equivalence of their monodromies. I demonstrated that if two torus fibrations share the same type of singularities, their global monodromies can be shown to be Hurwitz equivalent through the addition of specific torus Lefschetz fibrations, particularly in cases of \"simple\" singularities.\n\nIn another area of my research, I investigated holomorphic curves in moduli spaces, revealing that when peripheral monodromies are of infinite order, the corresponding holomorphic map acts as a quasi-isometric immersion. This work also led to improvements in the Parshin-Arakelov finiteness theorem, establishing that there are finitely many monodromy homomorphisms induced by holomorphic curves of a certain type, bounded away from zero.\n\nAdditionally, I have contributed to the field of optimization by developing a derivative-free algorithm, LDGM, for monotone (weakly) DR-submodular continuous maximization. This innovative approach allows for effective optimization without relying on gradient information, achieving competitive approximation guarantees. My empirical studies on budget allocation further validate the robustness and effectiveness of LDGM compared to traditional gradient-based methods.\n\nThrough these diverse research endeavors, I aim to bridge theoretical insights with practical applications, contributing to the advancement of both mathematical understanding and algorithmic efficiency.",
                "collaborators": [
                    "Chao Qian",
                    "Ke Tang"
                ],
                "pub_titles": [
                    "Classification of Torus Fibrations Over $S^2$ Up to Fibre Sum Stabilisation",
                    "Holomorphic Curves in Moduli Spaces Are Quasi-Isometrically Immersed",
                    "Maximizing Monotone DR-submodular Continuous Functions by Derivative-free Optimization"
                ],
                "pub_abstracts": [
                    "We study torus fibrations over the 2-sphere and Hurwitz equivalence of their monodromies. We show that, if two torus fibrations over $S^2$ have the same type of singularities, then their global monodromies are Hurwitz equivalent after performing direct sums with certain torus Lefschetz fibrations. The additional torus Lefschetz fibration is universal when the type of singularities is \"simple\".",
                    "A holomorphic curve in moduli spaces is the image of a non-constant holomorphic map from a hyperbolic surface $B$ of type $(g,n)$ to the moduli space $\\mathcal{M}_h$ of closed Riemann surfaces of genus $h$. We show that, when all peripheral monodromies are of infinite order, the holomorphic map is a quasi-isometric immersion with parameters depending only on $g$, $n$, $h$ and the systole of $B$. When peripheral monodromies also satisfy an additional condition, we find a lift quasi-isometrically embedding a fundamental polygon of the hyperbolic surface $B$ into the Teichm\\\"uller space. We further improve the Parshin-Arakelov finiteness theorem, by proving that there are only finitely many monodromy homomorphisms induced by holomorphic curves of type $(g,n)$ in $\\mathcal{M}_h$ where systole is bounded away from $0$, up to equivalence.",
                    "In this paper, we study the problem of monotone (weakly) DR-submodular continuous maximization. While previous methods require the gradient information of the objective function, we propose a derivative-free algorithm LDGM for the first time. We define $\\beta$ and $\\alpha$ to characterize how close a function is to continuous DR-submodulr and submodular, respectively. Under a convex polytope constraint, we prove that LDGM can achieve a $(1-e^{-\\beta}-\\epsilon)$-approximation guarantee after $O(1/\\epsilon)$ iterations, which is the same as the best previous gradient-based algorithm. Moreover, in some special cases, a variant of LDGM can achieve a $((\\alpha/2)(1-e^{-\\alpha})-\\epsilon)$-approximation guarantee for (weakly) submodular functions. We also compare LDGM with the gradient-based algorithm Frank-Wolfe under noise, and show that LDGM can be more robust. Empirical results on budget allocation verify the effectiveness of LDGM."
                ],
                "domain": [
                    "Topology",
                    "Algebraic Geometry",
                    "Optimization",
                    "Submodular Functions"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "a84f5ace-2cc1-49e3-bacf-bde4adcddc3b": {
                "pk": "a84f5ace-2cc1-49e3-bacf-bde4adcddc3b",
                "project_name": null,
                "name": "Jiadong Zang",
                "bio": "I am a researcher deeply engaged in the study of magnetic skyrmions and topological materials, with a focus on their unique properties and potential applications in spintronics. My recent work has explored the dynamics of skyrmions under various conditions, including temperature gradients and external magnetic fields, revealing counterintuitive behaviors that challenge conventional understanding. I have also investigated the role of spin-orbit coupling in B20 compounds and the effects of non-trivial topological textures in three-dimensional systems.\n\nMy research employs first-principles calculations and theoretical modeling to uncover the intricate relationships between electronic and magnetic structures in materials like molybdenum nitrides and transition metal thin films. I have contributed to the understanding of the topological magnetoelectric effect and its implications for axion insulators, as well as the dynamics of magnetic hopfions, which exhibit rich and complex motion driven by external currents.\n\nThrough my work, I aim to bridge the gap between theoretical predictions and experimental realizations, providing insights that could pave the way for novel magnetic materials and devices. I am particularly interested in how these topological phenomena can be harnessed for advanced applications in information technology and quantum computing. My ongoing research continues to explore the frontiers of magnetism and topology, seeking to uncover new materials and phenomena that can revolutionize the field.",
                "collaborators": [
                    "Sergey S. Pershoguba",
                    "Wei Li",
                    "Naoto Nagaosa",
                    "Yizhou Liu",
                    "Jie-Xiang Yu",
                    "Lingyao Kong",
                    "Jian Kang",
                    "Motohiko Ezawa",
                    "Takehito Yokoyama",
                    "Wen-Tao Hou"
                ],
                "pub_titles": [
                    "Systematic Search and A New Family of Skyrmion Materials",
                    "Monopole Current and Unconventional Hall Response on Topological Insulator",
                    "Dynamics of an Insulating Skyrmion under a Temperature Gradient",
                    "Transport Theory of Metallic B20 Helimagnets",
                    "Current Modulator based on Topological Insulator with Sliding Magnetic Superlattice",
                    "Dynamics of magnetization on the topological surface",
                    "Microscopic Theory of Nonlinear Hall Effect in Three-dimensional Magnetic Systems",
                    "Inertia in skyrmions confined to one-dimensional geometries",
                    "Slowly rotating neutron stars and hadronic stars in chiral SU(3) quark mean field model",
                    "Size Effects on Transport Properties in Topological Anderson Insulators",
                    "U(1) symmetry of the spin-orbit coupled Hubbard model on the Kagome lattice",
                    "Binding a Hopfion in Chiral Magnet Nanodisk",
                    "Giant perpendicular magnetic anisotropy in Fe/III-V nitride thin films",
                    "Magnetic-Resonance-Induced Pseudo-electric Field and Giant Current Response in Axion Insulators",
                    "Three-dimensional dynamics of magnetic hopfion driven by spin transfer torque",
                    "Electronic scattering off a magnetic hopfion",
                    "Hall Effect Induced by Topologically Trivial Target Skyrmions",
                    "Thermally Driven Topology in Frustrated Systems",
                    "Topological quantum phase transition in an S=2 spin chain"
                ],
                "pub_abstracts": [
                    "Magnetic skyrmions have recently attracted great attentions. However they are harbored in very limited numbers of magnets up to now. The search of new helimagnetic materials is thus an urgent topic in the field of skyrmion physics. In this letter, we provide a guideline on this issue, and discuss the possibility of realizing skyrmions in a new family of molybdenum nitrides $A_2$Mo$_3$N ($A$=Fe, Co, and Rh). By means of the first-principles calculations, the electronic and magnetic structures are calculated and the existence of strong Dzyaloshinskii-Moriya interaction is demonstrated.",
                    "We study theoretically the charged current above a topological insulator (TI) separated by a ferromagnetic insulating layer. An unconventional Hall response occurs in the conducting layer on top of the TI which approaches to a constant value independent of R for R<<l and decays with R^{-1} for R>>l, where R is the separation between TI and conducting layer and l is the screening length. In the comoving frame, it can be interpreted as a monopole current attached to the TI surface. The same mechanism gives the Hall response and deflection of the electron beam injected to the surface of insulating ferromagnet. A realistic estimate of an order of magnitude shows that both effects give reasonably large signal experimentally accessible.",
                    "We study the Skyrmion dynamics in thin films under a temperature gradient. Our numerical simulations show that both single and multiple Skyrmions in a crystal move towards the high temperature region, which is contrary to particle diffusion. Noticing a similar effect in the domain wall motion, we employ a theory based on magnon dynamics to explain this counterintuitive phenomenon. Unlike the temperature driven domain wall motion, the Skyrmion's topological charge plays an important role, and a transverse Skyrmion motion is observed. Our theory turns out to be in agreement with numerical simulations, both qualitatively and quantitatively. Our calculation indicated that a very promising Skyrmion dynamic phenomenon can be observed in experiments.",
                    "B20 compounds are a class of cubic helimagnets harboring nontrivial spin textures such as spin helices and skyrmions. It has been well understood that the Dzyaloshinskii-Moriya (DM) interaction is the origin of these textures, and the physics behind the DM interaction is the spin-orbital coupling (SOC). However the SOC shows its effect not only on the spins, but also on the electrons. In this paper, we will discuss effects of the SOC on the electron and spin transports in B20 compounds. An effective Hamiltonian is presented from symmetry analysis, and the spin-orbital coupling therein shows anomalous behaviors in anisotropic magnetoresistance (AMR) and helical resistance. New effects such as inverse spin-galvanic effect is proposed, and the origin of the DM interaction is discussed.",
                    "We study theoretically the surface of a topological insulator with a sliding magnetic superlattice coated above. By analyzing time-dependent Dirac equations, the dynamics of the zero mode is investigated. When the superlattice's sliding velocity is smaller (larger) than the Fermi velocity of topological insulator, the zero mode is perfectly (imperfectly) pumped. We also propose the application of this setup, which rectifies currents or generates pulse currents. It would provide a prototype of electronic devices based on topological insulator.",
                    "We investigate theoretically the dynamics of magnetization coupled to the surface Dirac fermions of a three dimensional topological insulator, by deriving the Landau-Lifshitz-Gilbert (LLG) equation in the presence of charge current. Both the inverse spin-Galvanic effect and the Gilbert damping coefficient $\\alpha$ are related to the two-dimensional diagonal conductivity $\\sigma_{xx}$ of the Dirac fermion, while the Berry phase of the ferromagnetic moment to the Hall conductivity $\\sigma_{xy}$. The spin transfer torque and the so-called $\\beta$-terms are shown to be negligibly small. Anomalous behaviors in various phenomena including the ferromagnetic resonance are predicted in terms of this LLG equation.",
                    "The nonlinear Hall effect (NLHE) has been detected in various of condensed matter systems. Unlike linear Hall effect, NLHE may exist in physical systems with broken inversion symmetry in the crystal. On the other hand, real space spin texture may also break inversion symmetry and result in NLHE. In this letter, we employ the Feynman diagramatic technique to calculate nonlinear Hall conductivity (NLHC) in three-dimensional magnetic systems. The results connect NLHE with the physical quantity of emergent electrodynamics which oringates from the magnetic texture. The leading order contribution of NLHC $\\chi_{abb}$ is proportional to the emergent toroidal moment $\\mathcal{T}_{a}^{e}$ which reflects how the spin textures wind in three dimension.",
                    "Magnetic skyrmions are conventionally attributed to having zero mass. In contrast, we show that skyrmions confined to one-dimensional geometries generically acquire mass (inertia) due to the combined effects of the skyrmion Hall effect and the elasticity of the system. We investigate the massive behavior of the skyrmion for a simplified periodic model of the disorder. We show that skyrmion mass lowers the critical depinning force and leads to a step-like behavior in the skyrmion velocity-vs-current curves, which were recently observed in experiments. Finite mass could also lead to hysteresis in the velocity-vs-current curves.",
                    "The equations of state for neutron matter, strange and non-strange hadronic matter in a chiral SU(3) quark mean field model are applied in the study of slowly rotating neutron stars and hadronic stars. The radius, mass, moment of inertia, and other physical quantities are carefully examined. The effect of nucleon crust for the strange hadronic star is exhibited. Our results show the rotation can increase the maximum mass of compact stars significantly. For big enough mass of pulsar which can not be explained as strange hadronic star, the theoretical approaches to increase the maximum mass are addressed.",
                    "We study the size effects on the transport properties in topological Anderson insulators by means of the Landauer-B\\\"uttiker formalism combined with the nonequilibrium Green function method. Conductances calculated for serval different widths of the nanoribbons reveal that there is no longer quantized plateaus for narrow nanoribbons. The local spin-resolved current distribution demonstrates that the edge states on the two sides can be coupled, leading to enhancement of backscattering as the width of the nanoribbon decreases, thus destroying the perfect quantization phenomena in the topological Anderson insulator. We also show that the main contribution to the nonquantized conductance also comes from edge states. Experiment proposals on topological Anderson insulator are discussed finally.",
                    "We study the symmetry properties of the single-band Hubbard model with general spin-orbit coupling (SOC) on the Kagome lattice. We show that the global U(1) spin-rotational symmetry is present in the Hubbard Hamiltonian owing to the inversion symmetry centered at sites. The corresponding spin Hamiltonian has, therefore, the SO(2) spin-rotational symmetry, which can be captured by including SOC non-perturbatively. The exact classical groundstates, which we obtain for arbitrary SOC, are governed by the SU(2) fluxes associated with SOC threading the constituent triangles. The groundstates break the SO(2) symmetry, and the associated Berezinsky-Kosterlitz-Thouless transition temperature is determined by the SU(2) fluxes through the triangles, which we confirm by finite temperature classical Monte Carlo simulation.",
                    "Hopfions are three-dimensional (3D) topological textures characterized by the integer Hopf invariant $Q_H$. Here, we present the realization of a zero--field, stable hopfion spin texture in a magnetic system consisting of a chiral magnet nanodisk sandwiched by two films with perpendicular magnetic anisotropy. The preimages of the spin texture and numerical calculations of $Q_H$ show that the hopfion has $Q_H=1$. Furthermore, another non-trivial state that includes a monopole--antimonopole pair (MAP) is also stabilized in this system. By applying an external magnetic field, hopfion and MAP states with the same polarization can be switched between each other. The topological transition between the hopfion and the MAP state involves a creation (annihilation) of the MAP and twist of the preimages. Our work paves the way to study non-trivial 3D topological spin textures and stimulates more investigations in the field of 3D spintronics.",
                    "Large perpendicular magnetic anisotropy (PMA) in transition metal thin films provides a pathway for enabling the intriguing physics of nanomagnetism and developing broad spintronics applications. After decades of searches for promising materials, the energy scale of PMA of transition metal thin films, unfortunately, remains only about 1 meV. This limitation has become a major bottleneck in the development of ultradense storage and memory devices. We discovered unprecedented PMA in Fe thin-film growth on the $(000\\bar{1})$ N-terminated surface of III-V nitrides from first-principles calculations. PMA ranges from 24.1 meV/u.c. in Fe/BN to 53.7 meV/u.c. in Fe/InN. Symmetry-protected degeneracy between $x^2-y^2$ and $xy$ orbitals and its lift by the spin-orbit coupling play a dominant role. As a consequence, PMA in Fe/III-V nitride thin films is dominated by first-order perturbation of the spin-orbit coupling, instead of second-order in conventional transition metal/oxide thin films. This game-changing scenario would also open a new field of magnetism on transition metal/nitride interfaces.",
                    "A quantized version of the magnetoelectric effect, known as the topological magnetoelectric effect, can exist in a time-reversal invariant topological insulator with all its surface states gapped out by magnetism. This topological phase, called the axion insulator phase, has been theoretically proposed but is still lack of conclusive experimental evidence due to the small signal of topological magnetoelectric effect. In this work, we propose that the dynamical in-plane magnetization in an axion insulator can generate a \"pseudo-electric field\", which acts on the surface state of topological insulator films and leads to the non-zero response current. Strikingly, we find that the current at magnetic resonance (either ferromagnetic or anti-ferromagnetic) is larger than that of topological magnetoelectric effect by several orders of magnitude, and thereby serves as a feasible smoking gun to confirm the axion insulator phase in the candidate materials.",
                    "Magnetic hopfion is three-dimensional (3D) topological soliton with novel spin structure that would enable exotic dynamics. Here we study the current driven 3D dynamics of a magnetic hopfion with unit Hopf index in a frustrated magnet. Attributed to spin Berry phase and symmetry of the hopfion, the phase space entangles multiple collective coordinates, thus the hopfion exhibits rich dynamics including longitudinal motion along the current direction, transverse motion perpendicular to the current direction, rotational motion and dilation. Furthermore, the characteristics of hopfion dynamics is determined by the ratio between the non-adiabatic spin transfer torque parameter and the damping parameter. Such peculiar 3D dynamics of magnetic hopfion could shed light on understanding the universal physics of hopfions in different systems and boost the prosperous development of 3D spintronics.",
                    "We study scattering of itinerant electrons off a magnetic hopfion in a three-dimensional metallic magnet described by a magnetization vector $\\mathbf S(\\mathbf r)$. A hopfion is a confined topological soliton of $\\mathbf S(\\mathbf r)$ characterized by an {\\it emergent} magnetic field $B_\\gamma(\\mathbf r) \\equiv \\epsilon_{\\alpha\\beta\\gamma} \\,\\mathbf S\\cdot(\\nabla_\\alpha \\mathbf S\\times \\nabla_\\beta \\mathbf S)/4 \\neq 0$ with vanishing average value $\\langle \\mathbf B(\\mathbf r)\\rangle = 0$. We evaluate the scattering amplitude in the opposite limits of large and small hopfion radius $R$ using the eikonal and Born approximations, respectively. In both limits, we find that the scattering cross-section contains a skew-scattering component giving rise to the Hall effect within a hopfion plane. That conclusion contests the popular notion that the topological Hall effect in non-collinear magnetic structures necessarily implies $\\langle \\mathbf B(\\mathbf r)\\rangle \\neq 0$. In the limit of small hopfion radius $pR \\ll 1$, we expand the Born series in powers of momentum $p$ and identify different expansion terms corresponding to the hopfion anisotropy, toroidal moment, and skew-scattering.",
                    "Electrons moving through a noncoplanar magnetic texture acquire a Berry phase, which can be described as an effective magnetic field. This effect is known as the topological Hall effect and has been observed in topological spin textures. Motivated by recent experimental realizations, here we study the Hall effect in a nontopological magnetic texture known as a target skyrmion. We start from a simplified semiclassical picture and show that the Hall signal is a nonmonotonic function of both the electronic energy and target skyrmion radius. That observation carries over to the fully quantum mechanical treatment in a Landauer-B\\\"uttiker formalism in a mesoscopic setting. Our conclusion challenges the popular opinion in the community that the Hall effect in such structures necessarily requires a nonzero skyrmion number.",
                    "Non-trivial topology in a two-dimensional frustrated spin system with the Dzyaloshinskii-Moriya (DM) interaction was investigated by Monto Carlo simulations. At finite temperatures, thermally driven topology was discovered and was found to be dominant at low magnetic field. This topological charge has a quadratic relation with the DM interaction and linear realtions with the external magnetic field or the uniaxial magnetic anisotropy. We also proposed a real frustrated system, the Mn-Bi mono-layer film with exceedingly large DM interaction, to enable thermally driven topology. Other topological non-trivial phases in high magnetic field region were also discussed in this real system.",
                    "We construct a model Hamiltonian for S = 2 spin chain, where a variable parameter $\\alpha$ is introduced. The edge spin is S = 1 for $\\alpha = 0$, and S = 3/2 for $\\alpha = 1$. Due to the topological distinction of the edge states, these two phases must be separated by one or several topological quantum phase transitions. We investigate the nature of the quantum phase transition by DMRG calculation, and propose a phase diagram for this model."
                ],
                "domain": [
                    "Magnetism",
                    "Topological Insulators",
                    "Skyrmions",
                    "Spintronics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can machine learning be effectively utilized to discover novel magnetic materials with enhanced properties, such as greater operating temperature ranges and reduced reliance on rare earth elements?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses the pressing need for efficient magnetic materials that can significantly impact various industries, including renewable energy, data storage, and advanced electronics. By advancing the discovery of new materials, this research could lead to breakthroughs in energy efficiency, reduced environmental impact, and enhanced technological capabilities. Furthermore, it could pave the way for future research in materials science, enabling the development of innovative applications and fostering interdisciplinary collaboration.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the vast combinatorial space of possible magnetic material compositions and the limitations of conventional methods, which are time-consuming and resource-intensive. Naive approaches may fail due to the inadequacy of standard density functional theory (DFT) in accurately predicting the magnetic properties of strongly correlated electron systems. Additionally, the need for prior knowledge of crystal structures and the high computational costs associated with large unit cells further complicate the discovery process.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has been limited by the reliance on traditional experimental and computational methods that do not effectively capture the complexities of magnetic materials. The barriers include the inadequacy of DFT in handling itinerant magnets and the lack of integration between electronic structure information and effective Hamiltonian models. Additionally, the existing approaches have primarily focused on a short list of materials, leaving a significant gap in the exploration of new compositions. My approach aims to leverage machine learning techniques to overcome these limitations by enabling faster and more accurate predictions of magnetic properties without the constraints of conventional methods.\n\n**[Question 5] - What are the key components of my approach and results?**  \nMy proposed methodology involves developing a machine learning model that utilizes a comprehensive dataset of known magnetic materials, including their compositions and properties. I will employ metrics such as accuracy and predictive power to evaluate the model's performance. The expected outcomes include the identification of novel magnetic materials with desirable properties, a reduction in the time and resources required for material discovery, and the establishment of a framework for future research in the field of magnetic materials. This approach aims to provide a more efficient pathway for exploring the vast compositional space and uncovering hidden patterns that traditional methods may overlook."
    },
    "2401.14818": {
        "paper_data": {
            "title": "ChemDFM: A Large Language Foundation Model for Chemistry",
            "url": "http://arxiv.org/abs/2401.14818v3",
            "arxiv_id": "2401.14818",
            "authors": [
                "Zihan Zhao",
                "Da Ma",
                "Lu Chen",
                "Liangtai Sun",
                "Zihao Li",
                "Yi Xia",
                "Bo Chen",
                "Hongshen Xu",
                "Zichen Zhu",
                "Su Zhu",
                "Shuai Fan",
                "Guodong Shen",
                "Kai Yu",
                "Xin Chen"
            ],
            "abstract": "Artificial intelligence (AI) has played an increasingly important role in chemical research. However, most models currently used in chemistry are specialist models that require training and tuning for specific tasks. A more generic and efficient solution would be an AI model that could address many tasks and support free-form dialogue in the broad field of chemistry. In its utmost form, such a generalist AI chemist could be referred to as Chemical General Intelligence. Large language models (LLMs) have recently logged tremendous success in the general domain of natural language processing, showing emerging task generalization and free-form dialogue capabilities. However, domain knowledge of chemistry is largely missing when training general-domain LLMs. The lack of such knowledge greatly hinders the performance of generalist LLMs in the field of chemistry. To this end, we develop ChemDFM, a pioneering LLM for chemistry trained on 34B tokens from chemical literature and textbooks, and fine-tuned using 2.7M instructions. As a result, it can understand and reason with chemical knowledge in free-form dialogue. Quantitative evaluations show that ChemDFM significantly surpasses most representative open-source LLMs. It outperforms GPT-4 on a great portion of chemical tasks, despite the substantial size difference. We have open-sourced the inference codes, evaluation datasets, and model weights of ChemDFM on Huggingface (https://huggingface.co/AI4Chem/ChemLLM-7B-Chat).",
            "introduction": "   1 Introduction  With the rapid development of artificial intelligence (AI), utilizing AI to assist chemical research has garnered increasing attention (Wang et al., 2023b; Back et al., 2024). Various AI models have been developed for tasks such as property prediction (Zhou et al., 2022; Wu et al., 2023b; Chen et al., 2023), molecular captioning and generation (Xu et al., 2021; Edwards et al., 2022; Perron et al., 2022; Du et al., 2024; Lu et al., 2024), and reaction predictions (Schwaller et al., 2020; Wang et al., 2021; Han et al., 2024). Since BERT (Devlin et al., 2019) and GPT (Radford et al., ), efforts have been made to fine-tune pre-trained models for specific chemical tasks (Zhou et al., 2022; Edwards et al., 2022; Liu et al., 2023; Luo et al., 2023; Zhang et al., 2024). However, these models are typically trained on a meticulously curated dataset to solve a designated task in a particular scenario, leading to a one-to-one relationship between models and tasks. Once out of that specific scenario, they are often not useful, even for highly related tasks. A more attractive and practical AI system should be capable of handling a wide range of chemical tasks under real-world scenarios and conducting free-form human-AI collaborations. Such an AI system necessitates a comprehensive array of chemical competencies, coupled with the ability to comprehend and reason in both chemical and natural languages. This would enable it to work as a research assistant or even collaborator alongside human researchers. This could be an essential step towards eventually achieving Chemical Artificial General Intelligence.   In pursuit of a highly integrated AI system for a broad range of chemical challenges, recent advancements in large language models (LLMs) (Du et al., 2022; Touvron et al., 2023a; Xu et al., 2023) brought great new hopes. Numerous studies have demonstrated the remarkable competencies of LLMs in natural language understanding and task generalization (Wei et al., 2021; Xu et al., 2023), deductive reasoning (Wei et al., 2022; Kojima et al., 2022), and tool utilization (Schick et al., 2023; Qin et al., 2024). These made LLMs shine in traditional natural language processing tasks and accomplish problems that were previously unimaginable and unsolvable, such as handling tasks in unknown scenarios or conducting free-form dialogues with humans. These inherent strengths underscore the viability of employing LLMs as AI-driven research collaborators in the field of chemistry.   Different from general domains, tasks in chemical domains necessitate models to possess additional chemical comprehension capabilities for understanding and reasoning over chemical-specialized language and knowledge. This hinders general domain LLMs from excelling in chemical tasks as they often lack in-depth chemical knowledge (Kristiadi et al., 2024). For example, molecules are a vital component of the chemical world. Although molecules can be conveyed through natural-language-like notations such as SMILES (Simplified Molecular Input Line Entry System), IUPAC names, and molecular formulas, their meanings and intrinsic structures are entirely different from those in natural language. CO represents carbon monoxide in chemistry, not Colorado, while Co represents Cobalt, not a company, and (CO) as part of a SMILES typically represents the carbonyl group. The lack of understanding of these molecular notations severely limits the applicability and performance of general domain LLMs in solving chemistry problems. Therefore, we believe that equipping general-domain LLMs with rich chemical knowledge of task-specific chemical models, as illustrated in Figure 1, is vital for developing LLMs useful in the field of chemistry.   Figure",
            "references": [
                {
                    "title": "UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation",
                    "abstract": "The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive training paradigm, enabling it to interpret molecules as a foreign language and generate them as text. Following a four-stage training scheme, UniMoT emerges as a multi-modal generalist capable of performing both molecule-to-text and text-to-molecule tasks. Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide range of molecule comprehension and generation tasks."
                },
                {
                    "title": "Retrosynthesis prediction with an iterative string editing model",
                    "abstract": null
                },
                {
                    "title": "Large Language Models for Inorganic Synthesis Predictions.",
                    "abstract": "We evaluate the effectiveness of pretrained and fine-tuned large language models (LLMs) for predicting the synthesizability of inorganic compounds and the selection of precursors needed to perform inorganic synthesis. The predictions of fine-tuned LLMs are comparable to─and sometimes better than─recent bespoke machine learning models for these tasks but require only minimal user expertise, cost, and time to develop. Therefore, this strategy can serve both as an effective and strong baseline for future machine learning studies of various chemical applications and as a practical tool for experimental chemists."
                },
                {
                    "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
                    "abstract": "Molecular modeling, a central topic in quantum mechanics, aims to accurately calculate the properties and simulate the behaviors of molecular systems. The molecular model is governed by physical laws, which impose geometric constraints such as invariance and equivariance to coordinate rotation and translation. While numerous deep learning approaches have been developed to learn molecular representations under these constraints, most of them are built upon heuristic and costly modules. We argue that there is a strong need for a general and flexible framework for learning both invariant and equivariant features. In this work, we introduce a novel Transformer-based molecular model called GeoMFormer to achieve this goal. Using the standard Transformer modules, two separate streams are developed to maintain and learn invariant and equivariant representations. Carefully designed cross-attention modules bridge the two streams, allowing information fusion and enhancing geometric modeling in each stream. As a general and flexible architecture, we show that many previous architectures can be viewed as special instantiations of GeoMFormer. Extensive experiments are conducted to demonstrate the power of GeoMFormer. All empirical results show that GeoMFormer achieves strong performance on both invariant and equivariant tasks of different types and scales. Code and models will be made publicly available at https://github.com/c-tl/GeoMFormer."
                },
                {
                    "title": "Machine learning-aided generative molecular design",
                    "abstract": null
                },
                {
                    "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
                    "abstract": "Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish this, we propose SMolInstruct, a large-scale, comprehensive, and high-quality dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral serves as the best base model for chemistry tasks. Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements."
                },
                {
                    "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
                    "abstract": "Automation is one of the cornerstones of contemporary material discovery. Bayesian optimization (BO) is an essential part of such workflows, enabling scientists to leverage prior domain knowledge into efficient exploration of a large molecular space. While such prior knowledge can take many forms, there has been significant fanfare around the ancillary scientific knowledge encapsulated in large language models (LLMs). However, existing work thus far has only explored LLMs for heuristic materials searches. Indeed, recent work obtains the uncertainty estimate -- an integral part of BO -- from point-estimated, non-Bayesian LLMs. In this work, we study the question of whether LLMs are actually useful to accelerate principled Bayesian optimization in the molecular space. We take a sober, dispassionate stance in answering this question. This is done by carefully (i) viewing LLMs as fixed feature extractors for standard but principled BO surrogate models and by (ii) leveraging parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate. Our extensive experiments with real-world chemistry problems show that LLMs can be useful for BO over molecules, but only if they have been pretrained or finetuned with domain-specific data."
                },
                {
                    "title": "Shaping the Water-Harvesting Behavior of Metal-Organic Frameworks Aided by Fine-Tuned GPT Models.",
                    "abstract": "We construct a data set of metal-organic framework (MOF) linkers and employ a fine-tuned GPT assistant to propose MOF linker designs by mutating and modifying the existing linker structures. This strategy allows the GPT model to learn the intricate language of chemistry in molecular representations, thereby achieving an enhanced accuracy in generating linker structures compared with its base models. Aiming to highlight the significance of linker design strategies in advancing the discovery of water-harvesting MOFs, we conducted a systematic MOF variant expansion upon state-of-the-art MOF-303 utilizing a multidimensional approach that integrates linker extension with multivariate tuning strategies. We synthesized a series of isoreticular aluminum MOFs, termed Long-Arm MOFs (LAMOF-1 to LAMOF-10), featuring linkers that bear various combinations of heteroatoms in their five-membered ring moiety, replacing pyrazole with either thiophene, furan, or thiazole rings or a combination of two. Beyond their consistent and robust architecture, as demonstrated by permanent porosity and thermal stability, the LAMOF series offers a generalizable synthesis strategy. Importantly, these 10 LAMOFs establish new benchmarks for water uptake (up to 0.64 g g-1) and operational humidity ranges (between 13 and 53%), thereby expanding the diversity of water-harvesting MOFs."
                },
                {
                    "title": "Accelerated chemical science with AI",
                    "abstract": "In light of the pressing need for practical materials and molecular solutions to renewable energy and health problems, to name just two examples, one wonders how to accelerate research and development in the chemical sciences, so as to address the time it takes to bring materials from initial discovery to commercialization. Artificial intelligence (AI)-based techniques, in particular, are having a transformative and accelerating impact on many if not most, technological domains. To shed light on these questions, the authors and participants gathered in person for the ASLLA Symposium on the theme of ‘Accelerated Chemical Science with AI’ at Gangneung, Republic of Korea. We present the findings, ideas, comments, and often contentious opinions expressed during four panel discussions related to the respective general topics: ‘Data’, ‘New applications’, ‘Machine learning algorithms’, and ‘Education’. All discussions were recorded, transcribed into text using Open AI's Whisper, and summarized using LG AI Research's EXAONE LLM, followed by revision by all authors. For the broader benefit of current researchers, educators in higher education, and academic bodies such as associations, publishers, librarians, and companies, we provide chemistry-specific recommendations and summarize the resulting conclusions."
                },
                {
                    "title": "Autonomous chemical research with large language models",
                    "abstract": null
                },
                {
                    "title": "InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery",
                    "abstract": "The rapid evolution of artificial intelligence in drug discovery encounters challenges with generalization and extensive training, yet Large Language Models (LLMs) offer promise in reshaping interactions with complex molecular data. Our novel contribution, InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information. InstructMol showcases substantial performance improvements in drug discovery-related molecular tasks, surpassing leading LLMs and significantly reducing the gap with specialized models, thereby establishing a robust foundation for a versatile and dependable drug discovery assistant."
                },
                {
                    "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
                    "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), leading to remarkable advancements in text understanding and generation. Nevertheless, alongside these strides, LLMs exhibit a critical tendency to produce hallucinations, resulting in content that is inconsistent with real-world facts or user inputs. This phenomenon poses substantial challenges to their practical deployment and raises concerns over the reliability of LLMs in real-world scenarios, which attracts increasing attention to detect and mitigate these hallucinations. In this survey, we aim to provide a thorough and in-depth overview of recent advances in the field of LLM hallucinations. We begin with an innovative taxonomy of LLM hallucinations, then delve into the factors contributing to hallucinations. Subsequently, we present a comprehensive overview of hallucination detection methods and benchmarks. Additionally, representative approaches designed to mitigate hallucinations are introduced accordingly. Finally, we analyze the challenges that highlight the current limitations and formulate open questions, aiming to delineate pathways for future research on hallucinations in LLMs."
                },
                {
                    "title": "Large language models for chemistry robotics",
                    "abstract": null
                },
                {
                    "title": "Prompt engineering of GPT-4 for chemical research: what can/cannot be done?",
                    "abstract": "ABSTRACT This paper evaluates the capabilities and limitations of the Generative Pre-trained Transformer 4 (GPT-4) in chemical research. Although GPT-4 exhibits remarkable proficiencies, it is evident that the quality of input data significantly affects its performance. We explore GPT-4’s potential in chemical tasks, such as foundational chemistry knowledge, cheminformatics, data analysis, problem prediction, and proposal abilities. While the language model partially outperformed traditional methods, such as black-box optimization, it fell short against specialized algorithms, highlighting the need for their combined use. The paper shares the prompts given to GPT-4 and its responses, providing a resource for prompt engineering within the community, and concludes with a discussion on the future of chemical research using large language models. GRAPHICAL ABSTRACT IMPACT STATEMENT This paper comprehensively reveals the advantages and limitations of GPT-4 in chemical research, such as expert knowledge, data analysis, prediction, suggestion, and autonomous experimentation."
                },
                {
                    "title": "MeSesamol, a bio-based and versatile polar aprotic solvent for organic synthesis and depolymerization",
                    "abstract": null
                },
                {
                    "title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
                    "abstract": "Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In natural science, traditional manual, serial, and labour-intensive work is being augmented by automated, parallel, and iterative processes driven by artificial intelligence-based experimental automation and more. To add new capabilities in natural science, enabling the acceleration and enrichment of automation of the discovery process, we present DARWIN, a series of tailored LLMs for natural science, mainly in physics, chemistry, and material science. This series relies on open-source LLM, incorporating structured and unstructured scientific knowledge from public datasets and literature. We fine-tuned the models using over 60,000 instruction data points, emphasizing factual correctness. During the fine-tuning, we introduce the Scientific Instruction Generation (SIG) model, automating instruction generation from scientific texts. This eliminates the need for manual extraction or domain-specific knowledge graphs and efficiently injects scientific knowledge into the model. We also explore multi-task training strategies, revealing interconnections between scientific tasks. DARWIN series not only achieves state-of-the-art results on various scientific tasks but also diminishes reliance on closed-source AI models. Our research showcases the ability of LLM in the scientific domain, with the overarching goal of fostering prosperity within the broader AI for science community."
                },
                {
                    "title": "Total Syntheses of Polycyclic Diterpenes Phomopsene, Methyl Phomopsenonate, and iso-Phomopsene via Reorganization of C-C Single Bonds.",
                    "abstract": "The first total syntheses of polycyclic diterpenes phomopsene (1), methyl phomopsenonate (2), and iso-phomopsene (3) have been accomplished through the unusual cascade reorganization of C-C single bonds. This approach features: (i) a synergistic Nazarov cyclization/double ring expansions in one-step, developed by authors, to rapid and stereospecific construction of the 5/5/5/5 tetraquinane scaffold bearing contiguous quaternary centers and (ii) a one-pot strategic ring expansion through Beckmann fragmentation/recombination to efficiently assemble the requisite 5/5/6/5 tetracyclic skeleton of the target molecules 1-3. This work enables us to determine that the correct structure of iso-phomopsene is, in fact, the C7 epimer of the originally assigned structure. Finally, the absolute configurations of three target molecules were confirmed through enantioselective synthesis."
                },
                {
                    "title": "Axially chiral styrene-based organocatalysts and their application in asymmetric cascade Michael/cyclization reaction",
                    "abstract": "An axially chiral styrene-based organocatalyst, featuring a combination of axially chiral styrene-based structure and a pyrrole ring, has been designed and synthesized. This catalyst demonstrates remarkable capabilities in producing a wide range of densely substituted spirooxindoles that feature an alkyne-substituted quaternary stereogenic center. These spirooxindoles are generated through mild cascade Michael/cyclization reactions, resulting in high conversion rates and exceptional enantioselectivity. Our catalytic model, based on experiments, X-ray structure analysis and DFT calculations suggests that chiral matched π–π interactions and multiple H-bonds between the organocatalyst and substrates play significant roles in controlling the stereoselectivity of the reaction."
                },
                {
                    "title": "EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education",
                    "abstract": "EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative aims to promote research and applications of LLMs for intelligent education."
                },
                {
                    "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales",
                    "abstract": "ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance. However, the current landscape lacks an accessible, efficient, and cost-effective end-to-end RLHF (Reinforcement Learning with Human Feedback) training pipeline for these powerful models, particularly when training at the scale of billions of parameters. This paper introduces DeepSpeed-Chat, a novel system that democratizes RLHF training, making it accessible to the AI community. DeepSpeed-Chat offers three key capabilities: an easy-to-use training and inference experience for ChatGPT-like models, a DeepSpeed-RLHF pipeline that replicates the training pipeline from InstructGPT, and a robust DeepSpeed-RLHF system that combines various optimizations for training and inference in a unified way. The system delivers unparalleled efficiency and scalability, enabling training of models with hundreds of billions of parameters in record time and at a fraction of the cost. With this development, DeepSpeed-Chat paves the way for broader access to advanced RLHF training, even for data scientists with limited resources, thereby fostering innovation and further development in the field of AI."
                },
                {
                    "title": "Scientific discovery in the age of artificial intelligence",
                    "abstract": null
                },
                {
                    "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
                    "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench."
                },
                {
                    "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
                    "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
                },
                {
                    "title": "Asymmetric Intramolecular Hydroalkylation of Internal Olefin with Cycloalkanone to Directly Access Polycyclic Systems.",
                    "abstract": "An asymmetric intramolecular hydroalkylation of unactivated internal olefins with tethered cyclic ketones was realized by the cooperative catalysis of a newly designed chiral amine (SPD-NH2) and Pd(II) complex, providing straightforward access to either bridged or fused bicyclic systems containing three stereogenic centers with excellent enantioselectivity (up to 99% ee) and diastereoselectivity (up to >20:1 dr). Notably, the bicyclic products could be conveniently transformed into a diverse range of key structures frequently found in bioactive terpenes, such as ∆6-protoilludene, cracroson D, and vulgarisins. The steric hindrance between the Ar group of the SPD-NH2 catalyst and the branched chain of the substrate, hydrogen-bonding interactions between the N-H of the enamine motif and the C=O of the directing group MQ, and the counterion of the Pd(II) complex were identified as key factors for excellent stereoinduction in this dual catalytic process by density functional theory calculations."
                },
                {
                    "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
                    "abstract": "Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability."
                },
                {
                    "title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
                    "abstract": "Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2"
                },
                {
                    "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
                    "abstract": "Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench."
                },
                {
                    "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
                    "abstract": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning."
                },
                {
                    "title": "DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs",
                    "abstract": "A ChatGPT-like system for drug compounds could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structure-activity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes the compound representation transformed by the adaptor and users' questions about this compound as inputs and generates answers. All these components are trained end-to-end. To train DrugChat, we collected instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs. The code and data is available at \\url{https://github.com/UCSD-AI4H/drugchat}"
                },
                {
                    "title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine",
                    "abstract": "Recently, Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding. While demonstrating proficiency in everyday conversations and question-answering situations, these models frequently struggle in domains that require precision, such as medical applications, due to their lack of domain-specific knowledge. In this paper, we describe the procedure for building a powerful, open-source language model specifically designed for medicine applications, termed as PMC-LLaMA. Our contributions are threefold: (i) we systematically investigate the process of adapting a general-purpose foundation language model towards medical domain, this involves data-centric knowledge injection through the integration of 4.8M biomedical academic papers and 30K medical textbooks, as well as comprehensive fine-tuning for alignment with domain-specific instructions; (ii) we contribute a large-scale, comprehensive dataset for instruction tuning. This dataset encompasses medical question-answering (QA), rationale for reasoning, and conversational dialogues, comprising a total of 202M tokens; (iii) we conduct thorough ablation studies to demonstrate the effectiveness of each proposed component. While evaluating on various public medical question-answering benchmarks, our lightweight PMCLLaMA, which consists of only 13 billion parameters, exhibits superior performance, even surpassing ChatGPT. All models, codes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA."
                },
                {
                    "title": "Augmenting large language models with chemistry tools",
                    "abstract": null
                },
                {
                    "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
                    "abstract": "Objective The primary aim of this research was to address the limitations observed in the medical knowledge of prevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with enhanced accuracy in medical advice. Methods We achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These conversations were cleaned and anonymized to respect privacy concerns. In addition to the model refinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access and utilize real-time information from online sources like Wikipedia and data from curated offline medical databases. Results The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice. By equipping the model with self-directed information retrieval from reliable online and offline sources, we observed substantial improvements in the accuracy of its responses. Conclusion Our proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a significant improvement in understanding patient inquiries and providing accurate advice. Given the high stakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable information are not only beneficial but essential."
                },
                {
                    "title": "GPT-4 Technical Report",
                    "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
                },
                {
                    "title": "LLaMA: Open and Efficient Foundation Language Models",
                    "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
                },
                {
                    "title": "3D Molecular Generation via Virtual Dynamics",
                    "abstract": "Structure-based drug design, i.e., finding molecules with high affinities to the target protein pocket, is one of the most critical tasks in drug discovery. Traditional solutions, like virtual screening, require exhaustively searching on a large molecular database, which are inefficient and cannot return novel molecules beyond the database. The pocket-based 3D molecular generation model, i.e., directly generating a molecule with a 3D structure and binding position in the pocket, is a new promising way to address this issue. Herein, we propose VD-Gen, a novel pocket-based 3D molecular generation pipeline. VD-Gen consists of several carefully designed stages to generate fine-grained 3D molecules with binding positions in the pocket cavity end-to-end. Rather than directly generating or sampling atoms with 3D positions in the pocket like in early attempts, in VD-Gen, we first randomly initialize many virtual particles in the pocket; then iteratively move these virtual particles, making the distribution of virtual particles approximate the distribution of molecular atoms. After virtual particles are stabilized in 3D space, we extract a 3D molecule from them. Finally, we further refine atoms in the extracted molecule by iterative movement again, to get a high-quality 3D molecule, and predict a confidence score for it. Extensive experiment results on pocket-based molecular generation demonstrate that VD-Gen can generate novel 3D molecules to fill the target pocket cavity with high binding affinities, significantly outperforming previous baselines."
                },
                {
                    "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
                    "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."
                },
                {
                    "title": "Unifying Molecular and Textual Representations via Multi-task Language Modelling",
                    "abstract": "The recent advances in neural language models have also been successfully applied to the field of chemistry, offering generative solutions for classical problems in molecular design and synthesis planning. These new methods have the potential to fuel a new era of data-driven automation in scientific discovery. However, specialized models are still typically required for each task, leading to the need for problem-specific fine-tuning and neglecting task interrelations. The main obstacle in this field is the lack of a unified representation between natural language and chemical representations, complicating and limiting human-machine interaction. Here, we propose the first multi-domain, multi-task language model that can solve a wide range of tasks in both the chemical and natural language domains. Our model can handle chemical and natural language concurrently, without requiring expensive pre-training on single domains or task-specific models. Interestingly, sharing weights across domains remarkably improves our model when benchmarked against state-of-the-art baselines on single-domain and cross-domain tasks. In particular, sharing information across domains and tasks gives rise to large improvements in cross-domain tasks, the magnitude of which increase with scale, as measured by more than a dozen of relevant metrics. Our work suggests that such models can robustly and efficiently accelerate discovery in physical sciences by superseding problem-specific fine-tuning and enhancing human-model interactions."
                },
                {
                    "title": "Large language models encode clinical knowledge",
                    "abstract": null
                },
                {
                    "title": "Galactica: A Large Language Model for Science",
                    "abstract": "Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community."
                },
                {
                    "title": "MOFormer: Self-Supervised Transformer Model for Metal–Organic Framework Property Prediction",
                    "abstract": "Metal–organic frameworks (MOFs) are materials with a high degree of porosity that can be used for many applications. However, the chemical space of MOFs is enormous due to the large variety of possible combinations of building blocks and topology. Discovering the optimal MOFs for specific applications requires an efficient and accurate search over countless potential candidates. Previous high-throughput screening methods using computational simulations like DFT can be time-consuming. Such methods also require the 3D atomic structures of MOFs, which adds one extra step when evaluating hypothetical MOFs. In this work, we propose a structure-agnostic deep learning method based on the Transformer model, named as MOFormer, for property predictions of MOFs. MOFormer takes a text string representation of MOF (MOFid) as input, thus circumventing the need of obtaining the 3D structure of a hypothetical MOF and accelerating the screening process. By comparing to other descriptors such as Stoichiometric-120 and revised autocorrelations, we demonstrate that MOFormer can achieve state-of-the-art structure-agnostic prediction accuracy on all benchmarks. Furthermore, we introduce a self-supervised learning framework that pretrains the MOFormer via maximizing the cross-correlation between its structure-agnostic representations and structure-based representations of the crystal graph convolutional neural network (CGCNN) on >400k publicly available MOF data. Benchmarks show that pretraining improves the prediction accuracy of both models on various downstream prediction tasks. Furthermore, we revealed that MOFormer can be more data-efficient on quantum-chemical property prediction than structure-based CGCNN when training data is limited. Overall, MOFormer provides a novel perspective on efficient MOF property prediction using deep learning."
                },
                {
                    "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
                    "abstract": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M."
                },
                {
                    "title": "Large Language Models are Zero-Shot Reasoners",
                    "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars."
                },
                {
                    "title": "Translation between Molecules and Natural Language",
                    "abstract": "We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality."
                },
                {
                    "title": "GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation",
                    "abstract": "Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules."
                },
                {
                    "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                    "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
                },
                {
                    "title": "Uncertainty-aware prediction of chemical reaction yields with graph neural networks",
                    "abstract": null
                },
                {
                    "title": "Molformer: Motif-Based Transformer on 3D Heterogeneous Molecular Graphs",
                    "abstract": "Procuring expressive molecular representations underpins AI-driven molecule design and scientific discovery. The research mainly focuses on atom-level homogeneous molecular graphs, ignoring the rich information in subgraphs or motifs. However, it has been widely accepted that substructures play a dominant role in identifying and determining molecular properties. To address such issues, we formulate heterogeneous molecular graphs (HMGs) and introduce a novel architecture to exploit both molecular motifs and 3D geometry. Precisely, we extract functional groups as motifs for small molecules and employ reinforcement learning to adaptively select quaternary amino acids as motif candidates for proteins. Then HMGs are constructed with both atom-level and motif-level nodes. To better accommodate those HMGs, we introduce a variant of the Transformer named Molformer, which adopts a heterogeneous self-attention layer to distinguish the interactions between multi-level nodes. Besides, it is also coupled with a multi-scale mechanism to capture fine-grained local patterns with increasing contextual scales. An attentive farthest point sampling algorithm is also proposed to obtain the molecular representations. We validate Molformer across a broad range of domains, including quantum chemistry, physiology, and biophysics. Extensive experiments show that Molformer outperforms or achieves the comparable performance of several state-of-the-art baselines. Our work provides a promising way to utilize informative motifs from the perspective of multi-level graph construction. The code is available at https://github.com/smiles724/Molformer."
                },
                {
                    "title": "Finetuned Language Models Are Zero-Shot Learners",
                    "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."
                },
                {
                    "title": "Chemformer: a pre-trained transformer for computational chemistry",
                    "abstract": "Transformer models coupled with a simplified molecular line entry system (SMILES) have recently proven to be a powerful combination for solving challenges in cheminformatics. These models, however, are often developed specifically for a single application and can be very resource-intensive to train. In this work we present the Chemformer model—a Transformer-based model which can be quickly applied to both sequence-to-sequence and discriminative cheminformatics tasks. Additionally, we show that self-supervised pre-training can improve performance and significantly speed up convergence on downstream tasks. On direct synthesis and retrosynthesis prediction benchmark datasets we publish state-of-the-art results for top-1 accuracy. We also improve on existing approaches for a molecular optimisation task and show that Chemformer can optimise on multiple discriminative tasks simultaneously. Models, datasets and code will be made available after publication."
                },
                {
                    "title": "RetroPrime: A Diverse, Plausible and Transformer-based Method for Single-Step Retrosynthesis Predictions",
                    "abstract": null
                },
                {
                    "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
                    "abstract": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25× parameters of BERT Large , demonstrating its generalizability to different downstream tasks."
                },
                {
                    "title": "Deep generative models for ligand‐based de novo design applied to multi‐parametric optimization",
                    "abstract": "Multi‐parameter optimization (MPO) is a major challenge in new chemical entity (NCE) drug discovery. Recently, promising results were reported for deep learning generative models applied to de novo molecular design, but, to our knowledge, until now no report was made of the value of this new technology for addressing MPO in an actual drug discovery project. In this study, we demonstrate the benefit of applying AI technology in a real drug discovery project. We evaluate the potential of a ligand‐based de novo design technology using deep learning generative models to accelerate the obtention of lead compounds meeting 11 different biological activity objectives simultaneously. Using the initial dataset of the project, we built QSAR models for all the 11 objectives, with moderate to high performance (precision between 0.67 and 1.0 on an independent test set). Our DL‐based AI de novo design algorithm, combined with the QSAR models, generated 150 virtual compounds predicted as active on all objectives. Eleven were synthetized and tested. The AI‐designed compounds met 9.5 objectives on average (i.e., 86% success rate) versus 6.4 (i.e., 58% success rate) for the initial molecules measured on all objectives. One of the AI‐designed molecules was active on all 11 measured objectives, and two were active on 10 objectives while being in the error margin of the assay for the last one. The AI algorithm designed compounds with functional groups, which, although being rare or absent in the initial dataset, turned out to be highly beneficial for the MPO."
                },
                {
                    "title": "Measuring Massive Multitask Language Understanding",
                    "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
                },
                {
                    "title": "Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy†",
                    "abstract": "We present an extension of our Molecular Transformer model combined with a hyper-graph exploration strategy for automatic retrosynthesis route planning without human intervention. The single-step retrosynthetic model sets a new state of the art for predicting reactants as well as reagents, solvents and catalysts for each retrosynthetic step. We introduce four metrics (coverage, class diversity, round-trip accuracy and Jensen–Shannon divergence) to evaluate the single-step retrosynthetic models, using the forward prediction and a reaction classification model always based on the transformer architecture. The hypergraph is constructed on the fly, and the nodes are filtered and further expanded based on a Bayesian-like probability. We critically assessed the end-to-end framework with several retrosynthesis examples from literature and academic exams. Overall, the frameworks have an excellent performance with few weaknesses related to the training data. The use of the introduced metrics opens up the possibility to optimize entire retrosynthetic frameworks by focusing on the performance of the single-step model only."
                },
                {
                    "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
                    "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains – such as news articles and encyclopedia entries, where text is plentiful – in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (∼75%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research."
                },
                {
                    "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models",
                    "abstract": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today’s hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8. 3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world’s largest language model at the time (17B parameters) with record breaking accuracy."
                },
                {
                    "title": "Remarkable Reactivity of Boron-Substituted Furans in the Diels-Alder Reactions with Maleic Anhydride.",
                    "abstract": "The reactivity of boron-substituted furans as dienes in the Diels-Alder reaction with maleic anhydride has been investigated. Gratifyingly, the furans with boryl substituents at C-3 gave the exo cycloadduct exclusively with excellent yields. In particular, the potassium trifluoroborate exhibited outstanding reactivity at room temperature. Theoretical calculations suggested that the trifluoroborate group is highly activating and also that the thermodynamics is the main factor that determines whether the products can be obtained efficiently or not."
                },
                {
                    "title": "Predicting reaction performance in C–N cross-coupling using machine learning",
                    "abstract": "A guide for catalyst choice in the forest Chemists often discover reactions by applying catalysts to a series of simple compounds. Tweaking those reactions to tolerate more structural complexity in pharmaceutical research is time-consuming. Ahneman et al. report that machine learning can help. Using a high-throughput data set, they trained a random forest algorithm to predict which specific palladium catalysts would best tolerate isoxazoles (cyclic structures with an N–O bond) during C–N bond formation. The predictions also helped to guide analysis of the catalyst inhibition mechanism. Science, this issue p. 186 A random forest algorithm trained on high-throughput data predicts which catalysts best tolerate certain heterocycles. Machine learning methods are becoming integral to scientific inquiry in numerous disciplines. We demonstrated that machine learning can be used to predict the performance of a synthetic reaction in multidimensional chemical space using data obtained via high-throughput experimentation. We created scripts to compute and extract atomic, molecular, and vibrational descriptors for the components of a palladium-catalyzed Buchwald-Hartwig cross-coupling of aryl halides with 4-methylaniline in the presence of various potentially inhibitory additives. Using these descriptors as inputs and reaction yield as output, we showed that a random forest algorithm provides significantly improved predictive performance over linear regression analysis. The random forest model was also successfully applied to sparse training sets and out-of-sample prediction, suggesting its value in facilitating adoption of synthetic methodology."
                },
                {
                    "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
                    "abstract": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community."
                },
                {
                    "title": "A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow",
                    "abstract": "A reaction screen in flowing solvent Chemists charged with manufacturing pharmaceuticals have recently been exploring the efficiency advantages of continuous flow techniques. Perera et al. now show that a flow apparatus can also accelerate reaction optimization earlier in the drug discovery process. They modified a high-performance liquid chromatography system to screen a wide variety of solvent, ligand, and base combinations to optimize carbon-carbon bond formation. Injecting stock solution aliquots of the catalyst and reactants into a carrier solvent stream let the authors vary the main solvent efficiently and scale up the optimal conditions for product isolation. Science, this issue p. 429 Chromatographic, flow-based screening of reaction conditions is demonstrated for Suzuki coupling in pharmaceutical research. The scarcity of complex intermediates in pharmaceutical research motivates the pursuit of reaction optimization protocols on submilligram scales. We report here the development of an automated flow-based synthesis platform, designed from commercially available components, that integrates both rapid nanomole-scale reaction screening and micromole-scale synthesis into a single modular unit. This system was validated by exploring a diverse range of reaction variables in a Suzuki-Miyaura coupling on nanomole scale at elevated temperatures, generating liquid chromatography–mass spectrometry data points for 5760 reactions at a rate of >1500 reactions per 24 hours. Through multiple injections of the same segment, the system directly produced micromole quantities of desired material. The optimal conditions were also replicated in traditional flow and batch mode at 50- to 200-milligram scale to provide good to excellent yields."
                },
                {
                    "title": "Decoupled Weight Decay Regularization",
                    "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at this https URL"
                },
                {
                    "title": "Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network",
                    "abstract": "The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts."
                },
                {
                    "title": "Crowdsourcing Multiple Choice Science Questions",
                    "abstract": "We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions. We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams."
                },
                {
                    "title": "MoleculeNet: a benchmark for molecular machine learning",
                    "abstract": "A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms."
                },
                {
                    "title": "What's What: The (Nearly) Definitive Guide to Reaction Role Assignment",
                    "abstract": "When analyzing chemical reactions it is essential to know which molecules are actively involved in the reaction and which educts will form the product molecules. Assigning reaction roles, like reactant, reagent, or product, to the molecules of a chemical reaction might be a trivial problem for hand-curated reaction schemes but it is more difficult to automate, an essential step when handling large amounts of reaction data. Here, we describe a new fingerprint-based and data-driven approach to assign reaction roles which is also applicable to rather unbalanced and noisy reaction schemes. Given a set of molecules involved and knowing the product(s) of a reaction we assign the most probable reactants and sort out the remaining reagents. Our approach was validated using two different data sets: one hand-curated data set comprising about 680 diverse reactions extracted from patents which span more than 200 different reaction types and include up to 18 different reactants. A second set consists of 50 000 randomly picked reactions from US patents. The results of the second data set were compared to results obtained using two different atom-to-atom mapping algorithms. For both data sets our method assigns the reaction roles correctly for the vast majority of the reactions, achieving an accuracy of 88% and 97% respectively. The median time needed, about 8 ms, indicates that the algorithm is fast enough to be applied to large collections. The new method is available as part of the RDKit toolkit and the data sets and Jupyter notebooks used for evaluation of the new method are available in the Supporting Information of this publication."
                },
                {
                    "title": "Suzuki–Miyaura cross-coupling optimization enabled by automated feedback",
                    "abstract": "An automated, droplet-flow microfluidic system explores and optimizes Pd-catalyzed Suzuki–Miyaura cross-coupling reactions."
                },
                {
                    "title": "Extraction of chemical structures and reactions from the literature",
                    "abstract": "........................................................................................................................................ II Table of"
                },
                {
                    "title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
                    "abstract": null
                },
                {
                    "title": "A Complex Catalytic Cycle Leading to a Regioselective Synthesis of o,o′‐Disubstituted Vinylarenes",
                    "abstract": null
                },
                {
                    "title": "The properties of known drugs. 1. Molecular frameworks.",
                    "abstract": "In order to better understand the common features present in drug molecules, we use shape description methods to analyze a database of commercially available drugs and prepare a list of common drug shapes. A useful way of organizing this structural data is to group the atoms of each drug molecule into ring, linker, framework, and side chain atoms. On the basis of the two-dimensional molecular structures (without regard to atom type, hybridization, and bond order), there are 1179 different frameworks among the 5120 compounds analyzed. However, the shapes of half of the drugs in the database are described by the 32 most frequently occurring frameworks. This suggests that the diversity of shapes in the set of known drugs is extremely low. In our second method of analysis, in which atom type, hybridization, and bond order are considered, more diversity is seen; there are 2506 different frameworks among the 5120 compounds in the database, and the most frequently occurring 42 frameworks account for only one-fourth of the drugs. We discuss the possible interpretations of these findings and the way they may be used to guide future drug discovery research."
                },
                {
                    "title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules",
                    "abstract": "18-24."
                },
                {
                    "title": "Readily accessible 12-I-5 oxidant for the conversion of primary and secondary alcohols to aldehydes and ketones",
                    "abstract": "Conversion de cyclohexanol, octanol, alcool benzylique et des alcools dimethoxy- et trimethoxy benzyliques par oxydation avec le triacetoxy-1,1,1 benzoiodoxole-1,2 one-3"
                },
                {
                    "title": "The Generation of a Unique Machine Description for Chemical Structures-A Technique Developed at Chemical Abstracts Service.",
                    "abstract": null
                },
                {
                    "title": "Uni-Mol: A Universal 3D Molecular Representation Learning Framework",
                    "abstract": "Molecular representation learning (MRL) has gained tremendous attention due to its critical role in learning from limited supervised data for applications like drug design. In most MRL methods, molecules are treated as 1D sequential tokens or 2D topology graphs, limiting their ability to incorporate 3D information for downstream tasks and, in particular, making it almost impossible for 3D geometry prediction or generation. Herein, we propose Uni-Mol, a universal MRL framework that significantly enlarges the representation ability and application scope of MRL schemes. Uni-Mol is composed of two models with the same SE(3)-equivariant transformer architecture: a molecular pretraining model trained by 209M molecular conformations; a pocket pretraining model trained by 3M candidate protein pocket data. The two models are used independently for separate tasks, and are combined when used in protein-ligand binding tasks. By properly incorporating 3D information, Uni-Mol outperforms SOTA in 14/15 molecular property prediction tasks. Moreover, Uni-Mol achieves superior performance in 3D spatial tasks, including protein-ligand binding pose prediction, molecular conformation generation, etc. Finally, we show that Uni-Mol can be successfully applied to the tasks with few-shot data like pocket druggability prediction. The code, model, and data are made publicly available at https://github.com/dptech-corp/Uni-Mol ."
                },
                {
                    "title": "Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries",
                    "abstract": "We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning."
                },
                {
                    "title": "WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models",
                    "abstract": null
                },
                {
                    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                {
                    "title": "Deep Learning for the Life Sciences",
                    "abstract": null
                },
                {
                    "title": "Improving Language Understanding by Generative Pre-Training",
                    "abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI)."
                },
                {
                    "title": "Pattern matching: The gestalt approach",
                    "abstract": null
                },
                {
                    "title": "Elementary mathematical theory of classification and prediction",
                    "abstract": null
                },
                {
                    "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "45d8f834-6183-4ab1-8d65-5d04d619cb02": {
                "pk": "45d8f834-6183-4ab1-8d65-5d04d619cb02",
                "project_name": null,
                "name": "Zihan Zhao",
                "bio": "I am a researcher dedicated to the field of multimodal emotion recognition, where I explore innovative methods to enhance the understanding of human emotions through various modalities. My recent work focuses on addressing the challenges posed by limited data in this domain. By leveraging transfer learning with state-of-the-art pre-trained models like wav2vec 2.0 and BERT, I have developed multi-level fusion approaches that combine early and late fusion techniques to improve performance. \n\nOne of my notable contributions is the introduction of a multi-granularity framework that extracts speech embeddings at different levels, which has led to a significant boost in accuracy on the IEMOCAP dataset. Additionally, I have tackled the limitations of attention mechanisms in existing models by integrating external emotion-related knowledge through a Bayesian attention module, resulting in improved performance metrics.\n\nMy research also extends to the realm of Large Language Models (LLMs), particularly in the context of Spoken Question Answering (SQA). I curated the LibriSQA dataset to facilitate this work and proposed a lightweight, end-to-end framework that effectively aligns speech and text features. This endeavor not only demonstrates the potential of LLMs in multimodal tasks but also sets the stage for the development of universal multimodal models. I am passionate about pushing the boundaries of emotion recognition and multimodal learning, and I am excited about the future directions this research can take.",
                "collaborators": [
                    "Yanfeng Wang",
                    "Yu Wang",
                    "Yiyang Jiang",
                    "Heyang Liu"
                ],
                "pub_titles": [
                    "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
                    "Knowledge-aware Bayesian Co-attention for Multimodal Emotion Recognition",
                    "LibriSQA: A Novel Dataset and Framework for Spoken Question Answering with Large Language Models"
                ],
                "pub_abstracts": [
                    "The research and applications of multimodal emotion recognition have become increasingly popular recently. However, multimodal emotion recognition faces the challenge of lack of data. To solve this problem, we propose to use transfer learning which leverages state-of-the-art pre-trained models including wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including coattention-based early fusion and late fusion with the models trained on both embeddings are explored. Also, a multi-granularity framework which extracts not only frame-level speech embeddings but also segment-level embeddings including phone, syllable and word-level speech embeddings is proposed to further boost the performance. By combining our coattention-based early fusion model and late fusion model with the multi-granularity feature extraction framework, we obtain result that outperforms best baseline approaches by 1.3% unweighted accuracy (UA) on the IEMOCAP dataset.",
                    "Multimodal emotion recognition is a challenging research area that aims to fuse different modalities to predict human emotion. However, most existing models that are based on attention mechanisms have difficulty in learning emotionally relevant parts on their own. To solve this problem, we propose to incorporate external emotion-related knowledge in the co-attention based fusion of pre-trained models. To effectively incorporate this knowledge, we enhance the co-attention model with a Bayesian attention module (BAM) where a prior distribution is estimated using the emotion-related knowledge. Experimental results on the IEMOCAP dataset show that the proposed approach can outperform several state-of-the-art approaches by at least 0.7% unweighted accuracy (UA).",
                    "While Large Language Models (LLMs) have demonstrated commendable performance across a myriad of domains and tasks, existing LLMs still exhibit a palpable deficit in handling multimodal functionalities, especially for the Spoken Question Answering (SQA) task which necessitates precise alignment and deep interaction between speech and text features. To address the SQA challenge on LLMs, we initially curated the free-form and open-ended LibriSQA dataset from Librispeech, comprising Part I with natural conversational formats and Part II encompassing multiple-choice questions followed by answers and analytical segments. Both parts collectively include 107k SQA pairs that cover various topics. Given the evident paucity of existing speech-text LLMs, we propose a lightweight, end-to-end framework to execute the SQA task on the LibriSQA, witnessing significant results. By reforming ASR into the SQA format, we further substantiate our framework's capability in handling ASR tasks. Our empirical findings bolster the LLMs' aptitude for aligning and comprehending multimodal information, paving the way for the development of universal multimodal LLMs. The dataset and demo can be found at https://github.com/ZihanZhaoSJTU/LibriSQA."
                ],
                "domain": [
                    "Multimodal Emotion Recognition",
                    "Transfer Learning",
                    "Large Language Models",
                    "Speech Processing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "7e1b6dac-6cc2-4ab6-bb43-281add2a485b": {
                "pk": "7e1b6dac-6cc2-4ab6-bb43-281add2a485b",
                "project_name": null,
                "name": "Da Ma",
                "bio": "I am a researcher deeply engaged in the intersection of condensed matter physics, materials science, and medical imaging, with a focus on leveraging advanced computational techniques to address complex problems. My recent work investigates the Kondo effect in Weyl semimetals, where I explore the anisotropic spin-spin correlation functions that arise from nontrivial spin textures. This research not only enhances our understanding of quantum materials but also highlights the unique properties of Fermi arcs.\n\nIn addition to my work in condensed matter physics, I have delved into the realm of electro-optic effects in metals, identifying novel mechanisms that can be harnessed in layered 2D materials like gapped bilayer graphene. This research opens new avenues for terahertz electro-optic modulation, showcasing the potential of metallic platforms in advanced applications.\n\nMy passion for applying cutting-edge technology to medicine is evident in my exploration of AI-driven precision medicine. I am particularly focused on utilizing 3D CT imaging and deep learning to automate the segmentation of internal anatomy, which can significantly enhance patient-specific treatment planning. My work aims to bridge the gap between advanced imaging techniques and clinical applications, ultimately improving outcomes for patients.\n\nFurthermore, I have developed a novel framework using Generative Adversarial Networks to differentiate between frontotemporal dementia and Alzheimer's disease, addressing a critical need in neuroimaging. By combining multi-scale structural features with synthetic data augmentation, I strive to enhance diagnostic accuracy and contribute to better patient care. My research reflects a commitment to innovation and interdisciplinary collaboration, driving advancements in both fundamental science and practical applications.",
                "collaborators": [
                    "Karteek Popuri",
                    "Mirza Faisal Beg",
                    "Hua Chen",
                    "Haiwen Liu",
                    "X. C. Xie",
                    "Ying Xiong",
                    "Justin C. W. Song",
                    "Vincent Chow",
                    "Donghuan Lu"
                ],
                "pub_titles": [
                    "Kondo Effect with Weyl Semimetal Fermi Arcs",
                    "Skew-scattering Pockels effect and metallic electro-optics in gapped bilayer graphene",
                    "Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition",
                    "Differential Diagnosis of Frontotemporal Dementia and Alzheimer's Disease using Generative Adversarial Network"
                ],
                "pub_abstracts": [
                    "We investigate the Kondo effect of the Fermi arcs in a time-reversal-invariant Weyl semimetal with the variational method. To show the consequence brought out by the nontrivial spin texture, we calculate the spatial spin-spin correlation functions. The correlation functions exhibit high anisotropy. The diagonal correlation functions are dominated by the antiferromagnetic correlation while the off-diagonal part has more complicated pattern. The correlation functions obey the same symmetry as the spin texture. Tuning chemical potential changes the pattern of the correlation functions and the correlation length. The correlation functions of the Weyl semimetal Fermi arcs and that from a Dirac semimetal show discrepancy.",
                    "We argue that a range of strong metallic electro-optic (EO) effects can be naturally realized from non-Drude dynamics of free carriers in metals. In particular, in clean metals we identify skew-scattering and a \"Snap\" (third-order derivative of velocity) dominating the Pockels and Kerr EO behavior of metals in the clean limit. Strikingly, we find that both Pockels and Kerr EO in metals play critical roles in metallic EO phenomena: for instance, metallic Pockels and Kerr EO effectively compete to produce a field-activated birefringence that is non-reciprocal in applied DC fields. Similarly, both contribute to sizeable field-induced modulations to transmission and reflection across a range of frequencies. We find metallic EO effects can be naturally realized in layered 2D materials such as gapped bilayer graphene producing pronounced values of EO coefficients in the terahertz -- an interesting new metallic platform for terahertz electro-optic modulation.",
                    "The latest advances in computer-assisted precision medicine are making it feasible to move from population-wide models that are useful to discover aggregate patterns that hold for group-based analysis to patient-specific models that can drive patient-specific decisions with regard to treatment choices, and predictions of outcomes of treatment. Body Composition is recognized as an important driver and risk factor for a wide variety of diseases, as well as a predictor of individual patient-specific clinical outcomes to treatment choices or surgical interventions. 3D CT images are routinely acquired in the oncological worklows and deliver accurate rendering of internal anatomy and therefore can be used opportunistically to assess the amount of skeletal muscle and adipose tissue compartments. Powerful tools of artificial intelligence such as deep learning are making it feasible now to segment the entire 3D image and generate accurate measurements of all internal anatomy. These will enable the overcoming of the severe bottleneck that existed previously, namely, the need for manual segmentation, which was prohibitive to scale to the hundreds of 2D axial slices that made up a 3D volumetric image. Automated tools such as presented here will now enable harvesting whole-body measurements from 3D CT or MRI images, leading to a new era of discovery of the drivers of various diseases based on individual tissue, organ volume, shape, and functional status. These measurements were hitherto unavailable thereby limiting the field to a very small and limited subset. These discoveries and the potential to perform individual image segmentation with high speed and accuracy are likely to lead to the incorporation of these 3D measures into individual specific treatment planning models related to nutrition, aging, chemotoxicity, surgery and survival after the onset of a major disease such as cancer.",
                    "Frontotemporal dementia and Alzheimer's disease are two common forms of dementia and are easily misdiagnosed as each other due to their similar pattern of clinical symptoms. Differentiating between the two dementia types is crucial for determining disease-specific intervention and treatment. Recent development of Deep-learning-based approaches in the field of medical image computing are delivering some of the best performance for many binary classification tasks, although its application in differential diagnosis, such as neuroimage-based differentiation for multiple types of dementia, has not been explored. In this study, a novel framework was proposed by using the Generative Adversarial Network technique to distinguish FTD, AD and normal control subjects, using volumetric features extracted at coarse-to-fine structural scales from Magnetic Resonance Imaging scans. Experiments of 10-folds cross-validation on 1,954 images achieved high accuracy. With the proposed framework, we have demonstrated that the combination of multi-scale structural features and synthetic data augmentation based on generative adversarial network can improve the performance of challenging tasks such as differentiating Dementia sub-types."
                ],
                "domain": [
                    "Condensed Matter Physics",
                    "Electro-Optics",
                    "Medical Imaging",
                    "Deep Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "18ad626a-3068-437c-9163-7ea02064bb9a": {
                "pk": "18ad626a-3068-437c-9163-7ea02064bb9a",
                "project_name": null,
                "name": "Lu Chen",
                "bio": "I am a philosopher and researcher deeply engaged in the foundations of space, geometry, and the interplay between physical theories and their underlying structures. My work critically examines traditional assumptions about the nature of space, particularly in response to Weyl's arguments regarding discrete geometry. I propose innovative frameworks such as Infinitesimal Gunk and Smooth Infinitesimal Geometry, which challenge conventional views by introducing infinitesimal regions and emphasizing the relational aspects of physical fields.\n\nMy research also explores the implications of algebraicism, arguing for a relationalist perspective that prioritizes structural relations over substantivalism. I delve into the philosophical significance of symmetries and isomorphisms in physical models, employing categorical strategies to enhance our understanding of these concepts. Additionally, I address the challenges posed by Zeno's paradox and the limitations of effective realism in the context of effective field theories, particularly in quantum gravity.\n\nThrough rigorous analysis and the development of new theoretical frameworks, I aim to contribute to a more nuanced understanding of the nature of space and its representation in physical theories. My work not only seeks to resolve longstanding philosophical dilemmas but also aspires to bridge the gap between abstract theoretical constructs and their practical implications in physics.",
                "collaborators": [],
                "pub_titles": [
                    "Why the Weyl Tile Argument is Wrong",
                    "Infinitesimal Gunk",
                    "A partial defense of algebraic relationalism",
                    "Symmetries as Isomorphisms",
                    "Univalence and Ontic Structuralism",
                    "KAM theorem with large twist and finite smooth large perturbation",
                    "Intrinsic Local Distances: A Mixed Solution to Weyl's Tile Argument",
                    "Smooth Infinitesimals in the Metaphysical Foundation of Spacetime Theories",
                    "Do Simple Infinitesimal Parts Solve Zeno's Paradox of Measure?",
                    "Can we \"effectivize'' spacetime?",
                    "On invariant tori in some reversible systems"
                ],
                "pub_abstracts": [
                    "Weyl famously argued that if space were discrete, then Euclidean geometry could not hold even approximately. Since then, many philosophers have responded to this argument by advancing alternative accounts of discrete geometry that recover approximately Euclidean space. However, they have missed an importantly flawed assumption in Weyl's argument: physical geometry is determined by fundamental spacetime structures independently from dynamical laws. In this paper, I aim to show its falsity through two rigorous examples: random walks in statistical physics and quantum mechanics.",
                    "In this paper, I advance an original view of the structure of space called \\textit{Infinitesimal Gunk}. This view says that every region of space can be further divided and some regions have infinitesimal size, where infinitesimals are understood in the framework of Robinson's (1966) nonstandard analysis. This view, I argue, provides a novel reply to the inconsistency arguments proposed by Arntzenius (2008) and Russell (2008), which have troubled a more familiar gunky approach. Moreover, it has important advantages over the alternative views these authors suggested. Unlike Arntzenius's proposal, it does not introduce regions with no interior. It also has a much richer measure theory than Russell's proposal and does not retreat to mere finite additivity.",
                    "I defend algebraicism, according to which physical fields can be understood in terms of their structural relations without reference to a spacetime manifold, as a genuine relationalist view against the conventional wisdom that it is equivalent to substantivalism, according to which spacetime exists fundamentally. I criticize the standard version of algebraicism that is considered equivalent to substantivalism. Furthermore, I present alternative examples of algebraicism that better implement relationalism with their conceptual advantages over substantivalism or its standard algebraic counterpart.",
                    "Symmetries and isomorphisms play similar conceptual roles when we consider how models represent physical situations, but they are formally distinct, as two models related by symmetries are not typically isomorphic. I offer a rigorous categorical strategy that formulate symmetries as isomorphisms between models and apply it to classical electromagnetism, and evaluate its philosophical significance in relation to the recent debate between `sophistication' and `reduction'. In addition to traditional spacetime models, I also consider algebraic models, in which case we can use the method of natural operators to address the problem of ontological nonperspicuity faced by the categorical strategy. Finally, I briefly expound on the significance of symmetries as isomorphisms in the framework of Univalent Foundations, in which isomorphic structures are formally identified.",
                    "The persistent challenge of formulating ontic structuralism in a rigorous manner, which prioritizes structures over the entities they contain, calls for a transformation of traditional logical frameworks. I argue that Univalent Foundations (UF), which feature the axiom that all isomorphic structures are identical, offer such a foundation and are more attractive than other proposed structuralist frameworks. Furthermore, I delve into the significance in the case of the hole argument and, very briefly, the nature of symmetries.",
                    "In the present paper, we will discuss the following non-degenerate Hamiltonian system \\begin{equation*} H(\\theta,t,I)=\\frac{H_0(I)}{\\varepsilon^{a}}+\\frac{P(\\theta,t,I)}{\\varepsilon^{b}}, \\end{equation*}   where $(\\theta,t,I)\\in\\mathbf{{T}}^{d+1}\\times[1,2]^d$ ($\\mathbf{{T}}:=\\mathbf{{R}}/{2\\pi \\mathbf{Z}}$), $a,b$ are given positive constants with $a>b$, $H_0: [1,2]^d\\rightarrow \\mathbf R$ is real analytic and $P: \\mathbf T^{d+1}\\times [1,2]^d\\rightarrow \\mathbf R$ is $C^{\\ell}$ with $\\ell=\\frac{2(d+1)(5a-b+2ad)}{a-b}+\\mu$, $0<\\mu\\ll1$. We prove that if $\\varepsilon$ is sufficiently small, there is an invariant torus with given Diophantine frequency vector for the above Hamiltonian system. As for application, we prove that a finite network of Duffing oscillators with periodic exterior forces possesses Lagrangian stability for almost all initial data.",
                    "Weyl's tile argument purports to show that there are no natural distance functions in atomistic space that approximate Euclidean geometry. I advance a response to this argument that relies on a new account of distance in atomistic space, called \\textit{the mixed account}, according to which \\textit{local distances} are primitive and other distances are derived from them. Under this account, atomistic space can approximate Euclidean space (and continuous space in general) very well. To motivate this account as a genuine solution to Weyl's tile argument, I argue that this account is no less natural than the standard account of distance in continuous space. I also argue that the mixed account has distinctive advantages over Forrest's (1995) account in response to Weyl's tile argument, which can be considered as a restricted version of the mixed account.",
                    "I propose a theory of space with infinitesimal regions called \\textit{smooth infinitesimal geometry} (SIG) based on certain algebraic objects (i.e., rings), which regiments a mode of reasoning heuristically used by geometricists and physicists (e.g., circle is composed of infinitely many straight lines). I argue that SIG has the following utilities. (1) It provides a simple metaphysics of vector fields and tangent space that are otherwise perplexing. A tangent space can be considered an infinitesimal region of space. (2) It generalizes a standard implementation of spacetime algebraicism (according to which physical fields exist fundamentally without an underlying manifold) called \\textit{Einstein algebras}. (3) It solves the long-standing problem of interpreting \\textit{smooth infinitesimal analysis} (SIA) realistically, an alternative foundation of spacetime theories to real analysis (Lawvere 1980). SIA is formulated in intuitionistic logic and is thought to have no classical reformulations (Hellman 2006). Against this, I argue that SIG is (part of) such a reformulation. But SIG has an unorthodox mereology, in which the principle of supplementation fails.",
                    "I develop a new view of the structure of space--called infinitesimal atomism--as a reply to Zeno's paradox of measure. According to this view, space is composed of ultimate parts with infinitesimal size, where infinitesimals are understood within the framework of Robinson's nonstandard analysis. Notably, this view satisfies a version of additivity: for every region that has a size, its size is the sum of the sizes of its disjoint parts. In particular, the size of a finite region is the sum of the sizes of its infinitesimal parts. Although this view is a coherent approach to Zeno's paradox and is preferable to Skyrms's (1983) infinitesimal approach, it faces both the main problem for the standard view (the problem of unmeasurable regions) and the main problem for finite atomism (Weyl's tile argument), leaving it with no clear advantage over these familiar alternatives.",
                    "According to \\textit{effective realism}, scientific theories give us knowledge about the unobservable world, but not at the fundamental level. This view is justified by the well-received \\textit{effective-field-theory} (EFT) approach to physics, according to which our best physical theories are only applicable up to a certain energy scale and expected to break down beyond that. In this paper, I explain the motivations for the EFT approach and effective realism and their benefits. I also raise new challenges for this approach. Applying effective realism to \\textit{effective quantum gravity} (EQG) reveals its shortcomings: EQG does not give us a realistic theory of spacetime even within its scope of validity. It also exposes a general interpretative dilemma faced by all EFTs concerning their indispensable references to classical spacetime beyond their scope of validity.",
                    "In the present paper, we consider the following reversible system \\begin{equation*} \\begin{cases} \\dot{x}=\\omega_0+f(x,y),\\\\ \\dot{y}=g(x,y), \\end{cases} \\end{equation*} where $x\\in\\mathbf{T}^{d}$, $y\\backsim0\\in \\mathbf{R}^{d}$, $\\omega_0$ is Diophantine, $f(x,y)=O(y)$, $g(x,y)=O(y^2)$ and $f$, $g$ are reversible with respect to the involution G: $(x,y)\\mapsto(-x,y)$, that is, $f(-x,y)=f(x,y)$, $g(-x,y)=-g(x,y)$. We study the accumulation of an analytic invariant torus $\\Gamma_0$ of the reversible system with Diophantine frequency $\\omega_0$ by other invariant tori. We will prove that if the Birkhoff normal form around $\\Gamma_0$ is 0-degenerate, then $\\Gamma_0$ is accumulated by other analytic invariant tori, the Lebesgue measure of the union of these tori being positive and the density of the union of these tori at $\\Gamma_0$ being one. We will also prove that if the Birkhoff normal form around $\\Gamma_0$ is $j$-degenerate ($1\\leq j\\leq d-1$) and condition (1.6) is satisfied, then through $\\Gamma_0$ there passes an analytic subvariety of dimension $d+j$ foliated into analytic invariant tori with frequency vector $\\omega_0$. If the Birkhoff normal form around $\\Gamma_0$ is $d-1$-degenerate, we will prove a stronger result, that is, a full neighborhood of $\\Gamma_0$ is foliated into analytic invariant tori with frequency vectors proportional to $\\omega_0$."
                ],
                "domain": [
                    "Philosophy of Science",
                    "Geometry",
                    "Nonstandard Analysis",
                    "Structural Realism"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "088f343b-0e69-4dc8-a748-4243aec5158c": {
                "pk": "088f343b-0e69-4dc8-a748-4243aec5158c",
                "project_name": null,
                "name": "Liangtai Sun",
                "bio": "I am a researcher dedicated to advancing the capabilities of task-oriented dialogue (TOD) systems and large language models (LLMs) in the context of mobile applications and scientific discovery. My recent work has focused on developing innovative architectures, such as the GUI-based task-oriented dialogue system (GUI-TOD), which allows intelligent assistants to perform tasks directly through GUI operations, bypassing the limitations of traditional API-based systems. \n\nI have also contributed to the evaluation of LLMs with the creation of benchmarks like SciEval and MULTI, which address the need for comprehensive assessments of scientific reasoning and multimodal understanding. My research emphasizes the importance of domain-specific knowledge, leading to the development of SciDFM, a mixture-of-experts LLM that excels in scientific reasoning and understanding complex molecular data.\n\nAdditionally, I have explored methods to enhance the training efficiency of LLMs through sparsity, proposing Sparsity-Accelerated Training (SAT) to reduce computational costs while maintaining performance. My latest project, MobA, integrates multimodal LLMs into mobile assistants, significantly improving their comprehension and task execution capabilities.\n\nThrough my work, I aim to bridge the gap between advanced AI models and practical applications, ultimately enhancing user experiences and facilitating scientific advancements. I am committed to open-sourcing my findings and tools to benefit the broader research community.",
                "collaborators": [
                    "Lu Chen",
                    "Kai Yu",
                    "Zichen Zhu",
                    "Zihan Zhao",
                    "Da Ma",
                    "Zhennan Shen",
                    "Baocai Chen",
                    "Situo Zhang",
                    "Su Zhu",
                    "Xingyu Chen"
                ],
                "pub_titles": [
                    "META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI",
                    "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
                    "MULTI: Multimodal Understanding Leaderboard with Text and Images",
                    "SciDFM: A Large Language Model with Mixture-of-Experts for Science",
                    "Sparsity-Accelerated Training for Large Language Models",
                    "MobA: A Two-Level Agent System for Efficient Mobile Task Automation"
                ],
                "pub_abstracts": [
                    "Task-oriented dialogue (TOD) systems have been widely used by mobile phone intelligent assistants to accomplish tasks such as calendar scheduling or hotel reservation. Current TOD systems usually focus on multi-turn text/speech interaction, then they would call back-end APIs designed for TODs to perform the task. However, this API-based architecture greatly limits the information-searching capability of intelligent assistants and may even lead to task failure if TOD-specific APIs are not available or the task is too complicated to be executed by the provided APIs. In this paper, we propose a new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A GUI-TOD system can directly perform GUI operations on real APPs and execute tasks without invoking TOD-specific backend APIs. Furthermore, we release META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile GUI. We also propose a multi-model action prediction and response model, which show promising results on META-GUI. The dataset, codes and leaderboard are publicly available.",
                    "Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a \"dynamic\" subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The data and codes are now publicly available.",
                    "Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community, while existing benchmarks primarily focus on understanding simple natural images and short context. In this paper, we present MULTI as a cutting-edge benchmark for evaluating MLLMs on understanding complex tables and images, and reasoning with long context. MULTI provides multimodal inputs and requires responses that are either precise or open-ended, reflecting real-life examination styles. MULTI includes over 18,000 questions and challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis and cross-modality reasoning. We also introduce MULTI-Elite, a 500-question selected hard subset, and MULTI-Extend, with more than 4,500 external knowledge context pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on MULTI, in contrast to other MLLMs scoring between 28.5% and 55.3%. MULTI serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.",
                    "Recently, there has been a significant upsurge of interest in leveraging large language models (LLMs) to assist scientific discovery. However, most LLMs only focus on general science, while they lack domain-specific knowledge, such as chemical molecules and amino acid sequences. To bridge these gaps, we introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and is able to conduct college-level scientific reasoning and understand molecules and amino acid sequences. We collect a large-scale training corpus containing numerous scientific papers and books from different disciplines as well as data from domain-specific databases. We further fine-tune the pre-trained model on lots of instruction data to improve performances on downstream benchmarks. From experiment results, we show that SciDFM achieves strong performance on general scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA performance on domain-specific benchmarks among models of similar size. We further analyze the expert layers and show that the results of expert selection vary with data from different disciplines. To benefit the broader research community, we open-source SciDFM at https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.",
                    "Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging \\emph{sparsity} in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process. Specifically, SAT achieves a $45\\%$ throughput improvement in continual pre-training and saves $38\\%$ training time in supervised fine-tuning in practice. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training. Our code is available at https://github.com/OpenDFM/SAT.",
                    "Current mobile assistants are limited by dependence on system APIs or struggle with complex user instructions and diverse interfaces due to restricted comprehension and decision-making abilities. To address these challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal large language models that enhances comprehension and planning capabilities through a sophisticated two-level agent architecture. The high-level Global Agent (GA) is responsible for understanding user commands, tracking history memories, and planning tasks. The low-level Local Agent (LA) predicts detailed actions in the form of function calls, guided by sub-tasks and memory from the GA. Integrating a Reflection Module allows for efficient task completion and enables the system to handle previously unseen complex tasks. MobA demonstrates significant improvements in task execution efficiency and completion rate in real-life evaluations, underscoring the potential of MLLM-empowered mobile assistants."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Multimodal Learning",
                    "Task-oriented Dialogue",
                    "Large Language Models"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "7dab9645-cb0f-43a4-8b35-3dde4a73d7c6": {
                "pk": "7dab9645-cb0f-43a4-8b35-3dde4a73d7c6",
                "project_name": null,
                "name": "Zihao Li",
                "bio": "As a researcher deeply engaged in the intersection of technology, law, and ethics, my work focuses on the implications of emerging technologies, particularly Large Language Models (LLMs) and their societal impacts. I have critically examined the regulatory landscape surrounding AI, emphasizing the need for the European Union to adapt its frameworks to address the unique risks posed by LLMs, such as hallucination and ethical concerns.\n\nMy research also delves into the manipulation of user behavior through dark patterns in digital interfaces. By systematically reviewing existing literature, I have identified key issues and proposed regulatory solutions that balance legal theory with practical design guidelines. This interdisciplinary approach allows me to bridge the gap between law and human-computer interaction.\n\nIn addition to my work on regulation, I am exploring innovative methodologies in machine learning, such as DiffuRec, which leverages diffusion models for sequential recommendation tasks. This research reflects my commitment to advancing the capabilities of AI while ensuring ethical considerations are at the forefront.\n\nI am also passionate about quantum computing and its applications, having developed robust protocols for verifying quantum states in blind quantum computation. My diverse research portfolio underscores my dedication to addressing complex challenges at the intersection of technology, ethics, and law, ultimately aiming to foster responsible innovation in our rapidly evolving digital landscape.",
                "collaborators": [
                    "Weiwei Yi",
                    "Huangjun Zhu",
                    "Aixin Sun",
                    "Chenliang Li",
                    "Wenchuan Wu",
                    "Boming Zhang",
                    "Benjamin Horowitz",
                    "Zheng Cai",
                    "Pan Gao",
                    "Hui Yuan"
                ],
                "pub_titles": [
                    "The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination",
                    "Mapping the Scholarship of Dark Pattern Regulation: A Systematic Review of Concepts, Regulatory Paradigms, and Solutions from an Interdisciplinary Perspective",
                    "DiffuRec: A Diffusion Model for Sequential Recommendation",
                    "A Kullback-Leibler Divergence-based Distributionally Robust Optimization Model for Heat Pump Day-ahead Operational Schedule in Distribution Networks",
                    "Improved Lyman Alpha Tomography using Optimized Reconstruction with Constraints on Absorption (ORCA)",
                    "Dynamic Local Feature Aggregation for Learning on Point Clouds",
                    "Robust and efficient verification of graph states in blind measurement-based quantum computation",
                    "Allocating Mixed Goods with Customized Fairness and Indivisibility Ratio",
                    "Accuracy of training data and model outputs in Generative AI: CREATe Response to the Information Commissioner Office Consultation",
                    "Implementation of generalized measurements on a qudit via quantum walks",
                    "Sequentially Optimal Pricing under Informational Robustness",
                    "Generalized matrix nearness problems"
                ],
                "pub_abstracts": [
                    "With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our whole society, rapidly altering the way we think, create and live. For instance, the GPT integration in Bing has altered our approach to online searching. While nascent LLMs have many advantages, new legal and ethical risks are also emerging, stemming in particular from stochastic parrots and hallucination. The EU is the first and foremost jurisdiction that has focused on the regulation of AI models. However, the risks posed by the new LLMs are likely to be underestimated by the emerging EU regulatory paradigm. Therefore, this correspondence warns that the European AI regulatory paradigm must evolve further to mitigate such risks.",
                    "Dark patterns, design tricks used on online interfaces to manipulate users decision-making process, have raised public concerns. However, research on regulation of dark pattern remains underdeveloped and scattered, particularly regarding scholars views on the concept, regulatory paradigms, and solutions. Following PRISMA guidelines, this paper systematically reviews the formats and content of regulatory discussions on dark patterns from the interdisciplinary scholarship of Law and Human-Computer Interaction. A total of 65 studies were analysed through content and thematic analysis. This study synthesises the unique trends and characteristics of legal scholarship on dark patterns, identifying five root problems and triple layered harms. It critiques current regulations in terms of legal theories and sectoral legislations, highlighting their inadequacies in addressing dark patterns. The paper also critically examines existing proposed solutions, including paradigmatic shifts in legal doctrines, refinements to existing frameworks, technical design-embedded solutions, and accountability measures for design practices. This research critically discusses the current barriers to effective dark pattern regulations and explores promising regulatory solutions. The difficulty in identifying the normative nature of various forms of dark patterns, in identifying evident and actionable harm, and the expanding scope of dark patterns connotation inherently hinders effective regulation. However, technical design-embedded solutions, accountability frameworks, and practical design guidelines offer potential routes for more proactive regulation, while legal pluralism stands as a promising macro-level change in regulatory paradigms for dark pattern regulation.",
                    "Mainstream solutions to Sequential Recommendation (SR) represent items with fixed vectors. These vectors have limited capability in capturing items' latent aspects and users' diverse preferences. As a new generative paradigm, Diffusion models have achieved excellent performance in areas like computer vision and natural language processing. To our understanding, its unique merit in representation generation well fits the problem setting of sequential recommendation. In this paper, we make the very first attempt to adapt Diffusion model to SR and propose DiffuRec, for item representation construction and uncertainty injection. Rather than modeling item representations as fixed vectors, we represent them as distributions in DiffuRec, which reflect user's multiple interests and item's various aspects adaptively. In diffusion phase, DiffuRec corrupts the target item embedding into a Gaussian distribution via noise adding, which is further applied for sequential item distribution representation generation and uncertainty injection. Afterward, the item representation is fed into an Approximator for target item representation reconstruction. In reverse phase, based on user's historical interaction behaviors, we reverse a Gaussian noise into the target item representation, then apply a rounding operation for target item prediction. Experiments over four datasets show that DiffuRec outperforms strong baselines by a large margin.",
                    "For its high coefficient of performance and zero local emissions, the heat pump (HP) has recently become popular in North Europe and China. However, the integration of HPs may aggravate the daily peak-valley gap in distribution networks significantly.",
                    "In this work, we propose an improved approach to reconstruct the three-dimensional intergalactic medium from observed Lyman-$\\alpha$ forest absorption features. We present our new method, the Optimized Reconstruction with Constraints on Absorption (ORCA), which outperforms the current baseline Wiener Filter (WF) when tested on mock Lyman Alpha forest data generated from hydrodynamical simulations. We find that both reconstructed flux errors and cosmic web classification improve substantially with ORCA, equivalent to 30-40\\% additional sight-lines with the standard WF. We use this method to identify and classify extremal objects, i.e. voids and (proto)-clusters, and find improved reconstruction across all summary statistics explored. We apply ORCA to existing Lyman Alpha forest data from the COSMOS Lyman Alpha Mapping and Tomography Observations (CLAMATO) Survey and compare it to the WF reconstruction.",
                    "Existing point cloud learning methods aggregate features from neighbouring points relying on constructing graph in the spatial domain, which results in feature update for each point based on spatially-fixed neighbours throughout layers. In this paper, we propose a dynamic feature aggregation (DFA) method that can transfer information by constructing local graphs in the feature domain without spatial constraints. By finding k-nearest neighbors in the feature domain, we perform relative position encoding and semantic feature encoding to explore latent position and feature similarity information, respectively, so that rich local features can be learned. At the same time, we also learn low-dimensional global features from the original point cloud for enhancing feature representation. Between DFA layers, we dynamically update the constructed local graph structure, so that we can learn richer information, which greatly improves adaptability and efficiency. We demonstrate the superiority of our method by conducting extensive experiments on point cloud classification and segmentation tasks. Implementation code is available: https://github.com/jiamang/DFA.",
                    "Blind quantum computation (BQC) is a secure quantum computation method that protects the privacy of clients. Measurement-based quantum computation (MBQC) is a promising approach for realizing BQC. To obtain reliable results in blind MBQC, it is crucial to verify whether the resource graph states are accurately prepared in the adversarial scenario. However, previous verification protocols for this task are too resource consuming or noise susceptible to be applied in practice. Here, we propose a robust and efficient protocol for verifying arbitrary graph states with any prime local dimension in the adversarial scenario, which leads to a robust and efficient protocol for verifying the resource state in blind MBQC. Our protocol requires only local Pauli measurements and is thus easy to realize with current technologies. Nevertheless, it can achieve the optimal scaling behaviors with respect to the system size and the target precision as quantified by the infidelity and significance level, which has never been achieved before. Notably, our protocol can exponentially enhance the scaling behavior with the significance level.",
                    "We consider the problem of fairly allocating a combination of divisible and indivisible goods. While fairness criteria like envy-freeness (EF) and proportionality (PROP) can always be achieved for divisible goods, only their relaxed versions, such as the ''up to one'' relaxations EF1 and PROP1, can be satisfied when the goods are indivisible. The ''up to one'' relaxations require the fairness conditions to be satisfied provided that one good can be completely eliminated or added in the comparison. In this work, we bridge the gap between the two extremes and propose ''up to a fraction'' relaxations for the allocation of mixed divisible and indivisible goods. The fraction is determined based on the proportion of indivisible goods, which we call the indivisibility ratio. The new concepts also introduce asymmetric conditions that are customized for individuals with varying indivisibility ratios. We provide both upper and lower bounds on the fractions of the modified item in order to satisfy the fairness criterion. Our results are tight up to a constant for EF and asymptotically tight for PROP.",
                    "The accuracy of Generative AI is increasingly critical as Large Language Models become more widely adopted. Due to potential flaws in training data and hallucination in outputs, inaccuracy can significantly impact individuals interests by distorting perceptions and leading to decisions based on flawed information. Therefore, ensuring these models accuracy is not only a technical necessity but also a regulatory imperative. ICO call for evidence on the accuracy of Generative AI marks a timely effort in ensuring responsible Generative AI development and use.   CREATe, as the Centre for Regulation of the Creative Economy based at the University of Glasgow, has conducted relevant research involving intellectual property, competition, information and technology law. We welcome the ICO call for evidence on the accuracy of Generative AI, and we are happy to highlight aspects of data protection law and AI regulation that we believe should receive attention.",
                    "Quantum measurements play a fundamental role in quantum mechanics and quantum information processing, but it is not easy to implement generalized measurements, the most powerful measurements allowed by quantum mechanics. Here we propose a simple recipe for implementing generalized measurements on a qudit via quantum walks. With this recipe, any discrete quantum measurement can be implemented via a one-dimensional discrete quantum walk; the number of steps is only two times the number of measurement outcomes. As an illustration, we present a unified solution for implementing arbitrary symmetric informationally complete measurements in dimension 3.",
                    "A seller sells an object over time but is uncertain how the buyer learns their willingness-to-pay. We consider informational robustness under \\textit{limited commitment}, where the seller offers a price \\textit{each period} to maximize continuation profit against worst-case information arrival. Our formulation maintains dynamic consistency by considering the worst case \\textit{sequentially}. Under general conditions, we characterize an essentially unique equilibrium where the buyer does not delay to learn more later. Furthermore, we identify a condition that ensures the equilibrium price path is ``reinforcing,'' so even dynamically inconsistent information arrival would not lower the seller's payoff below the equilibrium level.",
                    "We show that the global minimum solution of $\\lVert A - BXC \\rVert$ can be found in closed-form with singular value decompositions and generalized singular value decompositions for a variety of constraints on $X$ involving rank, norm, symmetry, two-sided product, and prescribed eigenvalue. This extends the solution of Friedland--Torokhti for the generalized rank-constrained approximation problem to other constraints as well as provides an alternative solution for rank constraint in terms of singular value decompositions. For more complicated constraints on $X$ involving structures such as Toeplitz, Hankel, circulant, nonnegativity, stochasticity, positive semidefiniteness, prescribed eigenvector, etc, we prove that a simple iterative method is linearly and globally convergent to the global minimum solution."
                ],
                "domain": [
                    "Artificial Intelligence",
                    "Quantum Computing",
                    "Recommendation Systems",
                    "Regulation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f14a8974-0eec-4491-9386-65247b6af211": {
                "pk": "f14a8974-0eec-4491-9386-65247b6af211",
                "project_name": null,
                "name": "Yi Xia",
                "bio": "I am a researcher specializing in the intricate interplay between lattice dynamics and thermal transport properties in crystalline materials, particularly in the context of thermoelectrics. My recent work has focused on understanding the mechanisms behind low lattice thermal conductivity in materials like Ag$_{6}$Ge$_{10}$P$_{12}$ and Cu$_{12}$Sb$_{4}$S$_{13}$, where I employ first-principles calculations and self-consistent phonon theory to reveal the role of anharmonic interactions and rattling phonons.\n\nI have also explored the potential of machine learning to enhance our understanding of phonon behavior, developing a method to transform phonon datasets into a format suitable for training universal interatomic potentials. This approach not only predicts harmonic phonon spectra but also provides insights into thermodynamic properties, which is crucial for advancing materials science.\n\nIn addition to my work on thermoelectrics, I have delved into quantum sensing, demonstrating how entangled quantum sensors can achieve superior measurement sensitivity through the use of noiseless linear amplifiers. My research extends to the study of phase transitions in materials like vanadium dioxide (VO$_{2}$), where I investigate the temperature-dependent behavior of phonons across the metal-insulator transition.\n\nOverall, my research aims to bridge theoretical insights with practical applications, paving the way for the development of high-performance materials and innovative quantum technologies.",
                "collaborators": [
                    "Vidvuds Ozoliņš",
                    "Maria K. Y. Chan",
                    "Chris Wolverton",
                    "Zongxia Liang",
                    "Huiju Lee",
                    "Quntao Zhuang",
                    "William Clark",
                    "Zheshen Zhang",
                    "Junsoo Park",
                    "Guohui Guan"
                ],
                "pub_titles": [
                    "Impact of Temperature-Dependent Rattling Phonons on Lattice Dynamics and Thermal Transport in Ag$_{6}$Ge$_{10}$P$_{12}$",
                    "Machine Learning a Universal Harmonic Interatomic Potential for Predicting Phonons in Crystalline Solids",
                    "Repeater-enhanced distributed quantum sensing based on continuous-variable multipartite entanglement",
                    "Renormalized Lattice Dynamics and Thermal Transport in VO$_{2}$",
                    "Leveraging Electron-Phonon Interaction to Enhance Thermoelectric Power Factor in Graphene-Like Semimetals",
                    "Optimal management of DB pension fund under both underfunded and overfunded cases",
                    "A Two-layer Stochastic Game Approach to Reinsurance Contracting and Competition",
                    "Anharmonic stabilization and lattice heat transport in rocksalt $β$-GeTe",
                    "Microscopic Mechanisms of Glass-Like Lattice Thermal Transport in Cubic Cu$_{12}$Sb$_{4}$S$_{13}$ Tetrahedrites"
                ],
                "pub_abstracts": [
                    "Crystalline compounds exhibiting low-frequency rattling phonons constitute an important class of high-performance thermoelectrics owning to their intrinsically very low lattice thermal conductivity ($\\kappa_{l}$). Theoretical approach that is capable of revealing the physical origin and accurately predicting $\\kappa_{l}$ is of particular interest, which, however, still remains an outstanding challenge. In this study, we perform a case study of lattice dynamics and thermal transport properties of Ag$_{6}$Ge$_{10}$P$_{12}$, which has recently been identified as a high-performance thermoelectric phosphide due to low $\\kappa_{l}$, arising from rattling vibrations associated with Ag$_{6}$ clusters. Analysis within a first-principles-based lattice-dynamics framework based on self-consistent phonon theory reveals a strong temperature dependence of rattling phonons due to high-order anharmonic interactions. Anharmonic hardening of the rattling optical modes has a strong effect on the lifetimes of heat-carrying acoustic phonons by decreasing the rate of three-phonon combination processes. This mechanism results in a significant increase in $\\kappa_l$ and changes its temperature dependence to $1/T^{0.64}$.",
                    "Phonons, as quantized vibrational modes in crystalline materials, play a crucial role in determining a wide range of physical properties, such as thermal and electrical conductivity, making their study a cornerstone in materials science. In this study, we present a simple yet effective strategy for deep learning harmonic phonons in crystalline solids by leveraging existing phonon databases and state-of-the-art machine learning techniques. The key of our method lies in transforming existing phonon datasets, primarily represented in interatomic force constants, into a force-displacement representation suitable for training machine learning universal interatomic potentials. By applying our approach to one of the largest phonon databases publicly available, we demonstrate that the resultant machine learning universal harmonic interatomic potential not only accurately predicts full harmonic phonon spectra but also calculates key thermodynamic properties with remarkable precision. Furthermore, the restriction to a harmonic potential energy surface in our model provides a way of assessing uncertainty in machine learning predictions of vibrational properties, essential for guiding further improvements and applications in materials science.",
                    "Entanglement is a unique resource for quantum-enhanced applications. When employed in sensing, shared entanglement between distributed quantum sensors enables a substantial gain in the measurement sensitivity in estimating global parameters of the quantum sensor network. Loss incurred in the distribution of entanglement, however, quickly dissipates the measurement-sensitivity advantage enjoyed by the entangled quantum sensors over sensors supplied with local quantum resources. Here, we present a viable approach to overcome the entanglement-distribution loss and show that the measurement sensitivity enabled by entangled quantum sensors beat that afforded by the optimum local resource. Our approach relies on noiseless linear amplifiers (NLAs) to serve as quantum repeaters. We show that unlike the outstanding challenge of building quantum repeaters to suppress the repeaterless bound for quantum key distribution, NLA-based quantum repeaters for distributed quantum sensing are realizable by available technology. As such, distributed quantum sensing would become the first application instance that benefits from quantum repeaters.",
                    "Vanadium dioxide (VO$_{2}$) undergoes a first-order metal-insulator transition (MIT) upon cooling near room temperature, concomitant with structural change from rutile to monoclinic. Accurate characterization of lattice vibrations is vital for elucidating the transition mechanism. To investigate the lattice dynamics and thermal transport properties of VO$_{2}$ across the MIT, we present a phonon renormalization scheme based on self-consistent phonon theory through iteratively refining vibrational free energy. Using this technique, we compute temperature-dependent phonon dispersion and lifetimes, and point out the importance of both magnetic and vibrational entropy in driving the MIT. The predicted phonon dispersion and lifetimes show quantitative agreement with experimental measurements. We demonstrate that lattice thermal conductivity of rutile VO$_{2}$ is nearly temperature independent as a result of strong intrinsic anharmonicity, while that of monoclinic VO$_{2}$ varies according to $1/T$. Due to phonon softening and enhanced scattering rates, the lattice thermal conductivity is deduced to be substantially lower in the rutile phase, suggesting that Wiedemann-Franz law might not be strongly violated in rutile VO$_{2}$.",
                    "Electron-phonon interaction (EPI) is presumably detrimental for thermoelectric performance in semiconductors because it limits carrier mobility. Here we show that enhanced EPI with strong energy dependence offers an intrinsic pathway to significant increase in the Seebeck coefficient and the thermoelectric power factor, particularly in the context of two-dimensional (2D) graphene-like Dirac bands. The increase is realized by enabling electron energy filtering through preferential scattering of electron/hole carriers. We prove this concept by implementing state-of-the-art first-principles computational methods with explicit treatment of EPI for a 2D gapless MoS$_{2}$ allotrope, which has both massless Dirac bands and a heavy-fermion state that acts as the filter. We determine that the optimal location of the heavy state and hence the onset of the filtering process is at the Dirac point. Our study opens a new avenue for attaining ultrahigh power factors via engineering the EPI in graphene-like semimetals or identifying new compounds that intrinsically possess the featured electronic structure.",
                    "This paper investigates the optimal management of an aggregated defined benefit pension plan in a stochastic environment. The interest rate follows the Ornstein-Uhlenbeck model, the benefits follow the geometric Brownian motion while the contribution rate is determined by the spread method of fund amortization. The pension manager invests in the financial market with three assets: cash, bond and stock. Regardless of the initial status of the plan, we suppose that the pension fund may become underfunded or overfunded in the planning horizon. The optimization goal of the manager is to maximize the expected utility in the overfunded region minus the weighted solvency risk in the underfunded region. By introducing an auxiliary process and related equivalent optimization problems and using the martingale method, the optimal wealth process, optimal portfolio and efficient frontier are obtained under four cases (high tolerance towards solvency risk, low tolerance towards solvency risk, a specific lower bound, and high lower bound). Moreover, we also obtain the probabilities that the optimal terminal wealth falls in the overfunded and underfunded regions. At last, we present numerical analyses to illustrate the manager's economic behaviors.",
                    "We propose a two-layer stochastic game model to study reinsurance contracting and competition in a market with one insurer and two competing reinsurers. The insurer negotiates with both reinsurers simultaneously for proportional reinsurance contracts that are priced using the variance premium principle. The reinsurance contracting between the insurer and each reinsurer is modeled as a Stackelberg game. The two reinsurers compete for business from the insurer and optimize the so-called relative performance, instead of their own surplus, and their competition is settled by a noncooperative Nash game. We obtain a sufficient and necessary condition, related to the competition degrees of the two reinsurers, for the existence of an equilibrium. We show that the equilibrium, if exists, is unique, and the equilibrium strategy of each player is constant, fully characterized in semiclosed form. Furthermore, we obtain interesting sensitivity results for the equilibrium strategies through both analytical and numerical studies.",
                    "Peierls-Boltzmann transport equation, coupled with third-order anharmonic lattice dynamics calculations, has been widely used to model lattice thermal conductivity ($\\kappa_{l}$) in bulk crystals. However, its application to materials with structural phase transition at relatively high temperature is fundamentally challenged by the presence of lattice instabilities (imaginary phonon modes). Additionally, its accuracy suffers from the absence of higher-than-third-order phonon scattering processes, which are important near/above the Debye temperature. In this letter, we present an effective scheme that combines temperature-induced anharmonic phonon renormalization and four-phonon scattering to resolve these two theoretical challenges. We apply this scheme to investigate the lattice dynamics and thermal transport properties of GeTe, which undergoes a second-order ferroelectric phase transition from rhombohedral $\\alpha$-GeTe to rocksalt $\\beta$-GeTe at about 700~K. Our results on the high-temperature phase $\\beta$-GeTe at 800~K confirm the stabilization of $\\beta$-GeTe by temperature effects. We find that considering only three-phonon scattering leads to significantly overestimated $\\kappa_{l}$ of 3.8~W/mK at 800~K, whereas including four-phonon scattering reduces $\\kappa_{l}$ to 1.7~W/mK, a value comparable with experiments. To explore the possibility to further suppress $\\kappa_{l}$, we show that alloying $\\beta$-GeTe with heavy cations such as Pb and Bi can effectively reduce $\\kappa_{l}$ to about 1.0~W/mK, whereas sample size needs to be around 10nm through nanostructuring to achieve a comparable reduction of $\\kappa_{l}$.",
                    "Materials based on cubic tetrahedrites (Cu$_{12}$Sb$_{4}$S$_{13}$) are useful thermoelectrics with unusual thermal and electrical transport properties, such as very low and nearly temperature-independent lattice thermal conductivity ($\\kappa_{L}$). We explain the microscopic origin of the glass-like $\\kappa_{L}$ in Cu$_{12}$Sb$_{4}$S$_{13}$ by explicitly treating anharmonicity up to quartic terms for both phonon energies and phonon scattering rates. We show that the strongly unstable phonon modes associated with trigonally coordinated Cu atoms are anharmonically stabilized above approximately $100$ K and continue hardening with increasing temperature, in accord with experimental data. This temperature induced hardening effect reduces scattering of heat carrying acoustic modes by reducing the available phase space for three-phonon processes, thereby balancing the conventional $\\propto T$ increase in scattering due to phonon population and yielding nearly temperature-independent $\\kappa_{L}$. Furthermore, we find that very strong phonon broadening lead to a qualitative breakdown of the conventional phonon-gas model and modify the dominant heat transport mechanism from the particle-like phonon wave packet propagation to incoherent tunneling described by off-diagonal terms in the heat-flux operator, which are typically prevailing in glasses and disordered crystals. Our work paves the way to a deeper understanding of glass-like thermal conductivity in complex crystals with strong anharmonicity."
                ],
                "domain": [
                    "Thermoelectrics",
                    "Lattice Dynamics",
                    "Quantum Sensing",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "3db04cb1-ce83-4a7c-8a8c-b991fce3dd13": {
                "pk": "3db04cb1-ce83-4a7c-8a8c-b991fce3dd13",
                "project_name": null,
                "name": "Bo Chen",
                "bio": "I am a researcher with a strong focus on representation theory, particularly in the context of quivers and Artin algebras. My recent work has delved into the intricate relationships between Gabriel-Roiter measures and the representation types of algebras, exploring the existence of infinitely many GR segments and their implications for wild representation types. I have investigated the properties of Kronecker quivers, demonstrating how the structure of these quivers influences the behavior of indecomposable modules and their associated measures.\n\nIn my studies, I have characterized tame quivers and examined the unique determination of Gabriel-Roiter measures through dimension vectors, particularly in regular components of Auslander-Reiten quivers. My research also extends to the development of efficient algorithms for solving optimal control problems involving $L^1$-cost functionals, utilizing advanced techniques like accelerated block coordinate descent.\n\nAdditionally, I have ventured into the realm of mobile computing, proposing a secure system for plausible deniable encryption that addresses the challenges posed by coercive attacks. My interdisciplinary approach combines theoretical insights with practical applications, aiming to bridge gaps between abstract algebraic concepts and real-world computational challenges.\n\nOverall, my work reflects a commitment to deepening the understanding of representation theory while also addressing contemporary issues in data security and optimization. I am passionate about exploring new avenues of research that can contribute to both theoretical advancements and practical solutions in these fields.",
                "collaborators": [
                    "Xiaoliang Song",
                    "Bo Yu",
                    "Le Sun",
                    "Xianpei Han",
                    "Bo An",
                    "Yuxiang Li",
                    "Youde Wang",
                    "Chong Song"
                ],
                "pub_titles": [
                    "The Gabriel-Roiter measures admitting no direct predecessors over $n$-Kronecker quivers",
                    "The Gabriel-Roiter measure for $\\widetilde{\\mathbb{A}}_n$ II",
                    "The Gabriel-Roiter measures of the indecomposables in a regular component of the 3-Kronecker quiver",
                    "The GR-segments for tame quivers",
                    "Dimension vectors in regular components over wild Kronecker quivers",
                    "The number of the Gabriel-Roiter measures admitting no direct predecessors over a wild quiver",
                    "Regular modules with preprojective Gabriel-Roiter submodules over $n$-Kronecker quivers",
                    "The Gabriel-Roiter measures and representation type",
                    "Towards Designing A Secure Plausibly Deniable System for Mobile Devices against Multi-snapshot Adversaries -- A Preliminary Design",
                    "Sentence Rewriting for Semantic Parsing",
                    "Mesh Independence of an Accelerated Block Coordinate Descent Method for Sparse Optimal Control Problems",
                    "Error Estimates for Sparse Optimal Control Problems by Piecewise Linear Finite Element Approximation",
                    "Huber's theorem for manifolds with $L^\\frac{n}{2}$ integrable Ricci curvatures",
                    "An efficient duality-based approach for PDE-constrained sparse optimization",
                    "Very regular solution to Landau-Lifshitz system with spin-polarized transport",
                    "Isolated Singularities of Yang-Mills-Higgs fields on surfaces"
                ],
                "pub_abstracts": [
                    "Let $Q$ be an $n$-Kronecker quiver, i.e., a quiver with two vertices, labeled by 1 and 2, and $n$ arrows from 2 to 1. We show that there are infinitely many Gabriel-Roiter measures admitting no direct predecessors.",
                    "Let $Q$ be a tame quiver of type $\\widetilde{\\mathbb{A}}_n$ and $\\Rep(Q)$ the category of finite dimensional representations over an algebraically closed field. A representation is simply called a module. It will be shown that a regular string module has, up to isomorphism, at most two Gabriel-Roiter submodules. The quivers $Q$ with sink-source orientations will be characterized as those, whose central parts do not contain preinjective modules. It will also be shown that there are only finitely many (central) Gabriel-Roiter measures admitting no direct predecessors. This fact will be generalized for all tame quivers.",
                    "Let $Q$ be the 3-Kronecker quiver, i.e., $Q$ has two vertices, labeled by 1 and 2, and three arrows from 2 to 1. Fix an algebraically closed field $k$. Let $\\mathcal{C}$ be a regular component of the Auslander-Reiten quiver containing an indecomposable module $X$ with dimension $(1,1)$ or $(2,1)$. Using the properties of the Fibonacci numbers, we show that the Gabriel-Roiter measures of the indecomposable modules in $\\mathcal{C}$ are uniquely determined by the dimension vectors. In other words, two indecomposable modules in $\\mathcal{C}$ are not isomorphic if and only if their Gabriel-Roiter measures are different.",
                    "A GR-segment for an artin algebra is a sequence of Gabriel-Roiter measures, which is closed under direct predecessors and successors. The number of the GR-segments indexed by natural numbers $\\mathbb{N}$ and integers $\\mathbb{Z}$ probably relates to the representation types of artin algebras. Let $k$ be an algebraically closed field and $Q$ be a tame quiver (of type $\\widetilde{\\mathbb{A}}_n$, $\\widetilde{\\mathbb{D}}_n$, $\\widetilde{\\mathbb{E}}_6$, $\\widetilde{\\mathbb{E}}_7$, or $\\widetilde{\\mathbb{E}}_8$). Let $b$ be the number of the isomorphism classes of the exceptional quasi-simple modules over the path algebra $\\Lambda=kQ$. We show that the number of the $\\mathbb{N}$- and $\\mathbb{Z}$-indexed GR-segments in the central part for $Q$ is bounded by $b+1$. Therefore, there are at most $b+3$ GR segments.",
                    "Let $\\mathcal{K}_n$ be the so-called wild Kronecker quiver, i.e., a quiver with one source and one sink and $n\\geq 3$ arrows from the source to the sink. The following problems will be studied for an arbitrary regular component $\\mathcal{C}$ of the Auslander-Reiten quiver: (1) What is the relationship between dimension vectors and quasi-lengths of the indecomposable regular representations in $\\mathcal{C}$? (2) For a given natural number $d$, is there an upper bound of the number of indecomposable representations in $\\mathcal{C}$ with the same length $d$? (3) When do the sets of the dimension vectors of indecomposable representations in different regular components coincide?",
                    "A famous result by Drozd says that a finite-dimensional representation-infinite algebra is of either tame or wild representation type. But one has to make assumption on the ground field. The Gabriel-Roiter measure might be an alternative approach to extend these concepts of tame and wild to arbitrary artin algebras. In particular, the infiniteness of the number of GR segments, i.e. sequences of Gabriel-Roiter measures which are closed under direct predecessors and successors, might relate to the wildness of artin algebras. As the first step, we are going to study the wild quiver with three vertices, labeled by $1$,$2$ and $3$, and one arrow from $1$ to $2$ and two arrows from $2$ to $3$. The Gabriel-Roiter submodules of the indecomposable preprojective modules and quasi-simple modules $\\tau^{-i}M$, $i\\geq 0$ are described, where $M$ is a Kronecker module and $\\tau=DTr$ is the Auslander-Reiten translation. Based on these calculations, the existence of infinitely many GR segments will be shown. Moreover, it will be proved that there are infinitely many Gabriel-Roiter measures admitting no direct predecessors.",
                    "Let $Q$ be a wild $n$-Kronecker quiver, i.e., a quiver with two vertices, labeled by 1 and 2, and $n\\geq 3$ arrows from 2 to 1. The indecomposable regular modules with preprojective Gabriel-Roiter submodules, in particular, those $\\tau^{-i}X$ with $\\udim X=(1,c)$ for $i\\geq 0$ and some $1\\leq c\\leq n-1$ will be studied. It will be shown that for each $i\\geq 0$ the irreducible monomorphisms starting with $\\tau^{-i}X$ give rise to a sequence of Gabriel-Roiter inclusions, and moreover, the Gabriel-Roiter measures of those produce a sequence of direct successors. In particular, there are infinitely many GR-segments, i.e., a sequence of Gabriel-Roiter measures closed under direct successors and predecessors. The case $n=3$ will be studied in detail with the help of Fibonacci numbers. It will be proved that for a regular component containing some indecomposable module with dimension vector $(1,1)$ or $(1,2)$, the Gabriel-Roiter measures of the indecomposable modules are uniquely determined by their dimension vectors.",
                    "Let $\\Lambda$ be an Artin algebra. A GR segment of $\\Lambda$ is a sequence of GR measures which is closed under direct successors and direct predecessors. The number of the GR segments was conjectured to relate to the representation type of $\\Lambda$. In this paper, let $k$ be an algebraically closed field and $\\Lambda$ be a finite-dimensional hereditary $k$-algebra. We show that $\\Lambda$ admits infinitely many GR segments if and only if $\\Lambda$ is of wild representation type. Thus the finiteness of the number of the GR segments might be an alternative characterization of the tameness of finite dimensional algebras over algebraically closed fields. Therefore, this might give a possibility to generalize Drozd's tameness and wildness to arbitrary Artin algebras.",
                    "Mobile computing devices have been used broadly to store, manage and process sensitive or even mission critical data. To protect confidentiality of data stored in mobile devices, major mobile operating systems use full disk encryption, which relies on traditional encryption mechanisms and requires that decryption keys will not be disclosed. This however, is not necessarily true, since an active attacker may coerce victims for decryption keys. Plausibly deniable encryption (PDE) can defend against such a coercive attacker by disguising the true secret key with a decoy key. Leveraging concept of PDE, various deniable storage systems have been built for both PC and mobile platforms. However, a secure PDE system for mobile devices is still missing which can be compatible with mainstream mobile devices and, meanwhile, remains secure when facing a strong multi-snapshot adversary. In this work, we propose a preliminary PDE system design for mobile computing devices using flash memory as underlying storage medium. Ours is the first secure PDE system for mobile devices which has the following new design features: 1) it is compatible with mainstream mobile devices due to its integration of PDE into flash translation layer (FTL), the most popular form of flash memory being used by modern mobile devices; and 2) it can defend against the multi-snapshot adversary by denying hidden writes (over the flash memory) caused by hidden sensitive data using random dummy writes.",
                    "A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontology. In this paper, we propose a sentence rewriting based semantic parsing method, which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form. Specifically, we propose two sentence-rewriting methods for two common types of mismatch: a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch. We evaluate our entence rewriting based semantic parser on the benchmark semantic parsing dataset -- WEBQUESTIONS. Experimental results show that our system outperforms the base system with a 3.4% gain in F1, and generates logical forms more accurately and parses sentences more robustly.",
                    "An accelerated block coordinate descent (ABCD) method in Hilbert space is analyzed to solve the sparse optimal control problem via its dual. The finite element approximation of this method is investigated and convergence results are presents. Based on the second order growth condition of the dual objective function, we show that iteration sequence of dual variables has the iteration complexity of $O(1/k)$. Moreover, we also prove iteration complexity for the primal problem. Two types of mesh-independence for ABCD method are proved, which asserts that asymptotically the infinite dimensional ABCD method and finite dimensional discretizations have the same convergence property, and the iterations of ABCD method remain nearly constant as the discretization is refined.",
                    "Optimization problems with $L^1$-control cost functional subject to an elliptic partial differential equation (PDE) are considered. However, different from the finite dimensional $l^1$-regularization optimization, the resulting discretized $L^1$-norm does not have a decoupled form when the standard piecewise linear finite element is employed to discretize the continuous problem. A common approach to overcome this difficulty is employing a nodal quadrature formula to approximately discretize the $L^1$-norm. It is inevitable that this technique will incur an additional error. Different from the traditional approach, a duality-based approach and an accelerated block coordinate descent (ABCD) method is introduced to solve this type of problem via its dual. Based on the discretized dual problem, a new discretized scheme for the $L^1$-norm is presented. Compared new discretized scheme for $L^1$-norm with the nodal quadrature formula, the advantages of our new discretized scheme can be demonstrated in terms of the approximation order. More importantly, finite element error estimates results for the primal problem with the new discretized scheme for the $L^1$-norm are provided, which confirm that this approximation scheme will not change the order of error estimates.",
                    "In this paper, we generalize Huber's finite point conformal compactification theorem to a higher dimensional manifold, which is conformally compact with $L^\\frac{n}{2}$ integrable Ricci curvatures.",
                    "In this paper, elliptic optimal control problems involving the $L^1$-control cost ($L^1$-EOCP) is considered. To numerically discretize $L^1$-EOCP, the standard piecewise linear finite element is employed. However, different from the finite dimensional $l^1$-regularization optimization, the resulting discrete $L^1$-norm does not have a decoupled form. A common approach to overcome this difficulty is employing a nodal quadrature formula to approximately discretize the $L^1$-norm. It is clear that this technique will incur an additional error. To avoid the additional error, solving $L^1$-EOCP via its dual, which can be reformulated as a multi-block unconstrained convex composite minimization problem, is considered. Motivated by the success of the accelerated block coordinate descent (ABCD) method for solving large scale convex minimization problems in finite dimensional space, we consider extending this method to $L^1$-EOCP. Hence, an efficient inexact ABCD method is introduced for solving $L^1$-EOCP. The design of this method combines an inexact 2-block majorized ABCD and the recent advances in the inexact symmetric Gauss-Seidel (sGS) technique for solving a multi-block convex composite quadratic programming whose objective contains a nonsmooth term involving only the first block. The proposed algorithm (called sGS-imABCD) is illustrated at two numerical examples. Numerical results not only confirm the finite element error estimates, but also show that our proposed algorithm is more efficient than (a) the ihADMM (inexact heterogeneous alternating direction method of multipliers), (b) the APG (accelerated proximal gradient) method.",
                    "In this paper, we provide a precise description of the compatibility conditions for the initial data so that one can show the existence and uniqueness of regular short-time solution to the Neumann initial-boundary problem of a class of Landau-Lifshitz-Gilbert system with spin-polarized transport, which is a strong nonlinear coupled parabolic system with non-local energy.",
                    "We study isolated singularities of two dimensional Yang-Mills-Higgs fields defined on a fiber bundle, where the fiber space is a compact Riemannian manifold and the structure group is a compact connected Lie group. In general the singularity can not be removed due to possibly non-vanishing limit holonomy around the singular points. We establish a sharp asymptotic decay estimate of the Yang-Mills-Higgs field near a singular point, where the decay rate is precisely determined by the limit holonomy. Our result can be viewed as a generalization of the classical removable singularity theorem of two dimensional harmonic maps."
                ],
                "domain": [
                    "Representation Theory",
                    "Algebra",
                    "Optimization",
                    "Mobile Computing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "1be95db5-7e05-439b-b96f-f7c4f7573cce": {
                "pk": "1be95db5-7e05-439b-b96f-f7c4f7573cce",
                "project_name": null,
                "name": "Hongshen Xu",
                "bio": "I am a researcher deeply engaged in the field of Natural Language Processing (NLP), with a particular focus on enhancing the capabilities of Large Language Models (LLMs). My recent work has centered around addressing the challenges posed by multilingual contexts and low-resource languages, exemplified by my development of the Multilingual Brain Surgeon (MBS) method, which improves model compression techniques by sampling calibration data proportionally to language distributions.\n\nI have also explored innovative approaches to prompt design in text-to-SQL tasks, introducing methods like ACT-SQL and CoE-SQL that enhance reasoning capabilities and streamline the query generation process. My research extends to improving LLM reliability through the Reinforcement Learning from Knowledge Feedback (RLKF) framework, which dynamically adjusts the model's knowledge boundaries to minimize hallucinations.\n\nAdditionally, I have contributed to the understanding of structural reading comprehension with the Topological Information Enhanced model (TIE) and have developed the REMEMBERER framework, which integrates long-term memory into LLMs for improved adaptability across tasks. My work on multimodal document understanding led to the creation of WebLM, a pre-training network that effectively models the interactions among text, structure, and image modalities.\n\nThrough these diverse projects, I aim to push the boundaries of what LLMs can achieve, making them more efficient, reliable, and capable of understanding complex, real-world data. My research not only addresses theoretical challenges but also seeks practical solutions that can be readily applied in various NLP applications.",
                "collaborators": [
                    "Lu Chen",
                    "Kai Yu",
                    "Ruisheng Cao",
                    "Hanchong Zhang",
                    "Da Ma",
                    "Zihan Zhao",
                    "Su Zhu",
                    "Zichen Zhu",
                    "Situo Zhang",
                    "Shuai Fan"
                ],
                "pub_titles": [
                    "Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind",
                    "ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought",
                    "CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions",
                    "Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback",
                    "TIE: Topological Information Enhanced Structural Reading Comprehension on Web Pages",
                    "Large Language Models Are Semi-Parametric Reinforcement Learning Agents",
                    "A BiRGAT Model for Multi-intent Spoken Language Understanding with Hierarchical Semantic Frames",
                    "Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding",
                    "ASTormer: An AST Structure-aware Transformer Decoder for Text-to-SQL",
                    "On the Structural Generalization in Text-to-SQL",
                    "Sparsity-Accelerated Training for Large Language Models"
                ],
                "pub_abstracts": [
                    "Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality. Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression. MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets. Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages. We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression. In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques.",
                    "Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn't need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.",
                    "Recently, Large Language Models (LLMs) have been demonstrated to possess impressive capabilities in a variety of domains and tasks. We investigate the issue of prompt design in the multi-turn text-to-SQL task and attempt to enhance the LLMs' reasoning capacity when generating SQL queries. In the conversational context, the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency. We introduce our method called CoE-SQL which can prompt LLMs to generate the SQL query based on the previously generated SQL query with an edition chain. We also conduct extensive ablation studies to determine the optimal configuration of our approach. Our approach outperforms different in-context learning baselines stably and achieves state-of-the-art performances on two benchmarks SParC and CoSQL using LLMs, which is also competitive to the SOTA fine-tuned models.",
                    "Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.",
                    "Recently, the structural reading comprehension (SRC) task on web pages has attracted increasing research interests. Although previous SRC work has leveraged extra information such as HTML tags or XPaths, the informative topology of web pages is not effectively exploited. In this work, we propose a Topological Information Enhanced model (TIE), which transforms the token-level task into a tag-level task by introducing a two-stage process (i.e. node locating and answer refining). Based on that, TIE integrates Graph Attention Network (GAT) and Pre-trained Language Model (PLM) to leverage the topological information of both logical structures and spatial structures. Experimental results demonstrate that our model outperforms strong baselines and achieves state-of-the-art performances on the web-based SRC benchmark WebSRC at the time of writing. The code of TIE will be publicly available at https://github.com/X-LANCE/TIE.",
                    "Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.",
                    "Previous work on spoken language understanding (SLU) mainly focuses on single-intent settings, where each input utterance merely contains one user intent. This configuration significantly limits the surface form of user utterances and the capacity of output semantics. In this work, we first propose a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue System, called MIVS. The target semantic frame is organized in a 3-layer hierarchical structure to tackle the alignment and assignment problems in multi-intent cases. Accordingly, we devise a BiRGAT model to encode the hierarchy of ontology items, the backbone of which is a dual relational graph attention network. Coupled with the 3-way pointer-generator decoder, our method outperforms traditional sequence labeling and classification-based schemes by a large margin.",
                    "The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. The pre-trained models and code are available at https://github.com/X-LANCE/weblm.",
                    "Text-to-SQL aims to generate an executable SQL program given the user utterance and the corresponding database schema. To ensure the well-formedness of output SQLs, one prominent approach adopts a grammar-based recurrent decoder to produce the equivalent SQL abstract syntax tree (AST). However, previous methods mainly utilize an RNN-series decoder, which 1) is time-consuming and inefficient and 2) introduces very few structure priors. In this work, we propose an AST structure-aware Transformer decoder (ASTormer) to replace traditional RNN cells. The structural knowledge, such as node types and positions in the tree, is seamlessly incorporated into the decoder via both absolute and relative position embeddings. Besides, the proposed framework is compatible with different traversing orders even considering adaptive node selection. Extensive experiments on five text-to-SQL benchmarks demonstrate the effectiveness and efficiency of our structured decoder compared to competitive baselines.",
                    "Exploring the generalization of a text-to-SQL parser is essential for a system to automatically adapt the real-world databases. Previous works provided investigations focusing on lexical diversity, including the influence of the synonym and perturbations in both natural language questions and databases. However, research on the structure variety of database schema~(DS) is deficient. Specifically, confronted with the same input question, the target SQL is probably represented in different ways when the DS comes to a different structure. In this work, we provide in-deep discussions about the structural generalization of text-to-SQL tasks. We observe that current datasets are too templated to study structural generalization. To collect eligible test data, we propose a framework to generate novel text-to-SQL data via automatic and synchronous (DS, SQL) pair altering. In the experiments, significant performance reduction when evaluating well-trained text-to-SQL models on the synthetic samples demonstrates the limitation of current research regarding structural generalization. According to comprehensive analysis, we suggest the practical reason is the overfitting of (NL, SQL) patterns.",
                    "Large language models (LLMs) have demonstrated proficiency across various natural language processing (NLP) tasks but often require additional training, such as continual pre-training and supervised fine-tuning. However, the costs associated with this, primarily due to their large parameter count, remain high. This paper proposes leveraging \\emph{sparsity} in pre-trained LLMs to expedite this training process. By observing sparsity in activated neurons during forward iterations, we identify the potential for computational speed-ups by excluding inactive neurons. We address associated challenges by extending existing neuron importance evaluation metrics and introducing a ladder omission rate scheduler. Our experiments on Llama-2 demonstrate that Sparsity-Accelerated Training (SAT) achieves comparable or superior performance to standard training while significantly accelerating the process. Specifically, SAT achieves a $45\\%$ throughput improvement in continual pre-training and saves $38\\%$ training time in supervised fine-tuning in practice. It offers a simple, hardware-agnostic, and easily deployable framework for additional LLM training. Our code is available at https://github.com/OpenDFM/SAT."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Large Language Models",
                    "Model Compression",
                    "Reinforcement Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ea7a3656-5ca2-48f6-9ac5-7f494ef70419": {
                "pk": "ea7a3656-5ca2-48f6-9ac5-7f494ef70419",
                "project_name": null,
                "name": "Zichen Zhu",
                "bio": "I am a researcher dedicated to advancing the fields of human-object interaction (HOI) detection, database systems, and conversational AI. My recent work focuses on improving the detection of rare HOI classes through the integration of human prior knowledge into existing models, resulting in notable enhancements in performance. I have also developed NOCAP, a near-optimal correlation-aware partitioning join algorithm that significantly outperforms traditional methods in handling skewed correlations in database joins.\n\nIn the realm of task-oriented dialogue systems, I introduced a GUI-based architecture that allows intelligent assistants to perform tasks directly on applications, bypassing the limitations of traditional API-based systems. My research extends to large language models (LLMs), where I explore the concept of model reliability and propose a novel framework, Reinforcement Learning from Knowledge Feedback (RLKF), to enhance their performance and minimize hallucinations.\n\nAdditionally, I am passionate about mental health and have created the Agent Mental Clinic (AMC), a self-improving conversational agent designed to assist in diagnosing depression through simulated dialogues. My work also includes the development of WebLM, a multimodal pre-training network that enhances document understanding by integrating various modalities.\n\nThrough my research, I aim to bridge the gap between theoretical advancements and practical applications, contributing to the development of systems that can effectively address real-world challenges across diverse domains.",
                "collaborators": [
                    "Lu Chen",
                    "Kai Yu",
                    "Manos Athanassoulis",
                    "Hongshen Xu",
                    "Da Ma",
                    "Shenyu Zhang",
                    "Qingquan Bao",
                    "Xiao Hu",
                    "Subhadeep Sarkar",
                    "Dimitris Staratzis"
                ],
                "pub_titles": [
                    "Rb-PaStaNet: A Few-Shot Human-Object Interaction Detection Based on Rules and Part States",
                    "NOCAP: Near-Optimal Correlation-Aware Partitioning Joins",
                    "Constructing and Analyzing the LSM Compaction Design Space (Updated Version)",
                    "META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI",
                    "Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback",
                    "Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory",
                    "Hierarchical Multimodal Pre-training for Visually Rich Webpage Understanding",
                    "Multi-Relational Algebra and Its Applications to Data Insights"
                ],
                "pub_abstracts": [
                    "Existing Human-Object Interaction (HOI) Detection approaches have achieved great progress on nonrare classes while rare HOI classes are still not well-detected. In this paper, we intend to apply human prior knowledge into the existing work. So we add human-labeled rules to PaStaNet and propose Rb-PaStaNet aimed at improving rare HOI classes detection. Our results show a certain improvement of the rare classes, while the non-rare classes and the overall improvement is more considerable.",
                    "Storage-based joins are still commonly used today because the memory budget does not always scale with the data size. One of the many join algorithms developed that has been widely deployed and proven to be efficient is the Hybrid Hash Join (HHJ), which is designed to exploit any available memory to maximize the data that is joined directly in memory. However, HHJ cannot fully exploit detailed knowledge of the join attribute correlation distribution.   In this paper, we show that given a correlation skew in the join attributes, HHJ partitions data in a suboptimal way. To do that, we derive the optimal partitioning using a new cost-based analysis of partitioning-based joins that is tailored for primary key - foreign key (PK-FK) joins, one of the most common join types. This optimal partitioning strategy has a high memory cost, thus, we further derive an approximate algorithm that has tunable memory cost and leads to near-optimal results. Our algorithm, termed NOCAP (Near-Optimal Correlation-Aware Partitioning) join, outperforms the state-of-the-art for skewed correlations by up to $30\\%$, and the textbook Grace Hash Join by up to $4\\times$. Further, for a limited memory budget, NOCAP outperforms HHJ by up to $10\\%$, even for uniform correlation. Overall, NOCAP dominates state-of-the-art algorithms and mimics the best algorithm for a memory budget varying from below $\\sqrt{\\|\\text{relation}\\|}$ to more than $\\|\\text{relation}\\|$.",
                    "Log-structured merge (LSM) trees offer efficient ingestion by appending incoming data, and thus, are widely used as the storage layer of production NoSQL data stores. To enable competitive read performance, LSM-trees periodically re-organize data to form a tree with levels of exponentially increasing capacity, through iterative compactions. Compactions fundamentally influence the performance of an LSM-engine in terms of write amplification, write throughput, point and range lookup performance, space amplification, and delete performance. Hence, choosing the appropriate compaction strategy is crucial and, at the same time, hard as the LSM-compaction design space is vast, largely unexplored, and has not been formally defined in the literature. As a result, most LSM-based engines use a fixed compaction strategy, typically hand-picked by an engineer, which decides how and when to compact data.   In this paper, we present the design space of LSM-compactions, and evaluate state-of-the-art compaction strategies with respect to key performance metrics. Toward this goal, our first contribution is to introduce a set of four design primitives that can formally define any compaction strategy: (i) the compaction trigger, (ii) the data layout, (iii) the compaction granularity, and (iv) the data movement policy. Together, these primitives can synthesize both existing and completely new compaction strategies. Our second contribution is to experimentally analyze 10 compaction strategies. We present 12 observations and 7 high-level takeaway messages, which show how LSM systems can navigate the compaction design space.",
                    "Task-oriented dialogue (TOD) systems have been widely used by mobile phone intelligent assistants to accomplish tasks such as calendar scheduling or hotel reservation. Current TOD systems usually focus on multi-turn text/speech interaction, then they would call back-end APIs designed for TODs to perform the task. However, this API-based architecture greatly limits the information-searching capability of intelligent assistants and may even lead to task failure if TOD-specific APIs are not available or the task is too complicated to be executed by the provided APIs. In this paper, we propose a new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A GUI-TOD system can directly perform GUI operations on real APPs and execute tasks without invoking TOD-specific backend APIs. Furthermore, we release META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile GUI. We also propose a multi-model action prediction and response model, which show promising results on META-GUI. The dataset, codes and leaderboard are publicly available.",
                    "Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and trains a reliable reward model to encourage the refusal of out-of-knowledge questions. Experimental results on mathematical questions affirm the substantial efficacy of RLKF in significantly enhancing LLM reliability.",
                    "Mental health issues, particularly depressive disorders, present significant challenges in contemporary society, necessitating the development of effective automated diagnostic methods. This paper introduces the Agent Mental Clinic (AMC), a self-improving conversational agent system designed to enhance depression diagnosis through simulated dialogues between patient and psychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we design a psychiatrist agent consisting of a tertiary memory structure, a dialogue control and reflect plugin that acts as ``supervisor'' and a memory sampling module, fully leveraging the skills reflected by the psychiatrist agent, achieving great accuracy on depression risk and suicide risk diagnosis via conversation. Experiment results on datasets collected in real-life scenarios demonstrate that the system, simulating the procedure of training psychiatrists, can be a promising optimization method for aligning LLMs with real-life distribution in specific domains without modifying the weights of LLMs, even when only a few representative labeled cases are available.",
                    "The growing prevalence of visually rich documents, such as webpages and scanned/digital-born documents (images, PDFs, etc.), has led to increased interest in automatic document understanding and information extraction across academia and industry. Although various document modalities, including image, text, layout, and structure, facilitate human information retrieval, the interconnected nature of these modalities presents challenges for neural networks. In this paper, we introduce WebLM, a multimodal pre-training network designed to address the limitations of solely modeling text and structure modalities of HTML in webpages. Instead of processing document images as unified natural images, WebLM integrates the hierarchical structure of document images to enhance the understanding of markup-language-based documents. Additionally, we propose several pre-training tasks to model the interaction among text, structure, and image modalities effectively. Empirical results demonstrate that the pre-trained WebLM significantly surpasses previous state-of-the-art pre-trained models across several webpage understanding tasks. The pre-trained models and code are available at https://github.com/X-LANCE/weblm.",
                    "A range of data insight analytical tasks involves analyzing a large set of tables of different schemas, possibly induced by various groupings, to find salient patterns. This paper presents Multi-Relational Algebra, an extension of the classic Relational Algebra, to facilitate such transformations and their compositions. Multi-Relational Algebra has two main characteristics: (1) Information Unit. The information unit is a slice $(r, X)$, where $r$ is a (region) tuple, and $X$ is a (feature) table. Specifically, a slice can encompass multiple columns, which surpasses the information unit of \"a single tuple\" or \"a group of tuples of one column\" in the classic relational algebra, (2) Schema Flexibility. Slices can have varying schemas, not constrained to a single schema. This flexibility further expands the expressive power of the algebra. Through various examples, we show that multi-relational algebra can effortlessly express many complex analytic problems, some of which are beyond the scope of traditional relational analytics. We have implemented and deployed a service for multi-relational analytics. Due to a unified logical design, we are able to conduct systematic optimization for a variety of seemingly different tasks. Our service has garnered interest from numerous internal teams who have developed data-insight applications using it, and serves millions of operators daily."
                ],
                "domain": [
                    "Human-Object Interaction",
                    "Database Systems",
                    "Task-Oriented Dialogue",
                    "Multimodal Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "d4d74a4e-c3a5-4bb2-b51f-33ee2a0efe99": {
                "pk": "d4d74a4e-c3a5-4bb2-b51f-33ee2a0efe99",
                "project_name": null,
                "name": "Su Zhu",
                "bio": "I am a researcher dedicated to enhancing language understanding (LU) through innovative approaches in transfer learning and deep learning architectures. My recent work focuses on addressing the challenges posed by concept definition differences in LU adaptation, which can lead to data sparsity despite semantic correlations between datasets. I developed a novel concept transfer learning approach that investigates substructures within literal concept definitions, resulting in a hierarchical semantic representation that effectively captures the relationships between concepts.\n\nIn my exploration of sequence labeling for spoken language understanding, I introduced a Bidirectional Long Short Term Memory - Long Short Term Memory (BLSTM-LSTM) model with a novel focus mechanism. This advancement allows for better alignment in sequence tasks, significantly improving performance on standard benchmarks like ATIS, where we achieved a state-of-the-art F1-score of 96.08%. My research not only demonstrates the effectiveness of these methods but also highlights their robustness against speech recognition errors, paving the way for more reliable language understanding systems. I am passionate about pushing the boundaries of what is possible in LU and am committed to developing solutions that bridge the gap between theoretical advancements and practical applications.",
                "collaborators": [
                    "Kai Yu"
                ],
                "pub_titles": [
                    "Concept Transfer Learning for Adaptive Language Understanding",
                    "Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken Language Understanding"
                ],
                "pub_abstracts": [
                    "Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of {\\em atomic concepts}. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2\\&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance ($F_1$-score 96.08\\%) on ATIS by only using lexicon features.",
                    "This paper investigates the framework of encoder-decoder with attention for sequence labelling based spoken language understanding. We introduce Bidirectional Long Short Term Memory - Long Short Term Memory networks (BLSTM-LSTM) as the encoder-decoder model to fully utilize the power of deep learning. In the sequence labelling task, the input and output sequences are aligned word by word, while the attention mechanism cannot provide the exact alignment. To address this limitation, we propose a novel focus mechanism for encoder-decoder framework. Experiments on the standard ATIS dataset showed that BLSTM-LSTM with focus mechanism defined the new state-of-the-art by outperforming standard BLSTM and attention based encoder-decoder. Further experiments also show that the proposed model is more robust to speech recognition errors."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Transfer Learning",
                    "Speech Recognition"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "13802c53-dc0c-4fff-9fcd-ef12f0a69e66": {
                "pk": "13802c53-dc0c-4fff-9fcd-ef12f0a69e66",
                "project_name": null,
                "name": "Shuai Fan",
                "bio": "I am a researcher specializing in wireless multi-hop networks, with a particular focus on link scheduling. My work diverges from traditional approaches that primarily utilize the protocol interference model, which only accounts for packet collisions. Instead, I employ the physical interference model, which provides a more accurate representation of real-world scenarios by considering the aggregated signal to interference and noise ratio (SINR). \n\nIn my research, I developed a centralized scheduling method based on Integer Linear Programming (ILP), which I enhanced with an approximate solution using randomized rounding techniques. This approach not only offers a probability bound for achieving a guaranteed approximate factor but also lays the groundwork for extending the centralized algorithm into a distributed solution. This distributed method is particularly advantageous for wireless networks, as it ensures that all links can transmit without interference while maintaining a defined approximate ratio.\n\nThrough my work, I aim to contribute to the optimization of wireless communication systems, enhancing their efficiency and reliability in increasingly complex environments.",
                "collaborators": [
                    "Lin Zhang",
                    "Yong Ren"
                ],
                "pub_titles": [
                    "Approximation Algorithms for Link Scheduling with Physical Interference Model in Wireless Multi-hop Networks"
                ],
                "pub_abstracts": [
                    "The link scheduling in wireless multi-hop networks is addressed. Different from most of work that adopt the protocol interference model which merely take consideration of packet collisions, our proposed algorithms use the physical interference model to reflect the aggregated signal to interference and noise ratio (SINR), which is a more accurate abstraction of the real scenario. We first propose a centralized scheduling method based on the Integer Linear Programming (ILP) and resolve it by an approximate solution based on the randomized rounding method. The probability bound of getting a guaranteed approximate factor is given. We then extend the centralized algorithm to a distributed solution, which is favorable in wireless networks. It is proven that with the distributed scheduling method, all links can transmit without interference, and the approximate ratio of the algorithm is also given."
                ],
                "domain": [
                    "Wireless Networks",
                    "Link Scheduling",
                    "Optimization",
                    "Interference Model"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f61f9cb2-9393-47a8-9e61-8fe1a1b7c54f": {
                "pk": "f61f9cb2-9393-47a8-9e61-8fe1a1b7c54f",
                "project_name": null,
                "name": "Guodong Shen",
                "bio": "I am a researcher specializing in video anomaly detection, a field that presents unique challenges due to the scarcity and unpredictability of anomalies. My recent work focuses on developing innovative frameworks that enhance the detection of these elusive events. One of my notable contributions is a Convolutional LSTM Auto-Encoder prediction framework that leverages bi-directionality and a higher-order mechanism to improve spatio-temporal memory exchange. This approach not only captures temporal regularities through forward and backward predictions but also strengthens the interaction of spatial information between the encoder and decoder. \n\nAdditionally, I have explored online anomaly detection methods that eliminate the need for offline training. By utilizing a randomly-initialized multilayer perceptron optimized in real-time, my solution reconstructs video frames pixel-by-pixel based on frequency information. This incremental learning approach allows for the detection of anomalies throughout the video stream, overcoming the limitations of traditional methods that only function effectively with a few abnormal frames.\n\nThrough rigorous evaluations on popular benchmarks, my frameworks have consistently outperformed existing prediction-based anomaly detection methods, demonstrating my commitment to advancing the state of the art in this critical area of research.",
                "collaborators": [
                    "Yuqi Ouyang",
                    "Victor Sanchez"
                ],
                "pub_titles": [
                    "Video Anomaly Detection via Prediction Network with Enhanced Spatio-Temporal Memory Exchange",
                    "Look at Adjacent Frames: Video Anomaly Detection without Offline Training"
                ],
                "pub_abstracts": [
                    "Video anomaly detection is a challenging task because most anomalies are scarce and non-deterministic. Many approaches investigate the reconstruction difference between normal and abnormal patterns, but neglect that anomalies do not necessarily correspond to large reconstruction errors. To address this issue, we design a Convolutional LSTM Auto-Encoder prediction framework with enhanced spatio-temporal memory exchange using bi-directionalilty and a higher-order mechanism. The bi-directional structure promotes learning the temporal regularity through forward and backward predictions. The unique higher-order mechanism further strengthens spatial information interaction between the encoder and the decoder. Considering the limited receptive fields in Convolutional LSTMs, we also introduce an attention module to highlight informative features for prediction. Anomalies are eventually identified by comparing the frames with their corresponding predictions. Evaluations on three popular benchmarks show that our framework outperforms most existing prediction-based anomaly detection methods.",
                    "We propose a solution to detect anomalous events in videos without the need to train a model offline. Specifically, our solution is based on a randomly-initialized multilayer perceptron that is optimized online to reconstruct video frames, pixel-by-pixel, from their frequency information. Based on the information shifts between adjacent frames, an incremental learner is used to update parameters of the multilayer perceptron after observing each frame, thus allowing to detect anomalous events along the video stream. Traditional solutions that require no offline training are limited to operating on videos with only a few abnormal frames. Our solution breaks this limit and achieves strong performance on benchmark datasets."
                ],
                "domain": [
                    "Video Anomaly Detection",
                    "Deep Learning",
                    "Convolutional Neural Networks",
                    "Online Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "65663042-bf38-4302-a0c8-74d6da5849c3": {
                "pk": "65663042-bf38-4302-a0c8-74d6da5849c3",
                "project_name": null,
                "name": "Kai Yu",
                "bio": "I am a researcher specializing in dialogue state tracking (DST), natural language understanding (LU), and speech synthesis. My work has focused on bridging the gap between rule-based and statistical approaches in DST, exemplified by my development of the Recurrent Polynomial Network (RPN), which combines the efficiency of constrained Markov Bayesian polynomial frameworks with the advantages of statistical methods. I have also explored concept transfer learning to enhance language understanding adaptation, achieving state-of-the-art performance on benchmarks like ATIS.\n\nIn the realm of speech synthesis, I have pioneered methods to generate natural speech with diverse prosody patterns using Gaussian Mixture Models (GMM-MDN). This approach not only improves the naturalness of synthetic speech but also enhances prosody diversity, a critical aspect of human-like speech generation. My research extends to semi-supervised learning on high-dimensional manifolds, where I have developed techniques that transform complex nonlinear learning problems into simpler linear ones.\n\nAdditionally, I have contributed to advancements in sound event detection and speaker diarization, proposing new metrics like Balanced Error Rate (BER) to evaluate performance more comprehensively. My recent work on code-switching automatic speech recognition (ASR) leverages unsupervised monolingual data to enhance performance in scenarios with limited code-switching data, demonstrating the effectiveness of innovative training paradigms.\n\nOverall, my research aims to push the boundaries of how we understand and generate human language and speech, with a strong emphasis on practical applications and real-world challenges.",
                "collaborators": [
                    "Su Zhu",
                    "Chenpeng Du",
                    "Kai Sun",
                    "Qizhe Xie",
                    "Tong Zhang",
                    "Heinrich Dinkel",
                    "Tao Liu",
                    "Tianxing He",
                    "Yu Zhang",
                    "Jasha Droppo"
                ],
                "pub_titles": [
                    "Recurrent Polynomial Network for Dialogue State Tracking",
                    "Concept Transfer Learning for Adaptive Language Understanding",
                    "Rich Prosody Diversity Modelling with Phone-level Mixture Density Network",
                    "High Dimensional Nonlinear Learning using Local Coordinate Coding",
                    "Encoder-decoder with Focus-mechanism for Sequence Labelling Based Spoken Language Understanding",
                    "Duration robust weakly supervised sound event detection",
                    "BER: Balanced Error Rate For Speaker Diarization",
                    "Phone-Level Prosody Modelling with GMM-Based MDN for Diverse and Controllable Speech Synthesis",
                    "On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation",
                    "Semi-supervised Learning for Code-Switching ASR with Large Language Model Filter"
                ],
                "pub_abstracts": [
                    "Dialogue state tracking (DST) is a process to estimate the distribution of the dialogue states as a dialogue progresses. Recent studies on constrained Markov Bayesian polynomial (CMBP) framework take the first step towards bridging the gap between rule-based and statistical approaches for DST. In this paper, the gap is further bridged by a novel framework -- recurrent polynomial network (RPN). RPN's unique structure enables the framework to have all the advantages of CMBP including efficiency, portability and interpretability. Additionally, RPN achieves more properties of statistical approaches than CMBP. RPN was evaluated on the data corpora of the second and the third Dialog State Tracking Challenge (DSTC-2/3). Experiments showed that RPN can significantly outperform both traditional rule-based approaches and statistical approaches with similar feature set. Compared with the state-of-the-art statistical DST approaches with a lot richer features, RPN is also competitive.",
                    "Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated. To address this issue, in this paper, a novel concept transfer learning approach is proposed. Here, substructures within literal concept definition are investigated to reveal the relationship between concepts. A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of {\\em atomic concepts}. Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU. The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2\\&3. Thorough empirical studies validate both the efficiency and effectiveness of the proposed method. In particular, we achieve state-of-the-art performance ($F_1$-score 96.08\\%) on ATIS by only using lexicon features.",
                    "Generating natural speech with diverse and smooth prosody pattern is a challenging task. Although random sampling with phone-level prosody distribution has been investigated to generate different prosody patterns, the diversity of the generated speech is still very limited and far from what can be achieved by human. This is largely due to the use of uni-modal distribution, such as single Gaussian, in the prior works of phone-level prosody modelling. In this work, we propose a novel approach that models phone-level prosodies with GMM based mixture density network (GMM-MDN). Experiments on the LJSpeech dataset demonstrate that phone-level prosodies can precisely control the synthetic speech and GMM-MDN can generate more natural and smooth prosody pattern than a single Gaussian. Subjective evaluations further show that the proposed approach not only achieves better naturalness, but also significantly improves the prosody diversity in synthetic speech without the need of manual control.",
                    "This paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning. The learned bases provide a set of anchor points to form a local coordinate system, such that each data point $x$ on the manifold can be locally approximated by a linear combination of its nearby anchor points, with the linear weights offering a local-coordinate coding of $x$. We show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding. The method turns a difficult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods. The work also gives a theoretical justification to the empirical success of some biologically-inspired models using sparse coding of sensory data, since a local coding scheme must be sufficiently sparse. However, sparsity does not always satisfy locality conditions, and can thus possibly lead to suboptimal results. The properties and performances of the method are empirically verified on synthetic data, handwritten digit classification, and object recognition tasks.",
                    "This paper investigates the framework of encoder-decoder with attention for sequence labelling based spoken language understanding. We introduce Bidirectional Long Short Term Memory - Long Short Term Memory networks (BLSTM-LSTM) as the encoder-decoder model to fully utilize the power of deep learning. In the sequence labelling task, the input and output sequences are aligned word by word, while the attention mechanism cannot provide the exact alignment. To address this limitation, we propose a novel focus mechanism for encoder-decoder framework. Experiments on the standard ATIS dataset showed that BLSTM-LSTM with focus mechanism defined the new state-of-the-art by outperforming standard BLSTM and attention based encoder-decoder. Further experiments also show that the proposed model is more robust to speech recognition errors.",
                    "Task 4 of the DCASE2018 challenge demonstrated that substantially more research is needed for a real-world application of sound event detection. Analyzing the challenge results it can be seen that most successful models are biased towards predicting long (e.g., over 5s) clips. This work aims to investigate the performance impact of fixed-sized window median filter post-processing and advocate the use of double thresholding as a more robust and predictable post-processing method. Further, four different temporal subsampling methods within the CRNN framework are proposed: mean-max, alpha-mean-max, Lp-norm and convolutional. We show that for this task subsampling the temporal resolution by a neural network enhances the F1 score as well as its robustness towards short, sporadic sound events. Our best single model achieves 30.1% F1 on the evaluation set and the best fusion model 32.5%, while being robust to event length variations.",
                    "DER is the primary metric to evaluate diarization performance while facing a dilemma: the errors in short utterances or segments tend to be overwhelmed by longer ones. Short segments, e.g., `yes' or `no,' still have semantic information. Besides, DER overlooks errors in less-talked speakers. Although JER balances speaker errors, it still suffers from the same dilemma. Considering all those aspects, duration error, segment error, and speaker-weighted error constituting a complete diarization evaluation, we propose a Balanced Error Rate (BER) to evaluate speaker diarization. First, we propose a segment-level error rate (SER) via connected sub-graphs and adaptive IoU threshold to get accurate segment matching. Second, to evaluate diarization in a unified way, we adopt a speaker-specific harmonic mean between duration and segment, followed by a speaker-weighted average. Third, we analyze our metric via the modularized system, EEND, and the multi-modal method on real datasets. SER and BER are publicly available at https://github.com/X-LANCE/BER.",
                    "Generating natural speech with a diverse and smooth prosody pattern is a challenging task. Although random sampling with phone-level prosody distribution has been investigated to generate different prosody patterns, the diversity of the generated speech is still very limited and far from what can be achieved by humans. This is largely due to the use of uni-modal distribution, such as single Gaussian, in the prior works of phone-level prosody modelling. In this work, we propose a novel approach that models phone-level prosodies with a GMM-based mixture density network(MDN) and then extend it for multi-speaker TTS using speaker adaptation transforms of Gaussian means and variances. Furthermore, we show that we can clone the prosodies from a reference speech by sampling prosodies from the Gaussian components that produce the reference prosodies. Our experiments on LJSpeech and LibriTTS dataset show that the proposed method with GMM-based MDN not only achieves significantly better diversity than using a single Gaussian in both single-speaker and multi-speaker TTS, but also provides better naturalness. The prosody cloning experiments demonstrate that the prosody similarity of the proposed method with GMM-based MDN is comparable to recent proposed fine-grained VAE while the target speaker similarity is better.",
                    "We propose to train bi-directional neural network language model(NNLM) with noise contrastive estimation(NCE). Experiments are conducted on a rescore task on the PTB data set. It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training. But still(regretfully), it did not out-perform the baseline uni-directional NNLM.",
                    "Code-switching (CS) phenomenon occurs when words or phrases from different languages are alternated in a single sentence. Due to data scarcity, building an effective CS Automatic Speech Recognition (ASR) system remains challenging. In this paper, we propose to enhance CS-ASR systems by utilizing rich unsupervised monolingual speech data within a semi-supervised learning framework, particularly when access to CS data is limited. To achieve this, we establish a general paradigm for applying noisy student training (NST) to the CS-ASR task. Specifically, we introduce the LLM-Filter, which leverages well-designed prompt templates to activate the correction capability of large language models (LLMs) for monolingual data selection and pseudo-labels refinement during NST. Our experiments on the supervised ASRU-CS and unsupervised AISHELL-2 and LibriSpeech datasets show that our method not only achieves significant improvements over supervised and semi-supervised learning baselines for the CS task, but also attains better performance compared with the fully-supervised oracle upper-bound on the CS English part. Additionally, we further investigate the influence of accent on AESRC dataset and demonstrate that our method can get achieve additional benefits when the monolingual data contains relevant linguistic characteristic."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Speech Recognition",
                    "Machine Learning",
                    "Transfer Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "79ea1793-b628-4f28-bda3-4235ce20d135": {
                "pk": "79ea1793-b628-4f28-bda3-4235ce20d135",
                "project_name": null,
                "name": "Xin Chen",
                "bio": "I am a researcher with a diverse background in mathematics, physics, and computer science, focusing on the intersection of combinatorial optimization, quantum systems, and machine learning. My recent work has explored a range of topics, from the representation of large integers and the complexities of genetic map linearization to the intricacies of quantum path interferences in nanoscale systems. \n\nI have developed approximation algorithms for the minimum breakpoint linearization problem, addressing the challenges posed by unsigned genetic maps, and proposed innovative frameworks for carbon-aware demand response in power systems, emphasizing the integration of electric vehicles into the grid. My research also delves into explainable artificial intelligence (XAI), where I aim to bridge the gap between global and local explanations through cohort-based insights.\n\nIn the realm of quantum mechanics, I am particularly interested in the effects of functional quantum noise and have established frameworks for understanding the stochastic behaviors of open multilevel quantum systems. My work on secure multi-party computation has led to the development of robust frameworks for secure auctions, ensuring privacy and integrity in malicious environments.\n\nOverall, my research is driven by a desire to tackle complex problems through interdisciplinary approaches, combining theoretical rigor with practical applications. I am committed to advancing our understanding of these fields and contributing to the development of innovative solutions that address real-world challenges.",
                "collaborators": [
                    "Wenjie Ye",
                    "Fanyu Meng",
                    "Xin Liu",
                    "Zhaodan Kong",
                    "Jane Wang",
                    "Zhili Chen"
                ],
                "pub_titles": [
                    "On pairs of one prime, four prime cubes and powers of 2",
                    "Prospects of LHC Higgs Physics at the end of Run III - Talk presented at the International Workshop on Future Linear Colliders (LCWS2016), Morioka, Japan, 5-9 December 2016. C16-12-05.4",
                    "Approximating the Minimum Breakpoint Linearization Problem for Genetic Maps without Gene Strandedness",
                    "Prospects for Higgs CP property measurements at the LHC - Talk presented at the International Workshop on Future Linear Colliders (LCWS2016), Morioka, Japan, 5-9 December 2016. C16-12-05.4",
                    "Enhance Low-Carbon Power System Operation via Carbon-Aware Demand Response",
                    "SOC-Boundary and Battery Aging Aware Hierarchical Coordination of Multiple EV Aggregates Among Multi-stakeholders with Multi-Agent Constrained Deep Reinforcement Learning",
                    "Quantum Path Interference through Incoherent Motions in Multilevel Quantum Systems",
                    "The Rigorous Stochastic Matrix Multiplication Scheme for the Calculations of Reduced Equilibrium Density Matrices of Open Multilevel Quantum Systems",
                    "CohEx: A Generalized Framework for Cohort Explanation",
                    "The Super Catalan Numbers $S(m, m + s)$ for $s \\leq 4$",
                    "Secure Computation Framework for Multiple Data Providers Against Malicious Adversaries",
                    "A study of backward stochastic differential equation on a Riemannian manifold",
                    "A probabilistic representation for heat flow of harmonic map on manifolds with time-dependent Riemannian metric"
                ],
                "pub_abstracts": [
                    "In this paper, we consider the simultaneous representation of pairs of sufficiently large integers. We prove that every pair of large positive odd integers can be represented in the form of a pair of one prime, four cubes of primes and 231 powers of 2.",
                    "The document is prepared for the LCWS2016 conference proceedings. The expected status of Higgs physics at the end of Run-3 is presented. The current Run-2 status is briefly reviewed, and the expected Higgs reach after the HL-LHC period is also summarized for some channels.",
                    "The study of genetic map linearization leads to a combinatorial hard problem, called the {\\em minimum breakpoint linearization} (MBL) problem. It is aimed at finding a linearization of a partial order which attains the minimum breakpoint distance to a reference total order. The approximation algorithms previously developed for the MBL problem are only applicable to genetic maps in which genes or markers are represented as signed integers. However, current genetic mapping techniques generally do not specify gene strandedness so that genes can only be represented as unsigned integers. In this paper, we study the MBL problem in the latter more realistic case. An approximation algorithm is thus developed, which achieves a ratio of $(m^2+2m-1)$ and runs in $O(n^7)$ time, where $m$ is the number of genetic maps used to construct the input partial order and $n$ the total number of distinct genes in these maps.",
                    "This document is prepared for the LCWS2016 conference proceedings. It reviews the current results on the Higgs CP property measurements from both ATLAS and CMS experiments in the Higgs to diboson decays, and in the Vector Boson Fusion production of Higgs via the ditau decay channel. The projected sensitivity of the ditau signal in HL-LHC is briefly discussed. Finally, it gives the prospects for the CP measurement in the Higgs to ditau decay with the tau substructure developments at both collaborations.",
                    "As the electrification process advances, enormous power flexibility is becoming available on the demand side, which can be harnessed to facilitate power system decarbonization. Hence, this paper studies the carbon-aware demand response (C-DR) paradigm, where individual users aim to minimize their carbon footprints through the optimal scheduling of flexible load devices. The specific operational dynamics and constraints of deferrable loads and thermostatically controlled loads are considered, and the carbon emission flow method is employed to determine users' carbon footprints using nodal carbon intensities. Then, an optimal power dispatch model that integrates the C-DR mechanism is proposed for low-carbon power system operation, based on the carbon-aware optimal power flow (C-OPF) method. Two solution algorithms, including a centralized Karush-Kuhn-Tucker (KKT) reformulation algorithm and an iterative solution algorithm, are developed to solve the bi-level power dispatch optimization model. Numerical simulations on the IEEE New England 39-bus system demonstrate the effectiveness of the proposed methods.",
                    "As electric vehicles (EV) become more prevalent and advances in electric vehicle electronics continue, vehicle-to-grid (V2G) techniques and large-scale scheduling strategies are increasingly important to promote renewable energy utilization and enhance the stability of the power grid. This study proposes a hierarchical multistakeholder V2G coordination strategy based on safe multi-agent constrained deep reinforcement learning (MCDRL) and the Proof-of-Stake algorithm to optimize benefits for all stakeholders, including the distribution system operator (DSO), electric vehicle aggregators (EVAs) and EV users. For DSO, the strategy addresses load fluctuations and the integration of renewable energy. For EVAs, energy constraints and charging costs are considered. The three critical parameters of battery conditioning, state of charge (SOC), state of power (SOP), and state of health (SOH), are crucial to the participation of EVs in V2G. Hierarchical multi-stakeholder V2G coordination significantly enhances the integration of renewable energy, mitigates load fluctuations, meets the energy demands of the EVAs, and reduces charging costs and battery degradation simultaneously.",
                    "Quantum path interferences or resonances in multilevel dissipative quantum systems play an important and intriguing role in the transport processes of nanoscale systems. Many previous minimalistic models used to describe the quantum path interference driven by incoherent fields are based on the approximations including the second order perturbation for the weak coupling limit, the ad-hoc choices of two-time correlation functions and $\\it{etc}$. On the other hand, the similar model to study the non-adiabatic molecular electronic excitation have been extensively developed and many efficient quantum molecular dynamics simulation schemes, such as the Ehrenfest scheme, have been proposed.   In this paper, I aim to propose an unified model, extend the Ehrenfest scheme to study the interactions of system-light and system-phonon simultaneously and gain insight into and principles of the roles of quantum path interferences in the realistic molecular systems. I discuss how to derive the time-dependent stochastic Schr$\\ddot{o}$dinger equation from the Ehrenfest scheme as a foundation to discuss the detailed balance for the weak coupling limit and therefore the quantum correction in the Ehrenfest scheme. Different from the master equation technique, the Ehrenfest scheme doesn't need any specific assumptions about spectral densities and two time correlation functions. With simple open two-level and three-level quantum systems, I show the effect of the quantum path interference on the steady state populations. Currently I only focus on the role of the phonon thermal reservoir. The electromagnetic field (solar light) will be modeled as a thermal reservoir and discussed in detail in the future paper.",
                    "Understanding the roles of the temporary and spatial structures of quantum functional noise in open multilevel quantum molecular systems attracts a lot of theoretical interests. I want to establish a rigorous and general framework for functional quantum noises from the constructive and computational perspectives, $\\it{i.e.}$ how to generate the random trajectories to reproduce the kernel and path ordering of the influence functional with effective Monte Carlo methods for arbitrary spectral densities. This construction approach aims to unify the existing stochastic models to rigorously describe the temporary and spatial structure of Gaussian quantum noises. In this paper, I review the Euclidean imaginary time influence functional and propose the stochastic matrix multiplication scheme to calculate reduced equilibrium density matrices (REDM).   In addition, I review and discuss the Feynman-Vernom influence functional according to the Gaussian quadratic integral, particularly its imaginary part which is critical to the rigorous description of the quantum detailed balance. As a result, I establish the conditions under which the influence functional can be interpreted as the average of exponential functional operator over real-valued Gaussian processes for open multilevel quantum systems. I also show the difference between the local and nonlocal phonons within this framework. With the stochastic matrix multiplication scheme, I compare the normalized REDM with the Boltzmann equilibrium distribution for open multilevel quantum systems.",
                    "eXplainable Artificial Intelligence (XAI) has garnered significant attention for enhancing transparency and trust in machine learning models. However, the scopes of most existing explanation techniques focus either on offering a holistic view of the explainee model (global explanation) or on individual instances (local explanation), while the middle ground, i.e., cohort-based explanation, is less explored. Cohort explanations offer insights into the explainee's behavior on a specific group or cohort of instances, enabling a deeper understanding of model decisions within a defined context. In this paper, we discuss the unique challenges and opportunities associated with measuring cohort explanations, define their desired properties, and create a generalized framework for generating cohort explanations based on supervised clustering.",
                    "We give a combinatorial interpretation using lattice paths for the super Catalan number $S(m, m+s)$ for $s \\leq 3$ and a separate interpretation for $s = 4$.",
                    "Due to the great development of secure multi-party computation, many practical secure computation schemes have been proposed. As an example, different secure auction mechanisms have been widely studied, which can protect bid privacy while satisfying various economic properties. However, as far as we know, none of them solve the secure computation problems for multiple data providers (e.g., secure cloud resource auctions) in the malicious security model. In this paper, we use the techniques of cut-and-choose and garbled circuits to propose a general secure computation framework for multiple data providers against malicious adversaries. Specifically, our framework checks input consistency with the cut-and-choose paradigm, conducts maliciously secure computations by running two independent garbled circuits, and verifies the correctness of output by comparing two versions of outputs. Theoretical analysis shows that our framework is secure against a malicious computation party, or a subset of malicious data providers. Taking secure cloud resource auctions as an example, we implement our framework. Extensive experimental evaluations show that the performance of the proposed framework is acceptable in practice.",
                    "Suppose $N$ is a compact Riemannian manifold, in this paper we will introduce the definition of $N$-valued BSDE and $L^2(\\mathbb{T}^m;N)$-valued BSDE for which the solution are not necessarily staying in only one local coordinate. Moreover, the global existence of a solution to $L^2(\\mathbb{T}^m;N)$-valued BSDE will be proved without any convexity condition on $N$.",
                    "In this paper we will give a probabilistic representation for the heat flow of harmonic map with time-dependent Riemannian metric via a forward-backward stochastic differential equation on manifolds. Moreover, we can provide an alternative stochastic method for the proof of existence of a unique local solution for heat flow of harmonic map with time-dependent Riemannian metric."
                ],
                "domain": [
                    "Mathematics",
                    "Quantum Computing",
                    "Machine Learning",
                    "Secure Computation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we enhance general-domain large language models (LLMs) with specialized chemical knowledge to effectively perform a wide range of chemical tasks in real-world scenarios?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it could lead to the development of AI systems that function as versatile research assistants in chemistry, facilitating human-AI collaboration. This advancement could significantly accelerate chemical research, enabling more efficient property predictions, molecular generation, and reaction predictions. By addressing this question, we could pave the way for achieving Chemical Artificial General Intelligence, which would transform how chemical research is conducted and open new avenues for practical applications in drug discovery, materials science, and beyond.\n\n**[Question 3] - Why is it hard?**  \nThe challenge lies in the inherent complexity of chemical language and knowledge, which differs significantly from natural language. General-domain LLMs often lack the depth of understanding required to interpret chemical notations and concepts accurately. Naive approaches may fail because they do not account for the unique semantics of chemical representations, leading to misinterpretations and ineffective task performance. Overcoming these technical obstacles requires integrating specialized chemical knowledge into LLMs while ensuring they maintain their generalization capabilities across various tasks.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on fine-tuning LLMs for specific chemical tasks using curated datasets, resulting in models that are not adaptable to new scenarios. This one-to-one relationship between models and tasks has limited their applicability. Additionally, there has been a lack of comprehensive approaches that integrate both chemical knowledge and natural language processing capabilities. Our approach differs by proposing a framework that enriches general-domain LLMs with extensive chemical knowledge, enabling them to handle diverse chemical tasks and engage in free-form dialogues with researchers.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves fine-tuning general-domain LLMs with a diverse dataset that includes chemical texts, molecular notations, and task-specific examples. We will evaluate the models using metrics such as accuracy in property prediction, reaction prediction, and user satisfaction in human-AI interactions. The expected outcomes include a versatile AI system capable of performing a wide range of chemical tasks, demonstrating improved performance over existing models, and facilitating effective collaboration between AI and human researchers in the field of chemistry."
    },
    "2401.14656": {
        "paper_data": {
            "title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
            "url": "http://arxiv.org/abs/2401.14656v2",
            "arxiv_id": "2401.14656",
            "authors": [
                "Qiang Zhang",
                "Keyang Ding",
                "Tianwen Lyv",
                "Xinda Wang",
                "Qingyu Yin",
                "Yiwen Zhang",
                "Jing Yu",
                "Yuhao Wang",
                "Xiaotong Li",
                "Zhuoyi Xiang",
                "Kehua Feng",
                "Xiang Zhuang",
                "Zeyuan Wang",
                "Ming Qin",
                "Mengyao Zhang",
                "Jinlu Zhang",
                "Jiyu Cui",
                "Tao Huang",
                "Pengju Yan",
                "Renjun Xu",
                "Hongyang Chen",
                "Xiaolin Li",
                "Xiaohui Fan",
                "Huabin Xing",
                "Huajun Chen"
            ],
            "abstract": "Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of \"scientific language\", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.",
            "introduction": "   1. Introduction    “The limits of my language mean the limits of my world.”   — Ludwig Wittgenstein     Humanity acquires knowledge of the world through perception and cognition, where natural languages (i.e., human languages) stand as the quintessential medium for articulating this world knowledge. Historically, this plethora of world knowledge has been expressed, chronicled, and disseminated through natural languages. Currently, Large Language Models (LLMs) stand as cutting-edge tools in processing natural language and gathering world knowledge. Typically, LLMs refer to those based on Transformer-based architectures with hundreds of millions (or even billions) of trainable parameters, trained on extensive textual corpus (Shanahan, 2022). Typical examples include GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), Galactica (Taylor et al., 2022a), LLaMA (Touvron et al., 2023b), ChatGLM (Zeng et al., 2023a) and Baichuan 2 (Baichuan, 2023). They have exhibited strong capacities to understand natural language and address complex tasks (in the manner of text generation), and have incited substantial interest in both academic and industrial domains. The exceptional performance of LLMs sparks hope that they may evolve into Artificial General Intelligence (AGI) in our current era.   Besides natural languages, to encapsulate more specialized science knowledge, an assortment of scientific languages has been developed, as illustrated in Fig. 1. This encompasses textual expressions in the scientific research domains, mathematical languages to define mathematical formulas, chemical languages such as SMILES that represent molecular structures, and biological languages that describe proteins or genomes, and detail the complex constitution of living organisms. These scientific languages come with their distinct vocabularies, where each term holds a specific meaning that can be completely different from natural languages. For example, the character “C” in English represents the amino acid Cysteine in protein languages (GDR et al., 1984), while in the SMILES language system, it denotes a Carbon atom (Weininger, 1988). Furthermore, experts in specific domains establish grammatical rules to organize these terms, enabling the construction of sentences with precise semantic functions. For instance, computational chemists create grammatical rules ensuring the accuracy of machine-generated molecules in SELFIES format (Krenn et al., 2020). After decades of evolution, scientific languages have become invaluable tools, significantly accelerating scientific discoveries. Due to the potential semantic and grammatical differences between scientific and natural languages, existing general LLMs (such as ChatGPT 111https://chat.openai.com or GPT-4 (OpenAI, 2023)) often fail to properly deal with scientific data like molecules and proteins (AI4Science and Quantum, 2023). As the well-known Austrian philosopher Ludwig Wittgenstein indicates, “The limits of my language mean the limits of my world.” (Ramsey, 1923) The world of general LLMs can be limited to natural languages.   To facilitate the understanding of scientific languages, researchers have devised Scientific Large Language Models (Sci-LLMs) customized for various scientific domains and disciplines. For instance, molecular language models have been developed to represent molecule structures as a string of atoms and chemical bonds (Li and Jiang, 2021). These models aid in predicting molecular properties (Wang et al., 2019), designing new drugs (Zhang et al., 2022c), and proposing retrosynthesis routes (Schwaller et al., 2021). Similarly, protein language models operate based on sequences of amino acids (Rives et al., 2019; Brandes et al., 2022). They are used to forecast 3D protein structures and functions (Lin et al., 2022), enhance existing proteins for improved fitness (Notin et al., 2023b), and create new proteins with specific functionalities (Nijkamp et al., 2023). As a burgeoning area",
            "references": []
        },
        "author_data": {
            "581a866a-96de-4790-8873-02a0dfcd14ba": {
                "pk": "581a866a-96de-4790-8873-02a0dfcd14ba",
                "project_name": null,
                "name": "Qiang Zhang",
                "bio": "I am a researcher with a deep interest in the interplay between topology, algebra, and mathematical physics. My work primarily focuses on the study of Seifert manifolds, automorphisms of free groups, and the dynamics of fixed point classes in various mathematical structures. Recently, I have explored the rank of fixed subgroups induced by orientation-reversing homeomorphisms in compact connected orientable Seifert manifolds, providing significant bounds that parallel existing inequalities in surface and hyperbolic 3-manifold groups.\n\nIn addition to my topological research, I have delved into mechanism design without payments, particularly in scenarios where agents possess differing preferences. By introducing exchanging phases, I have demonstrated the limitations of traditional mechanisms and proposed truthful mechanisms that optimize social welfare in various contexts.\n\nMy investigations also extend to the realm of aspherical manifolds and their properties, including the Bounded Index Property for Homotopy Equivalence (BIPHE). I have contributed to the understanding of fixed point indices in self-maps of graphs and surfaces, and I have provided insights into the behavior of spin fluctuations in iron-based superconductors, bridging the gap between mathematics and physics.\n\nOverall, my research aims to uncover fundamental principles that govern complex systems, whether in algebraic structures, topological spaces, or physical models, and I am committed to advancing our understanding of these intricate relationships.",
                "collaborators": [
                    "Xuezhi Zhao",
                    "Dongxiao Zhao",
                    "Peng Wang",
                    "Jianchun Wu",
                    "Bin Yan",
                    "Jiangping Hu",
                    "Shengkui Ye",
                    "Jialin Lei",
                    "Yingting Miao"
                ],
                "pub_titles": [
                    "The fixed subgroups of homeomorphisms of Seifert manifolds",
                    "Mechanism Design with Exchangeable Allocations",
                    "Aspherical manifolds which do not have Bounded Index Property",
                    "Iwip endomorphisms of free groups and fixed points of graph selfmaps",
                    "A note on test elements for monomorphisms of free groups",
                    "The group fixed by a family of endomorphisms of a surface group",
                    "Many-Anyons Wavefunction, State Capacity and Gentile Statistics",
                    "Bounds for fixed points on products of hyperbolic surfaces",
                    "Longitudinal modes of spin fluctuations in iron-based superconductors",
                    "On the bounded index property for products of aspherical polyhedra",
                    "Explicit bounds for fixed subgroups of endomorphisms of free products",
                    "Optimal Investment and Consumption Strategies with General and Linear Transaction Costs under CRRA Utility",
                    "Howson groups which are not strongly Howson",
                    "Fixed point indices and fixed words at infinity of selfmaps of graphs"
                ],
                "pub_abstracts": [
                    "Let $M$ be a compact connected orientable Seifert manifold with hyperbolic orbifold $B_M$, and $f_{\\pi}: \\pi_1(M)\\rightarrow\\pi_1(M)$ be an automorphism induced by an orientation-reversing homeomorphism $f$ of $M$. We give a bound on the rank of the fixed subgroup of $f_{\\pi}$, namely, $\\rank\\fix(f_{\\pi})<2\\rank \\pi_1(M)$, which is similar to the inequalities on surface groups and hyperbolic 3-manifold groups.",
                    "We investigate mechanism design without payments when agents have different types of preferences. Contrary to most settings in the literature where agents have the same preference, e.g. in the facility location games all agents would like to stay close to (or away from) the facility, we demonstrate the limitation of mechanism design without payments when agents have different preferences by introducing exchanging phases. We consider two types of exchanging phases. The first model is called central exchanges where the exchanges are performed by a central authority. The other model is called individual exchanges where agents exchange their outcomes by themselves. By using facility location games as an example, we provide a truthful mechanism that optimizes social welfare in central exchanges. We also provide a universally truthful randomized mechanism that achieves at least a half of the optimal social welfare in individual exchanges.",
                    "We prove that some aspherical manifolds do not have BIP, negating a question of Jiang.",
                    "In the paper \\cite{JWZ}, Jiang, Wang and Zhang gave a bound for the fixed points and fixed subgroups of a graph selfmap. In this paper, we give a sufficient condition when the bound can be reached, by studying iwip outer endomorphisms of free groups acting on the outer spaces.",
                    "A word in a group is called a test element if any endomorphism fixing it is necessarily an automorphism. In this note, we give a sufficient condition in geometry to construct test elements for monomorphisms of a free group, by using the Whitehead graph and the action of the free group on its Cayley graph.",
                    "For a closed surface $S$ with $\\chi(S)<0$, we show that the fixed subgroup of a family $\\mathcal B$ of endomorphisms of $\\pi_1(S)$ has $\\rk \\fix\\mathcal B\\leq \\rk \\pi_1(S)$. In particular, if $\\mathcal B$ contains a non-epimorphic endomorphism, then $\\rk \\fix\\mathcal B\\leq \\frac{1}{2} \\rk \\pi_1(S)$. We also show that geometric subgroups of $\\pi_1(S)$ are inert, and hence the fixed subgroup of a family of epimorphisms of $\\pi_1(S)$ is also inert.",
                    "The many-anyons wavefunction is constructed via the superposition of all the permutations on the direct product of single anyon states and its interchange properties are examined. The phase of permutation is not a representation but the word metric of the permutation group . Amazingly the interchange phase yields a finite capacity of one quantum state interpolating between Fermion and Boson and the mutual exchange phase has no explicit effect on statistics. Finite capacity of quantum state is defined as Gentile statistics and it is different from the fractional exclusion statistics. Some discussion on the general model is also given.",
                    "For the product $S_1\\times S_2$ of any two connected compact hyperbolic surfaces $S_1$ and $S_2$, we give a finite bound $\\mathcal{B}$ such that for any self-homeomorphism $f$ of $S_1\\times S_2$ and any fixed point class $F$ of $f$, the index $|ind(f, F)|\\leq \\mathcal{B}$, which is an affirmative answer for a special case of a question asked by Boju Jiang. Moreover, we also give bounds for the Lefschetz number $L(f)$ and the Nielsen number $N(f)$ of the homeomorphism $f$.",
                    "Iron-based superconductors can exhibit different magnetic ground states and are in a critical magnetic region where frustrated magnetic interactions strongly compete with each other. Here we investigate the longitudinal modes of spin fluctuations in an unified effective magnetic model for iron-based superconductors. We focus on the collinear antiferromagnetic phase (CAF) and calculate the behavior of the longitudinal modes when different phase boundaries are approached. The results can help to determine the nature of the magnetic fluctuations in iron-based superconductors.",
                    "A compact polyhedron $X$ is said to have the Bounded Index Property for Homotopy Equivalence (BIPHE) if there is a finite bound $\\mathcal{B}$ such that for any homotopy equivalence $f:X\\rightarrow X$ and any fixed point class $\\mathbf{F}$ of $f$, the index $|\\mathrm{ind}(f,\\mathbf{F})|\\leq \\mathcal{B}$. In this note, we consider the product of compact polyhedra, and give some sufficient conditions for it to have BIPHE. Moreover, we show that the products of closed Riemannian manifolds with negative sectional curvature, in particular hyperbolic manifolds, have BIPHE, which gives an affirmative answer to a special case of a question asked by Boju Jiang.",
                    "For an automorphism $\\phi$ of a free group $F_n$ of rank $n$, Bestvina and Handel showed that the rank $rk Fix(\\phi)$ of the fixed subgroup is not greater than $n$ (the so-called Scott conjecture). Soon after Bestvina and Handel's announcement, their result was generalized by many authors in various directions. In this paper, we are interested in the fixed subgroups of endomorphisms of free products, focusing on explicit bounds for their ranks.",
                    "Transaction costs play a critical role in asset allocation and consumption strategies in portfolio management. We apply the methods of dynamic programming and singular perturbation expansion to derive the closed-form leading solutions to this problem for small transaction costs with arbitrary transaction cost structure by maximizing the expected CRRA (constant relative risk aversion) utility function for this problem. We also discuss in detail the case which consists of both fixed and proportional transaction costs.",
                    "A group $G$ is called a Howson group if the intersection $H\\cap K$ of any two finitely generated subgroups $H, K<G$ is again finitely generated, and called a strongly Howson group when a uniform bound for the rank of $H\\cap K$ can be obtained from the ranks of $H$ and $K$. Clearly, every strongly Howson group is a Howson group, but it is unclear in the literature whether the converse is true. In this note, we show that the converse is not true by constructing the first Howson groups which are not strongly Howson.",
                    "Indices of fixed point classes play a central role in Nielsen fixed point theory. Jiang-Wang-Zhang proved that for selfmaps of graphs and surfaces, the index of any fixed point class has an upper bound called its characteristic.   In this paper, we study the difference between the index and the characteristic for selfmaps of graphs. First, for free groups, we extend attracting fixed words at infinity of automorphisms into that of injective endomorphisms. Then, by using relative train track technique, we show that the difference mentioned above is quite likely to be the number of equivalence classes of attracting fixed words of the endomorphism induced on the fundamental group. Since both of attracting fixed words and the existed characteristic are totally determined by endomorphisms themselves, we give a new algebraic approach to estimate indices of fixed point classes of graph selfmaps.   As consequence, we obtain an upper bound for attracting fixed words of injective endomorphisms of free groups, generalizing the one for automorphisms due to Gaboriau-Jaeger-Levitt-Lustig. Furthermore, we give a simple approach to roughly detecting whether fixed words exist or not."
                ],
                "domain": [
                    "Topology",
                    "Group Theory",
                    "Mechanism Design",
                    "Quantum Physics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "9feb731e-f4a1-480e-94ab-b4e538d47e1f": {
                "pk": "9feb731e-f4a1-480e-94ab-b4e538d47e1f",
                "project_name": null,
                "name": "Keyang Ding",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance on tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimal model selection across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for effective neural architectures by leveraging prior knowledge and enhancing search efficiency.\n\nThrough these contributions, I strive to push the boundaries of what GNNs can achieve, fostering a deeper understanding of their structure and performance while making them more accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "5e00b98f-cc80-4304-ae68-1362c7b7702c": {
                "pk": "5e00b98f-cc80-4304-ae68-1362c7b7702c",
                "project_name": null,
                "name": "Tianwen Lyv",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, thereby enhancing their scalability and effectiveness.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimizing performance across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for optimal model designs, making it more efficient and insightful.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and machine learning, with the goal of making these technologies more accessible and effective for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "c01f8f3c-1c8f-4fef-b46b-2693bd90b8e1": {
                "pk": "c01f8f3c-1c8f-4fef-b46b-2693bd90b8e1",
                "project_name": null,
                "name": "Xinda Wang",
                "bio": "I am a researcher dedicated to enhancing software security through innovative machine learning techniques. My work primarily focuses on vulnerability detection and management, particularly in the context of open-source software (OSS) and binary code. I have developed several advanced systems, such as PatchRNN for identifying secret security patches and BinGo for detecting security patches in binary code using code property graphs. \n\nRecognizing the challenges posed by the lack of accurately labeled datasets, I proposed a novel data augmentation technique that significantly improves the performance of pre-trained language models for vulnerability detection. My recent efforts also led to the creation of PySecDB, the first comprehensive security commit dataset for Python, which aids in identifying hidden security commits that often go unnoticed.\n\nIn addition to my technical contributions, I have explored the barriers to adopting AI-based vulnerability management in industry, identifying key challenges such as scalability and customization. My goal is to bridge the gap between academic research and practical application, ensuring that our advancements in AI can effectively enhance security practices in real-world scenarios. Through my research, I aim to empower security engineers with tools that not only improve detection rates but also streamline their workflows, ultimately contributing to a more secure software ecosystem.",
                "collaborators": [
                    "Kun Sun",
                    "Shu Wang",
                    "Pengbin Feng",
                    "Shiyu Sun",
                    "Weiliang Qi",
                    "Jiahao Cao",
                    "Darsh Poddar",
                    "Sophia Li",
                    "Sushil Jajodia",
                    "Sanae Benchaaboun"
                ],
                "pub_titles": [
                    "Enhancing Pre-Trained Language Models for Vulnerability Detection via Semantic-Preserving Data Augmentation",
                    "PatchRNN: A Deep Learning-Based System for Security Patch Identification",
                    "BinGo: Identifying Security Patches in Binary Code with Graph Representation Learning",
                    "FedCAP: Robust Federated Learning via Customized Aggregation and Personalization",
                    "Exploring Security Commits in Python",
                    "Bridging the Gap: A Study of AI-based Vulnerability Management between Industry and Academia"
                ],
                "pub_abstracts": [
                    "With the rapid development and widespread use of advanced network systems, software vulnerabilities pose a significant threat to secure communications and networking. Learning-based vulnerability detection systems, particularly those leveraging pre-trained language models, have demonstrated significant potential in promptly identifying vulnerabilities in communication networks and reducing the risk of exploitation. However, the shortage of accurately labeled vulnerability datasets hinders further progress in this field. Failing to represent real-world vulnerability data variety and preserve vulnerability semantics, existing augmentation approaches provide limited or even counterproductive contributions to model training. In this paper, we propose a data augmentation technique aimed at enhancing the performance of pre-trained language models for vulnerability detection. Given the vulnerability dataset, our method performs natural semantic-preserving program transformation to generate a large volume of new samples with enriched data diversity and variety. By incorporating our augmented dataset in fine-tuning a series of representative code pre-trained models (i.e., CodeBERT, GraphCodeBERT, UnixCoder, and PDBERT), up to 10.1% increase in accuracy and 23.6% increase in F1 can be achieved in the vulnerability detection task. Comparison results also show that our proposed method can substantially outperform other prominent vulnerability augmentation approaches.",
                    "With the increasing usage of open-source software (OSS) components, vulnerabilities embedded within them are propagated to a huge number of underlying applications. In practice, the timely application of security patches in downstream software is challenging. The main reason is that such patches do not explicitly indicate their security impacts in the documentation, which would be difficult to recognize for software maintainers and users. However, attackers can still identify these \"secret\" security patches by analyzing the source code and generate corresponding exploits to compromise not only unpatched versions of the current software, but also other similar software packages that may contain the same vulnerability due to code cloning or similar design/implementation logic. Therefore, it is critical to identify these secret security patches to enable timely fixes. To this end, we propose a deep learning-based defense system called PatchRNN to automatically identify secret security patches in OSS. Besides considering descriptive keywords in the commit message (i.e., at the text level), we leverage both syntactic and semantic features at the source-code level. To evaluate the performance of our system, we apply it on a large-scale real-world patch dataset and conduct a case study on a popular open-source web server software - NGINX. Experimental results show that the PatchRNN can successfully detect secret security patches with a low false positive rate.",
                    "A timely software update is vital to combat the increasing security vulnerabilities. However, some software vendors may secretly patch their vulnerabilities without creating CVE entries or even describing the security issue in their change log. Thus, it is critical to identify these hidden security patches and defeat potential N-day attacks. Researchers have employed various machine learning techniques to identify security patches in open-source software, leveraging the syntax and semantic features of the software changes and commit messages. However, all these solutions cannot be directly applied to the binary code, whose instructions and program flow may dramatically vary due to different compilation configurations. In this paper, we propose BinGo, a new security patch detection system for binary code. The main idea is to present the binary code as code property graphs to enable a comprehensive understanding of program flow and perform a language model over each basic block of binary code to catch the instruction semantics. BinGo consists of four phases, namely, patch data pre-processing, graph extraction, embedding generation, and graph representation learning. Due to the lack of an existing binary security patch dataset, we construct such a dataset by compiling the pre-patch and post-patch source code of the Linux kernel. Our experimental results show BinGo can achieve up to 80.77% accuracy in identifying security patches between two neighboring versions of binary code. Moreover, BinGo can effectively reduce the false positives and false negatives caused by the different compilers and optimization levels.",
                    "Federated learning (FL), an emerging distributed machine learning paradigm, has been applied to various privacy-preserving scenarios. However, due to its distributed nature, FL faces two key issues: the non-independent and identical distribution (non-IID) of user data and vulnerability to Byzantine threats. To address these challenges, in this paper, we propose FedCAP, a robust FL framework against both data heterogeneity and Byzantine attacks. The core of FedCAP is a model update calibration mechanism to help a server capture the differences in the direction and magnitude of model updates among clients. Furthermore, we design a customized model aggregation rule that facilitates collaborative training among similar clients while accelerating the model deterioration of malicious clients. With a Euclidean norm-based anomaly detection mechanism, the server can quickly identify and permanently remove malicious clients. Moreover, the impact of data heterogeneity and Byzantine attacks can be further mitigated through personalization on the client side. We conduct extensive experiments, comparing multiple state-of-the-art baselines, to demonstrate that FedCAP performs well in several non-IID settings and shows strong robustness under a series of poisoning attacks.",
                    "Python has become the most popular programming language as it is friendly to work with for beginners. However, a recent study has found that most security issues in Python have not been indexed by CVE and may only be fixed by 'silent' security commits, which pose a threat to software security and hinder the security fixes to downstream software. It is critical to identify the hidden security commits; however, the existing datasets and methods are insufficient for security commit detection in Python, due to the limited data variety, non-comprehensive code semantics, and uninterpretable learned features. In this paper, we construct the first security commit dataset in Python, namely PySecDB, which consists of three subsets including a base dataset, a pilot dataset, and an augmented dataset. The base dataset contains the security commits associated with CVE records provided by MITRE. To increase the variety of security commits, we build the pilot dataset from GitHub by filtering keywords within the commit messages. Since not all commits provide commit messages, we further construct the augmented dataset by understanding the semantics of code changes. To build the augmented dataset, we propose a new graph representation named CommitCPG and a multi-attributed graph learning model named SCOPY to identify the security commit candidates through both sequential and structural code semantics. The evaluation shows our proposed algorithms can improve the data collection efficiency by up to 40 percentage points. After manual verification by three security experts, PySecDB consists of 1,258 security commits and 2,791 non-security commits. Furthermore, we conduct an extensive case study on PySecDB and discover four common security fix patterns that cover over 85% of security commits in Python, providing insight into secure software maintenance, vulnerability detection, and automated program repair.",
                    "Recent research advances in Artificial Intelligence (AI) have yielded promising results for automated software vulnerability management. AI-based models are reported to greatly outperform traditional static analysis tools, indicating a substantial workload relief for security engineers. However, the industry remains very cautious and selective about integrating AI-based techniques into their security vulnerability management workflow. To understand the reasons, we conducted a discussion-based study, anchored in the authors' extensive industrial experience and keen observations, to uncover the gap between research and practice in this field. We empirically identified three main barriers preventing the industry from adopting academic models, namely, complicated requirements of scalability and prioritization, limited customization flexibility, and unclear financial implications. Meanwhile, research works are significantly impacted by the lack of extensive real-world security data and expertise. We proposed a set of future directions to help better understand industry expectations, improve the practical usability of AI-based security vulnerability research, and drive a synergistic relationship between industry and academia."
                ],
                "domain": [
                    "Vulnerability Detection",
                    "Machine Learning",
                    "Software Security",
                    "Federated Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "0b607260-5db7-46ad-8bc0-a8bfd5af1b82": {
                "pk": "0b607260-5db7-46ad-8bc0-a8bfd5af1b82",
                "project_name": null,
                "name": "Qingyu Yin",
                "bio": "I am a researcher dedicated to advancing the field of Natural Language Processing (NLP) with a strong focus on explainability, user intent understanding, and complex query answering. My recent work emphasizes the importance of generating human-readable explanations alongside classification decisions, as seen in my generative explanation framework that outperforms existing models. I have also tackled challenges in zero pronoun resolution by developing a neural recovery machine that effectively captures semantic information, significantly improving performance over state-of-the-art methods.\n\nMy research extends to enhancing self-attention networks by integrating multiple structural priors, leading to substantial improvements in text understanding. I have formulated innovative frameworks for tasks like table-based fact verification and temporal event forecasting, which leverage logical reasoning and user session data, respectively. Additionally, I have explored the intricacies of e-commerce query understanding, proposing context-aware models that utilize historical user behavior to refine search intents.\n\nI am particularly passionate about bridging the gap between complex logic queries and knowledge graphs, introducing frameworks that differentiate between entities and numerical values for more accurate reasoning. My work on multilingual keyphrase generation and novel pre-training tasks for short text further reflects my commitment to addressing the challenges of low-resource languages and the unique characteristics of user queries.\n\nThrough my research, I aim to create systems that not only perform well but also provide clear, interpretable insights, ultimately enhancing user experience and understanding in various applications.",
                "collaborators": [
                    "Bing Yin",
                    "Ting Liu",
                    "Yu Zhang",
                    "Zheng Li",
                    "Chen Luo",
                    "Haoming Jiang",
                    "Weinan Zhang",
                    "Tuo Zhao",
                    "Danqing Zhang",
                    "William Yang Wang"
                ],
                "pub_titles": [
                    "Towards Explainable NLP: A Generative Explanation Framework for Text Classification",
                    "A Deep Neural Network for Chinese Zero Pronoun Resolution",
                    "Multiple Structural Priors Guided Self Attention Network for Language Understanding",
                    "Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification",
                    "Neural Recovery Machine for Chinese Dropped Pronoun",
                    "Understanding Inter-Session Intentions via Complex Logical Reasoning",
                    "Deep Reinforcement Learning for Chinese Zero pronoun Resolution",
                    "CERES: Pretraining of Graph-Conditioned Transformer for Semi-Structured Session Data",
                    "DiP-GNN: Discriminative Pre-Training of Graph Neural Networks",
                    "StableMask: Refining Causal Masking in Decoder-only Transformer",
                    "RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph",
                    "SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models",
                    "Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites",
                    "Knowledge Graph Reasoning over Entities and Numerical Values",
                    "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution",
                    "XTQA: Span-Level Explanations of the Textbook Question Answering",
                    "Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training",
                    "Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding"
                ],
                "pub_abstracts": [
                    "Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information is often ignored, and the systems do not explicitly generate the human-readable explanations. To better alleviate this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.",
                    "Existing approaches for Chinese zero pronoun resolution overlook semantic information. This is because zero pronouns have no descriptive information, which results in difficulty in explicitly capturing their semantic similarities with antecedents. Moreover, when dealing with candidate antecedents, traditional systems simply take advantage of the local information of a single candidate antecedent while failing to consider the underlying information provided by the other candidates from a global perspective. To address these weaknesses, we propose a novel zero pronoun-specific neural network, which is capable of representing zero pronouns by utilizing the contextual information at the semantic level. In addition, when dealing with candidate antecedents, a two-level candidate encoder is employed to explicitly capture both the local and global information of candidate antecedents. We conduct experiments on the Chinese portion of the OntoNotes 5.0 corpus. Experimental results show that our approach substantially outperforms the state-of-the-art method in various experimental settings.",
                    "Self attention networks (SANs) have been widely utilized in recent NLP studies. Unlike CNNs or RNNs, standard SANs are usually position-independent, and thus are incapable of capturing the structural priors between sequences of words. Existing studies commonly apply one single mask strategy on SANs for incorporating structural priors while failing at modeling more abundant structural information of texts. In this paper, we aim at introducing multiple types of structural priors into SAN models, proposing the Multiple Structural Priors Guided Self Attention Network (MS-SAN) that transforms different structural priors into different attention heads by using a novel multi-mask based multi-head attention mechanism. In particular, we integrate two categories of structural priors, including the sequential order and the relative position of words. For the purpose of capturing the latent hierarchical structure of the texts, we extract these information not only from the word contexts but also from the dependency syntax trees. Experimental results on two tasks show that MS-SAN achieves significant improvements against other strong baselines.",
                    "Table-based fact verification task aims to verify whether the given statement is supported by the given semi-structured table. Symbolic reasoning with logical operations plays a crucial role in this task. Existing methods leverage programs that contain rich logical information to enhance the verification process. However, due to the lack of fully supervised signals in the program generation process, spurious programs can be derived and employed, which leads to the inability of the model to catch helpful logical operations. To address the aforementioned problems, in this work, we formulate the table-based fact verification task as an evidence retrieval and reasoning framework, proposing the Logic-level Evidence Retrieval and Graph-based Verification network (LERGV). Specifically, we first retrieve logic-level program-like evidence from the given table and statement as supplementary evidence for the table. After that, we construct a logic-level graph to capture the logical relations between entities and functions in the retrieved evidence, and design a graph-based verification network to perform logic-level graph-based reasoning based on the constructed graph to classify the final entailment relation. Experimental results on the large-scale benchmark TABFACT show the effectiveness of the proposed approach.",
                    "Dropped pronouns (DPs) are ubiquitous in pro-drop languages like Chinese, Japanese etc. Previous work mainly focused on painstakingly exploring the empirical features for DPs recovery. In this paper, we propose a neural recovery machine (NRM) to model and recover DPs in Chinese, so that to avoid the non-trivial feature engineering process. The experimental results show that the proposed NRM significantly outperforms the state-of-the-art approaches on both two heterogeneous datasets. Further experiment results of Chinese zero pronoun (ZP) resolution show that the performance of ZP resolution can also be improved by recovering the ZPs to DPs.",
                    "Understanding user intentions is essential for improving product recommendations, navigation suggestions, and query reformulations. However, user intentions can be intricate, involving multiple sessions and attribute requirements connected by logical operators such as And, Or, and Not. For instance, a user may search for Nike or Adidas running shoes across various sessions, with a preference for purple. In another example, a user may have purchased a mattress in a previous session and is now looking for a matching bed frame without intending to buy another mattress. Existing research on session understanding has not adequately addressed making product or attribute recommendations for such complex intentions. In this paper, we present the task of logical session complex query answering (LS-CQA), where sessions are treated as hyperedges of items, and we frame the problem of complex intention understanding as an LS-CQA task on an aggregated hypergraph of sessions, items, and attributes. This is a unique complex query answering task with sessions as ordered hyperedges. We also introduce a new model, the Logical Session Graph Transformer (LSGT), which captures interactions among items across different sessions and their logical connections using a transformer structure. We analyze the expressiveness of LSGT and prove the permutation invariance of the inputs for the logical operators. By evaluating LSGT on three datasets, we demonstrate that it achieves state-of-the-art results.",
                    "Deep neural network models for Chinese zero pronoun resolution learn semantic information for zero pronoun and candidate antecedents, but tend to be short-sighted---they often make local decisions. They typically predict coreference chains between the zero pronoun and one single candidate antecedent one link at a time, while overlooking their long-term influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is critical when later predicting zero pronoun-candidate antecedent pairs. In this study, we show how to integrate local and global decision-making by exploiting deep reinforcement learning models. With the help of the reinforcement learning agent, our model learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 dataset show that our technique surpasses the state-of-the-art models.",
                    "User sessions empower many search and recommendation tasks on a daily basis. Such session data are semi-structured, which encode heterogeneous relations between queries and products, and each item is described by the unstructured text. Despite recent advances in self-supervised learning for text or graphs, there lack of self-supervised learning models that can effectively capture both intra-item semantics and inter-item interactions for semi-structured sessions. To fill this gap, we propose CERES, a graph-based transformer model for semi-structured session data. CERES learns representations that capture both inter- and intra-item semantics with (1) a graph-conditioned masked language pretraining task that jointly learns from item text and item-item relations; and (2) a graph-conditioned transformer architecture that propagates inter-item contexts to item-level representations. We pretrained CERES using ~468 million Amazon sessions and find that CERES outperforms strong pretraining baselines by up to 9% in three session search and entity linking tasks.",
                    "Graph neural network (GNN) pre-training methods have been proposed to enhance the power of GNNs. Specifically, a GNN is first pre-trained on a large-scale unlabeled graph and then fine-tuned on a separate small labeled graph for downstream applications, such as node classification. One popular pre-training method is to mask out a proportion of the edges, and a GNN is trained to recover them. However, such a generative method suffers from graph mismatch. That is, the masked graph inputted to the GNN deviates from the original graph. To alleviate this issue, we propose DiP-GNN (Discriminative Pre-training of Graph Neural Networks). Specifically, we train a generator to recover identities of the masked edges, and simultaneously, we train a discriminator to distinguish the generated edges from the original graph's edges. In our framework, the graph seen by the discriminator better matches the original graph because the generator can recover a proportion of the masked edges. Extensive experiments on large-scale homogeneous and heterogeneous graphs demonstrate the effectiveness of the proposed framework.",
                    "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
                    "With the increasing demands on e-commerce platforms, numerous user action history is emerging. Those enriched action records are vital to understand users' interests and intents. Recently, prior works for user behavior prediction mainly focus on the interactions with product-side information. However, the interactions with search queries, which usually act as a bridge between users and products, are still under investigated. In this paper, we explore a new problem named temporal event forecasting, a generalized user behavior prediction task in a unified query product evolutionary graph, to embrace both query and product recommendation in a temporal manner. To fulfill this setting, there involves two challenges: (1) the action data for most users is scarce; (2) user preferences are dynamically evolving and shifting over time. To tackle those issues, we propose a novel Retrieval-Enhanced Temporal Event (RETE) forecasting framework. Unlike existing methods that enhance user representations via roughly absorbing information from connected entities in the whole graph, RETE efficiently and dynamically retrieves relevant entities centrally on each user as high-quality subgraphs, preventing the noise propagation from the densely evolutionary graph structures that incorporate abundant search queries. And meanwhile, RETE autoregressively accumulates retrieval-enhanced user representations from each time step, to capture evolutionary patterns for joint query and product prediction. Empirically, extensive experiments on both the public benchmark and four real-world industrial datasets demonstrate the effectiveness of the proposed RETE method.",
                    "Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from compositionally out-of-distribution generalization errors. To tackle these issues, we propose a novel few-shot semantic parsing method -- SeqZero. SeqZero decomposes the problem into a sequence of sub-problems, which correspond to the sub-clauses of the formal language. Based on the decomposition, the LMs only need to generate short answers using prompts for predicting sub-clauses. Thus, SeqZero avoids generating a long canonical utterance at once. Moreover, SeqZero employs not only a few-shot model but also a zero-shot model to alleviate the overfitting. In particular, SeqZero brings out the merits from both models via ensemble equipped with our proposed constrained rescaling. SeqZero achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split.",
                    "E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users' true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore users' history behaviors and consider only the instant search query, which is often a short string offering limited information about the true shopping intent.   We propose an end-to-end context-aware query rewriting model to bridge this gap, which takes the search context into account. Specifically, our model builds a session graph using the history search queries and their contained words. We then employ a graph attention mechanism that models cross-query relations and computes contextual information of the session. The model subsequently calculates session representations by combining the contextual information with the instant search query using an aggregation network. The session representations are then decoded to generate rewritten queries. Empirically, we demonstrate the superiority of our method to state-of-the-art approaches under various metrics. On in-house data from an online shopping platform, by introducing contextual information, our model achieves 11.6% improvement under the MRR (Mean Reciprocal Rank) metric and 20.1% improvement under the HIT@16 metric (a hit rate metric), in comparison with the best baseline method (Transformer-based model).",
                    "A complex logic query in a knowledge graph refers to a query expressed in logic form that conveys a complex meaning, such as where did the Canadian Turing award winner graduate from? Knowledge graph reasoning-based applications, such as dialogue systems and interactive search engines, rely on the ability to answer complex logic queries as a fundamental task. In most knowledge graphs, edges are typically used to either describe the relationships between entities or their associated attribute values. An attribute value can be in categorical or numerical format, such as dates, years, sizes, etc. However, existing complex query answering (CQA) methods simply treat numerical values in the same way as they treat entities. This can lead to difficulties in answering certain queries, such as which Australian Pulitzer award winner is born before 1927, and which drug is a pain reliever and has fewer side effects than Paracetamol. In this work, inspired by the recent advances in numerical encoding and knowledge graph reasoning, we propose numerical complex query answering. In this task, we introduce new numerical variables and operations to describe queries involving numerical attribute values. To address the difference between entities and numerical values, we also propose the framework of Number Reasoning Network (NRN) for alternatively encoding entities and numerical values into separate encoding structures. During the numerical encoding process, NRN employs a parameterized density function to encode the distribution of numerical values. During the entity encoding process, NRN uses established query encoding methods for the original CQA problem. Experimental results show that NRN consistently improves various query encoding methods on three different knowledge graphs and achieves state-of-the-art results.",
                    "Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.",
                    "Textbook Question Answering (TQA) is a task that one should answer a diagram/non-diagram question given a large multi-modal context consisting of abundant essays and diagrams. We argue that the explainability of this task should place students as a key aspect to be considered. To address this issue, we devise a novel architecture towards span-level eXplanations of the TQA (XTQA) based on our proposed coarse-to-fine grained algorithm, which can provide not only the answers but also the span-level evidences to choose them for students. This algorithm first coarsely chooses top $M$ paragraphs relevant to questions using the TF-IDF method, and then chooses top $K$ evidence spans finely from all candidate spans within these paragraphs by computing the information gain of each span to questions. Experimental results shows that XTQA significantly improves the state-of-the-art performance compared with baselines. The source code is available at https://github.com/keep-smile-001/opentqa",
                    "Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven't been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.",
                    "E-commerce query understanding is the process of inferring the shopping intent of customers by extracting semantic meaning from their search queries. The recent progress of pre-trained masked language models (MLM) in natural language processing is extremely attractive for developing effective query understanding models. Specifically, MLM learns contextual text embedding via recovering the masked tokens in the sentences. Such a pre-training process relies on the sufficient contextual information. It is, however, less effective for search queries, which are usually short text. When applying masking to short search queries, most contextual information is lost and the intent of the search queries may be changed. To mitigate the above issues for MLM pre-training on search queries, we propose a novel pre-training task specifically designed for short text, called Extended Token Classification (ETC). Instead of masking the input text, our approach extends the input by inserting tokens via a generator network, and trains a discriminator to identify which tokens are inserted in the extended input. We conduct experiments in an E-commerce store to demonstrate the effectiveness of ETC."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Explainable AI",
                    "Machine Learning",
                    "Graph Neural Network"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "e084f9ef-2a83-49b0-9a71-a9b24cd0d3c1": {
                "pk": "e084f9ef-2a83-49b0-9a71-a9b24cd0d3c1",
                "project_name": null,
                "name": "Yiwen Zhang",
                "bio": "I am a researcher with a diverse background in linguistics, recommendation systems, network analysis, and machine learning. My work has spanned various domains, from investigating phonetic changes in the Chengdu dialect of Mandarin to developing advanced frameworks for intent modeling in recommender systems. My recent research includes the Bilateral Intent-guided Graph Collaborative Filtering (BIGCF), which addresses the challenges of sparse implicit feedback by incorporating both individual and collective user intents.\n\nI have also contributed to the field of signed network embedding with MUSE, a framework that leverages multi-faceted attention mechanisms to enhance the representation of nodes in signed networks. My work on heterogeneous information networks (HINs) led to the development of HicRec, an interest composition model that improves recommendation accuracy by effectively learning user and item representations.\n\nIn addition, I have explored the potential of contrastive learning in heterogeneous graphs through Generative-Enhanced Heterogeneous Graph Contrastive Learning (GHGCL), which outperforms existing models in node classification and link prediction tasks. My research extends to cascade prediction, where I introduced the PersonalityGate to enhance GNN-based models by integrating personality traits into the prediction process.\n\nOverall, my interdisciplinary approach combines theoretical insights with practical applications, aiming to push the boundaries of understanding in both linguistic phenomena and complex data systems. I am passionate about leveraging my findings to create innovative solutions that address real-world challenges.",
                "collaborators": [
                    "Dengcheng Yan",
                    "Yi Zhang",
                    "Lei Sang",
                    "Hai Hu",
                    "Youwen Zhang",
                    "Wei Li",
                    "Wenxin Xie",
                    "Yu Wang",
                    "Jie Cao",
                    "Hong Zhong"
                ],
                "pub_titles": [
                    "Path of Vowel Raising in Chengdu Dialect of Mandarin",
                    "Exploring the Individuality and Collectivity of Intents behind Interactions for Graph Collaborative Filtering",
                    "MUSE: Multi-faceted Attention for Signed Network Embedding",
                    "Heterogeneous Information Network-based Interest Composition with Graph Neural Network for Recommendation",
                    "Generative-Enhanced Heterogeneous Graph Contrastive Learning",
                    "PersonalityGate: A General Plug-and-Play GNN Gate to Enhance Cascade Prediction with Personality Recognition Task",
                    "Model-Based and Model-Free point prediction algorithms for locally stationary random fields",
                    "Stochastic Description of Near-Horizon Fluctuations in Rindler-AdS"
                ],
                "pub_abstracts": [
                    "He and Rao (2013) reported a raising phenomenon of /a/ in /Xan/ (X being a consonant or a vowel) in Chengdu dialect of Mandarin, i.e. /a/ is realized as [epsilon] for young speakers but [ae] for older speakers, but they offered no acoustic analysis. We designed an acoustic study that examined the realization of /Xan/ in speakers of different age (old vs. young) and gender (male vs. female) groups, where X represents three conditions: 1) unaspirated consonants: C ([p], [t], [k]), 2) aspirated consonants: Ch ([ph], [th], [kh]), and 3) high vowels: V ([i], [y], [u]). 17 native speakers were asked to read /Xan/ characters and the F1 values are extracted for comparison. Our results confirmed the raising effect in He and Rao (2013), i.e., young speakers realize /a/ as [epsilon] in /an/, whereas older speakers in the most part realize it as [ae]. Also, female speakers raise more than male speakers within the same age group. Interestingly, within the /Van/ condition, older speakers do raise /a/ in /ian/ and /yan/. We interpret this as /a/ first assimilates to its preceding front high vowels /i/ and /y/ for older speakers, which then becomes phonologized in younger speakers in all conditions, including /Chan/ and /Can/. This shows a possible trajectory of the ongoing sound change in the Chengdu dialect.",
                    "Intent modeling has attracted widespread attention in recommender systems. As the core motivation behind user selection of items, intent is crucial for elucidating recommendation results. The current mainstream modeling method is to abstract the intent into unknowable but learnable shared or non-shared parameters. Despite considerable progress, we argue that it still confronts the following challenges: firstly, these methods only capture the coarse-grained aspects of intent, ignoring the fact that user-item interactions will be affected by collective and individual factors (e.g., a user may choose a movie because of its high box office or because of his own unique preferences); secondly, modeling believable intent is severely hampered by implicit feedback, which is incredibly sparse and devoid of true semantics. To address these challenges, we propose a novel recommendation framework designated as Bilateral Intent-guided Graph Collaborative Filtering (BIGCF). Specifically, we take a closer look at user-item interactions from a causal perspective and put forth the concepts of individual intent-which signifies private preferences-and collective intent-which denotes overall awareness. To counter the sparsity of implicit feedback, the feature distributions of users and items are encoded via a Gaussian-based graph generation strategy, and we implement the recommendation process through bilateral intent-guided graph reconstruction re-sampling. Finally, we propose graph contrastive regularization for both interaction and intent spaces to uniformize users, items, intents, and interactions in a self-supervised and non-augmented paradigm. Experimental results on three real-world datasets demonstrate the effectiveness of BIGCF compared with existing solutions.",
                    "Signed network embedding is an approach to learn low-dimensional representations of nodes in signed networks with both positive and negative links, which facilitates downstream tasks such as link prediction with general data mining frameworks. Due to the distinct properties and significant added value of negative links, existing signed network embedding methods usually design dedicated methods based on social theories such as balance theory and status theory. However, existing signed network embedding methods ignore the characteristics of multiple facets of each node and mix them up in one single representation, which limits the ability to capture the fine-grained attentions between node pairs. In this paper, we propose MUSE, a MUlti-faceted attention-based Signed network Embedding framework to tackle this problem. Specifically, a joint intra- and inter-facet attention mechanism is introduced to aggregate fine-grained information from neighbor nodes. Moreover, balance theory is also utilized to guide information aggregation from multi-order balanced and unbalanced neighbors. Experimental results on four real-world signed network datasets demonstrate the effectiveness of our proposed framework.",
                    "Heterogeneous information networks (HINs) are widely applied to recommendation systems due to their capability of modeling various auxiliary information with meta-paths. However, existing HIN-based recommendation models usually fuse the information from various meta-paths by simple weighted sum or concatenation, which limits performance improvement because it lacks the capability of interest compositions among meta-paths. In this article, we propose an HIN-based Interest Composition model for Recommendation (HicRec). Specifically, user and item representations are learned with a graph neural network on both the graph structure and features in each meta-path, and a parameter sharing mechanism is utilized here to ensure that the user and item representations are in the same latent space. Then, users' interests in each item from each pair of related meta-paths are calculated by a combination of the user and item representations. The composed user interests are obtained by their single interest from both intra- and inter-meta-paths for recommendation. Extensive experiments are conducted on three real-world datasets and the results demonstrate that our proposed HicRec model outperforms the baselines.",
                    "Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and contrastive discriminators for downstream tasks. However, data augmentation is still limited due to the graph data's integrity. Furthermore, the contrastive discriminators remain sampling bias and lack local heterogeneous information. To tackle the above limitations, we propose a novel Generative-Enhanced Heterogeneous Graph Contrastive Learning (GHGCL). Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using a masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generating hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global information. Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced contrastive discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest contrastive and generative baselines on node classification and link prediction tasks. To reproduce our work, we have open-sourced our code at https://anonymous.4open.science/r/GC-HGNN-E50C.",
                    "Cascade prediction estimates the size or the state of a cascade from either microscope or macroscope. It is of paramount importance for understanding the information diffusion process such as the spread of rumors and the propagation of new technologies in social networks. Recently, instead of extracting hand-crafted features or embedding cascade sequences into feature vectors for cascade prediction, graph neural networks (GNNs) are introduced to utilize the network structure which governs the cascade effect. However, these models do not take into account social factors such as personality traits which drive human's participation in the information diffusion process. In this work, we propose a novel multitask framework for enhancing cascade prediction with a personality recognition task. Specially, we design a general plug-and-play GNN gate, named PersonalityGate, to couple into existing GNN-based cascade prediction models to enhance their effectiveness and extract individuals' personality traits jointly. Experimental results on two real-world datasets demonstrate the effectiveness of our proposed framework in enhancing GNN-based cascade prediction models and in predicting individuals' personality traits as well.",
                    "The Model-free Prediction Principle has been successfully applied to general regression problems, as well as problems involving stationary and locally stationary time series. In this paper we demonstrate how Model-Free Prediction can be applied to handle random fields that are only locally stationary, i.e., they can be assumed to be stationary only across a limited part over their entire region of definition. We construct one-step-ahead point predictors and compare the performance of Model-free to Model-based prediction using models that incorporate a trend and/or heteroscedasticity. Both aspects of the paper, Model-free and Model-based, are novel in the context of random fields that are locally (but not globally) stationary. We demonstrate the application of our Model-based and Model-free point prediction methods to synthetic data as well as images from the CIFAR-10 dataset and in the latter case show that our best Model-free point prediction results outperform those obtained using Model-based prediction.",
                    "We study quantum spacetime fluctuations near light-sheet horizons associated with a Rindler wedge in AdS spacetime, in the context of AdS/CFT. In particular, we solve the vacuum Einstein equation near the light-sheet horizon, augmented with the Ansatz of a quantum source smeared out in a Planckian width along one of the light-cone directions. Such a source, whose physical interpretation is of gravitational shockwaves created by vacuum energy fluctuations, alters the Einstein equation to a stochastic partial differential equation taking the form of a Langevin equation. By integrating fluctuations along the light sheet, we find an accumulated effect in the round-trip time of a photon to traverse the horizon of the Rindler wedge that depends on both the $d$-dimensional Newton constant $G_N^{(d)}$ and the AdS curvature $L$, in agreement with previous literature utilizing different methods."
                ],
                "domain": [
                    "Linguistics",
                    "Recommender Systems",
                    "Graph Neural Network",
                    "Quantum Physics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "9a75dba7-839a-4902-bce3-9adff704e5f6": {
                "pk": "9a75dba7-839a-4902-bce3-9adff704e5f6",
                "project_name": null,
                "name": "Jing Yu",
                "bio": "I am a researcher deeply engaged in the intersection of mathematics, control theory, and graph theory, with a particular focus on Borel graphs and their embeddings. My recent work has explored the properties of graphs of polynomial growth, where I have strengthened existing results by demonstrating that these graphs can be embedded into the grid in a way that is both efficient and insightful. This research not only addresses fundamental questions in descriptive set theory but also provides a framework for understanding the universal properties of Borel graphs.\n\nIn addition to my work in graph theory, I have developed optimization algorithms for experimental design in Bayesian inverse problems, achieving significant computational efficiency through Chebyshev interpolation. My contributions to control theory include the design of nonlinear controllers that effectively balance performance and safety, as well as advancements in distributed linear-quadratic regulator problems under communication constraints.\n\nI am also actively involved in the classification of Hopf algebras, particularly those with the dual Chevalley property, where I have characterized their structures and explored their corepresentation types. My research is characterized by a blend of theoretical rigor and practical application, aiming to bridge gaps between abstract mathematical concepts and real-world systems. Through my work, I strive to contribute to a deeper understanding of complex systems and their underlying mathematical frameworks.",
                "collaborators": [
                    "Anton Bernshteyn",
                    "Gongxiang Liu",
                    "Mihai Anitescu",
                    "Dimitar Ho",
                    "Olle Kjellqvist"
                ],
                "pub_titles": [
                    "Embedding Borel graphs into grids of asymptotically optimal dimension",
                    "Solving Optimal Experimental Design with Sequential Quadratic Programming and Chebyshev Interpolation",
                    "Achieving Performance and Safety in Large Scale Systems with Saturation using a Nonlinear System Level Synthesis Approach",
                    "On Infinite-horizon System Level Synthesis Problems",
                    "Hopf algebras with the dual Chevalley property of discrete corepresentation type",
                    "Coradically graded Hopf algebras with the dual Chevalley property of tame corepresentation type",
                    "Large-scale geometry of Borel graphs of polynomial growth"
                ],
                "pub_abstracts": [
                    "Let $G$ be a Borel graph all of whose finite subgraphs embed into the $d$-dimensional grid with diagonals. We show that then $G$ itself admits a Borel embedding into the Schreier graph of a free Borel action of $\\mathbb Z^{O(d)}$. This strengthens an earlier result of the authors, in which $O(d)$ is replaced by $O(\\rho \\log \\rho)$, where $\\rho$ is the polynomial growth rate of $G$.",
                    "We propose an optimization algorithm to compute the optimal sensor locations in experimental design in the formulation of Bayesian inverse problems, where the parameter-to-observable mapping is described through an integral equation and its discretization results in a continuously indexed matrix whose size depends on the mesh size n. By approximating the gradient and Hessian of the objective design criterion from Chebyshev interpolation, we solve a sequence of quadratic programs and achieve the complexity $\\mathcal{O}(n\\log^2(n))$. An error analysis guarantees the integrality gap shrinks to zero as $n\\to\\infty$, and we apply the algorithm on a two-dimensional advection-diffusion equation, to determine the LIDAR's optimal sensing directions for data collection.",
                    "We present a novel class of nonlinear controllers that interpolates among differently behaving linear controllers as a case study for recently proposed Linear and Nonlinear System Level Synthesis framework. The structure of the nonlinear controller allows for simultaneously satisfying performance and safety objectives defined for small- and large-disturbance regimes. The proposed controller is distributed, handles delays, sparse actuation, and localizes disturbances. We show our nonlinear controller always outperforms its linear counterpart for constrained LQR problems. We further demonstrate the anti-windup property of an augmented control strategy based on the proposed controller for saturated systems via simulation.",
                    "System level synthesis is a promising approach that formulates structured optimal controller synthesis problems as convex problems. This work solves the distributed linear-quadratic regulator problem under communication constraints directly in infinite-dimensional space, without the finite-impulse response relaxation common in related work. Our method can also be used to construct optimal distributed Kalman filters with limited information exchange. We combine the distributed Kalman filter with state-feedback control to perform localized LQG control with communication constraints. We provide agent-level implementation details for the resulting output-feedback state-space controller.",
                    "We try to classify Hopf algebras with the dual Chevalley property of discrete corepresentation type over an algebraically closed field $\\Bbb{k}$ with characteristic 0. For such Hopf algebra $H$, we characterize the link quiver of $H$ and determine the structures of the link-indecomposable component $H_{(1)}$ containing $\\Bbb{k}1$. Besides, we construct an infinite-dimensional non-pointed non-cosemisimple link-indecomposable Hopf algebra $H(e_{\\pm 1}, f_{\\pm 1}, u, v)$ with the dual Chevalley property of discrete corepresentation type.",
                    "Let $\\Bbbk$ be an algebraically closed field of characteristic 0 and $H$ a finite-dimensional Hopf algebra over $\\Bbbk$ with the dual Chevalley property. In this paper, we show that $\\operatorname{gr}^c(H)$ is of tame corepresentation type if and only if $\\operatorname{gr}^c(H)\\cong (\\Bbbk\\langle x,y\\rangle/I)^* \\times H^\\prime$ for some finite-dimensional semisimple Hopf algebra $H^\\prime$ and some special ideals $I$. Then, by the method of link quiver and bosonization, we discuss which of the above ideals will occur when $(\\Bbbk\\langle x,y\\rangle/I)^* \\times H_0$ is a Hopf algebra of tame corepresentation type under some assumptions.",
                    "We study graphs of polynomial growth from the perspective of asymptotic geometry and descriptive set theory. The starting point of our investigation is a theorem of Krauthgamer and Lee who showed that every connected graph of polynomial growth admits an injective contraction mapping to $(\\mathbb Z^n, \\|\\cdot\\|_\\infty)$ for some $n\\in\\mathbb N$. We strengthen and generalize this result in a number of ways. In particular, answering a question of Papasoglu, we construct coarse embeddings from graphs of polynomial growth to $\\mathbb Z^n$. Moreover, we only require $n$ to be linear in the asymptotic polynomial growth rate of the graph; this confirms a conjecture of Levin and Linial, London, and Rabinovich \"in the asymptotic sense.\" (The exact form of the conjecture was refuted by Krauthgamer and Lee.) All our results are proved for Borel graphs, which allows us to settle a number of problems in descriptive combinatorics. Roughly, we prove that graphs generated by free Borel actions of $\\mathbb Z^n$ are universal for the class of Borel graphs of polynomial growth. This provides a general method for extending results about $\\mathbb Z^n$-actions to all Borel graphs of polynomial growth. For example, an immediate consequence of our main result is that all Borel graphs of polynomial growth are hyperfinite, which answers a well-known question in the area. As another illustration, we show that Borel graphs of polynomial growth support a certain combinatorial structure called toast. An important technical tool in our arguments is the notion of padded decomposition from computer science, which is closely related to the concept of asymptotic dimension due to Gromov. Along the way we find an alternative, probabilistic proof of a theorem of Papasoglu that graphs of asymptotic polynomial growth rate $\\rho<\\infty$ have asymptotic dimension at most $\\rho$ and establish the same bound in the Borel setting."
                ],
                "domain": [
                    "Graph Theory",
                    "Control Theory",
                    "Algebra",
                    "Descriptive Set Theory"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ea1db525-16a0-4dd9-9389-3813a20d8ef4": {
                "pk": "ea1db525-16a0-4dd9-9389-3813a20d8ef4",
                "project_name": null,
                "name": "Yuhao Wang",
                "bio": "I am a researcher dedicated to advancing the fields of machine learning and causal inference, with a particular focus on medical applications and high-dimensional data analysis. My recent work has centered on developing innovative frameworks for medical image-text pre-training, specifically the Unified Medical Contrastive Learning (UMCL) framework, which addresses challenges in medical data scarcity and fine-grained differences in medical images. This framework has shown promising results in downstream tasks, such as automated radiology report generation, where I have also introduced a disease-oriented retrieval approach to enhance report accuracy.\n\nIn addition to my work in medical imaging, I have explored the identifiability of complex graph models and the robustness of reinforcement learning algorithms. My research on Bayesian risk MDPs has led to the development of a multi-stage Bayesian risk-averse Q-learning algorithm that effectively balances robustness and optimal policy learning. I have also contributed to the understanding of rerandomization in experimental design, demonstrating how it can achieve optimal precision while maintaining robustness against model misspecification.\n\nMy work extends to high-dimensional causal inference, where I have proposed a Double-Calibration strategy to improve treatment effect estimation. I am passionate about bridging theoretical advancements with practical applications, and I strive to create methodologies that enhance the reliability and efficiency of data-driven decision-making in various domains. Through my research, I aim to contribute to the development of robust, interpretable, and efficient machine learning models that can tackle real-world challenges.",
                "collaborators": [
                    "Enlu Zhou",
                    "Xinran Li",
                    "Caroline Uhler",
                    "Arnab Bhattacharyya",
                    "Kaiyue Wen",
                    "Tengyao Wang",
                    "Lin Liu",
                    "Xinbo Wang",
                    "Di Wu",
                    "Santiago Segarra"
                ],
                "pub_titles": [
                    "Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt",
                    "Reading Radiology Imaging Like The Radiologist",
                    "Identifiability of AMP chain graph models",
                    "Bayesian Risk-Averse Q-Learning with Streaming Observations",
                    "Optimal Computing Budget Allocation for Data-driven Ranking and Selection",
                    "Rerandomization with Diminishing Covariate Imbalance and Diverging Number of Covariates",
                    "Asymptotic Theory of the Best-Choice Rerandomization using the Mahalanobis Distance",
                    "Residual Permutation Test for High-Dimensional Regression Coefficient Testing",
                    "Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity",
                    "Data-driven Ranking and Selection under Input Uncertainty",
                    "High-Dimensional Joint Estimation of Multiple Directed Gaussian Graphical Models",
                    "Learning High-dimensional Gaussian Graphical Models under Total Positivity without Adjustment of Tuning Parameters",
                    "Amortized Variational Deep Q Network"
                ],
                "pub_abstracts": [
                    "Contrastive language-image Pre-training (CLIP) [13] can leverage large datasets of unlabeled Image-Text pairs, which have demonstrated impressive performance in various downstream tasks. Given that annotating medical data is time-consuming and laborious, Image-Text Pre-training has promising applications in exploiting large-scale medical image and radiology report datasets. However, medical Image-Text Pre-training faces several challenges, as follows: (1) Due to privacy concerns, the amount of available medical data is relatively small compared to natural data, leading to weaker generalization ability of the model. (2) Medical images are highly similar with only fine-grained differences in subtleties, resulting in a large number of false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt usually differs from the natural medical image report, Subtle changes in wording can lead to significant differences in performance. In this paper, we propose a unified Image-Text-Label contrastive learning framework based on continuous prompts, with three main contributions. First, We unified the data of images, text, and labels, which greatly expanded the training data that the model could utilize. Second, we address the issue of data diversity and the impact of hand-crafted prompts on model performance by introducing continuous implicit prompts. Lastly, we propose a ImageText-Label contrastive Training to mitigate the problem of too many false-negative samples. We demonstrate through sufficient experiments that the Unified Medical Contrastive Learning (UMCL) framework exhibits excellent performance on several downstream tasks.",
                    "Automated radiology report generation aims to generate radiology reports that contain rich, fine-grained descriptions of radiology imaging. Compared with image captioning in the natural image domain, medical images are very similar to each other, with only minor differences in the occurrence of diseases. Given the importance of these minor differences in the radiology report, it is crucial to encourage the model to focus more on the subtle regions of disease occurrence. Secondly, the problem of visual and textual data biases is serious. Not only do normal cases make up the majority of the dataset, but sentences describing areas with pathological changes also constitute only a small part of the paragraph. Lastly, generating medical image reports involves the challenge of long text generation, which requires more expertise and empirical training in medical knowledge. As a result, the difficulty of generating such reports is increased. To address these challenges, we propose a disease-oriented retrieval framework that utilizes similar reports as prior knowledge references. We design a factual consistency captioning generator to generate more accurate and factually consistent disease descriptions. Our framework can find most similar reports for a given disease from the CXR database by retrieving a disease-oriented mask consisting of the position and morphological characteristics. By referencing the disease-oriented similar report and the visual features, the factual consistency model can generate a more accurate radiology report.",
                    "We study identifiability of Andersson-Madigan-Perlman (AMP) chain graph models, which are a common generalization of linear structural equation models and Gaussian graphical models. AMP models are described by DAGs on chain components which themselves are undirected graphs.   For a known chain component decomposition, we show that the DAG on the chain components is identifiable if the determinants of the residual covariance matrices of the chain components are monotone non-decreasing in topological order. This condition extends the equal variance identifiability criterion for Bayes nets, and it can be generalized from determinants to any super-additive function on positive semidefinite matrices. When the component decomposition is unknown, we describe conditions that allow recovery of the full structure using a polynomial time algorithm based on submodular function minimization. We also conduct experiments comparing our algorithm's performance against existing baselines.",
                    "We consider a robust reinforcement learning problem, where a learning agent learns from a simulated training environment. To account for the model mis-specification between this training environment and the real environment due to lack of data, we adopt a formulation of Bayesian risk MDP (BRMDP) with infinite horizon, which uses Bayesian posterior to estimate the transition model and impose a risk functional to account for the model uncertainty. Observations from the real environment that is out of the agent's control arrive periodically and are utilized by the agent to update the Bayesian posterior to reduce model uncertainty. We theoretically demonstrate that BRMDP balances the trade-off between robustness and conservativeness, and we further develop a multi-stage Bayesian risk-averse Q-learning algorithm to solve BRMDP with streaming observations from real environment. The proposed algorithm learns a risk-averse yet optimal policy that depends on the availability of real-world observations. We provide a theoretical guarantee of strong convergence for the proposed algorithm.",
                    "In a fixed budget ranking and Selection (R&S) problem, one aims to identify the best design among a finite number of candidates by efficiently allocating the given computing budget to evaluate design performance. Classical methods for R&S usually assume the distribution of the randomness in the system is exactly known. In this paper, we consider the practical scenario where the true distribution is unknown but can be estimated from streaming input data that arrive in batches over time. We formulate the R&S problem in this dynamic setting as a multi-stage problem, where we adopt the Bayesian approach to estimate the distribution and formulate a stage-wise optimization problem to allocate the computing budget. We characterize the optimality conditions for the stage-wise problem by applying the large deviations theory to maximize the decay rate of probability of false selection. Based on the optimality conditions and combined with the updating of distribution estimates, we design two sequential budget allocation procedures for R&S under streaming input data. We theoretically guarantee the consistency and asymptotic optimality of the proposed procedures. We demonstrate the practical efficiency through numerical experiments in comparison with the equal allocation policy and an extension of the Optimal Computing Budget Allocation algorithm.",
                    "Completely randomized experiments have been the gold standard for drawing causal inference because they can balance all potential confounding on average. However, they may suffer from unbalanced covariates for realized treatment assignments. Rerandomization, a design that rerandomizes the treatment assignment until a prespecified covariate balance criterion is met, has recently got attention due to its easy implementation, improved covariate balance and more efficient inference. Researchers have then suggested to use the treatment assignments that minimize the covariate imbalance, namely the optimally balanced design. This has caused again the long-time controversy between two philosophies for designing experiments: randomization versus optimal and thus almost deterministic designs. Existing literature argued that rerandomization with overly balanced observed covariates can lead to highly imbalanced unobserved covariates, making it vulnerable to model misspecification. On the contrary, rerandomization with properly balanced covariates can provide robust inference for treatment effects while sacrificing some efficiency compared to the ideally optimal design. In this paper, we show it is possible that, by making the covariate imbalance diminishing at a proper rate as the sample size increases, rerandomization can achieve its ideally optimal precision that one can expect with perfectly balanced covariates, while still maintaining its robustness. We further investigate conditions on the number of covariates for achieving the desired optimality. Our results rely on a more delicate asymptotic analysis for rerandomization. The derived theory for rerandomization provides a deeper understanding of its large-sample property and can better guide its practical implementation. Furthermore, it also helps reconcile the controversy between randomized and optimal designs in an asymptotic sense.",
                    "Rerandomization, a design that utilizes pretreatment covariates and improves their balance between different treatment groups, has received attention recently in both theory and practice. There are at least two types of rerandomization that are used in practice: the first rerandomizes the treatment assignment until covariate imbalance is below a prespecified threshold; the second randomizes the treatment assignment multiple times and chooses the one with the best covariate balance. In this paper we will consider the second type of rerandomization, namely the best-choice rerandomization, whose theory and inference are still lacking in the literature. In particular, we will focus on the best-choice rerandomization that uses the Mahalanobis distance to measure covariate imbalance, which is one of the most commonly used imbalance measure for multivariate covariates and is invariant to affine transformations of covariates. We will study the large-sample repeatedly sampling properties of the best-choice rerandomization, allowing both the number of covariates and the number of tried complete randomizations to increase with the sample size. We show that the asymptotic distribution of the difference-in-means estimator is more concentrated around the true average treatment effect under rerandomization than under the complete randomization, and propose large-sample accurate confidence intervals for rerandomization that are shorter than that for the completely randomized experiment. We further demonstrate that, with moderate number of covariates and with the number of tried randomizations increasing polynomially with the sample size, the best-choice rerandomization can achieve the ideally optimal precision that one can expect even with perfectly balanced covariates. The developed theory and methods for rerandomization are also illustrated using real field experiments.",
                    "We consider the problem of testing whether a single coefficient is equal to zero in fixed-design linear models under a moderately high-dimensional regime, where the dimension of covariates $p$ is allowed to be in the same order of magnitude as sample size $n$. In this regime, to achieve finite-population validity, existing methods usually require strong distributional assumptions on the noise vector (such as Gaussian or rotationally invariant), which limits their applications in practice. In this paper, we propose a new method, called residual permutation test (RPT), which is constructed by projecting the regression residuals onto the space orthogonal to the union of the column spaces of the original and permuted design matrices. RPT can be proved to achieve finite-population size validity under fixed design with just exchangeable noises, whenever $p < n / 2$. Moreover, RPT is shown to be asymptotically powerful for heavy tailed noises with bounded $(1+t)$-th order moment when the true coefficient is at least of order $n^{-t/(1+t)}$ for $t \\in [0,1]$. We further proved that this signal size requirement is essentially rate-optimal in the minimax sense. Numerical studies confirm that RPT performs well in a wide range of simulation settings with normal and heavy-tailed noise distributions.",
                    "Treatment effect estimation under unconfoundedness is a fundamental task in causal inference. In response to the challenge of analyzing high-dimensional datasets collected in substantive fields such as epidemiology, genetics, economics, and social sciences, various methods for treatment effect estimation with high-dimensional nuisance parameters (the outcome regression and the propensity score) have been developed in recent years. However, it is still unclear what is the necessary and sufficient sparsity condition on the nuisance parameters such that we can estimate the treatment effect at $1 / \\sqrt{n}$-rate. In this paper, we propose a new Double-Calibration strategy that corrects the estimation bias of the nuisance parameter estimates computed by regularized high-dimensional techniques and demonstrate that the corresponding Doubly-Calibrated estimator achieves $1 / \\sqrt{n}$-rate as long as one of the nuisance parameters is sparse with sparsity below $\\sqrt{n} / \\log p$, where $p$ denotes the ambient dimension of the covariates, whereas the other nuisance parameter can be arbitrarily complex and completely misspecified. The Double-Calibration strategy can also be applied to settings other than treatment effect estimation, e.g. regression coefficient estimation in the presence of a diverging number of controls in a semiparametric partially linear model, and local average treatment effect estimation with instrumental variables.",
                    "We consider a simulation-based Ranking and Selection (R&S) problem with input uncertainty, where unknown input distributions can be estimated using input data arriving in batches of varying sizes over time. Each time a batch arrives, additional simulations can be run using updated input distribution estimates. The goal is to confidently identify the best design after collecting as few batches as possible. We first introduce a moving average estimator for aggregating simulation outputs generated under heterogenous input distributions. Then, based on a Sequential Elimination framework, we devise two major R&S procedures by establishing exact and asymptotic confidence bands for the estimator. In deriving the latter confidence bands, we incorporate the result of \"Multiple Comparison with Best\" and establish an asymptotic normality result which explicitly characterizes the tradeoff between input uncertainty and stochastic uncertainty in an online environment. We also extend our procedures to the indifference zone setting, which helps save simulation effort for practical usage. Numerical results show the effectiveness and necessity of our procedures. Moreover, the efficiency can be further boosted through optimizing the \"drop rate\" parameter of the estimator.",
                    "We consider the problem of jointly estimating multiple related directed acyclic graph (DAG) models based on high-dimensional data from each graph. This problem is motivated by the task of learning gene regulatory networks based on gene expression data from different tissues, developmental stages or disease states. We prove that under certain regularity conditions, the proposed $\\ell_0$-penalized maximum likelihood estimator converges in Frobenius norm to the adjacency matrices consistent with the data-generating distributions and has the correct sparsity. In particular, we show that this joint estimation procedure leads to a faster convergence rate than estimating each DAG model separately. As a corollary, we also obtain high-dimensional consistency results for causal inference from a mix of observational and interventional data. For practical purposes, we propose \\emph{jointGES} consisting of Greedy Equivalence Search (GES) to estimate the union of all DAG models followed by variable selection using lasso to obtain the different DAGs, and we analyze its consistency guarantees. The proposed method is illustrated through an analysis of simulated data as well as epithelial ovarian cancer gene expression data.",
                    "We consider the problem of estimating an undirected Gaussian graphical model when the underlying distribution is multivariate totally positive of order 2 (MTP2), a strong form of positive dependence. Such distributions are relevant for example for portfolio selection, since assets are usually positively dependent. A large body of methods have been proposed for learning undirected graphical models without the MTP2 constraint. A major limitation of these methods is that their structure recovery guarantees in the high-dimensional setting usually require a particular choice of a tuning parameter, which is unknown a priori in real world applications. We here propose a new method to estimate the underlying undirected graphical model under MTP2 and show that it is provably consistent in structure recovery without adjusting the tuning parameters. This is achieved by a constraint-based estimator that infers the structure of the underlying graphical model by testing the signs of the empirical partial correlation coefficients. We evaluate the performance of our estimator in simulations and on financial data.",
                    "Efficient exploration is one of the most important issues in deep reinforcement learning. To address this issue, recent methods consider the value function parameters as random variables, and resort variational inference to approximate the posterior of the parameters. In this paper, we propose an amortized variational inference framework to approximate the posterior distribution of the action value function in Deep Q Network. We establish the equivalence between the loss of the new model and the amortized variational inference loss. We realize the balance of exploration and exploitation by assuming the posterior as Cauchy and Gaussian, respectively in a two-stage training process. We show that the amortized framework can results in significant less learning parameters than existing state-of-the-art method. Experimental results on classical control tasks in OpenAI Gym and chain Markov Decision Process tasks show that the proposed method performs significantly better than state-of-art methods and requires much less training time."
                ],
                "domain": [
                    "Medical Imaging",
                    "Causal Inference",
                    "Reinforcement Learning",
                    "Graphical Models"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ef70dc01-bb70-448f-b051-b39c567051f0": {
                "pk": "ef70dc01-bb70-448f-b051-b39c567051f0",
                "project_name": null,
                "name": "Xiaotong Li",
                "bio": "I am a researcher with a diverse background in mathematics, statistics, and applied technology, focusing on areas such as algebra, stochastic differential equations, and assistive technologies for individuals with visual impairments. My work on the twisted Heisenberg-Virasoro algebra has led to the discovery of non-inner and non-skew-symmetric biderivations, contributing to the understanding of linear commuting maps and post-Lie algebra structures.\n\nIn the realm of graph theory, I have utilized matrix techniques to compute the Ihara-zeta function and analyze spanning trees in semi-regular bipartite graphs, revealing the intricate relationship between graph spectra and zeta functions. My research also extends to numerical methods for stochastic differential equations, where I have developed strong convergence techniques for semi-implicit methods driven by Lévy processes, uncovering new insights into convergence rates and invariant measures.\n\nBeyond theoretical work, I am passionate about applying my research to real-world challenges. I have developed a vision-based wearable steering assistance system for individuals with impaired vision, focusing on enhancing mobility in athletic environments. This project involved creating a lightweight multitask network for obstacle detection and collecting a new dataset to support multi-task detection in athletics.\n\nAdditionally, I have tackled complex logistical problems by proposing a joint model for hub network design, integrating strategic and operational decisions through innovative mixed-integer programming techniques. My work aims to provide practical solutions that enhance efficiency in transportation networks, ultimately contributing to improved accessibility and mobility for all.",
                "collaborators": [
                    "Wei Liu",
                    "Xiaotong Liu",
                    "Xiaomin Tang",
                    "Xian'an Jin",
                    "Qi Yan",
                    "Tianjiao Tang",
                    "Hongjiong Tian",
                    "Binglu Wang",
                    "Zhijun Li"
                ],
                "pub_titles": [
                    "Biderivations of the twisted Heisenberg-Virasoro algebra and their applications",
                    "The Ihara-zeta function and the spectrum of the join of two semi-regular bipartite graphs",
                    "Truncated Euler-Maruyama method for time-changed stochastic differential equations with super-linear state variables and Hölder's continuous time variables",
                    "The semi-implicit Euler-Maruyama method for nonlinear non-autonomous stochastic differential equations driven by a class of Lévy processes",
                    "Vision-based Wearable Steering Assistance for People with Impaired Vision in Jogging",
                    "Hub Network Design Problem with Capacity, Congestion and Heterogeneous Economies of Scale"
                ],
                "pub_abstracts": [
                    "In this paper, the biderivations without the skew-symmetric condition of the twisted Heisenberg-Virasoro algebra are presented. We find some non-inner and non-skew-symmetric biderivations. As applications, the characterizations of the forms of linear commuting maps and the commutative post-Lie algebra structures on the twisted Heisenberg-Virasoro algebra are given. It also is proved that every biderivation of the graded twisted Heisenberg-Virasoro left-symmetric algebra is trivial.",
                    "In this paper, using matrix techniques, we compute the Ihara-zeta function and the number of spanning trees of the join of two semi-regular bipartite graphs. Furthermore, we show that the spectrum and the zeta function of the join of two semi-regular bipartite graphs can determine each other.",
                    "An explicit numerical method is developed for a class of non-autonomous time-changed stochastic differential equations, whose coefficients obey H\\\"older's continuity in terms of the time variables and are allowed to grow super-linearly in terms of the state variables. The strong convergence of the method in the finite time interval is proved and the convergence rate is obtained. Numerical simulations are provided.",
                    "The strong convergence of the semi-implicit Euler-Maruyama (EM) method for stochastic differential equations with non-linear coefficients driven by a class of L\\'evy processes is investigated. The dependence of the convergence order of the numerical scheme on the parameters of the class of L\\'evy processes is discovered, which is different from existing results. In addition, the existence and uniqueness of numerical invariant measure of the semi-implicit EM method is studied and its convergence to the underlying invariant measure is also proved. Numerical examples are provided to confirm our theoretical results.",
                    "Outdoor sports pose a challenge for people with impaired vision. The demand for higher-speed mobility inspired us to develop a vision-based wearable steering assistance. To ensure broad applicability, we focused on a representative sports environment, the athletics track. Our efforts centered on improving the speed and accuracy of perception, enhancing planning adaptability for the real world, and providing swift and safe assistance for people with impaired vision. In perception, we engineered a lightweight multitask network capable of simultaneously detecting track lines and obstacles. Additionally, due to the limitations of existing datasets for supporting multi-task detection in athletics tracks, we diligently collected and annotated a new dataset (MAT) containing 1000 images. In planning, we integrated the methods of sampling and spline curves, addressing the planning challenges of curves. Meanwhile, we utilized the positions of the track lines and obstacles as constraints to guide people with impaired vision safely along the current track. Our system is deployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it demonstrated adaptability in different sports scenarios, assisting users in achieving free movement of 400-meter at an average speed of 1.34 m/s, meeting the level of normal people in jogging. Our MAT dataset is publicly available from https://github.com/snoopy-l/MAT",
                    "We propose a joint model that links the strategic level location and capacity decisions with the operational level routing and hub assignment decisions to solve hub network design problem with congestion and heterogeneous economics of scale. We also develop a novel flow-based mixed-integer second-order cone programming (MISOCP) formulation. We perform numerical experiments on a real-world data set to validate the efficiency of solving the MISOCP reformulation. The numerical studies yield observations can be used as guidelines in the design of transportation network for a logistics company."
                ],
                "domain": [
                    "Algebra",
                    "Stochastic Processes",
                    "Computer Vision",
                    "Optimization"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b5ec9787-1532-4e14-b9d8-a840f5201967": {
                "pk": "b5ec9787-1532-4e14-b9d8-a840f5201967",
                "project_name": null,
                "name": "Zhuoyi Xiang",
                "bio": "I am a researcher dedicated to the intersection of artificial intelligence and biomolecular design, with a focus on enhancing drug discovery, synthetic biology, and enzyme engineering. My recent work centers around bridging the gap between AI's computational capabilities and the nuanced understanding of biomolecular complexity that researchers possess. I have developed InstructBioMol, a novel large language model (LLM) that facilitates an any-to-any alignment between natural language and biomolecules. This innovative model allows researchers to express their design goals in natural language, enabling the generation of biomolecular outputs that align with specific biological needs.\n\nThrough InstructBioMol, I have demonstrated the ability to generate drug molecules with a 10% improvement in binding affinity and design enzymes that achieve an ESP Score of 70.4, surpassing the recommended threshold for enzyme-substrate interactions. My work aims to transform the landscape of biomolecular research by making it more intuitive and accessible, ultimately driving advancements in the field. I am passionate about leveraging AI to unlock new possibilities in biomolecular design and contribute to the future of drug discovery and synthetic biology.",
                "collaborators": [
                    "Xiang Zhuang",
                    "Keyan Ding",
                    "Tianwen Lyu",
                    "Yinuo Jiang",
                    "Xiaotong Li",
                    "Zeyuan Wang",
                    "Ming Qin",
                    "Kehua Feng",
                    "Jike Wang",
                    "Qiang Zhang"
                ],
                "pub_titles": [
                    "InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions"
                ],
                "pub_abstracts": [
                    "Understanding and designing biomolecules, such as proteins and small molecules, is central to advancing drug discovery, synthetic biology, and enzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have revolutionized biomolecular research, achieving remarkable accuracy in biomolecular prediction and design. However, a critical gap remains between AI's computational power and researchers' intuition, using natural language to align molecular complexity with human intentions. Large Language Models (LLMs) have shown potential to interpret human intentions, yet their application to biomolecular research remains nascent due to challenges including specialized knowledge requirements, multimodal data integration, and semantic alignment between natural language and biomolecules. To address these limitations, we present InstructBioMol, a novel LLM designed to bridge natural language and biomolecules through a comprehensive any-to-any alignment of natural language, molecules, and proteins. This model can integrate multimodal biomolecules as input, and enable researchers to articulate design goals in natural language, providing biomolecular outputs that meet precise biological needs. Experimental results demonstrate InstructBioMol can understand and design biomolecules following human instructions. Notably, it can generate drug molecules with a 10% improvement in binding affinity and design enzymes that achieve an ESP Score of 70.4, making it the only method to surpass the enzyme-substrate interaction threshold of 60.0 recommended by the ESP developer. This highlights its potential to transform real-world biomolecular research."
                ],
                "domain": [
                    "Biomolecular Design",
                    "Large Language Models",
                    "Drug Discovery",
                    "Synthetic Biology"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "24c237b6-6295-4fab-8e41-cbff68150b47": {
                "pk": "24c237b6-6295-4fab-8e41-cbff68150b47",
                "project_name": null,
                "name": "Kehua Feng",
                "bio": "I am a researcher dedicated to advancing the capabilities of large language models (LLMs) and their applications in scientific domains. My recent work addresses the challenges of evaluating LLMs, particularly in terms of their performance in understanding and generating scientific knowledge. I developed a sample-efficient human evaluation method based on Maximum Discrepancy (MAD) competition, which allows for reliable comparisons of LLMs across various skills, including knowledge understanding and mathematical reasoning.\n\nRecognizing the need for comprehensive benchmarks, I introduced the SciKnowEval framework, which systematically evaluates LLMs on their scientific knowledge across five progressive levels. This initiative not only benchmarks 26 advanced LLMs but also highlights areas for improvement, particularly in scientific reasoning.\n\nAdditionally, I have tackled the issue of factual accuracy in generated texts with the FacTool framework, designed to detect factual errors across diverse tasks. My research also extends to biomolecular design, where I developed InstructBioMol, a novel LLM that bridges the gap between natural language and biomolecular research. This model enables researchers to articulate design goals in natural language, resulting in significant advancements in drug molecule design and enzyme engineering.\n\nThrough my work, I aim to enhance the understanding and utility of LLMs in scientific research, ultimately contributing to the development of more robust and effective AI tools in various fields.",
                "collaborators": [
                    "Keyan Ding",
                    "Qiang Zhang",
                    "Huajun Chen",
                    "Xiang Zhuang",
                    "Zeyuan Wang",
                    "Ming Qin",
                    "Kede Ma",
                    "Zhihua Wang",
                    "Weijie Wang",
                    "Yu Zhao"
                ],
                "pub_titles": [
                    "Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition",
                    "SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models",
                    "FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
                    "InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions"
                ],
                "pub_abstracts": [
                    "The past years have witnessed a proliferation of large language models (LLMs). Yet, automated and unbiased evaluation of LLMs is challenging due to the inaccuracy of standard metrics in reflecting human preferences and the inefficiency in sampling informative and diverse test examples. While human evaluation remains the gold standard, it is expensive and time-consuming, especially when dealing with a large number of testing samples. To address this problem, we propose a sample-efficient human evaluation method based on MAximum Discrepancy (MAD) competition. MAD automatically selects a small set of informative and diverse instructions, each adapted to two LLMs, whose responses are subject to three-alternative forced choice by human subjects. The pairwise comparison results are then aggregated into a global ranking using the Elo rating system. We select eight representative LLMs and compare them in terms of four skills: knowledge understanding, mathematical reasoning, writing, and coding. Experimental results show that the proposed method achieves a reliable and sensible ranking of LLMs' capabilities, identifies their relative strengths and weaknesses, and offers valuable insights for further LLM advancement.",
                    "Large language models (LLMs) have gained increasing prominence in scientific research, but there is a lack of comprehensive benchmarks to fully evaluate their proficiency in understanding and mastering scientific knowledge. To address this need, we introduce the SciKnowEval benchmark, a novel framework that systematically evaluates LLMs across five progressive levels of scientific knowledge: studying extensively, inquiring earnestly, thinking profoundly, discerning clearly, and practicing assiduously. These levels aim to assess the breadth and depth of scientific knowledge in LLMs, including memory, comprehension, reasoning, discernment, and application. Specifically, we first construct a large-scale evaluation dataset encompassing 70K multi-level scientific problems and solutions in the domains of biology, chemistry, physics, and materials science. By leveraging this dataset, we benchmark 26 advanced open-source and proprietary LLMs using zero-shot and few-shot prompting strategies. The results reveal that despite the state-of-the-art performance of proprietary LLMs, there is still significant room for improvement, particularly in addressing scientific reasoning and applications. We anticipate that SciKnowEval will establish a standard for benchmarking LLMs in science research and promote the development of stronger scientific LLMs. The dataset and code are publicly available at https://scimind.ai/sciknoweval .",
                    "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
                    "Understanding and designing biomolecules, such as proteins and small molecules, is central to advancing drug discovery, synthetic biology, and enzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have revolutionized biomolecular research, achieving remarkable accuracy in biomolecular prediction and design. However, a critical gap remains between AI's computational power and researchers' intuition, using natural language to align molecular complexity with human intentions. Large Language Models (LLMs) have shown potential to interpret human intentions, yet their application to biomolecular research remains nascent due to challenges including specialized knowledge requirements, multimodal data integration, and semantic alignment between natural language and biomolecules. To address these limitations, we present InstructBioMol, a novel LLM designed to bridge natural language and biomolecules through a comprehensive any-to-any alignment of natural language, molecules, and proteins. This model can integrate multimodal biomolecules as input, and enable researchers to articulate design goals in natural language, providing biomolecular outputs that meet precise biological needs. Experimental results demonstrate InstructBioMol can understand and design biomolecules following human instructions. Notably, it can generate drug molecules with a 10% improvement in binding affinity and design enzymes that achieve an ESP Score of 70.4, making it the only method to surpass the enzyme-substrate interaction threshold of 60.0 recommended by the ESP developer. This highlights its potential to transform real-world biomolecular research."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Large Language Models",
                    "Biomolecular Research",
                    "Automated Evaluation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "16198a61-d1f9-4ea7-a6f6-def64c727db4": {
                "pk": "16198a61-d1f9-4ea7-a6f6-def64c727db4",
                "project_name": null,
                "name": "Xiang Zhuang",
                "bio": "I am a researcher dedicated to advancing the intersection of artificial intelligence and biomolecular science. My recent work focuses on leveraging graph-based approaches and large language models (LLMs) to enhance molecular property prediction and biomolecular design. I developed the Graph Sampling-based Meta-learning (GS-Meta) framework, which effectively utilizes many-to-many correlations between molecules and properties for few-shot learning, achieving significant improvements in predictive performance.\n\nRecognizing the potential of LLMs in biomolecular research, I introduced InstructBioMol, a model that bridges natural language and biomolecular design, enabling researchers to articulate design goals and generate optimized drug molecules and enzymes. My work also addresses the critical challenge of out-of-distribution generalization in molecular representation learning, proposing a novel framework that identifies invariant features to enhance robustness against distribution shifts.\n\nAdditionally, I have explored the integration of domain knowledge into molecular representation through the Contrastive Knowledge-aware GNN (CKGNN), which improves the ability to distinguish between chemically similar molecules. My research extends to optimizing transformer architectures for language modeling, where I introduced StableMask to refine attention mechanisms and enhance model performance.\n\nThrough these contributions, I aim to transform how we understand and design biomolecules, bridging the gap between computational power and human intuition in the field of drug discovery and synthetic biology.",
                "collaborators": [
                    "Qiang Zhang",
                    "Huajun Chen",
                    "Keyan Ding",
                    "Yin Fang",
                    "Xiaotong Li",
                    "Zeyuan Wang",
                    "Ming Qin",
                    "Bin Wu",
                    "Tianwen Lyu",
                    "Yinuo Jiang"
                ],
                "pub_titles": [
                    "Graph Sampling-based Meta-Learning for Molecular Property Prediction",
                    "InstructBioMol: Advancing Biomolecule Understanding and Design Following Human Instructions",
                    "Learning Invariant Molecular Representation in Latent Discrete Space",
                    "Knowledge-aware Contrastive Molecular Graph Learning",
                    "StableMask: Refining Causal Masking in Decoder-only Transformer",
                    "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction"
                ],
                "pub_abstracts": [
                    "Molecular property is usually observed with a limited number of samples, and researchers have considered property prediction as a few-shot problem. One important fact that has been ignored by prior works is that each molecule can be recorded with several different properties simultaneously. To effectively utilize many-to-many correlations of molecules and properties, we propose a Graph Sampling-based Meta-learning (GS-Meta) framework for few-shot molecular property prediction. First, we construct a Molecule-Property relation Graph (MPG): molecule and properties are nodes, while property labels decide edges. Then, to utilize the topological information of MPG, we reformulate an episode in meta-learning as a subgraph of the MPG, containing a target property node, molecule nodes, and auxiliary property nodes. Third, as episodes in the form of subgraphs are no longer independent of each other, we propose to schedule the subgraph sampling process with a contrastive loss function, which considers the consistency and discrimination of subgraphs. Extensive experiments on 5 commonly-used benchmarks show GS-Meta consistently outperforms state-of-the-art methods by 5.71%-6.93% in ROC-AUC and verify the effectiveness of each proposed module. Our code is available at https://github.com/HICAI-ZJU/GS-Meta.",
                    "Understanding and designing biomolecules, such as proteins and small molecules, is central to advancing drug discovery, synthetic biology, and enzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have revolutionized biomolecular research, achieving remarkable accuracy in biomolecular prediction and design. However, a critical gap remains between AI's computational power and researchers' intuition, using natural language to align molecular complexity with human intentions. Large Language Models (LLMs) have shown potential to interpret human intentions, yet their application to biomolecular research remains nascent due to challenges including specialized knowledge requirements, multimodal data integration, and semantic alignment between natural language and biomolecules. To address these limitations, we present InstructBioMol, a novel LLM designed to bridge natural language and biomolecules through a comprehensive any-to-any alignment of natural language, molecules, and proteins. This model can integrate multimodal biomolecules as input, and enable researchers to articulate design goals in natural language, providing biomolecular outputs that meet precise biological needs. Experimental results demonstrate InstructBioMol can understand and design biomolecules following human instructions. Notably, it can generate drug molecules with a 10% improvement in binding affinity and design enzymes that achieve an ESP Score of 70.4, making it the only method to surpass the enzyme-substrate interaction threshold of 60.0 recommended by the ESP developer. This highlights its potential to transform real-world biomolecular research.",
                    "Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts. Our code is available at https://github.com/HICAI-ZJU/iMoLD.",
                    "Leveraging domain knowledge including fingerprints and functional groups in molecular representation learning is crucial for chemical property prediction and drug discovery. When modeling the relation between graph structure and molecular properties implicitly, existing works can hardly capture structural or property changes and complex structure, with much smaller atom vocabulary and highly frequent atoms. In this paper, we propose the Contrastive Knowledge-aware GNN (CKGNN) for self-supervised molecular representation learning to fuse domain knowledge into molecular graph representation. We explicitly encode domain knowledge via knowledge-aware molecular encoder under the contrastive learning framework, ensuring that the generated molecular embeddings equipped with chemical domain knowledge to distinguish molecules with similar chemical formula but dissimilar functions. Extensive experiments on 8 public datasets demonstrate the effectiveness of our model with a 6\\% absolute improvement on average against strong competitors. Ablation study and further investigation also verify the best of both worlds: incorporation of chemical domain knowledge into self-supervised learning.",
                    "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
                    "Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding."
                ],
                "domain": [
                    "Molecular Property Prediction",
                    "Graph Neural Network",
                    "Large Language Models",
                    "Self-Supervised Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "988b75f5-dbd3-4287-ab08-a0e7c7a779b0": {
                "pk": "988b75f5-dbd3-4287-ab08-a0e7c7a779b0",
                "project_name": null,
                "name": "Zeyuan Wang",
                "bio": "I am a researcher dedicated to advancing methodologies in control systems, machine learning, and medical diagnostics. My recent work focuses on leader-following consensus control for multi-agent systems, where I developed a dynamic event-triggered control framework that significantly reduces communication frequency while ensuring convergence. \n\nIn the realm of medical data analysis, I introduced AMI-Net+, a multi-instance neural network that effectively handles incomplete and imbalanced datasets, outperforming existing models. My approach integrates advanced techniques like multi-head attention and self-adaptive pooling to enhance diagnostic accuracy. Additionally, I have explored syndrome differentiation in Traditional Chinese Medicine using a convolutional neural network framework, demonstrating its effectiveness even with noisy and incomplete data.\n\nI also investigate adversarial attacks on video recognition models, employing reinforcement learning for efficient frame selection to optimize the attack process. My work extends to the engineering domain, where I analyze coaxial compound helicopters, proposing innovative control strategies that improve flight performance and reduce power requirements.\n\nThrough my research, I aim to bridge theoretical advancements with practical applications, contributing to fields ranging from control theory to healthcare and machine learning. My goal is to develop robust, efficient solutions that address real-world challenges while pushing the boundaries of current methodologies.",
                "collaborators": [
                    "Josiah Poon",
                    "Simon Poon",
                    "Shiding Sun",
                    "Mohammed Chadli",
                    "Chaofeng Sha",
                    "Su Yang",
                    "Yuan Su",
                    "Yihua Cao",
                    "Yifan Zhao",
                    "Jia Li"
                ],
                "pub_titles": [
                    "Dynamic event-triggered control for multi-agent systems with adjustable inter-event time: a moving average approach",
                    "AMI-Net+: A Novel Multi-Instance Neural Network for Medical Diagnosis from Incomplete and Imbalanced Data",
                    "CNN based Multi-Instance Multi-Task Learning for Syndrome Differentiation of Diabetic Patients",
                    "Attention-based Multi-instance Neural Network for Medical Diagnosis from Incomplete and Low Quality Data",
                    "Reinforcement Learning Based Sparse Black-box Adversarial Attack on Video Recognition Models",
                    "A Hybrid Trim Strategy for Coaxial Compound Helicopter",
                    "Cooperative Bi-path Metric for Few-shot Learning"
                ],
                "pub_abstracts": [
                    "This extended abstract presents our recent work on the leader-following consensus control for generic linear multi-agent systems. An improved dynamic event-triggered control framework are proposed, based on a moving average approach. The proposed methods involve model-based estimation and clock-like auxiliary dynamic variables to increase the inter-event time as long as possible eventually. Compared to the static event-triggered strategy and the existing state-of-the-art dynamic event-triggered mechanism, the proposed approach significantly reduces the communication frequency while still guaranteeing asymptotic convergence. Numerical simulations demonstrate the validity of the proposed theoretical results.",
                    "In medical real-world study (RWS), how to fully utilize the fragmentary and scarce information in model training to generate the solid diagnosis results is a challenging task. In this work, we introduce a novel multi-instance neural network, AMI-Net+, to train and predict from the incomplete and extremely imbalanced data. It is more effective than the state-of-art method, AMI-Net. First, we also implement embedding, multi-head attention and gated attention-based multi-instance pooling to capture the relations of symptoms themselves and with the given disease. Besides, we propose var-ious improvements to AMI-Net, that the cross-entropy loss is replaced by focal loss and we propose a novel self-adaptive multi-instance pooling method on instance-level to obtain the bag representation. We validate the performance of AMI-Net+ on two real-world datasets, from two different medical domains. Results show that our approach outperforms other base-line models by a considerable margin.",
                    "Syndrome differentiation in Traditional Chinese Medicine (TCM) is the process of understanding and reasoning body condition, which is the essential step and premise of effective treatments. However, due to its complexity and lack of standardization, it is challenging to achieve. In this study, we consider each patient's record as a one-dimensional image and symptoms as pixels, in which missing and negative values are represented by zero pixels. The objective is to find relevant symptoms first and then map them to proper syndromes, that is similar to the object detection problem in computer vision. Inspired from it, we employ multi-instance multi-task learning combined with the convolutional neural network (MIMT-CNN) for syndrome differentiation, which takes region proposals as input and output image labels directly. The neural network consists of region proposals generation, convolutional layer, fully connected layer, and max pooling (multi-instance pooling) layer followed by the sigmoid function in each syndrome prediction task for image representation learning and final results generation. On the diabetes dataset, it performs better than all other baseline methods. Moreover, it shows stability and reliability to generate results, even on the dataset with small sample size, a large number of missing values and noises.",
                    "One way to extract patterns from clinical records is to consider each patient record as a bag with various number of instances in the form of symptoms. Medical diagnosis is to discover informative ones first and then map them to one or more diseases. In many cases, patients are represented as vectors in some feature space and a classifier is applied after to generate diagnosis results. However, in many real-world cases, data is often of low-quality due to a variety of reasons, such as data consistency, integrity, completeness, accuracy, etc. In this paper, we propose a novel approach, attention based multi-instance neural network (AMI-Net), to make the single disease classification only based on the existing and valid information in the real-world outpatient records. In the context of a patient, it takes a bag of instances as input and output the bag label directly in end-to-end way. Embedding layer is adopted at the beginning, mapping instances into an embedding space which represents the individual patient condition. The correlations among instances and their importance for the final classification are captured by multi-head attention transformer, instance-level multi-instance pooling and bag-level multi-instance pooling. The proposed approach was test on two non-standardized and highly imbalanced datasets, one in the Traditional Chinese Medicine (TCM) domain and the other in the Western Medicine (WM) domain. Our preliminary results show that the proposed approach outperforms all baselines results by a significant margin.",
                    "We explore the black-box adversarial attack on video recognition models. Attacks are only performed on selected key regions and key frames to reduce the high computation cost of searching adversarial perturbations on a video due to its high dimensionality. To select key frames, one way is to use heuristic algorithms to evaluate the importance of each frame and choose the essential ones. However, it is time inefficient on sorting and searching. In order to speed up the attack process, we propose a reinforcement learning based frame selection strategy. Specifically, the agent explores the difference between the original class and the target class of videos to make selection decisions. It receives rewards from threat models which indicate the quality of the decisions. Besides, we also use saliency detection to select key regions and only estimate the sign of gradient instead of the gradient itself in zeroth order optimization to further boost the attack process. We can use the trained model directly in the untargeted attack or with little fine-tune in the targeted attack, which saves computation time. A range of empirical results on real datasets demonstrate the effectiveness and efficiency of the proposed method.",
                    "Interest in the coaxial compound helicopter (CCH) has been increasing in the civil aviation and engineering community for its high-speed and high-maneuverability features, and is likely to continue to do so for the foreseeable future. Since the control in CCH is totally different from the conventional helicopter, the redundant control strategy design is one of the biggest challenges. In this study, the CCH model based on XH-59A is built to investigate the impact of the propeller and the elevator on the flight performance. Four trim strategies with different objectives are proposed and then compared to find the optimal control allocation. A heuristic descent search method is applied to search the optimal velocity at which the propeller and the elevator are engaged. A significant improvement of power required at medium and high speed with acceptable rotor airloads increment was found by using the Hybrid Trim strategy in the speed range of 0-100m/s, with regard to a pre-configured pitch angle schedule. The corresponding control variables obtained locate in a reasonable control range, with a maximum power reduced of 13% at 100m/s, which showcases the potential of the Hybrid Trim strategy.",
                    "Given base classes with sufficient labeled samples, the target of few-shot classification is to recognize unlabeled samples of novel classes with only a few labeled samples. Most existing methods only pay attention to the relationship between labeled and unlabeled samples of novel classes, which do not make full use of information within base classes. In this paper, we make two contributions to investigate the few-shot classification problem. First, we report a simple and effective baseline trained on base classes in the way of traditional supervised learning, which can achieve comparable results to the state of the art. Second, based on the baseline, we propose a cooperative bi-path metric for classification, which leverages the correlations between base classes and novel classes to further improve the accuracy. Experiments on two widely used benchmarks show that our method is a simple and effective framework, and a new state of the art is established in the few-shot classification field."
                ],
                "domain": [
                    "Control Systems",
                    "Machine Learning",
                    "Medical Diagnosis",
                    "Adversarial Attacks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "9bd372c7-a9f7-49e5-b779-70e73e26da05": {
                "pk": "9bd372c7-a9f7-49e5-b779-70e73e26da05",
                "project_name": null,
                "name": "Ming Qin",
                "bio": "I am a researcher dedicated to the field of image restoration, particularly through the lens of deep convolutional neural networks (DCNNs). My recent work challenges the conventional approach of merely stacking layers in neural networks for super-resolution tasks. Instead, I advocate for a model architecture that integrates physical principles and a priori knowledge of the image restoration process. By employing a symmetric non-linear color space and introducing innovative techniques like the \"reuse plus patch\" method, I aim to enhance the performance of super-resolution across various scaling factors. My findings demonstrate that this approach not only improves the quality of image restoration but also offers a fresh perspective on how we can construct neural networks that are more aligned with the underlying physics of the tasks they are designed to solve. I am passionate about pushing the boundaries of what is possible in image processing and continually seek to refine my models for even greater efficacy.",
                "collaborators": [
                    "Yiwen Huang"
                ],
                "pub_titles": [
                    "Densely Connected High Order Residual Network for Single Frame Image Super Resolution"
                ],
                "pub_abstracts": [
                    "Deep convolutional neural networks (DCNN) have been widely adopted for research on super resolution recently, however previous work focused mainly on stacking as many layers as possible in their model, in this paper, we present a new perspective regarding to image restoration problems that we can construct the neural network model reflecting the physical significance of the image restoration process, that is, embedding the a priori knowledge of image restoration directly into the structure of our neural network model, we employed a symmetric non-linear colorspace, the sigmoidal transfer, to replace traditional transfers such as, sRGB, Rec.709, which are asymmetric non-linear colorspaces, we also propose a \"reuse plus patch\" method to deal with super resolution of different scaling factors, our proposed methods and model show generally superior performance over previous work even though our model was only roughly trained and could still be underfitting the training set."
                ],
                "domain": [
                    "Image Restoration",
                    "Super Resolution",
                    "Deep Learning",
                    "Convolutional Neural Networks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "8b58ab8e-206d-4f06-a5d0-c9475e998321": {
                "pk": "8b58ab8e-206d-4f06-a5d0-c9475e998321",
                "project_name": null,
                "name": "Mengyao Zhang",
                "bio": "I am a researcher dedicated to enhancing robotic object grasping technologies, focusing on the challenges posed by missing or noisy ground truth data during the training of convolutional neural networks (CNNs). My work addresses these critical issues by developing innovative loss functions that improve model accuracy and robustness. Specifically, I introduced a novel predicted category probability method for handling unlabeled samples, which synergizes effectively with pseudo-labeling techniques. Additionally, I proposed a symmetric loss function designed to mitigate the impact of label noise, ensuring that the training process remains resilient against data corruption.\n\nThrough rigorous experimentation with typical grasping neural networks, I have demonstrated that these new loss functions can lead to performance improvements ranging from 2% to 13%. My research not only contributes to the theoretical understanding of loss functions in machine learning but also has practical implications for the deployment of more reliable and efficient robotic systems in real-world applications. I am passionate about pushing the boundaries of what robots can achieve in terms of perception and interaction with their environments, and I am excited to continue exploring innovative solutions in this dynamic field.",
                "collaborators": [
                    "Yangfan Deng",
                    "Yong Zhao"
                ],
                "pub_titles": [
                    "Robust Loss Functions for Object Grasping under Limited Ground Truth"
                ],
                "pub_abstracts": [
                    "Object grasping is a crucial technology enabling robots to perceive and interact with the environment sufficiently. However, in practical applications, researchers are faced with missing or noisy ground truth while training the convolutional neural network, which decreases the accuracy of the model. Therefore, different loss functions are proposed to deal with these problems to improve the accuracy of the neural network. For missing ground truth, a new predicted category probability method is defined for unlabeled samples, which works effectively in conjunction with the pseudo-labeling method. Furthermore, for noisy ground truth, a symmetric loss function is introduced to resist the corruption of label noises. The proposed loss functions are powerful, robust, and easy to use. Experimental results based on the typical grasping neural network show that our method can improve performance by 2 to 13 percent."
                ],
                "domain": [
                    "Computer Vision",
                    "Deep Learning",
                    "Robotics",
                    "Loss Functions"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "63a6853d-e5e8-46b6-8e01-420f66d36ebb": {
                "pk": "63a6853d-e5e8-46b6-8e01-420f66d36ebb",
                "project_name": null,
                "name": "Jinlu Zhang",
                "bio": "I am a researcher dedicated to enhancing skeleton-based action recognition, particularly in capturing the nuances of human movement. My recent work introduces the concept of Expressive Keypoints, which integrates detailed hand and foot information into skeletal representations. This innovation significantly improves the discriminative power of existing models, allowing for a more nuanced understanding of intricate actions.\n\nTo effectively model these Expressive Keypoints, I developed the Skeleton Transformation strategy, which intelligently down-samples keypoints while prioritizing the most significant joints through importance weighting. Additionally, I designed a plug-and-play Instance Pooling module that enables our approach to scale efficiently in multi-person scenarios without incurring excessive computational costs.\n\nMy research has been validated through extensive experiments across seven datasets, demonstrating the superiority of my method over state-of-the-art techniques in skeleton-based human action recognition. I am passionate about pushing the boundaries of what is possible in this field and am committed to making my work accessible, as evidenced by the code I have shared on GitHub.",
                "collaborators": [
                    "Yijie Yang",
                    "Jiaxu Zhang",
                    "Zhigang Tu"
                ],
                "pub_titles": [
                    "Expressive Keypoints for Skeleton-based Action Recognition via Skeleton Transformation"
                ],
                "pub_abstracts": [
                    "In the realm of skeleton-based action recognition, the traditional methods which rely on coarse body keypoints fall short of capturing subtle human actions. In this work, we propose Expressive Keypoints that incorporates hand and foot details to form a fine-grained skeletal representation, improving the discriminative ability for existing models in discerning intricate actions. To efficiently model Expressive Keypoints, the Skeleton Transformation strategy is presented to gradually downsample the keypoints and prioritize prominent joints by allocating the importance weights. Additionally, a plug-and-play Instance Pooling module is exploited to extend our approach to multi-person scenarios without surging computation costs. Extensive experimental results over seven datasets present the superiority of our method compared to the state-of-the-art for skeleton-based human action recognition. Code is available at https://github.com/YijieYang23/SkeleT-GCN."
                ],
                "domain": [
                    "Computer Vision",
                    "Action Recognition",
                    "Skeleton-based Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b9e39786-e793-414d-84b0-9fbb72047f0c": {
                "pk": "b9e39786-e793-414d-84b0-9fbb72047f0c",
                "project_name": null,
                "name": "Jiyu Cui",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work focuses on enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I am passionate about understanding the broader design space of GNNs. My work on AutoTransfer and FALCON aims to streamline the process of finding optimal model designs by leveraging prior knowledge and efficiently navigating the design space. I believe that by systematically studying these dimensions, we can unlock new potentials in machine learning applications.\n\nOverall, my research is driven by a desire to push the boundaries of GNNs, making them more effective and applicable to real-world challenges, while also contributing to the theoretical understanding of their underlying structures.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "2d6bd5be-3dcb-4bcd-a4a6-b16bf2b94beb": {
                "pk": "2d6bd5be-3dcb-4bcd-a4a6-b16bf2b94beb",
                "project_name": null,
                "name": "Tao Huang",
                "bio": "I am a researcher with a strong focus on the mathematical modeling of complex fluid dynamics, particularly in the context of nematic liquid crystal flows and particle physics. My recent work has established significant criteria for the regularity and uniqueness of weak solutions to the nematic liquid crystal flow, contributing to a deeper understanding of the interior smoothness of these solutions. I have also explored the implications of decentralized social networks, critically assessing their potential to enhance online communication and freedom of expression through value-based design.\n\nIn the realm of particle physics, I have investigated the distribution amplitudes of heavy pseudoscalars and their implications for decay processes, employing light-cone sum rules to derive key parameters. My research on the pion-photon transition form factor has aimed to reconcile discrepancies in experimental data, providing a comprehensive analysis that could lead to a more accurate determination of the pion distribution amplitude.\n\nAdditionally, I have delved into the theoretical aspects of content moderation using large language models, advocating for a legitimacy-based framework that emphasizes user participation and transparency over mere accuracy. My work seeks to bridge the gap between theoretical insights and practical applications, ultimately aiming to inform future research directions and policy interventions in both fluid dynamics and digital communication.",
                "collaborators": [
                    "Tao Zhong",
                    "Xing-Gang Wu",
                    "Changyou Wang",
                    "Huifang Wu",
                    "Alberto Bressan",
                    "Peiyong Wang",
                    "Fen Zuo",
                    "Chengyuan Qu"
                ],
                "pub_titles": [
                    "Regularity and uniqueness for a class of solutions to the hydrodynamic flow of nematic liquid crystals",
                    "Decentralized Social Networks and the Future of Free Speech Online",
                    "Content Moderation by LLM: From Accuracy to Legitimacy",
                    "Heavy Pseudoscalar Leading-Twist Distribution Amplitudes within QCD Theory in Background Fields",
                    "The longitudinal and transverse distributions of the pion wavefunction from the present experimental data on the pion-photon transition form factor",
                    "Finding a way to determine the pion distribution amplitude from the experimental data",
                    "A Phenomenological Analysis of Higher Fock State Contributions to the $χ_{cJ}$ Decays",
                    "Representation of Dissipative Solutions to a Nonlinear Variational Wave Equation",
                    "On singularities of Ericksen-Leslie system in dimension three",
                    "Information on the Pion Distribution Amplitude from the Pion-Photon Transition Form Factor with the Belle and BaBar Data",
                    "Determination of the pion distribution amplitude",
                    "Boundary bubbling analysis of approximate harmonic maps under either weak or strong anchoring conditions in dimension two",
                    "Semileptonic $B_c$ decays and Charmonium distribution amplitude",
                    "Blow up criterion for nematic liquid crystal flows",
                    "Long-time dynamics of Ericksen-Leslie system on $\\mathbb S^2$"
                ],
                "pub_abstracts": [
                    "In this paper, we establish an $\\epsilon$-regularity criterion for any weak solution $(u,d)$ to the nematic liquid crystal flow (1.1) such that $(u,\\nabla d)\\in L^p_tL^q_x$ for some $p\\ge 2$ and $q\\ge n$ satisfying the condition (1.2). As consequences, we prove the interior smoothness of any such a solution when $p>2$ and $q>n$. We also show that uniqueness holds for the class of weak solutions $(u,d)$ the Cauchy problem of the nematic liquid crystal flow (1.1) that satisfy $(u,\\nabla d)\\in L^p_tL^q_x$ for some $p>2$ and $q>n$ satisfying (1.2).",
                    "Decentralized social networks like Mastodon and BlueSky are trending topics that have drawn much attention and discussion in recent years. By devolving powers from the central node to the end users, decentralized social networks aim to cure existing pathologies on the centralized platforms and have been viewed by many as the future of the Internet. This article critically and systematically assesses the decentralization project's prospect for communications online. It uses normative theories of free speech to examine whether and how the decentralization design could facilitate users' freedom of expression online. The analysis shows that both promises and pitfalls exist, highlighting the importance of value-based design in this area. Two most salient issues for the design of the decentralized networks are: how to balance the decentralization ideal with constant needs of centralization on the network, and how to empower users to make them truly capable of exercising their control. The article then uses some design examples, such as the shared blocklist and the opt-in search function, to illustrate the value considerations underlying the design choices. Some tentative proposals for law and policy interventions are offered to better facilitate the design of the new network. Rather than providing clear answers, the article seeks to map the value implications of the design choices, highlight the stakes, and point directions for future research.",
                    "One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy - the extent to which LLM makes correct decisions about content. This article argues that accuracy is insufficient and misleading, because it fails to grasp the distinction between easy cases and hard cases as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLM is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework of evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLM's real potential in moderation is not accuracy improvement. Rather, LLM can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLM's role in content moderation and redirect relevant research in this field.",
                    "In this paper, we study the leading-twist distribution amplitude (DA) of the heavy pseudoscalars (HPs), such as $\\eta_c$, $\\eta_b$ and $B_c$, within the QCD theory in the background fields. New sum rules up to dimension-six condensates for both the HP decay constants and their leading-twist DA moments are presented. From the sum rules for the HP decay constants, we obtain $f_{\\eta_c} = 453 \\pm 4 \\textrm{MeV}$, $f_{B_c} = 498 \\pm 14 \\textrm{MeV}$, and $f_{\\eta_b} = 811 \\pm 34 \\textrm{MeV}$. Basing on the sum rules for the HPs' leading-twist DA moments, we construct a new model for the $\\eta_c$, $\\eta_b$ and $B_c$ leading-twist DAs. Our present HP DA model can also be adaptable for the light pseudo-scalar DAs, such as the pion and kaon DAs. Thus, it shall be applicable for a wide range of QCD exclusive processes. As an application, we apply the $\\eta_c$ leading-twist DA to calculate the $B_c \\to \\eta_c$ transition form factor $f_+^{B_c \\to \\eta_c}(q^2)$. At the maximum recoil region, we obtain $f_+^{B_c \\to \\eta_c}(0) = 0.612^{+0.053}_{-0.052}$. After further extrapolating the TFF $f_+^{B_c \\to \\eta_c}(q^2)$ to its allowable $q^2$ region, we predict the branching ratio for the semi-leptonic decay $B_c \\to \\eta_c l \\nu$. We obtain ${\\cal B}(B_c \\to \\eta_c l \\nu)=\\left(7.70^{+1.65}_{-1.48}\\right) \\times 10^{-3}$ for massless leptons, which is consistent with the LCSRs estimation obtained in the literature.",
                    "It is noted that the low-energy behavior of the pion-photon transition form factor $F_{\\pi\\gamma}(Q^2)$ is sensitive to the transverse distribution of the pion wavefunction, and its high-energy behavior is sensitive to the longitudinal one. Thus a careful study on $F_{\\pi\\gamma}(Q^2)$ can provide helpful information on the pion wavefunction precisely. In this paper, we present a combined analysis of the data on $F_{\\pi\\gamma}(Q^2)$ reported by the CELLO, the CLEO, the BABAR and the BELLE collaborations. It is performed by using the method of least squares. By using the combined measurements of BELLE and CLEO Collaborations, the pion wavefunction longitudinal and transverse behavior can be fixed to a certain degree, i.e. we obtain $\\beta \\in [0.691,0.757] \\rm GeV$ and $B \\in [0.00,0.235]$ for $P_{\\chi^2} \\geq 90\\%$, where $\\beta$ and $B$ are two parameters of a convenient pion wavefunction model whose distribution amplitude can mimic the various longitudinal behavior under proper choice of parameters. We observe that the CELLO, CLEO and BELLE data are consistent with each other, all of which prefers the asymptotic-like distribution amplitude; while the BABAR data prefers a more broad distribution amplitude, such as the CZ-like one.",
                    "It is believed that one can extract more accurate information of the pion distribution amplitude from the pion-photon transition form factor (TFF) due to the single pion in this process. However the BABAR and Belle data of the pion-photon TFF have a big difference for $Q^2\\in [15,40]$ GeV$^2$, and at present, the pion DA can not be definitely determined from the pion-photon TFF. it is crucial to find the right pion DA behavior and to determine which data is more reliable. In this letter, we perform a combined analysis of the most existing data of the processes involving pion by using a general model for the pion wavefunction/DA. Such a DA model can mimic all the existed pion DA behaviors, whose parameters can be fixed by the constraints from the processes $\\pi^0\\to\\gamma\\gamma$, $\\pi\\to\\mu\\nu$, $B\\to\\pi l \\nu$, and etc. Especially, we examine the $B \\rightarrow \\pi$ transition form factors that provides another constraint to the parameter $B$ in our DA model, which results in $B \\in[0.00,0.29]$. This inversely shows that the predicted curve for the pion-photon TFF is between the BABAR and Belle data in the region $Q^2\\in$ $[15,40]$ GeV$^2$. It will be tested by coming more accurate data at large $Q^2$ region, and the definite behavior of pion DA can be concluded finally.",
                    "We present a phenomenological analysis of higher Fack state contributions to the $\\chi_{cJ}$ decays by using the recent BES experimental data. It is found that the higher Fock state $\\mid (c\\bar{c})_{8}g>$ makes an important contributions to the inclusive and exclusive processes with respect to that from the valence Fock state $\\mid c\\bar{c} >$ of the $\\chi_{cJ}$ and some constraints of these contributions are obtained for the $\\chi_{c0}$ and $\\chi_{c2}$ states in order to fit the experimental data.",
                    "The paper introduces a new way to construct dissipative solutions to a second order variational wave equation. By a variable transformation, from the nonlinear PDE one obtains a semilinear hyperbolic system with sources. In contrast with the conservative case, here the source terms are discontinuous and the discontinuities are not always crossed transversally. Solutions to the semilinear system are obtained by an approximation argument, relying on Kolmogorov's compactness theorem. Reverting to the original variables, one recovers a solution to the nonlinear wave equation where the total energy is a monotone decreasing function of time.",
                    "In this paper, we consider the initial and boundary value problem of Ericksen-Leslie system modeling nematic liquid crystal flows in dimension three. Two examples of singularity at finite time are constructed. The first example is constructed in a special axisymmetric class with suitable axisymmetric initial and boundary data, while the second example is constructed for an initial data with small energy but nontrivial topology. A counter example of maximum principle to the system is constructed by utilizing the Poiseuille flow in dimension one.",
                    "The pion-photon transition form factor (TFF) provides strong constraints on the pion distribution amplitude (DA). We perform an analysis of all existing data (CELLO, CLEO, BaBar, Belle) on the pion-photon TFF by means of light-cone pQCD approach in which we include the next-to-leading order correction to the valence-quark contribution and estimate the non-valence-quark contribution by a phenomenological model based on the TFF's limiting behavior at both $Q^2\\to 0$ and $Q^2\\to\\infty$. At present, the pion DA is not definitely determined, it is helpful to have a pion DA model that can mimic all the suggested behaviors, especially to agree with the constraints from the pion-photon TFF in whole measured region within a consistent way. For the purpose, we adopt the conventional model for pion wavefunction/DA that has been constructed in our previous paper \\cite{hw1}, whose broadness is controlled by a parameter $B$. We fix the DA parameters by using the CELLO, CLEO, BABAR and Belle data within the smaller $Q^2$ region ($Q^2 \\leq 15$ GeV$^2$), where all the data are consistent with each other. And then the pion-photon TFF is extrapolated into larger $Q^2$ region. We observe that the BABAR favors $B=0.60$ which has the behavior close to the Chernyak-Zhitnitsky DA, whereas the recent Belle favors $B=0.00$ which is close to the asymptotic DA. We need more accurate data at large $Q^2$ region to determine the precise value of $B$, and the definite behavior of pion DA can be concluded finally by the consistent data in the coming future.",
                    "Right now, we have not enough knowledge to determine the hadron distribution amplitudes (DAs) which are universal physical quantities in the high energy processes involving hadron for applying pQCD to exclusive processes. Even for the simplest pion, one can't discriminate from different DA models. Inversely, one expects that processes involving pion can in principle provide strong constraints on the pion DA. For example, the pion-photon transition form factor (TFF) can get accurate information of the pion wave function or DA, due to the single pion in this process. However, the data from Belle and BABAR have a big difference on TFF in high $Q^2$ regions, at present, they are helpless for determining the pion DA. At the present paper, we think it is still possible to determine the pion DA as long as we perform a combined analysis of the most existing data of the processes involving pion such as $\\pi \\to \\mu \\bar{\\nu}$, $\\pi^0 \\to \\gamma \\gamma$, $B\\to \\pi l \\nu$, $D \\to \\pi l \\nu$, and etc. Based on the revised light-cone harmonic oscillator model, a convenient DA model has been suggested, whose parameter $B$ which dominates its longitudinal behavior for $\\phi_{\\pi}(x,\\mu^2)$ can be determined in a definite range by those processes. A light-cone sum rule analysis of the semi-leptonic processes $B \\to \\pi l \\nu$ and $D \\to \\pi l \\nu$ leads to a narrow region $B = [0.01,0.14]$, which indicate a slight deviation from the asymptotic DA. Then, one can predict the behavior of the pion-photon TFF in high $Q^2$ regions which can be tested in the future experiments. Following this way it provides the possibility that the pion DA will be determined by the global fit finally.",
                    "In this paper, we will study the bubbling phenomena of approximate harmonic maps in dimension two that have either (i) bounded $L^2$-tension fields under the weak anchoring condition, or (ii) bounded $L\\log L \\cap M^{1,\\delta}$-tension fields under the strong anchoring condition.",
                    "In this paper we study the semileptonic decays of the $B_c$ meson in the Light-Cone Sum Rule (LCSR) approach. The result for each channel depends on the corresponding distribution amplitude of the final meson. For the case of $B_c$ decaying into a pseudoscalar meson, to twist-3 accuracy only the leading twist distribution amplitude (DA) is involved if we start from a chiral current. If we choose a suitable chiral current in the vector meson case, the main twist-3 contributions are also eliminated and we can consider the leading twist contribution only. The leading twist distribution amplitudes of the charmonium and other heavy mesons are given by a model approach in the reasonable way. Employing this charmonium distribution amplitude we find the cross section $\\sigma(e^+e^-\\to J/\\psi+\\eta_c)\\simeq22.8 {fb}$ which is consistent with Belle and BaBar's data. Based on this model, we calculate the form factors for various $B_c$ decay modes in the corresponding regions. Extrapolating the form factors to the whole kinetic regions, we get the decay widths and branching ratios for various $B_c$ decay modes including their $\\tau$ modes when they are kinematically accessible.",
                    "In this paper, we establish a blow up criterion for the short time classical solution of the nematic liquid crystal ow, a simplified version of Ericksen-Leslie system modeling the hydrodynamic evolution of nematic liquid crystals, in dimensions two and three. More precisely, $0<T_*<+\\infty$ is the maximal time interval iff (i) for $n=3$, $|\\omega|+|\\nabla d|^2\\notin L^1_tL^\\infty_x(\\mathbb R^3\\times [0,T_*])$, and (ii) for $n=2$, $|\\nabla|^2\\notin L^1_tL^\\infty_x(\\mathbb R^2\\times [0,T_*])$.",
                    "In this paper, we study the long-time behavior of full Ericksen-Leslie system modeling the hydrodynamics of nematic liquid crystals between two dimensional unit spheres. Under a weaker assumption for Leslie's coefficients, we give the key energy inequality for the global weak solution. At last, inspired by the conditions on the simplified system, we establish several sufficient conditions which guarantee the uniform convergence of the system in $L^2$ and $H^k$ spaces as time tends to infinity under small initial data."
                ],
                "domain": [
                    "Nematic Liquid Crystals",
                    "Quantum Chromodynamics",
                    "Content Moderation",
                    "Decentralized Networks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "35b46026-ed38-45a7-9e64-3575faa0f7b5": {
                "pk": "35b46026-ed38-45a7-9e64-3575faa0f7b5",
                "project_name": null,
                "name": "Pengju Yan",
                "bio": "I am a researcher dedicated to advancing the fields of ordinal regression and multiple instance learning (MIL), particularly in the context of computational pathology. My recent work has focused on improving adaptive label distribution learning (ALDL) for ordinal regression tasks, such as facial age and head pose estimation. I have identified key principles that should guide the development of ALDL methods, leading to the creation of a novel unimodal-concentrated loss function. This approach not only enhances the accuracy of predictions but also accounts for the inherent uncertainties in the data.\n\nIn addition to my work on ALDL, I have developed an innovative framework called M4, which adapts the Multi-gate Mixture-of-experts architecture for simultaneous prediction of multiple genetic mutations from whole slide images (WSIs). This model leverages a mixture of experts and multi-proxy networks to effectively capture the complex information present in pathological images. My research has demonstrated significant improvements over existing single-task methods across multiple datasets, showcasing the potential of my approach in enhancing diagnostic accuracy in computational pathology.\n\nI am passionate about bridging theoretical advancements with practical applications, and I strive to contribute to the development of more efficient and effective machine learning models that can tackle real-world challenges in healthcare and beyond.",
                "collaborators": [
                    "Qiang Li",
                    "Jingjing Wang",
                    "Zhaoliang Yao",
                    "Yachun Li",
                    "Pengju Yang",
                    "Jingwei Yan",
                    "Chunmao Wang",
                    "Shiliang Pu",
                    "Junyu Li",
                    "Ye Zhang"
                ],
                "pub_titles": [
                    "Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression",
                    "M4: Multi-Proxy Multi-Gate Mixture of Experts Network for Multiple Instance Learning in Histopathology Image Analysis"
                ],
                "pub_abstracts": [
                    "Learning from a label distribution has achieved promising results on ordinal regression tasks such as facial age and head pose estimation wherein, the concept of adaptive label distribution learning (ALDL) has drawn lots of attention recently for its superiority in theory. However, compared with the methods assuming fixed form label distribution, ALDL methods have not achieved better performance. We argue that existing ALDL algorithms do not fully exploit the intrinsic properties of ordinal regression. In this paper, we emphatically summarize that learning an adaptive label distribution on ordinal regression tasks should follow three principles. First, the probability corresponding to the ground-truth should be the highest in label distribution. Second, the probabilities of neighboring labels should decrease with the increase of distance away from the ground-truth, i.e., the distribution is unimodal. Third, the label distribution should vary with samples changing, and even be distinct for different instances with the same label, due to the different levels of difficulty and ambiguity. Under the premise of these principles, we propose a novel loss function for fully adaptive label distribution learning, namely unimodal-concentrated loss. Specifically, the unimodal loss derived from the learning to rank strategy constrains the distribution to be unimodal. Furthermore, the estimation error and the variance of the predicted distribution for a specific sample are integrated into the proposed concentrated loss to make the predicted distribution maximize at the ground-truth and vary according to the predicting uncertainty. Extensive experimental results on typical ordinal regression tasks including age and head pose estimation, show the superiority of our proposed unimodal-concentrated loss compared with existing loss functions.",
                    "Multiple instance learning (MIL) has been successfully applied for whole slide images (WSIs) analysis in computational pathology, enabling a wide range of prediction tasks from tumor subtyping to inferring genetic mutations and multi-omics biomarkers. However, existing MIL methods predominantly focus on single-task learning, resulting in not only overall low efficiency but also the overlook of inter-task relatedness. To address these issues, we proposed an adapted architecture of Multi-gate Mixture-of-experts with Multi-proxy for Multiple instance learning (M4), and applied this framework for simultaneous prediction of multiple genetic mutations from WSIs. The proposed M4 model has two main innovations: (1) utilizing a mixture of experts with multiple gating strategies for multi-genetic mutation prediction on a single pathological slide; (2) constructing multi-proxy expert network and gate network for comprehensive and effective modeling of pathological image information. Our model achieved significant improvements across five tested TCGA datasets in comparison to current state-of-the-art single-task methods. The code is available at:https://github.com/Bigyehahaha/M4."
                ],
                "domain": [
                    "Ordinal Regression",
                    "Multiple Instance Learning",
                    "Computational Pathology",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "4fbf143a-9000-4c61-9388-d2feeccbbe7f": {
                "pk": "4fbf143a-9000-4c61-9388-d2feeccbbe7f",
                "project_name": null,
                "name": "Renjun Xu",
                "bio": "I am a researcher with a diverse background in theoretical physics, machine learning, and materials science. My work primarily focuses on the intersection of these fields, particularly in understanding complex systems through advanced computational methods. Recently, I have delved into the zero mode cohomology of pure spinors in superstring theory, which has implications for massless vertex operators in Type IIB superstrings. \n\nIn addition to my theoretical pursuits, I have developed innovative computational techniques for analyzing electronic configurations, leading to significant advancements in understanding LS spectral terms. My research also extends to the study of superconductivity, where I created the S2SNet model, achieving state-of-the-art predictions based solely on crystal structures. This work not only contributes to the fundamental understanding of superconductivity but also aligns with sustainable development goals.\n\nMoreover, I have explored the optical properties of ice under high pressure, revealing critical insights into its behavior that differ significantly from ambient conditions. My recent endeavors in user modeling and federated learning highlight my commitment to applying machine learning in healthcare, addressing challenges like data privacy and personalization. Through these varied research experiences, I aim to bridge theoretical insights with practical applications, fostering advancements across multiple domains.",
                "collaborators": [
                    "Albert Schwarz",
                    "Ke Liu",
                    "Andrei Mikhailov",
                    "Michael Movshev",
                    "Kaifan Yang",
                    "Zhiming Liu",
                    "Yanming Ma",
                    "Tian Cui",
                    "Bingbing Liu",
                    "Guangtian Zou"
                ],
                "pub_titles": [
                    "BRST cohomology of the sum of two pure spinors",
                    "Alternative Mathematical Technique to Determine LS Spectral Terms",
                    "Cohomology ring of the BRST operator associated to the sum of two pure spinors",
                    "Homology of Lie algebra of supersymmetries",
                    "Integral invariants in flat superspace",
                    "Homology of Lie algebra of supersymmetries and of super Poincare Lie algebra",
                    "Weierstrass cycles and tautological rings in various moduli spaces of algebraic curves",
                    "$E(2)$-Equivariant Vision Transformer",
                    "S2SNet: A Pretrained Neural Network for Superconductivity Discovery",
                    "Ab initio investigation of optical properties of high-pressure phases of ice",
                    "Ab initio investigation of hydrogen bonding and electronic structure of high-pressure phases of ice",
                    "Empowering General-purpose User Representation with Full-life Cycle Behavior Modeling",
                    "Personalized Federated Learning with Adaptive Batchnorm for Healthcare"
                ],
                "pub_abstracts": [
                    "We study the zero mode cohomology of the sum of two pure spinors. The knowledge of this cohomology allows us to better understand the structure of the massless vertex operator of the Type IIB pure spinor superstring.",
                    "We presented an alternative computational method for determining the permitted LS spectral terms arising from $l^N$ electronic configurations. This method makes the direct calculation of LS terms possible. Using only basic algebra, we derived our theory from LS-coupling scheme and Pauli exclusion principle. As an application, we have performed the most complete set of calculations to date of the spectral terms arising from $l^N$ electronic configurations, and the representative results were shown. As another application on deducing LS-coupling rules, for two equivalent electrons, we deduced the famous Even Rule; for three equivalent electrons, we derived a new simple rule.",
                    "In the study of the Type II superstring, it is useful to consider the BRST complex associated to the sum of two pure spinors. The cohomology of this complex is an infinite-dimensional vector space. It is also a finite-dimensional algebra over the algebra of functions of a single pure spinor. In this paper we study the multiplicative structure.",
                    "We study the homology and cohomology groups of super Lie algebra of supersymmetries and of super Poincare algebra. We discuss in detail the calculation in dimensions D=10 and D=6. Our methods can be applied to extended supersymmetry algebra and to other dimensions.",
                    "We are solving for the case of flat superspace some homological problems that were formulated by Berkovits and Howe. (Our considerations can be applied also to the case of supertorus.) These problems arise in the attempt to construct integrals invariant with respect to supersymmetry. They appear also in other situations, in particular, in the pure spinor formalism in supergravity.",
                    "We study the homology and cohomology groups of super Lie algebra of supersymmetries and of super Poincare Lie algebra in various dimensions. We give complete answers for (non-extended) supersymmetry in all dimensions $\\leq 11$. For dimensions $D=10,11$ we describe also the cohomology of reduction of supersymmetry Lie algebra to lower dimensions. Our methods can be applied to extended supersymmetry algebra.",
                    "We analyze Weierstrass cycles and tautological rings in moduli space of smooth algebraic curves and in moduli spaces of integral algebraic curves with embedded disks with special attention to moduli spaces of curves having genus $\\leq 6$. In particular, we show that our general formula gives a good estimate for the dimension of Weierstrass cycles for lower genera.",
                    "Vision Transformer (ViT) has achieved remarkable performance in computer vision. However, positional encoding in ViT makes it substantially difficult to learn the intrinsic equivariance in data. Initial attempts have been made on designing equivariant ViT but are proved defective in some cases in this paper. To address this issue, we design a Group Equivariant Vision Transformer (GE-ViT) via a novel, effective positional encoding operator. We prove that GE-ViT meets all the theoretical requirements of an equivariant neural network. Comprehensive experiments are conducted on standard benchmark datasets, demonstrating that GE-ViT significantly outperforms non-equivariant self-attention networks. The code is available at https://github.com/ZJUCDSYangKaifan/GEVit.",
                    "Superconductivity allows electrical current to flow without any energy loss, and thus making solids superconducting is a grand goal of physics, material science, and electrical engineering. More than 16 Nobel Laureates have been awarded for their contribution to superconductivity research. Superconductors are valuable for sustainable development goals (SDGs), such as climate change mitigation, affordable and clean energy, industry, innovation and infrastructure, and so on. However, a unified physics theory explaining all superconductivity mechanism is still unknown. It is believed that superconductivity is microscopically due to not only molecular compositions but also the geometric crystal structure. Hence a new dataset, S2S, containing both crystal structures and superconducting critical temperature, is built upon SuperCon and Material Project. Based on this new dataset, we propose a novel model, S2SNet, which utilizes the attention mechanism for superconductivity prediction. To overcome the shortage of data, S2SNet is pre-trained on the whole Material Project dataset with Masked-Language Modeling (MLM). S2SNet makes a new state-of-the-art, with out-of-sample accuracy of 92% and Area Under Curve (AUC) of 0.92. To the best of our knowledge, S2SNet is the first work to predict superconductivity with only information of crystal structures. This work is beneficial to superconductivity discovery and further SDGs. Code and datasets are available in https://github.com/zjuKeLiu/S2SNet",
                    "We report a detailed ab initio investigation on the optical properties of ice under a wide high pressure range. The ice X phase (up to 380GPa), the theoretical proposed higher pressure phase ice XV (300GPa), as well as the ambient pressure low-temperature phase ice XI are involved. Our results show that the dispersion relations of optical properties of ice under high pressure are quite different from those under ambient pressure. Under higher pressure, there is whole tendency of blue shift in all optical properties of ice, and the energy region for optical response become broader, such as the absorption band and reflection band. In addition to the augmented absorption edge, all absorption peaks are found to be rising, and the reflection peaks are also enhanced a bit; hence the transmissivity of ice is inferred to be dropping. The photoconductivity is enhanced, and we explain this behavior from the increase of earner density. The static optical properties are found to be pressure independent, and principally due to the network topology of hydrogen bonding.",
                    "We report a detailed ab initio investigation on hydrogen bonding, geometry, electronic structure, and lattice dynamics of ice under a large high pressure range, including the ice X phase (55-380GPa), the previous theoretically proposed higher-pressure phase ice XIIIM (Refs. 1-2) (380GPa), ice XV (a new structure we derived from ice XIIIM) (300-380GPa), as well as the ambient pressure low-temperature phase ice XI. Different from many other materials, the band gap of ice X is found to be increasing linearly with pressure from 55GPa up to 290GPa, the electronic density of states (DOS) shows that the valence bands have a tendency of red shift (move to lower energies) referring to the Fermi energy while the conduction bands have a blue shift (move to higher energies). This behavior is interpreted as the high pressure induced change of s-p charge transfers between hydrogen and oxygen. It is found that ice X exists in the pressure range from 75GPa to about 290GPa. Beyond 300GPa, a new hydrogen-bonding structure with 50% hydrogen atoms in symmetric positions in O-H-O bonds and the other half being asymmetric, ice XV, is identified. The physical mechanism for this broken symmetry in hydrogen bonding is revealed.",
                    "User Modeling plays an essential role in industry. In this field, task-agnostic approaches, which generate general-purpose representation applicable to diverse downstream user cognition tasks, is a promising direction being more valuable and economical than task-specific representation learning. With the rapid development of Internet service platforms, user behaviors have been accumulated continuously. However, existing general-purpose user representation researches have little ability for full-life cycle modeling on extremely long behavior sequences since user registration. In this study, we propose a novel framework called full- Life cycle User Representation Model (LURM) to tackle this challenge. Specifically, LURM consists of two cascaded sub-models: (I) Bag-of-Interests (BoI) encodes user behaviors in any time period into a sparse vector with super-high dimension (e.g., 10^5); (II) Self-supervised Multi-anchor Encoder Network (SMEN) maps sequences of BoI features to multiple low-dimensional user representations. Specially, SMEN achieves almost lossless dimensionality reduction, benefiting from a novel multi-anchor module which can learn different aspects of user interests. Experiments on several benchmark datasets show that our approach outperforms state-of-the-art general-purpose representation methods.",
                    "There is a growing interest in applying machine learning techniques to healthcare. Recently, federated learning (FL) is gaining popularity since it allows researchers to train powerful models without compromising data privacy and security. However, the performance of existing FL approaches often deteriorates when encountering non-iid situations where there exist distribution gaps among clients, and few previous efforts focus on personalization in healthcare. In this article, we propose FedAP to tackle domain shifts and then obtain personalized models for local clients. FedAP learns the similarity between clients based on the statistics of the batch normalization layers while preserving the specificity of each client with different local batch normalization. Comprehensive experiments on five healthcare benchmarks demonstrate that FedAP achieves better accuracy compared to state-of-the-art methods (e.g., 10% accuracy improvement for PAMAP2) with faster convergence speed."
                ],
                "domain": [
                    "Mathematical Physics",
                    "Superstring Theory",
                    "Machine Learning",
                    "User Modeling"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "2849d0a4-6319-48f7-ae45-dc0ce106cb81": {
                "pk": "2849d0a4-6319-48f7-ae45-dc0ce106cb81",
                "project_name": null,
                "name": "Hongyang Chen",
                "bio": "I am a researcher dedicated to advancing the fields of deep learning, graph neural networks, and wireless communication systems. My recent work has focused on integrating deep learning into image signal processing (ISP) pipelines, where I developed LW-ISP, a novel architecture that significantly enhances real-time image processing efficiency while reducing computational load. I have also explored local graph features for missing link prediction, proposing the Circled Feature aware Graph transformer (CFG) model, which achieves state-of-the-art performance on benchmark datasets.\n\nIn addition to my work in image processing and graph neural networks, I am deeply invested in the ethical implications of AI-generated content. I introduced WaDiff, a watermarking framework that not only detects synthetic content but also identifies the users responsible for generating it, ensuring copyright protection in the context of diffusion models.\n\nMy research extends to molecular representation learning, where I have integrated Kolmogorov-Arnold Networks into graph neural networks to enhance molecular property prediction. I have also tackled challenges in wireless underground sensor networks, utilizing robust algorithms for improved localization accuracy.\n\nThrough my diverse research endeavors, I aim to bridge theoretical advancements with practical applications, contributing to the development of efficient algorithms and frameworks that address real-world challenges in technology and science.",
                "collaborators": [
                    "Minhao Cheng",
                    "H. Vincent Poor",
                    "Xiao Deng",
                    "Kaisheng Ma",
                    "Jingsong Lv",
                    "Yao Qi",
                    "Lei Yu",
                    "Rui Min",
                    "Sen Li",
                    "Ruifeng Li"
                ],
                "pub_titles": [
                    "LW-ISP: A Lightweight Model with ISP and Deep Learning",
                    "Circle Feature Graphormer: Can Circle Features Stimulate Graph Transformer?",
                    "A Watermark-Conditioned Diffusion Model for IP Protection",
                    "GNN-SKAN: Harnessing the Power of SwallowKAN to Advance Molecular Representation Learning with GNNs",
                    "Distributed Source Localization in Wireless Underground Sensor Networks",
                    "Range-Free Localization with the Radical Line",
                    "The integrable semi-discrete nonlinear Schrödinger equations with nonzero backgrounds: Bilinearization-reduction approach",
                    "Compressive Channel Estimation for Two-way Relay Network in a Frequency-Selective Channel with Compressed Sensing",
                    "Diffusion-based Graph Generative Methods",
                    "Input Snapshots Fusion for Scalable Discrete Dynamic Graph Nerual Networks",
                    "Bilinearization-reduction approach to the classical and nonlocal semi-discrete modified Korteweg-de Vries equations with nonzero backgrounds",
                    "Statistical CSI Based Hybrid mmWave MIMO-NOMA with Max-Min Fairness"
                ],
                "pub_abstracts": [
                    "The deep learning (DL)-based methods of low-level tasks have many advantages over the traditional camera in terms of hardware prospects, error accumulation and imaging effects. Recently, the application of deep learning to replace the image signal processing (ISP) pipeline has appeared one after another; however, there is still a long way to go towards real landing. In this paper, we show the possibility of learning-based method to achieve real-time high-performance processing in the ISP pipeline. We propose LW-ISP, a novel architecture designed to implicitly learn the image mapping from RAW data to RGB image. Based on U-Net architecture, we propose the fine-grained attention module and a plug-and-play upsampling block suitable for low-level tasks. In particular, we design a heterogeneous distillation algorithm to distill the implicit features and reconstruction information of the clean image, so as to guide the learning of the student model. Our experiments demonstrate that LW-ISP has achieved a 0.38 dB improvement in PSNR compared to the previous best method, while the model parameters and calculation have been reduced by 23 times and 81 times. The inference efficiency has been accelerated by at least 15 times. Without bells and whistles, LW-ISP has achieved quite competitive results in ISP subtasks including image denoising and enhancement.",
                    "In this paper, we introduce two local graph features for missing link prediction tasks on ogbl-citation2. We define the features as Circle Features, which are borrowed from the concept of circle of friends. We propose the detailed computing formulas for the above features. Firstly, we define the first circle feature as modified swing for common graph, which comes from bipartite graph. Secondly, we define the second circle feature as bridge, which indicates the importance of two nodes for different circle of friends. In addition, we firstly propose the above features as bias to enhance graph transformer neural network, such that graph self-attention mechanism can be improved. We implement a Circled Feature aware Graph transformer (CFG) model based on SIEG network, which utilizes a double tower structure to capture both global and local structure features. Experimental results show that CFG achieves the state-of-the-art performance on dataset ogbl-citation2.",
                    "The ethical need to protect AI-generated content has been a significant concern in recent years. While existing watermarking strategies have demonstrated success in detecting synthetic content (detection), there has been limited exploration in identifying the users responsible for generating these outputs from a single model (owner identification). In this paper, we focus on both practical scenarios and propose a unified watermarking framework for content copyright protection within the context of diffusion models. Specifically, we consider two parties: the model provider, who grants public access to a diffusion model via an API, and the users, who can solely query the model API and generate images in a black-box manner. Our task is to embed hidden information into the generated contents, which facilitates further detection and owner identification. To tackle this challenge, we propose a Watermark-conditioned Diffusion model called WaDiff, which manipulates the watermark as a conditioned input and incorporates fingerprinting into the generation process. All the generative outputs from our WaDiff carry user-specific information, which can be recovered by an image extractor and further facilitate forensic identification. Extensive experiments are conducted on two popular diffusion models, and we demonstrate that our method is effective and robust in both the detection and owner identification tasks. Meanwhile, our watermarking framework only exerts a negligible impact on the original generation and is more stealthy and efficient in comparison to existing watermarking strategies.",
                    "Effective molecular representation learning is crucial for advancing molecular property prediction and drug design. Mainstream molecular representation learning approaches are based on Graph Neural Networks (GNNs). However, these approaches struggle with three significant challenges: insufficient annotations, molecular diversity, and architectural limitations such as over-squashing, which leads to the loss of critical structural details. To address these challenges, we introduce a new class of GNNs that integrates the Kolmogorov-Arnold Networks (KANs), known for their robust data-fitting capabilities and high accuracy in small-scale AI + Science tasks. By incorporating KANs into GNNs, our model enhances the representation of molecular structures. We further advance this approach with a variant called SwallowKAN (SKAN), which employs adaptive Radial Basis Functions (RBFs) as the core of the non-linear neurons. This innovation improves both computational efficiency and adaptability to diverse molecular structures. Building on the strengths of SKAN, we propose a new class of GNNs, GNN-SKAN, and its augmented variant, GNN-SKAN+, which incorporates a SKAN-based classifier to further boost performance. To our knowledge, this is the first work to integrate KANs into GNN architectures tailored for molecular representation learning. Experiments across 6 classification datasets, 6 regression datasets, and 4 few-shot learning datasets demonstrate that our approach achieves new state-of-the-art performance in terms of accuracy and computational cost.",
                    "Node localization plays an important role in many practical applications of wireless underground sensor networks (WUSNs), such as finding the locations of earthquake epicenters, underground explosions, and microseismic events in mines. It is more difficult to obtain the time-difference-of-arrival (TDOA) measurements in WUSNs than in terrestrial wireless sensor networks because of the unfavorable channel characteristics in the underground environment. The robust Chinese remainder theorem (RCRT) has been shown to be an effective tool for solving the phase ambiguity problem and frequency estimation problem in wireless sensor networks. In this paper, the RCRT is used to robustly estimate TDOA or range difference in WUSNs and therefore improves the ranging accuracy in such networks. After obtaining the range difference, distributed source localization algorithms based on a diffusion strategy are proposed to decrease the communication cost while satisfying the localization accuracy requirement. Simulation results confirm the validity and efficiency of the proposed methods.",
                    "Due to hardware and computational constraints, wireless sensor networks (WSNs) normally do not take measurements of time-of-arrival or time-difference-of-arrival for rangebased localization. Instead, WSNs in some applications use rangefree localization for simple but less accurate determination of sensor positions. A well-known algorithm for this purpose is the centroid algorithm. This paper presents a range-free localization technique based on the radical line of intersecting circles. This technique provides greater accuracy than the centroid algorithm, at the expense of a slight increase in computational load. Simulation results show that for the scenarios studied, the radical line method can give an approximately 2 to 30% increase in accuracy over the centroid algorithm, depending on whether or not the anchors have identical ranges, and on the value of DOI.",
                    "In this paper the classical and nonlocal semi-discrete nonlinear Schr\\\"{o}dinger (sdNLS) equations with nonzero backgrounds are solved by means of the bilinearization-reduction approach. In the first step of this approach, the unreduced sdNLS system with a nonzero background is bilinearized and its solutions are presented in terms of quasi double Casoratians. Then, reduction techniques are implemented to deal with complex and nonlocal reductions, which yields solutions for the four classical and nonlocal sdNLS equations with a plane wave background or a hyperbolic function background. These solutions are expressed with explicit formulae and allow classifications according to canonical forms of certain spectral matrix. In particular, we present explicit formulae for general rogue waves for the classical focusing sdNLS equation. Some obtained solutions are analyzed and illustrated.",
                    "Two-way relay network (TWRN) was introduced to realize high-data rate transmission over the wireless frequency-selective channel. However, TWRC requires the knowledge of channel state information (CSI) not only for coherent data detection but also for the self-data removal. This is partial accomplished by training sequence-based linear channel estimation. However, conventional linear estimation techniques neglect anticipated sparsity of multipath channel. Unlike the previous methods, we propose a compressive channel estimation method which exploit the sparse structure and provide significant improvements in MSE performance when compared with traditional LSbased linear channel probing strategies. Simulation results confirm the proposed methods.",
                    "Being the most cutting-edge generative methods, diffusion methods have shown great advances in wide generation tasks. Among them, graph generation attracts significant research attention for its broad application in real life. In our survey, we systematically and comprehensively review on diffusion-based graph generative methods. We first make a review on three mainstream paradigms of diffusion methods, which are denoising diffusion probabilistic models, score-based genrative models, and stochastic differential equations. Then we further categorize and introduce the latest applications of diffusion models on graphs. In the end, we point out some limitations of current studies and future directions of future explorations. The summary of existing methods metioned in this survey is in https://github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.",
                    "Dynamic graphs are ubiquitous in the real world, yet there is a lack of suitable theoretical frameworks to effectively extend existing static graph models into the temporal domain. Additionally, for link prediction tasks on discrete dynamic graphs, the requirement of substantial GPU memory to store embeddings of all nodes hinders the scalability of existing models. In this paper, we introduce an Input {\\bf S}napshots {\\bf F}usion based {\\bf Dy}namic {\\bf G}raph Neural Network (SFDyG). By eliminating the partitioning of snapshots within the input window, we obtain a multi-graph (more than one edge between two nodes). Subsequently, by introducing a graph denoising problem with the assumption of temporal decayed smoothing, we integrate Hawkes process theory into Graph Neural Networks to model the generated multi-graph. Furthermore, based on the multi-graph, we propose a scalable three-step mini-batch training method and demonstrate its equivalence to full-batch training counterpart. Our experiments, conducted on eight distinct dynamic graph datasets for future link prediction tasks, revealed that SFDyG generally surpasses related methods.",
                    "Quasi double Casoratian solutions are derived for a bilinear system reformulated from the coupled semi-discrete modified Korteweg-de Vries equations with nonzero backgrounds. These solutions, when applied with the classical and nonlocal reduction techniques, also satisfy the corresponding classical and nonlocal semi-discrete modified Korteweg-de Vries equations with nonzero backgrounds. They can be expressed explicitly, allowing for an easy investigation of the dynamics of systems. As illustrative examples, the dynamics of solitonic, periodic and rational solutions with a plane wave background are examined for the focusing semi-discrete Korteweg-de Vries equation and the defocusing reverse-space-time complex semi-discrete Korteweg-de Vries equation.",
                    "Non-orthogonal multiple access (NOMA) and millimeter wave (mmWave) are two key enabling technologies for the fifth-generation (5G) mobile networks and beyond. In this paper, we consider mmWave NOMA systems with max-min fairness constraints. On the one hand, existing beamforming designs aiming at maximizing the spectrum efficiency (SE) are unsuitable for the NOMA systems with fairness in this paper. On the other hand, previous work on about mmWave NOMA mostly depends on full knowledge of channel state information (CSI) which is extremely difficult to obtain accurately in mmWave communication systems. To address this problem, we propose a heuristic hybrid beamforming design based on the statistical CSI (SCSI) user grouping strategy. An analog beamforming scheme is first proposed to integrate the whole cluster users to mitigate the inter-cluster interference in the first stage. Then two digital beamforming designs are proposed to further suppress the interference based on SCSI. One is the widely used zero forcing (ZF) approach and the other is derived from the signal-to leakage-plus-noise ratio (SLNR) metric extended from orthogonal multiple access (OMA) systems. The effective gains fed back from the users are used for the power allocation. We introduce the quadratic transform (QT) method and bisection approach to reformulate this complex problem so as to rend it solvable. Simulation results show that our proposed algorithms outperform the previous algorithms in term of user fairness."
                ],
                "domain": [
                    "Deep Learning",
                    "Graph Neural Network",
                    "Image Processing",
                    "Wireless Sensor Networks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "0d3f421a-7695-482c-b8ba-c8aec91c6faa": {
                "pk": "0d3f421a-7695-482c-b8ba-c8aec91c6faa",
                "project_name": null,
                "name": "Xiaolin Li",
                "bio": "I am a researcher with a diverse background in information theory, materials science, and machine learning, focusing on the intersection of these fields to address complex challenges. My work on two-user interference channels has led to significant advancements in secure communication, where I developed inner and outer bounds on capacity regions, achieving the complete Han-Kobayashi region while enhancing security conditions.\n\nIn the realm of materials science, I have explored the self-assembly of chemically derived graphene sheets onto gold structures, pioneering methods to create regular arrays of graphene devices. This work not only demonstrated the potential for high-performance molecular sensors but also highlighted the unique properties of graphene in electronic applications.\n\nAdditionally, I have ventured into the field of cancer diagnostics through my development of DeepCancer, a deep generative machine learning architecture. This model leverages unlabeled microarray data to extract meaningful features, significantly improving classification accuracy for cancerous versus non-cancerous tissue samples.\n\nMy research is characterized by a commitment to innovation and interdisciplinary collaboration, as I strive to push the boundaries of knowledge in both theoretical and applied domains. I am passionate about translating complex theoretical concepts into practical solutions that can have a real-world impact.",
                "collaborators": [
                    "Ryutaroh Matsumoto",
                    "Hailiang Wang",
                    "Xinran Wang",
                    "Hongjie Dai",
                    "Rajendra Rana Bhat",
                    "Vivek Viswanath",
                    "Xiao Chen",
                    "Jian-Jian Jiang"
                ],
                "pub_titles": [
                    "Secure Multiplex Coding Over Interference Channel with Confidential Messages",
                    "Chemical Self Assembly of Graphene Sheets",
                    "DeepCancer: Detecting Cancer through Gene Expressions via Deep Generative Learning",
                    "A Smiley-type theorem for spectral operators of finite type"
                ],
                "pub_abstracts": [
                    "In this paper, inner and outer bounds on the capacity region of two-user interference channels with two confidential messages have been proposed. By adding secure multiplex coding to the error correction method in [15] which achieves the best achievable capacity region for interference channel up to now, we have shown that the improved secure capacity region compared with [2] now is the whole Han-Kobayashi region. In addition, this construction not only removes the rate loss incurred by adding dummy messages to achieve security, but also change the original weak security condition in [2] to strong security. Then the equivocation rate for a collection of secret messages has also been evaluated, when the length of the message is finite or the information rate is high, our result provides a good approximation for bounding the worst case equivocation rate. Our results can be readily extended to the Gaussian interference channel with little efforts.",
                    "Chemically derived graphene sheets were found to self-assemble onto patterned gold structures via electrostatic interactions between noncovalent functional groups on GS and gold. This afforded regular arrays of single graphene sheets on large substrates, characterized by scanning electron and Auger microscopy imaging and Raman spectroscopy. Self assembly was used for the first time to produce on-substrate and fully-suspended graphene electrical devices. Molecular coatings on the GS were removed by high current electrical annealing, which recovered the high electrical conductance and Dirac point of the GS. Molecular sensors for highly sensitive gas detections are demonstrated with self-assembled GS devices.",
                    "Transcriptional profiling on microarrays to obtain gene expressions has been used to facilitate cancer diagnosis. We propose a deep generative machine learning architecture (called DeepCancer) that learn features from unlabeled microarray data. These models have been used in conjunction with conventional classifiers that perform classification of the tissue samples as either being cancerous or non-cancerous. The proposed model has been tested on two different clinical datasets. The evaluation demonstrates that DeepCancer model achieves a very high precision score, while significantly controlling the false positive and false negative scores.",
                    "In this short article, we mainly prove that, for any spectral operator $A$ of type $m$ on a complex Hilbert space, if a bounded operator $B$ lies in the collection of bounded linear operators that are in the $k$-centralizer of every bounded linear operator in the $l$-centralizer of $A$, where $k\\leqslant l$ is two arbitrary positive integers satisfying $l\\geqslant k$ as well as $l\\geqslant 2m+1$, then $B$ must belong to the von Neumann algebra generated by $A$ and identity operator. This result generalizes a matrix commutator theorem proved by M.\\ F.\\ Smiley. For this aim, Smiley-type operators are defined and studied."
                ],
                "domain": [
                    "Information Theory",
                    "Graphene",
                    "Machine Learning",
                    "Quantum Mechanics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "5f419461-4a19-4589-96cf-79a33ea445d0": {
                "pk": "5f419461-4a19-4589-96cf-79a33ea445d0",
                "project_name": null,
                "name": "Xiaohui Fan",
                "bio": "I am an astrophysicist with a keen interest in the study of quasars, stellar populations, and the evolution of the universe. My research spans a variety of topics, including the simulation of stellar objects, the measurement of Lyman Continuum (LyC) escape fractions in faint Lyman Alpha Emitters, and the discovery of high-redshift quasars. I have developed simulations that accurately model the spatial, luminosity, and spectral distributions of various stellar types, which serve as valuable tools for analyzing star counts and refining quasar selection algorithms.\n\nMy work on LyC escape fractions has revealed important insights into the properties of young dwarf galaxies, suggesting that traditional indicators may not accurately reflect their escape fractions. Additionally, I have contributed to the understanding of the mass density of the universe by analyzing the existence of massive galaxy clusters, which indicates a sub-critical density universe.\n\nI have also explored the properties of high-redshift quasars, uncovering their role in the formation of supermassive black holes and the evolution of the intergalactic medium during the epoch of reionization. My research employs a Bayesian approach to estimating luminosity functions, allowing for more accurate constraints on quasar populations beyond survey detection limits.\n\nThrough my work, I aim to deepen our understanding of cosmic evolution and the fundamental processes that govern the formation of galaxies and black holes in the early universe.",
                "collaborators": [
                    "SDSS Collaboration",
                    "Neta A. Bahcall",
                    "Minghao Yue",
                    "Jinyi Yang",
                    "Feige Wang",
                    "Fuyan Bian",
                    "Eduardo Banados",
                    "Robert A. Simcoe",
                    "Nickolay Y. Gnedin",
                    "Linhua Jiang"
                ],
                "pub_titles": [
                    "Simulation of Stellar Objects in SDSS Color Space",
                    "Lyman Continuum Escape Fraction in Ly$α$ Emitters at $z\\simeq3.1$",
                    "High-Redshift Quasars Found in Sloan Digital Sky Survey Commissioning Data II: The Spring Equatorial Stripe",
                    "The Most Massive Distant Clusters: Determining Omega and sigma_8",
                    "The Discovery of a High-redshift Quasar without Emission Lines from Sloan Digital Sky Survey Commissioning Data",
                    "Quasars and the Intergalactic Medium at Cosmic Dawn",
                    "A Lightweight Universe?",
                    "L Dwarfs Found in Sloan Digital Sky Survey Commissioning Imaging Data",
                    "Cosmic Reionization Redux",
                    "A Sample of Quasars with Strong Nitrogen Emission Lines from the Sloan Digital Sky Survey",
                    "Physical properties of the first quasars",
                    "A Candidate Kiloparsec-scale Quasar Pair at $z=5.66$",
                    "Revisiting the Lensed Fraction of High-Redshift Quasars",
                    "Observational constraints on Cosmic Reionization",
                    "A Flexible Method of Estimating Luminosity Functions"
                ],
                "pub_abstracts": [
                    "We present a simulation of the spatial, luminosity and spectral distributions of four types of stellar objects. We simulate: (1) Galactic stars, based on a Galactic structure model, a stellar population synthesis model, stellar isochrones, and stellar spectral libraries; (2) white dwarfs, based on model atmospheres, the observed luminosity function, mass distribution, and Galactic distribution of white dwarfs; (3) quasars, based on their observed luminosity function and its evolution, and models of emission and absorption spectra of quasars; and (4) compact emission line galaxies, based on the observed distribution of their spectral properties and sizes. The results are presented in the color system of the Sloan Digital Sky Survey (SDSS), with realistic photometric error and Galactic extinction. The simulated colors of stars and quasars are compared with observations in the SDSS system and show good agreement. The stellar simulation can be used as a tool to analyze star counts and constrain models of Galactic structure, as well as to identify stars with unusual colors. The simulation can also be used to establish the quasar target selection algorithm for the SDSS.",
                    "We measure the LyC escape fraction in 54 faint Lyman Alpha Emitters (LAEs) at $z\\simeq3.1$ in the GOODS-South field. With the average magnitude of $R=26.7$ AB ($M_{UV}=-18.8$, $L\\simeq0.1L^*$), these galaxies represent a population of compact young dwarf galaxies. Their properties are likely to resemble those in the galaxies responsible for reionising the Universe at $z>6$. We do not detect LyC emission in any individual LAEs in the deep {\\em HST} F336W images, which covers the rest-frame 820\\r{A}. We do not detect the LyC emission of these LAEs in the stacked F336W images, either. The $3\\sigma$ upper limit of LyC escape fractions is $f_{\\rm esc}<14-32\\%$. However, the high Ly$\\alpha$ rest-frame equivalent width, low stellar mass and UV luminosity of these LAEs suggest that they should have $f_{\\rm esc}>50\\%$. The low LyC escape fraction from this work and other stacking analysis suggest that the LyC leaking galaxies with $f_{\\rm esc} > 50\\%$ at $z=2-3$ do not follow the relation between the $f_{\\rm esc}$ and UV luminosity and Ly$\\alpha$ equivalent width (EW) derived from typical galaxies at similar redshift. Therefore, the UV luminosity and Ly$\\alpha$ equivalent width (EW) are not the best indicators for the LyC escape fraction.",
                    "This is the second paper in a series aimed at finding high-redshift quasars from five-color (u'g'r'i'z') imaging data taken along the Celestial Equator by the Sloan Digital Sky Survey (SDSS) during its commissioning phase. In this paper, we present 22 high-redshift quasars (z>3.6) discovered from ~250 deg^2 of data in the spring Equatorial Stripe, plus photometry for two previously known high-redshift quasars in the same region of sky. Our success rate of identifying high-redshift quasars is 68%. Five of the newly discovered quasars have redshifts higher than 4.6 (z=4.62, 4.69, 4.70, 4.92 and 5.03). All the quasars have i* < 20.2 with absolute magnitude -28.8 < M_B < -26.1 (h=0.5, q_0=0.5). Several of the quasars show unusual emission and absorption features in their spectra, including an object at z=4.62 without detectable emission lines, and a Broad Absorption Line (BAL) quasar at z=4.92.",
                    "The existence of the three most massive clusters of galaxies observed so far at z>0.5 is used to constrain the mass density parameter of the universe, Omega, and the amplitude of mass fluctuations, sigma_8. We find Omega=0.2 (+0.3,-0.1), and sigma_8=1.2 (+0.5,-0.4) (95 %). We show that the existence of even the single most distant cluster at z=0.83, MS1054-03, with its large gravitational lensing mass, high temperature, and large velocity dispersion, is sufficient to establish powerful constraints. High-density, Omega=1 (sigma_8 ~ 0.5-0.6) Gaussian models are ruled out by these data (< 10^{-6} probability); the Omega=1 models predict only ~10^{-5} massive clusters at z > 0.65 (~10^{-3} at z > 0.5) instead of the 1 (3) clusters observed.",
                    "We report observations of a luminous unresolved object at redshift z = 4.62, with a featureless optical spectrum redward of the Lyman alpha forest region, discovered from Sloan Digital Sky Survey (SDSS) commissioning data. The redshift is determined by the onset of the Lyman-alpha forest at lambda ~ 6800 A, and a Lyman Limit System at lambda = 5120 A. A strong Ly_alpha absorption system with weak metal absorption lines at z=4.58 is also identified in the spectrum. The object has a continuum absolute magnitude of -26.6 at 1450 A in the rest-frame (h_0=0.5, q_0=0.5), and therefore cannot be an ordinary galaxy. It shows no radio emission (the 3-sigma upper limit of its flux at 6 cm is 60 micron Jy), indicating an radio-to-optical flux ratio at least as small as that of the radio-weakest known BL Lacs. It is also not linearly polarized in the observed I band to a 3-sigma upper limit of 4%. Therefore, it is either the most distant BL Lac object known to date, with very weak radio emission, or a new type of unbeamed quasar, whose broad emission-line region is very weak or absent.",
                    "Quasars at cosmic dawn provide powerful probes of the formation and growth of the earliest supermassive black holes (SMBHs) in the universe, their connections to galaxy and structure formation, and the evolution of the intergalactic medium (IGM) at the epoch of reionization (EoR). Hundreds of quasars have been discovered in the first billion years of cosmic history, with the quasar redshift frontier extended to z~7.6. Observations of quasars at cosmic dawn show that: (1) The number density of luminous quasars declines exponentially at z>5, suggesting that the earliest quasars emerge at z~10; the lack of strong evolution in their average spectral energy distribution indicates a rapid buildup of the AGN environment. (2) Billion-solar-mass BHs already exist at z>7.5; they must form and grow in less than 700 Myr, by a combination of massive early BH seeds with highly efficient and sustained accretion. (3) The rapid quasar growth is accompanied by strong star formation and feedback activity in their host galaxies, which show diverse morphological and kinetic properties, with typical dynamical mass of lower than that implied by the local BH/galaxy scaling relations. (4) HI absorption in quasar spectra probes the tail end of cosmic reionization at z~5.3-6, and indicates the EoR midpoint at 6.9 < z < 7.6 with large spatial fluctuations in IGM ionization. Observations of heavy element absorption lines suggest that the circumgalactic medium also experiences evolution in its ionization structure and metal enrichment during the EoR.",
                    "How much matter is there in the universe? Does the universe have the critical density needed to stop its expansion, or is the universe underweight and destined to expand forever? We show that several independent measures, especially those utilizing the largest bound systems known - clusters of galaxies - all indicate that the mass-density of the universe is insufficient to halt the expansion. A promising new method, the evolution of the number density of clusters with time, provides the most powerful indication so far that the universe has a sub-critical density. We show that different techniques reveal a consistent picture of a lightweight universe with only ~20-30% of the critical density. Thus, the universe may expand forever.",
                    "This paper describes the discovery of seven dwarf objects of spectral type `L' (objects cooler than the latest M dwarfs) in commissioning imaging data taken by the Sloan Digital Sky Survey (SDSS). Low-resolution spectroscopy shows that these objects have spectral types from L0 to L8. Comparison of the SDSS and 2MASS photometry for several of these objects indicates the presence of significant opacity at optical wavelengths, perhaps due to atmospheric dust. This comparison also demonstrates the high astrometric accuracy (better than 1'' for these faint sources) of both surveys.   The L dwarfs are shown to occupy a distinctive region of color-color space as measured in the SDSS filters, which should enable their identification in a straightforward way. This should lead eventually to a complete sample of many hundreds of these low mass objects, or about one per 15 square degrees to i'~20, in the complete SDSS data set.",
                    "We show that numerical simulations of reionization that resolve the Lyman Limit systems (and, thus, correctly count absorptions of ionizing photons) have converged to about 10% level for 5<z<6.2 and are in reasonable agreement (within 10%) with the SDSS data in this redshift interval. The SDSS data thus constraint the redshift of overlap of cosmic HII regions to z_{OVL} = 6.1+-0.15. At higher redshifts, the simulations are far from convergence on the mean Gunn-Peterson optical depth, but achieve good convergence for the mean neutral hydrogen fraction. The simulations that fit the SDSS data, however, do not have nearly enough resolution to resolve the earliest episodes of star formation, and are very far from converging on the precise value of the optical depth to Thompson scattering - any value between 6 and 10% is possible, depending on the convergence rate of the simulations and the fractional contribution of PopIII stars. This is generally consistent with the third-year WMAP results, but much higher resolution simulation are required to come up with the sufficiently precise value for the Thompson optical depth that can be statistically compared with the WMAP data.",
                    "We report on 293 quasars with strong NIV] lambda 1486 or NIII] lambda 1750 emission lines (rest-frame equivalent width > 3 \\AA) at 1.7 < z < 4.0 selected from the Sloan Digital Sky Survey (SDSS) Fifth Data Release. These nitrogen-rich (N-rich) objects comprise ~1.1% of the SDSS quasars. The comparison between the N-rich quasars and other quasars shows that the two quasar subsets share many common properties. We also confirm previous results that N-rich quasars have much stronger Lya and NV lambda 1240 emission lines. Strong nitrogen emission in all ionization states indicates high overall nitrogen abundances in these objects. We find evidence that the nitrogen abundance is closely related to quasar radio properties. The radio-loud fraction in the NIII]-rich quasars is 26% and in the NIV]-rich quasars is 69%, significantly higher than ~8% measured in other quasars with similar redshift and luminosity. Therefore, the high nitrogen abundance in N-rich quasars could be an indicator of a special quasar evolution stage, in which the radio activity is also strong.",
                    "Since the beginning of the new millennium, more than 100 $z\\sim 6$ quasars have been discovered through several surveys and followed-up with multi-wavelength observations. These data provided a large amount of information on the growth of supermassive black holes at the early epochs, the properties of quasar host galaxies and the joint formation and evolution of these massive systems. We review the properties of the highest-$z$ quasars known so far, especially focusing on some of the most recent results obtained in (sub-)millimeter bands. We discuss key observational challenges and open issues in theoretical models and highlight possible new strategies to improve our understanding of the galaxy-black hole formation and evolution in the early Universe.",
                    "We report the discovery of a close quasar pair candidate at $z=5.66$, J2037--4537. J2037--4537 is resolved into two quasar images at the same redshift in ground-based observations. Followup spectroscopy shows significant differences in both the continuum slopes and emission line properties of the two images. The two quasar images have a projected separation of $1\\farcs24$ ($7.3\\text{~kpc}$ at $z=5.66$) and a redshift difference of $\\Delta z\\lesssim0.01$. High-resolution images taken by {\\em Hubble Space Telescope} do not detect the foreground lensing galaxy. The observational features of J2037--4537 strongly disfavor the lensing hypothesis. If J2037--4537 is a physical quasar pair, it indicates a quasar clustering signal of $\\sim10^5$ at a separation of $\\sim10$ proper kpc (pkpc), and gives the first observational constraint on the pair fraction of $z>5$ quasars, $f_\\text{pair}(r<30\\text{~pkpc})>0.3\\%$. The properties of J2037--4537 are consistent with those of merger-triggered quasar pairs in hydrodynamical simulations of galaxy mergers.",
                    "The observed lensed fraction of high-redshift quasars $(\\sim0.2\\%)$ is significantly lower than previous theoretical predictions $(\\gtrsim4\\%)$. We revisit the lensed fraction of high-redshift quasars predicted by theoretical models, where we adopt recent measurements of galaxy velocity dispersion functions (VDFs) and explore a wide range of quasar luminosity function (QLF) parameters. We use both analytical methods and mock catalogs which give consistent results. For ordinary QLF parameters and the depth of current high-redshift quasar surveys $(m_z\\lesssim22)$, our model suggests a multiply-imaged fraction of $F_\\text{multi}\\sim 0.4\\%-0.8\\%$. The predicted lensed fraction is $\\sim1\\%-6\\%$ for the brightest $z_s\\sim6$ quasars $(m_z\\lesssim19)$, depending on the QLF. The systematic uncertainties of the predicted lensed fraction in previous models can be as large as $2-4$ times and are dominated by the VDF. Applying VDFs from recent measurements decreases the predicted lensed fraction and relieves the tension between observations and theoretical models. Given the depth of current imaging surveys, there are $\\sim15$ lensed quasars at $z_s>5.5$ detectable over the sky. Upcoming sky surveys like the LSST survey and the {\\em Euclid} survey will find several tens of lensed quasars at this redshift range.",
                    "Recent observations have set the first constraints on the epoch of reionization (EoR), corresponding to the formation epoch of the first luminous objects. Studies of Gunn-Peterson (GP) absorption, and related phenomena, suggest a qualitative change in the state of the intergalactic medium (IGM) at $z \\sim 6$, indicating a rapid increase in the neutral fraction of the IGM, from $x_{HI} < 10^{-4}$ at $z \\le 5.5$, to $x_{HI} > 10^{-3}$, perhaps up to 0.1, at $z \\ge 6$. Conversely, transmission spikes in the GP trough, and the evolution of the \\lya galaxy luminosity function indicate $x_{HI} < 0.5$ at $z\\sim 6.5$, while the large scale polarization of the cosmic microwave background (CMB) implies a significant ionization fraction extending to higher redshifts, $z \\sim 11 \\pm 3$. The results suggest that reionization is less an event than a process, with the process beginning as early as $z \\sim 14$, and with the 'percolation', or 'overlap' phase ending at $z \\sim 6$. The data are consistent with low luminosity star forming galaxies as being the dominant sources of reionizing photons. Low frequency radio telescopes currently under construction should be able to make the first direct measurements of HI 21cm emission from the neutral IGM during the EoR, and upcoming measurements of secondary CMB temperature anisotropy will provide fine details of the dynamics of the reionized IGM.",
                    "We describe a Bayesian approach to estimating luminosity functions. We derive the likelihood function and posterior probability distribution for the luminosity function, given the observed data, and we compare the Bayesian approach with maximum-likelihood by simulating sources from a Schechter function. For our simulations confidence intervals derived from bootstrapping the maximum-likelihood estimate can be too narrow, while confidence intervals derived from the Bayesian approach are valid. We develop our statistical approach for a flexible model where the luminosity function is modeled as a mixture of Gaussian functions. Statistical inference is performed using Markov chain Monte Carlo (MCMC) methods, and we describe a Metropolis-Hastings algorithm to perform the MCMC. The MCMC simulates random draws from the probability distribution of the luminosity function parameters, given the data, and we use a simulated data set to show how these random draws may be used to estimate the probability distribution for the luminosity function. In addition, we show how the MCMC output may be used to estimate the probability distribution of any quantities derived from the luminosity function, such as the peak in the space density of quasars. The Bayesian method we develop has the advantage that it is able to place accurate constraints on the luminosity function even beyond the survey detection limits, and that it provides a natural way of estimating the probability distribution of any quantities derived from the luminosity function, including those that rely on information beyond the survey detection limits."
                ],
                "domain": [
                    "Astrophysics",
                    "Quasars",
                    "Galaxy Formation",
                    "Reionization"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "270946e2-6b8e-4f5e-a23d-4d5814063db4": {
                "pk": "270946e2-6b8e-4f5e-a23d-4d5814063db4",
                "project_name": null,
                "name": "Huabin Xing",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures. For instance, I developed Position-aware GNNs (P-GNNs) to better capture the positional context of nodes within graphs, significantly improving performance in tasks like link prediction and community detection.\n\nI also introduced Identity-aware GNNs (ID-GNNs), which extend the expressive power of traditional GNNs by incorporating node identities during message passing. This innovation has led to substantial accuracy improvements across various prediction tasks. My exploration of dynamic graphs culminated in the ROLAND framework, which allows static GNNs to adapt to dynamic environments, showcasing the scalability and efficiency of my approaches.\n\nBeyond architectural advancements, I have delved into the design space of GNNs, systematically studying over 315,000 designs to provide guidelines for optimizing performance across different tasks. My work on AutoML, particularly with FALCON and AutoTransfer, aims to streamline the search for optimal model designs, making it more efficient and insightful.\n\nOverall, my research is driven by a passion for pushing the boundaries of GNNs and machine learning, with the goal of making these technologies more effective and accessible for real-world applications.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "AutoML",
                    "Multi-task Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "05678b6b-982d-4488-9408-7180589987f9": {
                "pk": "05678b6b-982d-4488-9408-7180589987f9",
                "project_name": null,
                "name": "Huajun Chen",
                "bio": "As a researcher deeply engaged in the intersection of language, knowledge representation, and machine learning, my work focuses on enhancing our understanding of how human cognition and language can be modeled and improved through advanced computational techniques. My recent publications explore the capabilities of Large Language Models (LLMs) and their integration with symbolic knowledge structures, such as Knowledge Graphs (KGs). I advocate for the development of Large Knowledge Models (LKMs) that can effectively manage diverse knowledge structures, addressing challenges like cognitive alignment and commonsense reasoning.\n\nI have also delved into the realm of data stream learning, tackling concept drift in semantic web contexts, and have proposed frameworks for interpretable transfer learning using KGs. My contributions to generative Knowledge Graph Construction (KGC) highlight the potential of sequence-to-sequence frameworks in building knowledge graphs, while my work on the dual-encoding Transformer architecture addresses scalability issues in processing large graphs.\n\nIn addition, I have developed innovative methods for image editing through reinforcement learning, and I have tackled the challenges of few-shot link prediction and knowledge graph completion in federated settings. My research emphasizes the importance of multi-modal knowledge graph completion, where I introduced adaptive techniques to leverage imbalanced modality information effectively.\n\nOverall, my work aims to bridge the gap between human-like understanding and machine learning, fostering advancements that can lead to more intelligent and interpretable AI systems.",
                "collaborators": [
                    "Jiaoyan Chen",
                    "Wen Zhang",
                    "Ningyu Zhang",
                    "Mingyang Chen",
                    "Freddy Lecue",
                    "Jeff Pan",
                    "Yuxia Geng",
                    "Ernesto Jimenez-Ruiz",
                    "Hongbin Ye",
                    "Hui Chen"
                ],
                "pub_titles": [
                    "Large Knowledge Model: Perspectives and Challenges",
                    "Learning from Ontology Streams with Semantic Concept Drift",
                    "Human-centric Transfer Learning Explanation via Knowledge Graph [Extended Abstract]",
                    "Generative Knowledge Graph Construction: A Review",
                    "Unleashing the Power of Transformer for Graphs",
                    "InstructRL4Pix: Training Diffusion for Image Editing by Reinforcement Learning",
                    "Meta Relational Learning for Few-Shot Link Prediction in Knowledge Graphs",
                    "FedE: Embedding Knowledge Graphs in Federated Setting",
                    "When Low Resource NLP Meets Unsupervised Language Model: Meta-pretraining Then Meta-learning for Few-shot Text Classification",
                    "Unleashing the Power of Imbalanced Modality Information for Multi-modal Knowledge Graph Completion"
                ],
                "pub_abstracts": [
                    "Humankind's understanding of the world is fundamentally linked to our perception and cognition, with \\emph{human languages} serving as one of the major carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language Models} (LLMs) like ChatGPT epitomize the pre-training of extensive, sequence-based world knowledge into neural networks, facilitating the processing and manipulation of this knowledge in a parametric space. This article explores large models through the lens of \"knowledge\". We initially investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in enhancing LLMs, covering aspects like knowledge-augmented language model, structure-inducing pre-training, knowledgeable prompts, structured CoT, knowledge editing, semantic tools for LLM and knowledgeable AI agents. Subsequently, we examine how LLMs can boost traditional symbolic knowledge bases, encompassing aspects like using LLM as KG builder and controller, structured knowledge pretraining, and LLM-enhanced symbolic reasoning. Considering the intricate nature of human knowledge, we advocate for the creation of \\emph{Large Knowledge Models} (LKM), specifically engineered to manage diversified spectrum of knowledge structures. This promising undertaking would entail several key challenges, such as disentangling knowledge base from language models, cognitive alignment with human knowledge, integration of perception and cognition, and building large commonsense models for interacting with physical world, among others. We finally propose a five-\"A\" principle to distinguish the concept of LKM.",
                    "Data stream learning has been largely studied for extracting knowledge structures from continuous and rapid data records. In the semantic Web, data is interpreted in ontologies and its ordered sequence is represented as an ontology stream. Our work exploits the semantics of such streams to tackle the problem of concept drift i.e., unexpected changes in data distribution, causing most of models to be less accurate as time passes. To this end we revisited (i) semantic inference in the context of supervised stream learning, and (ii) models with semantic embeddings. The experiments show accurate prediction with data from Dublin and Beijing.",
                    "Transfer learning which aims at utilizing knowledge learned from one problem (source domain) to solve another different but related problem (target domain) has attracted wide research attentions. However, the current transfer learning methods are mostly uninterpretable, especially to people without ML expertise. In this extended abstract, we brief introduce two knowledge graph (KG) based frameworks towards human understandable transfer learning explanation. The first one explains the transferability of features learned by Convolutional Neural Network (CNN) from one domain to another through pre-training and fine-tuning, while the second justifies the model of a target domain predicted by models from multiple source domains in zero-shot learning (ZSL). Both methods utilize KG and its reasoning capability to provide rich and human understandable explanations to the transfer procedure.",
                    "Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.",
                    "Despite recent successes in natural language processing and computer vision, Transformer suffers from the scalability problem when dealing with graphs. The computational complexity is unacceptable for large-scale graphs, e.g., knowledge graphs. One solution is to consider only the near neighbors, which, however, will lose the key merit of Transformer to attend to the elements at any distance. In this paper, we propose a new Transformer architecture, named dual-encoding Transformer (DET). DET has a structural encoder to aggregate information from connected neighbors and a semantic encoder to focus on semantically useful distant nodes. In comparison with resorting to multi-hop neighbors, DET seeks the desired distant neighbors via self-supervised training. We further find these two encoders can be incorporated to boost each others' performance. Our experiments demonstrate DET has achieved superior performance compared to the respective state-of-the-art methods in dealing with molecules, networks and knowledge graphs with various sizes.",
                    "Instruction-based image editing has made a great process in using natural human language to manipulate the visual content of images. However, existing models are limited by the quality of the dataset and cannot accurately localize editing regions in images with complex object relationships. In this paper, we propose Reinforcement Learning Guided Image Editing Method(InstructRL4Pix) to train a diffusion model to generate images that are guided by the attention maps of the target object. Our method maximizes the output of the reward model by calculating the distance between attention maps as a reward function and fine-tuning the diffusion model using proximal policy optimization (PPO). We evaluate our model in object insertion, removal, replacement, and transformation. Experimental results show that InstructRL4Pix breaks through the limitations of traditional datasets and uses unsupervised learning to optimize editing goals and achieve accurate image editing based on natural human commands.",
                    "Link prediction is an important way to complete knowledge graphs (KGs), while embedding-based methods, effective for link prediction in KGs, perform poorly on relations that only have a few associative triples. In this work, we propose a Meta Relational Learning (MetaR) framework to do the common but challenging few-shot link prediction in KGs, namely predicting new triples about a relation by only observing a few associative triples. We solve few-shot link prediction by focusing on transferring relation-specific meta information to make model learn the most important knowledge and learn faster, corresponding to relation meta and gradient meta respectively in MetaR. Empirically, our model achieves state-of-the-art results on few-shot link prediction KG benchmarks.",
                    "Knowledge graphs (KGs) consisting of triples are always incomplete, so it's important to do Knowledge Graph Completion (KGC) by predicting missing triples. Multi-Source KG is a common situation in real KG applications which can be viewed as a set of related individual KGs where different KGs contains relations of different aspects of entities. It's intuitive that, for each individual KG, its completion could be greatly contributed by the triples defined and labeled in other ones. However, because of the data privacy and sensitivity, a set of relevant knowledge graphs cannot complement each other's KGC by just collecting data from different knowledge graphs together. Therefore, in this paper, we introduce federated setting to keep their privacy without triple transferring between KGs and apply it in embedding knowledge graph, a typical method which have proven effective for KGC in the past decade. We propose a Federated Knowledge Graph Embedding framework FedE, focusing on learning knowledge graph embeddings by aggregating locally-computed updates. Finally, we conduct extensive experiments on datasets derived from KGE benchmark datasets and results show the effectiveness of our proposed FedE.",
                    "Text classification tends to be difficult when data are deficient or when it is required to adapt to unseen classes. In such challenging scenarios, recent studies have often used meta-learning to simulate the few-shot task, thus negating implicit common linguistic features across tasks. This paper addresses such problems using meta-learning and unsupervised language models. Our approach is based on the insight that having a good generalization from a few examples relies on both a generic model initialization and an effective strategy for adapting this model to newly arising tasks. We show that our approach is not only simple but also produces a state-of-the-art performance on a well-studied sentiment classification dataset. It can thus be further suggested that pretraining could be a promising solution for few-shot learning of many other NLP tasks. The code and the dataset to replicate the experiments are made available at https://github.com/zxlzr/FewShotNLP.",
                    "Multi-modal knowledge graph completion (MMKGC) aims to predict the missing triples in the multi-modal knowledge graphs by incorporating structural, visual, and textual information of entities into the discriminant models. The information from different modalities will work together to measure the triple plausibility. Existing MMKGC methods overlook the imbalance problem of modality information among entities, resulting in inadequate modal fusion and inefficient utilization of the raw modality information. To address the mentioned problems, we propose Adaptive Multi-modal Fusion and Modality Adversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality information for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive modality weights and further generates adversarial samples by modality-adversarial training to enhance the imbalanced modality information. Our approach is a co-design of the MMKGC model and training strategy which can outperform 19 recent MMKGC methods and achieve new state-of-the-art results on three public MMKGC benchmarks. Our code and data have been released at https://github.com/zjukg/AdaMF-MAT."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Knowledge Graph",
                    "Transfer Learning",
                    "Multi-modal Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively develop and implement Scientific Large Language Models (Sci-LLMs) that accurately process and understand specialized scientific languages, such as those used in chemistry and biology, to enhance scientific discovery and innovation?\n\n---\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it addresses the limitations of existing general Large Language Models (LLMs) in handling specialized scientific data. By developing Sci-LLMs, we can significantly improve the accuracy and efficiency of scientific research, leading to advancements in drug discovery, molecular design, and protein engineering. This work could pave the way for more integrated and interdisciplinary approaches in science, fostering collaboration between computational and experimental scientists. Furthermore, it could lead to practical applications in various fields, including healthcare, environmental science, and materials science, ultimately enhancing our understanding of complex biological and chemical systems.\n\n---\n\n**[Question 3] - Why is it hard?**  \nThe challenges in developing Sci-LLMs stem from the inherent complexities of scientific languages, which have distinct vocabularies and grammatical rules that differ significantly from natural languages. Naive approaches may fail because they do not account for these differences, leading to misinterpretations of scientific data. Additionally, the vast diversity of scientific languages across different domains presents a technical obstacle in creating a unified model. Theoretical challenges include the need for robust training datasets that accurately represent the nuances of scientific language, as well as the development of metrics that can effectively evaluate the performance of Sci-LLMs in real-world applications.\n\n---\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on general LLMs, which are not tailored to the specific needs of scientific languages. Existing models often lack the necessary training on specialized datasets, leading to gaps in their ability to process scientific terminology accurately. Barriers such as the limited availability of annotated scientific data and the complexity of integrating diverse scientific languages have hindered progress. Our approach differs by focusing on the creation of domain-specific datasets and employing advanced training techniques that incorporate the unique grammatical structures and semantic meanings of scientific languages, thereby improving upon prior work.\n\n---\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves developing Sci-LLMs using a combination of transformer architectures and domain-specific training datasets that encompass various scientific languages, such as SMILES for chemistry and protein sequences for biology. We will utilize"
    },
    "2401.11052": {
        "paper_data": {
            "title": "Mining experimental data from Materials Science literature with Large Language Models: an evaluation study",
            "url": "http://arxiv.org/abs/2401.11052v3",
            "arxiv_id": "2401.11052",
            "authors": [
                "Luca Foppiano",
                "Guillaume Lambard",
                "Toshiyuki Amagasa",
                "Masashi Ishii"
            ],
            "abstract": "This study is dedicated to assessing the capabilities of large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in extracting structured information from scientific documents in materials science. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. Due to the evident lack of datasets within Materials Informatics (MI), we evaluated using SuperMat, based on superconductor research, and MeasEval, a generic measurement evaluation corpus. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches (baseline). We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, a GPT-3.5-Turbo fine-tuned with the appropriate strategy for RE outperforms all models, including the baseline. Without any fine-tuning, GPT-4 and GPT-4-Turbo display remarkable reasoning and relationship extraction capabilities after being provided with merely a couple of examples, surpassing the baseline. Overall, the results suggest that although LLMs demonstrate relevant reasoning skills in connecting concepts, specialised models are currently a better choice for tasks requiring extracting complex domain-specific entities like materials. These insights provide initial guidance applicable to other materials science sub-domains in future work.",
            "introduction": "   1 Introduction  Mining experimental data from literature has become increasingly popular in materials science due to the vast amount of information available and the need to accelerate materials discovery using data-driven techniques. Data for machine learning in materials science is often sourced from published papers, material databases, laboratory experiments, or first-principles calculations [1]. The introduction of big data in materials research has shifted from traditional random techniques to more efficient, data-driven methods. Data mining of computational screening libraries has been shown to identify different classes of strong CO2subscriptCO2\\rm CO_{2}roman_CO start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-binding sites, enabling materials to exhibit specific properties even in wet flue gases [2]. Machine learning techniques have been employed for high-entropy alloy discovery, focusing on probabilistic models and artificial neural networks [3]. However, the use of advanced machine learning algorithms in experimental materials science is limited by the lack of sufficiently large and diverse datasets amenable to data mining [4]. A present central tenet of data-driven materials discovery is that with a sufficiently large volume of accumulated data and suitable data-driven techniques, designing a new material could be more efficient and rational [5]. The materials science field is moving away from traditional manual, serial, and human-intensive work towards automated, parallel, and iterative processes driven by artificial intelligence, simulation, and experimental automation [6, 7]. But, materials science literature is a vast source of knowledge that remains relatively unexplored with data mining techniques [8], especially for the reason that materials science data come in diverse forms such as unstructured textual content and structured tables and graphs, adding complexity to the extraction process. As a result, nowadays, many projects still depend on manual data extraction. While extensive structured databases contain accumulated experimental data [9], they remain limited in number and highly costly due to the amount of human labour involved [10].   Additionally, addressing issues related to the quality and meaning of materials science data often demands a curation step assisted by a sub-domain knowledge frequently specific to the approached sub-field of materials science, e.g. polymers, metal-organic frameworks, high-entropy alloys, etc., with their own physical and chemical phenomena, methods and protocols, terminology and jargon. For instance, the classification of superconductors can be complex and sometimes arbitrary, blending compound-based classes like cuprates [11] and iron-based [12] materials with unconventional classes like heavy fermions [13]. The classification of superconductors can also be based on phenomena such as the Meissner effect, which describes how superconductors expel magnetic fields [14]. Superconductors can be divided into two classes according to how this breakdown occurs, the so-called type-I and type-II superconductors. As these classifications are not mutually exclusive certain materials could potentially fall into multiple categories, for example, a material can be both a cuprate and a type-II superconductor, the classification of superconductors is a complex task that demands an extensive knowledge of the developments and current state-of-the-arts. Moreover, substantial confusion may occur due to the cross-domain polysemy of used words, terms and symbols. In different sub-domains, the same term can take on specific nuances or meanings unique to a given sub-domain. This phenomenon is common in language and can lead to misunderstandings if the context of the sub-domain is unclear. For instance, the acronym",
            "references": [
                {
                    "title": "MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation Extraction for Material Science Knowledge-base Construction",
                    "abstract": null
                },
                {
                    "title": "Prompt engineering of GPT-4 for chemical research: what can/cannot be done?",
                    "abstract": "ABSTRACT This paper evaluates the capabilities and limitations of the Generative Pre-trained Transformer 4 (GPT-4) in chemical research. Although GPT-4 exhibits remarkable proficiencies, it is evident that the quality of input data significantly affects its performance. We explore GPT-4’s potential in chemical tasks, such as foundational chemistry knowledge, cheminformatics, data analysis, problem prediction, and proposal abilities. While the language model partially outperformed traditional methods, such as black-box optimization, it fell short against specialized algorithms, highlighting the need for their combined use. The paper shares the prompts given to GPT-4 and its responses, providing a resource for prompt engineering within the community, and concludes with a discussion on the future of chemical research using large language models. GRAPHICAL ABSTRACT IMPACT STATEMENT This paper comprehensively reveals the advantages and limitations of GPT-4 in chemical research, such as expert knowledge, data analysis, prediction, suggestion, and autonomous experimentation."
                },
                {
                    "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
                    "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation."
                },
                {
                    "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
                    "abstract": "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents."
                },
                {
                    "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
                    "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm."
                },
                {
                    "title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era",
                    "abstract": "OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is demonstrated to be one small step for generative AI (GAI), but one giant leap for artificial general intelligence (AGI). Since its official release in November 2022, ChatGPT has quickly attracted numerous users with extensive media coverage. Such unprecedented attention has also motivated numerous researchers to investigate ChatGPT from various aspects. According to Google scholar, there are more than 500 articles with ChatGPT in their titles or mentioning it in their abstracts. Considering this, a review is urgently needed, and our work fills this gap. Overall, this work is the first to survey ChatGPT with a comprehensive review of its underlying technology, applications, and challenges. Moreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI."
                },
                {
                    "title": "Small data machine learning in materials science",
                    "abstract": null
                },
                {
                    "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
                    "abstract": "Large Language Models (LLMs) have made remarkable strides in various tasks. Whether LLMs are competitive few-shot solvers for information extraction (IE) tasks, however, remains an open problem. In this work, we aim to provide a thorough answer to this question. Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings. Therefore, we conclude that LLMs are not effective few-shot information extractors in general. Nonetheless, we illustrate that with appropriate prompting strategies, LLMs can effectively complement SLMs and tackle challenging samples that SLMs struggle with. And moreover, we propose an adaptive filter-then-rerank paradigm to combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as filters and LLMs serve as rerankers. By prompting LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.4% F1-gain on average) on various IE tasks, with an acceptable time and cost investment."
                },
                {
                    "title": "ChatGPT: Jack of all trades, master of none",
                    "abstract": null
                },
                {
                    "title": "Automatic extraction of materials and properties from superconductors scientific literature",
                    "abstract": "ABSTRACT The automatic extraction of materials and related properties from the scientific literature is gaining attention in data-driven materials science (Materials Informatics). In this paper, we discuss Grobid-superconductors, our solution for automatically extracting superconductor material names and respective properties from text. Built as a Grobid module, it combines machine learning and heuristic approaches in a multi-step architecture that supports input data as raw text or PDF documents. Using Grobid-superconductors, we built SuperCon2, a database of 40,324 materials and properties records from 37,700 papers. The material (or sample) information is represented by name, chemical formula, and material class, and is characterized by shape, doping, substitution variables for components, and substrate as adjoined information. The properties include the Tc superconducting critical temperature and, when available, applied pressure with the Tc measurement method. Graphical Abstract"
                },
                {
                    "title": "Accelerating materials discovery using artificial intelligence, high performance computing and robotics",
                    "abstract": null
                },
                {
                    "title": "Machine learning–enabled high-entropy alloy discovery",
                    "abstract": "High-entropy alloys are solid solutions of multiple principal elements that are capable of reaching composition and property regimes inaccessible for dilute materials. Discovering those with valuable properties, however, too often relies on serendipity, because thermodynamic alloy design rules alone often fail in high-dimensional composition spaces. We propose an active learning strategy to accelerate the design of high-entropy Invar alloys in a practically infinite compositional space based on very sparse data. Our approach works as a closed-loop, integrating machine learning with density-functional theory, thermodynamic calculations, and experiments. After processing and characterizing 17 new alloys out of millions of possible compositions, we identified two high-entropy Invar alloys with extremely low thermal expansion coefficients around 2 × 10−6 per degree kelvin at 300 kelvin. We believe this to be a suitable pathway for the fast and automated discovery of high-entropy alloys with optimal thermal, magnetic, and electrical properties. Description A little expansive Invar alloys have extremely low thermal expansion, making them attractive for several types of applications. Finding these types of alloys in a complex compositional space, however, is challenging. Rao et al. used an iterative scheme that combines machine learning, density functional theory, experiments, and thermodynamic calculation to find two new invar alloys out of millions of candidates (see the Perspective by Hu and Yang). The alloys are both compositionally complex, high entropy materials, thus demonstrating the power of this approach for materials discovery. —BG Two high-entropy alloys with extremely low thermal expansion were found with the help of machine learning."
                },
                {
                    "title": "Big Data Mining and Classification of Intelligent Material Science Data Using Machine Learning",
                    "abstract": "There is a high need for a big data repository for material compositions and their derived analytics of metal strength, in the material science community. Currently, many researchers maintain their own excel sheets, prepared manually by their team by tabulating the experimental data collected from scientific journals, and analyzing the data by performing manual calculations using formulas to determine the strength of the material. In this study, we propose a big data storage for material science data and its processing parameters information to address the laborious process of data tabulation from scientific articles, data mining techniques to retrieve the information from databases to perform big data analytics, and a machine learning prediction model to determine material strength insights. Three models are proposed based on Logistic regression, Support vector Machine SVM and Random Forest Algorithms. These models are trained and tested using a 10-fold cross validation approach. The Random Forest classification model performed better on the independent dataset, with 87% accuracy in comparison to Logistic regression and SVM with 72% and 78%, respectively."
                },
                {
                    "title": "GPT-3 Models are Poor Few-Shot Learners in the Biomedical Domain",
                    "abstract": "Deep neural language models have set new breakthroughs in many tasks of Natural Language Processing (NLP). Recent work has shown that deep transformer language models (pretrained on large amounts of texts) can achieve high levels of task-specific few-shot performance comparable to state-of-the-art models. However, the ability of these large language models in few-shot transfer learning has not yet been explored in the biomedical domain. We investigated the performance of two powerful transformer language models, i.e. GPT-3 and BioBERT, in few-shot settings on various biomedical NLP tasks. The experimental results showed that, to a great extent, both the models underperform a language model fine-tuned on the full training data. Although GPT-3 had already achieved near state-of-the-art results in few-shot knowledge transfer on open-domain NLP tasks, it could not perform as effectively as BioBERT, which is orders of magnitude smaller than GPT-3. Regarding that BioBERT was already pretrained on large biomedical text corpora, our study suggests that language models may largely benefit from in-domain pretraining in task-specific few-shot learning. However, in-domain pretraining seems not to be sufficient; novel pretraining and few-shot learning strategies are required in the biomedical NLP domain."
                },
                {
                    "title": "Advances in scientific literature mining for interpreting materials characterization",
                    "abstract": null
                },
                {
                    "title": "SuperMat: construction of a linked annotated dataset from superconductors-related publications",
                    "abstract": "ABSTRACT A growing number of papers are published in the area of superconducting materials science. However, novel text and data mining (TDM) processes are still needed to efficiently access and exploit this accumulated knowledge, paving the way towards data-driven materials design. Herein, we present SuperMat (Superconductor Materials), an annotated corpus of linked data derived from scientific publications on superconductors, which comprises 142 articles, 16052 entities, and 1398 links that are characterised into six categories: the names, classes, and properties of materials; links to their respective superconducting critical temperature (Tc); and parametric conditions such as applied pressure or measurement methods. The construction of SuperMat resulted from a fruitful collaboration between computer scientists and material scientists, and its high quality is ensured through validation by domain experts. The quality of the annotation guidelines was ensured by satisfactory Inter Annotator Agreement (IAA) between the annotators and the domain experts. SuperMat includes the dataset, annotation guidelines, and annotation support tools that use automatic suggestions to help minimise human errors."
                },
                {
                    "title": "Electron Doping of the Iron-Arsenide Superconductor CeFeAsO Controlled by Hydrostatic Pressure.",
                    "abstract": "In the iron-pnictide material CeFeAsO not only the Fe moments, but also the local 4f moments of the Ce order antiferromagnetically at low temperatures. We elucidate on the peculiar role of the Ce on the emergence of superconductivity. While application of pressure suppresses the iron SDW ordering temperature monotonously up to 4 GPa, the Ce-4f magnetism is stabilized until both types of magnetic orders disappear abruptly and a narrow SC dome develops. With further increasing pressure characteristics of a Kondo-lattice system become more and more apparent in the electrical resistivity. This suggests a connection of the emergence of superconductivity with the extinction of the magnetic order and the onset of Kondo screening of the Ce-4f moments."
                },
                {
                    "title": "Data augmentation in microscopic images for material data mining",
                    "abstract": null
                },
                {
                    "title": "Data-driven design of metal–organic frameworks for wet flue gas CO2 capture",
                    "abstract": null
                },
                {
                    "title": "Automatic Identification and Normalisation of Physical Measurements in Scientific Literature",
                    "abstract": "We present Grobid-quantities, an open-source application for extracting and normalising measurements from scientific and patent literature. Tools of this kind, aiming to understand and make unstructured information accessible, represent the building blocks for large-scale Text and Data Mining (TDM) systems. Grobid-quantities is a module built on top of Grobid [6] [13], a machine learning framework for parsing and structuring PDF documents. Designed to process large quantities of data, it provides a robust implementation accessible in batch mode or via a REST API. The machine learning engine architecture follows the cascade approach, where each model is specialised in the resolution of a specific task. The models are trained using CRF (Conditional Random Field) algorithm [12] for extracting quantities (atomic values, intervals and lists), units (such as length, weight) and different value representations (numeric, alphabetic or scientific notation). Identified measurements are normalised according to the International System of Units (SI). Thanks to its stable recall and reliable precision, Grobid-quantities has been integrated as the measurement-extraction engine in various TDM projects, such as Marve (Measurement Context Extraction from Text), for extracting semantic measurements and meaning in Earth Science [10]. At the National Institute for Materials Science in Japan (NIMS), it is used in an ongoing project to discover new superconducting materials. Normalised materials characteristics (such as critical temperature, pressure) extracted from scientific literature are a key resource for materials informatics (MI) [9]."
                },
                {
                    "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
                    "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
                },
                {
                    "title": "SciBERT: A Pretrained Language Model for Scientific Text",
                    "abstract": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/."
                },
                {
                    "title": "An open experimental database for exploring inorganic materials",
                    "abstract": null
                },
                {
                    "title": "A polymer dataset for accelerated property prediction and design",
                    "abstract": null
                },
                {
                    "title": "Exploration of new superconductors and functional materials, and fabrication of superconducting tapes and wires of iron pnictides",
                    "abstract": "Abstract This review shows the highlights of a 4-year-long research project supported by the Japanese Government to explore new superconducting materials and relevant functional materials. The project found several tens of new superconductors by examining ∼1000 materials, each of which was chosen by Japanese experts with a background in solid state chemistry. This review summarizes the major achievements of the project in newly found superconducting materials, and the fabrication wires and tapes of iron-based superconductors; it incorporates a list of ∼700 unsuccessful materials examined for superconductivity in the project. In addition, described are new functional materials and functionalities discovered during the project."
                },
                {
                    "title": "Microstructure and Properties of High-Temperature Superconductors",
                    "abstract": null
                },
                {
                    "title": "Theory of Superconductivity",
                    "abstract": null
                },
                {
                    "title": "Using GPT-4 in Parameter Selection of Polymer Informatics: Improving Predictive Accuracy Amidst Data Scarcity and 'Ugly Duckling' Dilemma",
                    "abstract": "Materials informatics and cheminformatics struggle with data scarcity, hindering the extraction of significant relationships between structures and properties. The \"Ugly Duckling\" theorem, suggesting the difficulty of data processing without assumptions..."
                },
                {
                    "title": "SemEval-2021 Task 8: MeasEval – Extracting Counts and Measurements and their Related Contexts",
                    "abstract": "We describe MeasEval, a SemEval task of extracting counts, measurements, and related context from scientific documents, which is of significant importance to the creation of Knowledge Graphs that distill information from the scientific literature. This is a new task in 2021, for which over 75 submissions from 25 participants were received. We expect the data developed for this task and the findings reported to be valuable to the scientific knowledge extraction, metrology, and automated knowledge base construction communities."
                },
                {
                    "title": "Machine Learning and Data Mining in Materials Science",
                    "abstract": null
                },
                {
                    "title": "A survey of named entity recognition and classification",
                    "abstract": "This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress."
                },
                {
                    "title": "[Yes... but].",
                    "abstract": null
                },
                {
                    "title": "Pattern matching: the gestalt approach",
                    "abstract": null
                },
                {
                    "title": "for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \" , → description\": \"a list of strings\", \"type\": \"array\", \"items\": {\" , → type\": \"string\"}}}, \"required\": [\"foo\"]}",
                    "abstract": null
                },
                {
                    "title": "material chemical form with no variables e.g. LaFe03NaCl2 where the , → doping rates are included in the name",
                    "abstract": null
                },
                {
                    "title": "You are a useful assistant",
                    "abstract": null
                },
                {
                    "title": "chemical substitution or replacements, like (A is a random variable, , → can be any symbol): A = Ni, Cu, A = Ni, Ni substituted (which , → means A = Ni)",
                    "abstract": null
                },
                {
                    "title": "Wiktoria Mieleszczenko-Kowszewicz",
                    "abstract": null
                },
                {
                    "title": "You will be asked to compute relation extraction given a text and lists , → of entities",
                    "abstract": null
                },
                {
                    "title": "Include relevant text that indicates the application of a modifier, such , → as \"between\", \"less than\", \"approximately\",",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "f556e69a-2990-4127-8db6-c14f2897dd23": {
                "pk": "f556e69a-2990-4127-8db6-c14f2897dd23",
                "project_name": null,
                "name": "Luca Foppiano",
                "bio": "I am a researcher dedicated to advancing the field of materials informatics, particularly in the extraction and analysis of superconducting materials from scientific literature. My recent work has focused on developing Grobid-superconductors, a robust solution that leverages machine learning and heuristic methods to automatically extract material names and properties from raw text and PDF documents. This effort culminated in the creation of SuperCon2, a comprehensive database containing over 40,000 records of superconductors, which enhances the existing SuperCon database.\n\nI am passionate about improving the efficiency and accuracy of data curation processes. To this end, I designed a semi-automatic curation interface that integrates anomaly detection and a training data collector, significantly boosting the quality of our dataset while minimizing manual corrections. My work emphasizes collaboration between computer scientists and material scientists, as exemplified by the development of SuperMat, an annotated corpus that links data from scientific publications on superconductors. This project not only showcases my commitment to high-quality data annotation but also highlights the importance of interdisciplinary collaboration in achieving reliable and actionable insights in materials science.\n\nThrough my research, I aim to pave the way for data-driven materials design, making it easier for scientists to access and utilize the wealth of knowledge contained in the growing body of literature on superconducting materials.",
                "collaborators": [
                    "Kensei Terashima",
                    "Yoshihiko Takano",
                    "Masashi Ishii",
                    "Pedro Baptista de Castro",
                    "Pedro Ortiz Suarez",
                    "Tomoya Mato",
                    "Taku Tou",
                    "Chikako Sakai",
                    "Wei-Sheng Wang",
                    "Toshiyuki Amagasa"
                ],
                "pub_titles": [
                    "Automatic extraction of materials and properties from superconductors scientific literature",
                    "Semi-automatic staging area for high-quality structured data extraction from scientific literature",
                    "SuperMat: Construction of a linked annotated dataset from superconductors-related publications"
                ],
                "pub_abstracts": [
                    "The automatic extraction of materials and related properties from the scientific literature is gaining attention in data-driven materials science (Materials Informatics). In this paper, we discuss Grobid-superconductors, our solution for automatically extracting superconductor material names and respective properties from text. Built as a Grobid module, it combines machine learning and heuristic approaches in a multi-step architecture that supports input data as raw text or PDF documents. Using Grobid-superconductors, we built SuperCon2, a database of 40324 materials and properties records from 37700 papers. The material (or sample) information is represented by name, chemical formula, and material class, and is characterized by shape, doping, substitution variables for components, and substrate as adjoined information. The properties include the Tc superconducting critical temperature and, when available, applied pressure with the Tc measurement method.",
                    "We propose a semi-automatic staging area for efficiently building an accurate database of experimental physical properties of superconductors from literature, called SuperCon2, to enrich the existing manually-built superconductor database SuperCon. Here we report our curation interface (SuperCon2 Interface) and a workflow managing the state transitions of each examined record, to validate the dataset of superconductors from PDF documents collected using Grobid-superconductors in a previous work. This curation workflow allows both automatic and manual operations, the former contains ``anomaly detection'' that scans new data identifying outliers, and a ``training data collector'' mechanism that collects training data examples based on manual corrections. Such training data collection policy is effective in improving the machine-learning models with a reduced number of examples. For manual operations, the interface (SuperCon2 interface) is developed to increase efficiency during manual correction by providing a smart interface and an enhanced PDF document viewer. We show that our interface significantly improves the curation quality by boosting precision and recall as compared with the traditional ``manual correction''. Our semi-automatic approach would provide a solution for achieving a reliable database with text-data mining of scientific documents.",
                    "A growing number of papers are published in the area of superconducting materials science. However, novel text and data mining (TDM) processes are still needed to efficiently access and exploit this accumulated knowledge, paving the way towards data-driven materials design. Herein, we present SuperMat (Superconductor Materials), an annotated corpus of linked data derived from scientific publications on superconductors, which comprises 142 articles, 16052 entities, and 1398 links that are characterised into six categories: the names, classes, and properties of materials; links to their respective superconducting critical temperature (Tc); and parametric conditions such as applied pressure or measurement methods. The construction of SuperMat resulted from a fruitful collaboration between computer scientists and material scientists, and its high quality is ensured through validation by domain experts. The quality of the annotation guidelines was ensured by satisfactory Inter Annotator Agreement (IAA) between the annotators and the domain experts. SuperMat includes the dataset, annotation guidelines, and annotation support tools that use automatic suggestions to help minimise human errors."
                ],
                "domain": [
                    "Materials Informatics",
                    "Text Mining",
                    "Machine Learning",
                    "Superconductors"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "cd303160-3fd4-4d66-b9f6-2fdc4bd001a4": {
                "pk": "cd303160-3fd4-4d66-b9f6-2fdc4bd001a4",
                "project_name": null,
                "name": "Guillaume Lambard",
                "bio": "I am a researcher with a strong focus on the intersection of particle physics and machine learning, particularly in the context of neutrino detection and materials science. My work with the ANTARES neutrino telescope involved a comprehensive search for high-energy neutrinos originating from the Sun, where I developed selection criteria to effectively distinguish potential signals from atmospheric background noise. This research not only yielded competitive upper limits for weakly interactive massive particles (WIMPs) but also positioned ANTARES alongside leading neutrino observatories like IceCube and SuperKamiokande.\n\nIn addition to my work in particle physics, I am passionate about applying machine learning techniques to materials science, especially in scenarios where data is scarce. I developed the SMILES-X pipeline, an innovative approach that leverages a neural architecture to characterize molecular compounds using only SMILES strings as input. By incorporating an attention mechanism, SMILES-X not only enhances predictive accuracy but also provides interpretability without additional computational overhead. The results achieved with SMILES-X, including state-of-the-art performance in predicting aqueous solubility and hydration free energy, demonstrate its potential as a valuable tool for materials scientists and chemists. I am committed to advancing these fields through the integration of cutting-edge computational methods and experimental insights.",
                "collaborators": [
                    "Ekaterina Gracheva"
                ],
                "pub_titles": [
                    "Indirect dark matter search with the ANTARES neutrino telescope",
                    "SMILES-X: autonomous molecular compounds characterization for small datasets without descriptors"
                ],
                "pub_abstracts": [
                    "Using the data recorded by the ANTARES neutrino telescope during 2007 and 2008, a search for high energy neutrinos coming from the direction of the Sun has been performed. The neutrino selection criteria have been chosen so as to maximize the rejection of the atmospheric background with respect to possible signals produced by the self-annihilation of weakly interactive massive particles accumulated in the centre of the Sun. After data unblinding, the number of neutrinos observed was found to be compatible with background expectations. The results obtained were compared to the fluxes predicted by the Constrained Minimal Supersymmetric Standard Model, and 90% upper limits for this model were obtained. Our limits are competitive with those obtained by other neutrino telescopes such as IceCube and SuperKamiokande, which give ANTARES limits for the spin-dependent WIMP-proton cross-section that are more stringent than those obtained by direct search experiments.",
                    "There is more and more evidence that machine learning can be successfully applied in materials science and related fields. However, datasets in these fields are often quite small ($\\ll1000$ samples). It makes the most advanced machine learning techniques remain neglected, as they are considered to be applicable to big data only. Moreover, materials informatics methods often rely on human-engineered descriptors, that should be carefully chosen, or even created, to fit the physicochemical property that one intends to predict. In this article, we propose a new method that tackles both the issue of small datasets and the difficulty of task-specific descriptors development. The SMILES-X is an autonomous pipeline for molecular compounds characterisation based on a \\{Embed-Encode-Attend-Predict\\} neural architecture with a data-specific Bayesian hyper-parameters optimisation. The only input to the architecture -- the SMILES strings -- are de-canonicalised in order to efficiently augment the data. One of the key features of the architecture is the attention mechanism, which enables the interpretation of output predictions without extra computational cost. The SMILES-X shows new state-of-the-art results in the inference of aqueous solubility ($\\overline{RMSE}_{test} \\simeq 0.57 \\pm 0.07$ mols/L), hydration free energy ($\\overline{RMSE}_{test} \\simeq 0.81 \\pm 0.22$ kcal/mol, which is $\\sim 24.5\\%$ better than molecular dynamics simulations), and octanol/water distribution coefficient ($\\overline{RMSE}_{test} \\simeq 0.59 \\pm 0.02$ for LogD at pH 7.4) of molecular compounds. The SMILES-X is intended to become an important asset in the toolkit of materials scientists and chemists. The source code for the SMILES-X is available at \\href{https://github.com/GLambard/SMILES-X}{github.com/GLambard/SMILES-X}."
                ],
                "domain": [
                    "Neutrino Physics",
                    "Machine Learning",
                    "Materials Science",
                    "Computational Chemistry"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f18424da-eb80-4ff1-b136-47761fcd3964": {
                "pk": "f18424da-eb80-4ff1-b136-47761fcd3964",
                "project_name": null,
                "name": "Toshiyuki Amagasa",
                "bio": "I am a researcher dedicated to tackling complex challenges in graph analysis, software engineering, and data integration. My recent work has focused on developing innovative solutions that enhance the efficiency and accuracy of various computational tasks. For instance, I introduced gScarf, a modularity clustering algorithm that effectively identifies fine-grained clusters in massive graphs while significantly reducing computational time. This work addresses the limitations of existing methods, particularly the resolution limit problem and high computational costs.\n\nIn the realm of software development, I have explored the Extract Method refactoring technique, proposing a novel approach that emphasizes semantic coherence in code fragments. By leveraging state-of-the-art method name prediction techniques, my method improves the accuracy of recommendations, making refactoring more intuitive and effective for developers.\n\nAdditionally, I have delved into the challenges of Entity Matching (EM), particularly in low-resource scenarios. My research on parameter-efficient fine-tuning of pre-trained language models using adapters has shown promising results, achieving competitive performance while minimizing computational overhead. This work not only advances the field of EM but also contributes to the broader understanding of transfer learning in heterogeneous data environments.\n\nThrough these diverse projects, I aim to bridge the gap between theoretical advancements and practical applications, ultimately enhancing the tools and techniques available for researchers and practitioners alike.",
                "collaborators": [
                    "Hiroaki Shiokawa",
                    "Hiroyuki Kitagawa",
                    "Jinto Yamanaka",
                    "Yasuhiro Hayase",
                    "John Bosco Mugeni",
                    "Steven Lynden",
                    "Akiyoshi Matono"
                ],
                "pub_titles": [
                    "Scaling Fine-grained Modularity Clustering for Massive Graphs",
                    "Recommending Extract Method Refactoring Based on Confidence of Predicted Method Name",
                    "AdapterEM: Pre-trained Language Model Adaptation for Generalized Entity Matching using Adapter-tuning"
                ],
                "pub_abstracts": [
                    "Modularity clustering is an essential tool to understand complicated graphs. However, existing methods are not applicable to massive graphs due to two serious weaknesses. (1) It is difficult to fully reproduce ground-truth clusters due to the resolution limit problem. (2) They are computationally expensive because all nodes and edges must be computed iteratively. This paper proposes gScarf, which outputs fine-grained clusters within a short running time. To overcome the aforementioned weaknesses, gScarf dynamically prunes unnecessary nodes and edges, ensuring that it captures fine-grained clusters. Experiments show that gScarf outperforms existing methods in terms of running time while finding clusters with high accuracy.",
                    "Refactoring is an important activity that is frequently performed in software development, and among them, Extract Method is known to be one of the most frequently performed refactorings. The existing techniques for recommending Extract Method refactoring calculate metrics from the source method and the code fragments to be extracted to order the recommendation candidates. This paper proposes a new technique for accurately recommending Extract Method refactoring by considering whether code fragments are semantically coherent chunks that can be given clear method names, in addition to the metrics used in previous studies. As a criterion for the semantic coherency, the proposed technique employs the probability (i.e. confidence) of the predicted method names for the code fragments output by code2seq, which is a state-of-the-art method name prediction technique. The evaluation experiment confirmed that the proposed technique has higher correctness of recommendation than the existing techniques.",
                    "Entity Matching (EM) involves identifying different data representations referring to the same entity from multiple data sources and is typically formulated as a binary classification problem. It is a challenging problem in data integration due to the heterogeneity of data representations. State-of-the-art solutions have adopted NLP techniques based on pre-trained language models (PrLMs) via the fine-tuning paradigm, however, sequential fine-tuning of overparameterized PrLMs can lead to catastrophic forgetting, especially in low-resource scenarios. In this study, we propose a parameter-efficient paradigm for fine-tuning PrLMs based on adapters, small neural networks encapsulated between layers of a PrLM, by optimizing only the adapter and classifier weights while the PrLMs parameters are frozen. Adapter-based methods have been successfully applied to multilingual speech problems achieving promising results, however, the effectiveness of these methods when applied to EM is not yet well understood, particularly for generalized EM with heterogeneous data. Furthermore, we explore using (i) pre-trained adapters and (ii) invertible adapters to capture token-level language representations and demonstrate their benefits for transfer learning on the generalized EM benchmark. Our results show that our solution achieves comparable or superior performance to full-scale PrLM fine-tuning and prompt-tuning baselines while utilizing a significantly smaller computational footprint $\\approx 13\\%$ of the PrLM parameters."
                ],
                "domain": [
                    "Graph Clustering",
                    "Software Refactoring",
                    "Entity Matching",
                    "Natural Language Processing"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "0c259275-8f5b-43a0-8a99-37fb391667d4": {
                "pk": "0c259275-8f5b-43a0-8a99-37fb391667d4",
                "project_name": null,
                "name": "Masashi Ishii",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures and exploring innovative solutions.\n\nOne of my notable contributions is the development of Position-aware GNNs (P-GNNs), which effectively capture the positional context of nodes within a graph, significantly improving performance in tasks like link prediction and community detection. I also introduced Identity-aware GNNs (ID-GNNs), which enhance the expressive power of message-passing frameworks by incorporating node identities, leading to substantial accuracy improvements across various prediction tasks.\n\nRecognizing the challenges posed by dynamic graphs, I proposed the ROLAND framework, which allows for the seamless adaptation of static GNNs to dynamic environments, ensuring scalability and efficiency. My research also delves into the architectural design space of GNNs, where I systematically explored over 315,000 designs to provide guidelines for optimizing performance across different tasks.\n\nIn addition to my work on GNNs, I have ventured into automated machine learning (AutoML) with methods like FALCON and AutoTransfer, which aim to streamline the search for optimal model designs by leveraging prior knowledge and enhancing efficiency.\n\nOverall, my research is driven by a passion for pushing the boundaries of what GNNs can achieve, fostering a deeper understanding of their structures, and developing tools that empower researchers and practitioners alike.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively mine and extract meaningful data from the vast and diverse materials science literature to accelerate materials discovery using machine learning techniques?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for the research community as it can significantly enhance the efficiency of materials discovery processes, leading to the development of new materials with tailored properties. By automating data extraction and mining from literature, researchers can leverage existing knowledge more effectively, fostering innovation and collaboration across sub-fields. This advancement could lead to practical applications in various industries, such as energy, electronics, and manufacturing, ultimately contributing to technological progress and sustainability.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in solving this problem stem from the complexity and diversity of materials science data, which includes unstructured textual content, structured tables, and graphs. Naive approaches may fail due to the need for domain-specific knowledge to accurately interpret and curate the data, as well as the intricacies involved in classifying materials based on various phenomena. Additionally, the presence of cross-domain polysemy can lead to misunderstandings and misinterpretations of terms, complicating the extraction process further. Overcoming these technical and theoretical obstacles requires sophisticated algorithms capable of handling diverse data formats and contexts.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has been limited by the lack of sufficiently large and diverse datasets suitable for machine learning applications in materials science. Existing solutions often rely on manual data extraction, which is labor-intensive and costly. Additionally, the complexity of materials classification and the need for sub-domain expertise have hindered progress. My approach differs by integrating advanced machine learning techniques with automated data mining processes, aiming to create a more efficient and scalable solution that addresses the limitations of prior work.\n\n**[Question 5] - What are the key components of my approach and results?**  \nMy proposed methodology involves developing a machine learning framework that utilizes natural language processing (NLP) techniques to extract and classify data from materials science literature. I will use a diverse dataset sourced from published papers and existing databases, focusing on specific materials such as superconductors. The evaluation metric will include precision, recall, and F1-score to assess the accuracy of data extraction and classification. The expected outcomes include a comprehensive, structured dataset that enhances the accessibility of materials science knowledge and facilitates further research and discovery in the field."
    },
    "2311.12410": {
        "paper_data": {
            "title": "nach0: Multimodal Natural and Chemical Languages Foundation Model",
            "url": "http://arxiv.org/abs/2311.12410v3",
            "arxiv_id": "2311.12410",
            "authors": [
                "Micha Livne",
                "Zulfat Miftahutdinov",
                "Elena Tutubalina",
                "Maksim Kuznetsov",
                "Daniil Polykovskiy",
                "Annika Brundyn",
                "Aastha Jhunjhunwala",
                "Anthony Costa",
                "Alex Aliper",
                "Alán Aspuru-Guzik",
                "Alex Zhavoronkov"
            ],
            "abstract": "Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.",
            "introduction": "   1 Introduction   Large-scale pre-training of language models (LMs), such as BERT 1, T5 2, BART 3 and GPT 4, on vast amounts of text data has yielded impressive results on a variety of natural language processing (NLP) tasks. These models’ success can be attributed to their ability to learn deeply contextualized representations of input tokens through self-supervision at scale 1. Recently, foundation models have built upon the concept of self-supervised learning by pre-training a single model over unlabeled data that can be easily adapted to any task 5.   The application of neural network architectures and LMs has significantly advanced the field of chemistry, particularly in domain-specific information retrieval, drug development, and clinical trial design 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. These developments include neural molecular fingerprinting, generative approaches to small molecule design 11, 12, 13, prediction of pharmacological properties, and drug repurposing 13, 14. The clinical development of a drug is a time and money consuming process that typically requires several years and a billion-dollar budget to progress from phase 1 clinical trials to the patients 16. The use of state-of-the-art neural network approaches and language models has the potential to facilitate the drug development process considerably.   A number of LMs have been proposed for the biomedical domain, utilizing a variety of model families: for instance, researchers have developed BioBERT 17, based on BERT with 110 million parameters, and SciFive, based on T5-base and T5-large with 220 and 770 million parameters respectively, using biomedical literature from PubMed. NVIDIA has also developed BioMegatron models in the biomedical domain using a more extensive set of PubMed-derived free text, ranging from 345 million to 1.2 billion parameters. However, the datasets used in these models cover mainly biomedical natural language texts and contain biomedical named entities like drugs, genes, and cell lines names but omit important chemical structure descriptions in SMILES format. Enriching biomedical datasets with chemical structures is an important and challenging task. Recently, LMs such as Galactica 18, based on Transformer architecture in a decoder-only setup 19 with 120 billion parameters in its largest setup, and MolT5 20, based on T5-base and T5-large, were proposed to address this limitation. Both modes were pre-trained with natural language and chemical data, creating a shared representation space, yet were not fine-tuned on a diverse set of chemical tasks with instruction tuning in a multi-task fashion. The Venn diagram in Fig. 1 provides a summary of the existing LMs. Furthermore, simple language models trained with molecular structures can reproduce complex molecular distributions 21, and even their 3D structure of molecules, materials and proteins using a GPT framework 22.   In this paper, we propose a unified encoder-decoder transformer named nach0 for natural language, chemical generalization and cross-domain tasks. We pre-train on both natural language and chemical data using Self Supervised Learning and employ nach0 as the foundation model for a wide range of downstream tasks (Fig. 2). The tasks include well-known NLP problems such as information extraction, question answering, textual entailment, molecular structures and description generation, chemical property prediction, and reaction predictions. Inspired by Raffel et al. 2, Chung et al. 23, we",
            "references": []
        },
        "author_data": {
            "5db61aea-2449-4df1-a9c3-e94caa6da1b4": {
                "pk": "5db61aea-2449-4df1-a9c3-e94caa6da1b4",
                "project_name": null,
                "name": "Micha Livne",
                "bio": "I am a researcher specializing in generative models and representation learning, with a focus on developing innovative frameworks that leverage probabilistic approaches to enhance data generation and understanding. My recent work includes the formulation of conditional generative models based on probability flows, which allows for efficient inference and sampling without requiring prior knowledge of class structures. This has enabled me to train models on heterogeneous datasets while maintaining strong performance on specific subsets.\n\nOne of my notable contributions is the TzK model, which utilizes attributes to create tight conditional priors around data manifolds, demonstrating impressive results on benchmark datasets like MNIST and Omniglot. Additionally, I introduced the Mutual Information Machine (MIM), a probabilistic auto-encoder that emphasizes low divergence and high mutual information, leading to effective latent clustering and robust representations. My work extends to language data with SentenceMIM, which addresses challenges in variational autoencoders by providing informative representations while avoiding posterior collapse.\n\nI have also explored applications in motion capture and drug discovery, developing methods that incorporate physics for motion tracking and a probabilistic auto-encoder for small molecule generation, respectively. My research aims to bridge the gap between complex data structures and practical applications, ultimately contributing to advancements in machine learning and generative modeling.",
                "collaborators": [
                    "David J. Fleet",
                    "Kevin Swersky",
                    "David Fleet",
                    "Leonid Sigal",
                    "Marcus A. Brubaker",
                    "Danny Reidenbach",
                    "Rajesh K. Ilango",
                    "Michelle Gill",
                    "Johnny Israeli"
                ],
                "pub_titles": [
                    "TzK: Flow-Based Conditional Generative Model",
                    "TzK Flow - Conditional Generative Model",
                    "MIM: Mutual Information Machine",
                    "High Mutual Information in Representation Learning with Symmetric Variational Inference",
                    "SentenceMIM: A Latent Variable Language Model",
                    "Walking on Thin Air: Environment-Free Physics-based Markerless Motion Capture",
                    "Improving Small Molecule Generation using Mutual Information Machine"
                ],
                "pub_abstracts": [
                    "We formulate a new class of conditional generative models based on probability flows. Trained with maximum likelihood, it provides efficient inference and sampling from class-conditionals or the joint distribution, and does not require a priori knowledge of the number of classes or the relationships between classes. This allows one to train generative models from multiple, heterogeneous datasets, while retaining strong prior models over subsets of the data (e.g., from a single dataset, class label, or attribute). In this paper, in addition to end-to-end learning, we show how one can learn a single model from multiple datasets with a relatively weak Glow architecture, and then extend it by conditioning on different knowledge types (e.g., a single dataset). This yields log likelihood comparable to state-of-the-art, compelling samples from conditional priors.",
                    "We introduce TzK (pronounced \"task\"), a conditional probability flow-based model that exploits attributes (e.g., style, class membership, or other side information) in order to learn tight conditional prior around manifolds of the target observations. The model is trained via approximated ML, and offers efficient approximation of arbitrary data sample distributions (similar to GAN and flow-based ML), and stable training (similar to VAE and ML), while avoiding variational approximations. TzK exploits meta-data to facilitate a bottleneck, similar to autoencoders, thereby producing a low-dimensional representation. Unlike autoencoders, the bottleneck does not limit model expressiveness, similar to flow-based ML. Supervised, unsupervised, and semi-supervised learning are supported by replacing missing observations with samples from learned priors. We demonstrate TzK by training jointly on MNIST and Omniglot datasets with minimal preprocessing, and weak supervision, with results comparable to state-of-the-art.",
                    "We introduce the Mutual Information Machine (MIM), a probabilistic auto-encoder for learning joint distributions over observations and latent variables. MIM reflects three design principles: 1) low divergence, to encourage the encoder and decoder to learn consistent factorizations of the same underlying distribution; 2) high mutual information, to encourage an informative relation between data and latent variables; and 3) low marginal entropy, or compression, which tends to encourage clustered latent representations. We show that a combination of the Jensen-Shannon divergence and the joint entropy of the encoding and decoding distributions satisfies these criteria, and admits a tractable cross-entropy bound that can be optimized directly with Monte Carlo and stochastic gradient descent. We contrast MIM learning with maximum likelihood and VAEs. Experiments show that MIM learns representations with high mutual information, consistent encoding and decoding distributions, effective latent clustering, and data log likelihood comparable to VAE, while avoiding posterior collapse.",
                    "We introduce the Mutual Information Machine (MIM), a novel formulation of representation learning, using a joint distribution over the observations and latent state in an encoder/decoder framework. Our key principles are symmetry and mutual information, where symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, and mutual information, to encourage the learning of useful representations for downstream tasks. Our starting point is the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, plus a mutual information encouraging regularizer. We show that this can be bounded by a tractable cross entropy loss function between the true model and a parameterized approximation, and relate this to the maximum likelihood framework. We also relate MIM to variational autoencoders (VAEs) and demonstrate that MIM is capable of learning symmetric factorizations, with high mutual information that avoids posterior collapse.",
                    "SentenceMIM is a probabilistic auto-encoder for language data, trained with Mutual Information Machine (MIM) learning to provide a fixed length representation of variable length language observations (i.e., similar to VAE). Previous attempts to learn VAEs for language data faced challenges due to posterior collapse. MIM learning encourages high mutual information between observations and latent variables, and is robust against posterior collapse. As such, it learns informative representations whose dimension can be an order of magnitude higher than existing language VAEs. Importantly, the SentenceMIM loss has no hyper-parameters, simplifying optimization. We compare sentenceMIM with VAE, and AE on multiple datasets. SentenceMIM yields excellent reconstruction, comparable to AEs, with a rich structured latent space, comparable to VAEs. The structured latent representation is demonstrated with interpolation between sentences of different lengths. We demonstrate the versatility of sentenceMIM by utilizing a trained model for question-answering and transfer learning, without fine-tuning, outperforming VAE and AE with similar architectures.",
                    "We propose a generative approach to physics-based motion capture. Unlike prior attempts to incorporate physics into tracking that assume the subject and scene geometry are calibrated and known a priori, our approach is automatic and online. This distinction is important since calibration of the environment is often difficult, especially for motions with props, uneven surfaces, or outdoor scenes. The use of physics in this context provides a natural framework to reason about contact and the plausibility of recovered motions. We propose a fast data-driven parametric body model, based on linear-blend skinning, which decouples deformations due to pose, anthropometrics and body shape. Pose (and shape) parameters are estimated using robust ICP optimization with physics-based dynamic priors that incorporate contact. Contact is estimated from torque trajectories and predictions of which contact points were active. To our knowledge, this is the first approach to take physics into account without explicit {\\em a priori} knowledge of the environment or body dimensions. We demonstrate effective tracking from a noisy single depth camera, improving on state-of-the-art results quantitatively and producing better qualitative results, reducing visual artifacts like foot-skate and jitter.",
                    "We address the task of controlled generation of small molecules, which entails finding novel molecules with desired properties under certain constraints (e.g., similarity to a reference molecule). Here we introduce MolMIM, a probabilistic auto-encoder for small molecule drug discovery that learns an informative and clustered latent space. MolMIM is trained with Mutual Information Machine (MIM) learning, and provides a fixed length representation of variable length SMILES strings. Since encoder-decoder models can learn representations with ``holes'' of invalid samples, here we propose a novel extension to the training procedure which promotes a dense latent space, and allows the model to sample valid molecules from random perturbations of latent codes. We provide a thorough comparison of MolMIM to several variable-size and fixed-size encoder-decoder models, demonstrating MolMIM's superior generation as measured in terms of validity, uniqueness, and novelty. We then utilize CMA-ES, a naive black-box and gradient free search algorithm, over MolMIM's latent space for the task of property guided molecule optimization. We achieve state-of-the-art results in several constrained single property optimization tasks as well as in the challenging task of multi-objective optimization, improving over previous success rate SOTA by more than 5\\% . We attribute the strong results to MolMIM's latent representation which clusters similar molecules in the latent space, whereas CMA-ES is often used as a baseline optimization method. We also demonstrate MolMIM to be favourable in a compute limited regime, making it an attractive model for such cases."
                ],
                "domain": [
                    "Generative Models",
                    "Probabilistic Modeling",
                    "Autoencoders",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "9e8d6a2c-f4b3-4980-b8ec-54aeaca2b611": {
                "pk": "9e8d6a2c-f4b3-4980-b8ec-54aeaca2b611",
                "project_name": null,
                "name": "Zulfat Miftahutdinov",
                "bio": "I am a researcher dedicated to advancing the field of medical text mining and natural language processing, particularly in the context of health-related data. My work primarily focuses on the challenges of medical concept normalization, where I leverage powerful neural architectures, including recurrent neural networks and BERT, to map health-related entities from free-form text to standardized vocabularies like the Unified Medical Language System (UMLS). \n\nI have developed innovative end-to-end models that significantly outperform existing state-of-the-art approaches, achieving impressive F-measure scores in tasks such as ICD-10 coding and entity recognition. My research also extends to creating the Russian Drug Reaction Corpus (RuDReC), which facilitates the detection of health-related entities in consumer reviews, and I have established baseline models that demonstrate substantial improvements over previous benchmarks.\n\nRecently, I have explored the integration of language models into drug discovery pipelines, addressing the limitations of traditional chemical representations by introducing a novel model, nach0-pc. This model effectively combines spatial features with textual representations, showcasing my commitment to bridging the gap between computational methods and real-world applications in healthcare.\n\nThrough my work, I aim to enhance the understanding and utilization of medical data, ultimately contributing to improved healthcare outcomes and drug discovery processes. I am passionate about making my research accessible, as evidenced by my efforts to share datasets and pretrained models with the broader community.",
                "collaborators": [
                    "Elena Tutubalina",
                    "Sergey Nikolenko",
                    "Valentin Malykh",
                    "Eugene Beloded",
                    "Artur Kadurin",
                    "Roman Kudrin",
                    "Ilseyar Alimova",
                    "Andrey Sakhovskiy",
                    "Maksim Kuznetsov",
                    "Airat Valiev"
                ],
                "pub_titles": [
                    "Deep Neural Models for Medical Concept Normalization in User-Generated Texts",
                    "An Encoder-Decoder Model for ICD-10 Coding of Death Certificates",
                    "Sequence Learning with RNNs for Medical Concept Normalization in User-Generated Texts",
                    "CommentsRadar: Dive into Unique Data on All Comments on the Web",
                    "Drug and Disease Interpretation Learning with Biomedical Entity Representation Transformer",
                    "The Russian Drug Reaction Corpus and Neural Models for Drug Reactions and Effectiveness Detection in User Reviews",
                    "nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder"
                ],
                "pub_abstracts": [
                    "In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This is a challenging task since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful neural networks such as recurrent neural networks and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform an existing state of the art models.",
                    "Information extraction from textual documents such as hospital records and healthrelated user discussions has become a topic of intense interest. The task of medical concept coding is to map a variable length text to medical concepts and corresponding classification codes in some external system or ontology. In this work, we utilize recurrent neural networks to automatically assign ICD-10 codes to fragments of death certificates written in English. We develop end-to-end neural architectures directly tailored to the task, including basic encoder-decoder architecture for statistical translation. In order to incorporate prior knowledge, we concatenate cosine similarities vector among the text and dictionary entry to the encoded state. Being applied to a standard benchmark from CLEF eHealth 2017 challenge, our model achieved F-measure of 85.01% on a full test set with significant improvement as compared to the average score of 62.2% for all official participants approaches.",
                    "In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a disease mention in free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This task is challenging since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem, with recurrent neural networks trained to obtain semantic representations of one- and multi-word expressions. We develop end-to-end neural architectures tailored specifically to medical concept normalization, including bidirectional LSTM and GRU with an attention mechanism and additional semantic similarity features based on UMLS. Our evaluation over a standard benchmark shows that our model improves over a state of the art baseline for classification based on CNNs.",
                    "We introduce an entity-centric search engineCommentsRadarthatpairs entity queries with articles and user opinions covering a widerange of topics from top commented sites. The engine aggregatesarticles and comments for these articles, extracts named entities,links them together and with knowledge base entries, performssentiment analysis, and aggregates the results, aiming to mine fortemporal trends and other insights. In this work, we present thegeneral engine, discuss the models used for all steps of this pipeline,and introduce several case studies that discover important insightsfrom online commenting data.",
                    "Concept normalization in free-form texts is a crucial step in every text-mining pipeline. Neural architectures based on Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art results in the biomedical domain. In the context of drug discovery and development, clinical trials are necessary to establish the efficacy and safety of drugs. We investigate the effectiveness of transferring concept normalization from the general biomedical domain to the clinical trials domain in a zero-shot setting with an absence of labeled data. We propose a simple and effective two-stage neural approach based on fine-tuned BERT architectures. In the first stage, we train a metric learning model that optimizes relative similarity of mentions and concepts via triplet loss. The model is trained on available labeled corpora of scientific abstracts to obtain vector embeddings of concept names and entity mentions from texts. In the second stage, we find the closest concept name representation in an embedding space to a given clinical mention. We evaluated several models, including state-of-the-art architectures, on a dataset of abstracts and a real-world dataset of trial records with interventions and conditions mapped to drug and disease terminologies. Extensive experiments validate the effectiveness of our approach in knowledge transfer from the scientific literature to clinical trials.",
                    "The Russian Drug Reaction Corpus (RuDReC) is a new partially annotated corpus of consumer reviews in Russian about pharmaceutical products for the detection of health-related named entities and the effectiveness of pharmaceutical products. The corpus itself consists of two parts, the raw one and the labelled one. The raw part includes 1.4 million health-related user-generated texts collected from various Internet sources, including social media. The labelled part contains 500 consumer reviews about drug therapy with drug- and disease-related information. Labels for sentences include health-related issues or their absence. The sentences with one are additionally labelled at the expression level for identification of fine-grained subtypes such as drug classes and drug forms, drug indications, and drug reactions. Further, we present a baseline model for named entity recognition (NER) and multi-label sentence classification tasks on this corpus. The macro F1 score of 74.85% in the NER task was achieved by our RuDR-BERT model. For the sentence classification task, our model achieves the macro F1 score of 68.82% gaining 7.47% over the score of BERT model trained on Russian data. We make the RuDReC corpus and pretrained weights of domain-specific BERT models freely available at https://github.com/cimm-kzn/RuDReC",
                    "Recent advancements have integrated Language Models (LMs) into a drug discovery pipeline. However, existing models mostly work with SMILES and SELFIES chemical string representations, which lack spatial features vital for drug discovery. Additionally, attempts to translate chemical 3D structures into text format encounter issues such as excessive length and insufficient atom connectivity information. To address these issues, we introduce nach0-pc, a model combining domain-specific encoder and textual representation to handle spatial arrangement of atoms effectively. Our approach utilizes a molecular point cloud encoder for concise and order-invariant structure representation. We introduce a novel pre-training scheme for molecular point clouds to distillate the knowledge from spatial molecular structures datasets. After fine-tuning within both single-task and multi-task frameworks, nach0-pc demonstrates performance comparable with other diffusion models in terms of generated samples quality across several established spatial molecular generation tasks. Notably, our model is a multi-task approach, in contrast to diffusion models being limited to single tasks. Additionally, it is capable of processing point cloud-related data, which language models are not capable of handling due to memory limitations. These lead to our model having reduced training and inference time while maintaining on par performance."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Medical Informatics",
                    "Machine Learning",
                    "Named Entity Recognition"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "aa9bf396-d2f4-4141-aeab-f28dbcaf3b3e": {
                "pk": "aa9bf396-d2f4-4141-aeab-f28dbcaf3b3e",
                "project_name": null,
                "name": "Elena Tutubalina",
                "bio": "I am a researcher dedicated to advancing the field of biomedical text mining and natural language processing, particularly in the context of drug safety and health-related information extraction. My recent work has focused on leveraging user-generated data from social media to automatically monitor adverse drug events (ADEs) and drug reactions (ADRs). I have developed multimodal models that integrate language understanding with molecular property predictions, achieving state-of-the-art results in various multilingual benchmarks.\n\nI am particularly interested in the challenges of medical concept normalization, where I have employed advanced neural architectures to map health-related mentions in free-form texts to standardized vocabularies. My research has led to the creation of valuable resources, such as the Russian Drug Reaction Corpus (RuDReC) and the NEREL-BIO corpus, which facilitate further exploration in named entity recognition and classification tasks.\n\nAdditionally, I have explored innovative approaches to zero-shot learning and aspect-based rating prediction, aiming to improve the efficiency of classifiers while minimizing annotation efforts. My work also extends to the COVID-19 pandemic, where I contributed to the development of a large-scale dataset of tweets to support research on public sentiment and misinformation.\n\nThrough my research, I strive to bridge the gap between computational methods and real-world health applications, providing tools and insights that can enhance our understanding of drug safety and public health dynamics.",
                "collaborators": [
                    "Zulfat Miftahutdinov",
                    "Sergey Nikolenko",
                    "Valentin Malykh",
                    "Anton Alekseev",
                    "Vladimir Ivanov",
                    "Tatiana Batura",
                    "Ekaterina Artemova",
                    "Ilseyar Alimova",
                    "Andrey Sakhovskiy",
                    "Natalia Loukachevitch"
                ],
                "pub_titles": [
                    "Selection of pseudo-annotated data for adverse drug reaction classification across drug groups",
                    "Multimodal Model with Text and Drug Embeddings for Adverse Drug Reaction Classification",
                    "Near-Zero-Shot Suggestion Mining with a Little Help from WordNet",
                    "An Encoder-Decoder Model for ICD-10 Coding of Death Certificates",
                    "Deep Neural Models for Medical Concept Normalization in User-Generated Texts",
                    "CommentsRadar: Dive into Unique Data on All Comments on the Web",
                    "Drug and Disease Interpretation Learning with Biomedical Entity Representation Transformer",
                    "Sequence Learning with RNNs for Medical Concept Normalization in User-Generated Texts",
                    "Improving unsupervised neural aspect extraction for online discussions using out-of-domain classification",
                    "Data and models for stance and premise detection in COVID-19 tweets: insights from the Social Media Mining for Health (SMM4H) 2022 shared task",
                    "The Russian Drug Reaction Corpus and Neural Models for Drug Reactions and Effectiveness Detection in User Reviews",
                    "NEREL-BIO: A Dataset of Biomedical Abstracts Annotated with Nested Named Entities",
                    "AspeRa: Aspect-based Rating Prediction Model",
                    "RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback",
                    "RuNNE-2022 Shared Task: Recognizing Nested Named Entities",
                    "nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder",
                    "So What's the Plan? Mining Strategic Planning Documents",
                    "RuREBus: a Case Study of Joint Named Entity Recognition and Relation Extraction from e-Government Domain",
                    "A large-scale COVID-19 Twitter chatter dataset for open scientific research -- an international collaboration"
                ],
                "pub_abstracts": [
                    "Automatic monitoring of adverse drug events (ADEs) or reactions (ADRs) is currently receiving significant attention from the biomedical community. In recent years, user-generated data on social media has become a valuable resource for this task. Neural models have achieved impressive performance on automatic text classification for ADR detection. Yet, training and evaluation of these methods are carried out on user-generated texts about a targeted drug. In this paper, we assess the robustness of state-of-the-art neural architectures across different drug groups. We investigate several strategies to use pseudo-labeled data in addition to a manually annotated train set. Out-of-dataset experiments diagnose the bottleneck of supervised models in terms of breakdown performance, while additional pseudo-labeled data improves overall results regardless of the text selection strategy.",
                    "In this paper, we focus on the classification of tweets as sources of potential signals for adverse drug effects (ADEs) or drug reactions (ADRs). Following the intuition that text and drug structure representations are complementary, we introduce a multimodal model with two components. These components are state-of-the-art BERT-based models for language understanding and molecular property prediction. Experiments were carried out on multilingual benchmarks of the Social Media Mining for Health Research and Applications (#SMM4H) initiative. Our models obtained state-of-the-art results of 0.61 F1 and 0.57 F1 on #SMM4H 2021 Shared Tasks 1a and 2 in English and Russian, respectively. On the classification of French tweets from SMM4H 2020 Task 1, our approach pushes the state of the art by an absolute gain of 8% F1. Our experiments show that the molecular information obtained from neural networks is more beneficial for ADE classification than traditional molecular descriptors. The source code for our models is freely available at https://github.com/Andoree/smm4h_2021_classification.",
                    "In this work, we explore the constructive side of online reviews: advice, tips, requests, and suggestions that users provide about goods, venues, services, and other items of interest. To reduce training costs and annotation efforts needed to build a classifier for a specific label set, we present and evaluate several entailment-based zero-shot approaches to suggestion classification in a label-fully-unseen fashion. In particular, we introduce the strategy of assigning target class labels to sentences in English language with user intentions, which significantly improves prediction quality. The proposed strategies are evaluated with a comprehensive experimental study that validated our results both quantitatively and qualitatively.",
                    "Information extraction from textual documents such as hospital records and healthrelated user discussions has become a topic of intense interest. The task of medical concept coding is to map a variable length text to medical concepts and corresponding classification codes in some external system or ontology. In this work, we utilize recurrent neural networks to automatically assign ICD-10 codes to fragments of death certificates written in English. We develop end-to-end neural architectures directly tailored to the task, including basic encoder-decoder architecture for statistical translation. In order to incorporate prior knowledge, we concatenate cosine similarities vector among the text and dictionary entry to the encoded state. Being applied to a standard benchmark from CLEF eHealth 2017 challenge, our model achieved F-measure of 85.01% on a full test set with significant improvement as compared to the average score of 62.2% for all official participants approaches.",
                    "In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This is a challenging task since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful neural networks such as recurrent neural networks and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform an existing state of the art models.",
                    "We introduce an entity-centric search engineCommentsRadarthatpairs entity queries with articles and user opinions covering a widerange of topics from top commented sites. The engine aggregatesarticles and comments for these articles, extracts named entities,links them together and with knowledge base entries, performssentiment analysis, and aggregates the results, aiming to mine fortemporal trends and other insights. In this work, we present thegeneral engine, discuss the models used for all steps of this pipeline,and introduce several case studies that discover important insightsfrom online commenting data.",
                    "Concept normalization in free-form texts is a crucial step in every text-mining pipeline. Neural architectures based on Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art results in the biomedical domain. In the context of drug discovery and development, clinical trials are necessary to establish the efficacy and safety of drugs. We investigate the effectiveness of transferring concept normalization from the general biomedical domain to the clinical trials domain in a zero-shot setting with an absence of labeled data. We propose a simple and effective two-stage neural approach based on fine-tuned BERT architectures. In the first stage, we train a metric learning model that optimizes relative similarity of mentions and concepts via triplet loss. The model is trained on available labeled corpora of scientific abstracts to obtain vector embeddings of concept names and entity mentions from texts. In the second stage, we find the closest concept name representation in an embedding space to a given clinical mention. We evaluated several models, including state-of-the-art architectures, on a dataset of abstracts and a real-world dataset of trial records with interventions and conditions mapped to drug and disease terminologies. Extensive experiments validate the effectiveness of our approach in knowledge transfer from the scientific literature to clinical trials.",
                    "In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a disease mention in free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This task is challenging since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem, with recurrent neural networks trained to obtain semantic representations of one- and multi-word expressions. We develop end-to-end neural architectures tailored specifically to medical concept normalization, including bidirectional LSTM and GRU with an attention mechanism and additional semantic similarity features based on UMLS. Our evaluation over a standard benchmark shows that our model improves over a state of the art baseline for classification based on CNNs.",
                    "Deep learning architectures based on self-attention have recently achieved and surpassed state of the art results in the task of unsupervised aspect extraction and topic modeling. While models such as neural attention-based aspect extraction (ABAE) have been successfully applied to user-generated texts, they are less coherent when applied to traditional data sources such as news articles and newsgroup documents. In this work, we introduce a simple approach based on sentence filtering in order to improve topical aspects learned from newsgroups-based content without modifying the basic mechanism of ABAE. We train a probabilistic classifier to distinguish between out-of-domain texts (outer dataset) and in-domain texts (target dataset). Then, during data preparation we filter out sentences that have a low probability of being in-domain and train the neural model on the remaining sentences. The positive effect of sentence filtering on topic coherence is demonstrated in comparison to aspect extraction models trained on unfiltered texts.",
                    "The COVID-19 pandemic has sparked numerous discussions on social media platforms, with users sharing their views on topics such as mask-wearing and vaccination. To facilitate the evaluation of neural models for stance detection and premise classification, we organized the Social Media Mining for Health (SMM4H) 2022 Shared Task 2. This competition utilized manually annotated posts on three COVID-19-related topics: school closures, stay-at-home orders, and wearing masks. In this paper, we extend the previous work and present newly collected data on vaccination from Twitter to assess the performance of models on a different topic. To enhance the accuracy and effectiveness of our evaluation, we employed various strategies to aggregate tweet texts with claims, including models with feature-level (early) fusion and dual-view architectures from SMM4H 2022 leaderboard. Our primary objective was to create a valuable dataset and perform an extensive experimental evaluation to support future research in argument mining in the health domain.",
                    "The Russian Drug Reaction Corpus (RuDReC) is a new partially annotated corpus of consumer reviews in Russian about pharmaceutical products for the detection of health-related named entities and the effectiveness of pharmaceutical products. The corpus itself consists of two parts, the raw one and the labelled one. The raw part includes 1.4 million health-related user-generated texts collected from various Internet sources, including social media. The labelled part contains 500 consumer reviews about drug therapy with drug- and disease-related information. Labels for sentences include health-related issues or their absence. The sentences with one are additionally labelled at the expression level for identification of fine-grained subtypes such as drug classes and drug forms, drug indications, and drug reactions. Further, we present a baseline model for named entity recognition (NER) and multi-label sentence classification tasks on this corpus. The macro F1 score of 74.85% in the NER task was achieved by our RuDR-BERT model. For the sentence classification task, our model achieves the macro F1 score of 68.82% gaining 7.47% over the score of BERT model trained on Russian data. We make the RuDReC corpus and pretrained weights of domain-specific BERT models freely available at https://github.com/cimm-kzn/RuDReC",
                    "This paper describes NEREL-BIO -- an annotation scheme and corpus of PubMed abstracts in Russian and smaller number of abstracts in English. NEREL-BIO extends the general domain dataset NEREL by introducing domain-specific entity types. NEREL-BIO annotation scheme covers both general and biomedical domains making it suitable for domain transfer experiments. NEREL-BIO provides annotation for nested named entities as an extension of the scheme employed for NEREL. Nested named entities may cross entity boundaries to connect to shorter entities nested within longer entities, making them harder to detect.   NEREL-BIO contains annotations for 700+ Russian and 100+ English abstracts. All English PubMed annotations have corresponding Russian counterparts. Thus, NEREL-BIO comprises the following specific features: annotation of nested named entities, it can be used as a benchmark for cross-domain (NEREL -> NEREL-BIO) and cross-language (English -> Russian) transfer. We experiment with both transformer-based sequence models and machine reading comprehension (MRC) models and report their results.   The dataset is freely available at https://github.com/nerel-ds/NEREL-BIO.",
                    "We propose a novel end-to-end Aspect-based Rating Prediction model (AspeRa) that estimates user rating based on review texts for the items and at the same time discovers coherent aspects of reviews that can be used to explain predictions or profile users. The AspeRa model uses max-margin losses for joint item and user embedding learning and a dual-headed architecture; it significantly outperforms recently proposed state-of-the-art models such as DeepCoNN, HFT, NARRE, and TransRev on two real world data sets of user reviews. With qualitative examination of the aspects and quantitative evaluation of rating prediction models based on these aspects, we show how aspect embeddings can be used in a recommender system.",
                    "Recent research has shown the advantages of using autoencoders based on deep neural networks for collaborative filtering. In particular, the recently proposed Mult-VAE model, which used the multinomial likelihood variational autoencoders, has shown excellent results for top-N recommendations. In this work, we propose the Recommender VAE (RecVAE) model that originates from our research on regularization techniques for variational autoencoders. RecVAE introduces several novel ideas to improve Mult-VAE, including a novel composite prior distribution for the latent codes, a new approach to setting the $\\beta$ hyperparameter for the $\\beta$-VAE framework, and a new approach to training based on alternating updates. In experimental evaluation, we show that RecVAE significantly outperforms previously proposed autoencoder-based models, including Mult-VAE and RaCT, across classical collaborative filtering datasets, and present a detailed ablation study to assess our new developments. Code and models are available at https://github.com/ilya-shenbin/RecVAE.",
                    "The RuNNE Shared Task approaches the problem of nested named entity recognition. The annotation schema is designed in such a way, that an entity may partially overlap or even be nested into another entity. This way, the named entity \"The Yermolova Theatre\" of type \"organization\" houses another entity \"Yermolova\" of type \"person\". We adopt the Russian NEREL dataset for the RuNNE Shared Task. NEREL comprises news texts written in the Russian language and collected from the Wikinews portal. The annotation schema includes 29 entity types. The nestedness of named entities in NEREL reaches up to six levels. The RuNNE Shared Task explores two setups. (i) In the general setup all entities occur more or less with the same frequency. (ii) In the few-shot setup the majority of entity types occur often in the training set. However, some of the entity types are have lower frequency, being thus challenging to recognize. In the test set the frequency of all entity types is even.   This paper reports on the results of the RuNNE Shared Task. Overall the shared task has received 156 submissions from nine teams. Half of the submissions outperform a straightforward BERT-based baseline in both setups. This paper overviews the shared task setup and discusses the submitted systems, discovering meaning insights for the problem of nested NER. The links to the evaluation platform and the data from the shared task are available in our github repository: https://github.com/dialogue-evaluation/RuNNE.",
                    "Recent advancements have integrated Language Models (LMs) into a drug discovery pipeline. However, existing models mostly work with SMILES and SELFIES chemical string representations, which lack spatial features vital for drug discovery. Additionally, attempts to translate chemical 3D structures into text format encounter issues such as excessive length and insufficient atom connectivity information. To address these issues, we introduce nach0-pc, a model combining domain-specific encoder and textual representation to handle spatial arrangement of atoms effectively. Our approach utilizes a molecular point cloud encoder for concise and order-invariant structure representation. We introduce a novel pre-training scheme for molecular point clouds to distillate the knowledge from spatial molecular structures datasets. After fine-tuning within both single-task and multi-task frameworks, nach0-pc demonstrates performance comparable with other diffusion models in terms of generated samples quality across several established spatial molecular generation tasks. Notably, our model is a multi-task approach, in contrast to diffusion models being limited to single tasks. Additionally, it is capable of processing point cloud-related data, which language models are not capable of handling due to memory limitations. These lead to our model having reduced training and inference time while maintaining on par performance.",
                    "In this paper we present a corpus of Russian strategic planning documents, RuREBus. This project is grounded both from language technology and e-government perspectives. Not only new language sources and tools are being developed, but also their applications to e-goverment research. We demonstrate the pipeline for creating a text corpus from scratch. First, the annotation schema is designed. Next texts are marked up using human-in-the-loop strategy, so that preliminary annotations are derived from a machine learning model and are manually corrected. The amount of annotated texts is large enough to showcase what insights can be gained from RuREBus.",
                    "We show-case an application of information extraction methods, such as named entity recognition (NER) and relation extraction (RE) to a novel corpus, consisting of documents, issued by a state agency. The main challenges of this corpus are: 1) the annotation scheme differs greatly from the one used for the general domain corpora, and 2) the documents are written in a language other than English. Unlike expectations, the state-of-the-art transformer-based models show modest performance for both tasks, either when approached sequentially, or in an end-to-end fashion. Our experiments have demonstrated that fine-tuning on a large unlabeled corpora does not automatically yield significant improvement and thus we may conclude that more sophisticated strategies of leveraging unlabelled texts are demanded. In this paper, we describe the whole developed pipeline, starting from text annotation, baseline development, and designing a shared task in hopes of improving the baseline. Eventually, we realize that the current NER and RE technologies are far from being mature and do not overcome so far challenges like ours.",
                    "As the COVID-19 pandemic continues its march around the world, an unprecedented amount of open data is being generated for genetics and epidemiological research. The unparalleled rate at which many research groups around the world are releasing data and publications on the ongoing pandemic is allowing other scientists to learn from local experiences and data generated in the front lines of the COVID-19 pandemic. However, there is a need to integrate additional data sources that map and measure the role of social dynamics of such a unique world-wide event into biomedical, biological, and epidemiological analyses. For this purpose, we present a large-scale curated dataset of over 152 million tweets, growing daily, related to COVID-19 chatter generated from January 1st to April 4th at the time of writing. This open dataset will allow researchers to conduct a number of research projects relating to the emotional and mental responses to social distancing measures, the identification of sources of misinformation, and the stratified measurement of sentiment towards the pandemic in near real time."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Biomedical Informatics",
                    "Adverse Drug Events",
                    "Information Extraction"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b7262af7-481d-4ef4-bdb9-6f75ead3d68c": {
                "pk": "b7262af7-481d-4ef4-bdb9-6f75ead3d68c",
                "project_name": null,
                "name": "Maksim Kuznetsov",
                "bio": "I am a researcher dedicated to the field of generative models, particularly in the context of molecular graph generation and advanced prior distributions for GANs and VAEs. My recent work introduces a hierarchical normalizing flow model that innovatively generates molecular structures by recursively splitting nodes in a graph. This model not only excels in distribution learning tasks but also allows for precise control over molecular properties through its hierarchical latent codes, enabling both global and marginal structural changes.\n\nIn addition to my work on molecular graphs, I have explored the limitations of traditional Gaussian priors in generative models. I proposed the Tensor Ring Induced Prior (TRIP), a novel family of priors that enhances the performance of GANs and VAEs by packing an exponential number of Gaussians into a high-dimensional lattice. This approach addresses issues like mode collapse in GANs and improves the evidence lower bound in VAEs, demonstrating significant advancements in generative modeling.\n\nMy research emphasizes the development of flexible, plug-and-play frameworks that can be integrated into existing architectures, paving the way for more robust and efficient generative models across various applications. I am passionate about pushing the boundaries of what generative models can achieve, particularly in the realm of molecular design and synthesis.",
                "collaborators": [
                    "Daniil Polykovskiy",
                    "Dmitry Vetrov",
                    "Alexander Zhebrak"
                ],
                "pub_titles": [
                    "MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation",
                    "A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models"
                ],
                "pub_abstracts": [
                    "We propose a hierarchical normalizing flow model for generating molecular graphs. The model produces new molecular structures from a single-node graph by recursively splitting every node into two. All operations are invertible and can be used as plug-and-play modules. The hierarchical nature of the latent codes allows for precise changes in the resulting graph: perturbations in the top layer cause global structural changes, while perturbations in the consequent layers change the resulting molecule marginally. The proposed model outperforms existing generative graph models on the distribution learning task. We also show successful experiments on global and constrained optimization of chemical properties using latent codes of the model.",
                    "Generative models produce realistic objects in many domains, including text, image, video, and audio synthesis. Most popular models---Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)---usually employ a standard Gaussian distribution as a prior. Previous works show that the richer family of prior distributions may help to avoid the mode collapse problem in GANs and to improve the evidence lower bound in VAEs. We propose a new family of prior distributions---Tensor Ring Induced Prior (TRIP)---that packs an exponential number of Gaussians into a high-dimensional lattice with a relatively small number of parameters. We show that these priors improve Fr\\'echet Inception Distance for GANs and Evidence Lower Bound for VAEs. We also study generative models with TRIP in the conditional generation setup with missing conditions. Altogether, we propose a novel plug-and-play framework for generative models that can be utilized in any GAN and VAE-like architectures."
                ],
                "domain": [
                    "Generative Models",
                    "Molecular Graphs",
                    "Normalizing Flows",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "2840566f-e0d0-4035-8abd-e8ed6d428735": {
                "pk": "2840566f-e0d0-4035-8abd-e8ed6d428735",
                "project_name": null,
                "name": "Daniil Polykovskiy",
                "bio": "I am a researcher dedicated to advancing the field of generative models, particularly in the context of molecular design and drug discovery. My work has focused on developing innovative architectures such as the Deterministic Decoder Variational Autoencoder (DD-VAE) and hierarchical normalizing flow models, which enhance the generation of molecular graphs and structures. I have explored the integration of dynamic selection mechanisms in neural networks, improving performance in image classification tasks.\n\nMy recent contributions include BindGPT, a generative model that creates 3D molecular structures directly within protein binding sites, and nach0-pc, which effectively combines spatial representations with language models for drug discovery. I am also the creator of Chemistry42, a software platform that merges AI with medicinal chemistry to design novel molecules with specific properties.\n\nRecognizing the need for standardized evaluation in generative chemistry, I developed the Molecular Sets (MOSES) benchmarking platform, which facilitates the comparison of generative models. My research aims to bridge the gap between computational methods and practical applications in drug discovery, ultimately contributing to more efficient and effective therapeutic development. Through my work, I strive to push the boundaries of what generative models can achieve in the realm of molecular science.",
                "collaborators": [
                    "Maksim Kuznetsov",
                    "Dmitry Vetrov",
                    "Alex Zhavoronkov",
                    "Alexander Zhebrak",
                    "Rim Shayakhmetov",
                    "Vladimir Aladinskiy",
                    "Iurii Kemaev",
                    "Artem Zholus",
                    "Roman Schutski",
                    "Sarath Chandar"
                ],
                "pub_titles": [
                    "Deterministic Decoding for Discrete Data in Variational Autoencoders",
                    "MolGrow: A Graph Normalizing Flow for Hierarchical Molecular Generation",
                    "ReSet: Learning Recurrent Dynamic Routing in ResNet-like Neural Networks",
                    "A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models",
                    "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
                    "nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder",
                    "Chemistry42: An AI-based platform for de novo molecular design",
                    "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models"
                ],
                "pub_abstracts": [
                    "Variational autoencoders are prominent generative models for modeling discrete data. However, with flexible decoders, they tend to ignore the latent codes. In this paper, we study a VAE model with a deterministic decoder (DD-VAE) for sequential data that selects the highest-scoring tokens instead of sampling. Deterministic decoding solely relies on latent codes as the only way to produce diverse objects, which improves the structure of the learned manifold. To implement DD-VAE, we propose a new class of bounded support proposal distributions and derive Kullback-Leibler divergence for Gaussian and uniform priors. We also study a continuous relaxation of deterministic decoding objective function and analyze the relation of reconstruction accuracy and relaxation parameters. We demonstrate the performance of DD-VAE on multiple datasets, including molecular generation and optimization problems.",
                    "We propose a hierarchical normalizing flow model for generating molecular graphs. The model produces new molecular structures from a single-node graph by recursively splitting every node into two. All operations are invertible and can be used as plug-and-play modules. The hierarchical nature of the latent codes allows for precise changes in the resulting graph: perturbations in the top layer cause global structural changes, while perturbations in the consequent layers change the resulting molecule marginally. The proposed model outperforms existing generative graph models on the distribution learning task. We also show successful experiments on global and constrained optimization of chemical properties using latent codes of the model.",
                    "Neural Network is a powerful Machine Learning tool that shows outstanding performance in Computer Vision, Natural Language Processing, and Artificial Intelligence. In particular, recently proposed ResNet architecture and its modifications produce state-of-the-art results in image classification problems. ResNet and most of the previously proposed architectures have a fixed structure and apply the same transformation to all input images. In this work, we develop a ResNet-based model that dynamically selects Computational Units (CU) for each input object from a learned set of transformations. Dynamic selection allows the network to learn a sequence of useful transformations and apply only required units to predict the image label. We compare our model to ResNet-38 architecture and achieve better results than the original ResNet on CIFAR-10.1 test set. While examining the produced paths, we discovered that the network learned different routes for images from different classes and similar routes for similar images.",
                    "Generative models produce realistic objects in many domains, including text, image, video, and audio synthesis. Most popular models---Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)---usually employ a standard Gaussian distribution as a prior. Previous works show that the richer family of prior distributions may help to avoid the mode collapse problem in GANs and to improve the evidence lower bound in VAEs. We propose a new family of prior distributions---Tensor Ring Induced Prior (TRIP)---that packs an exponential number of Gaussians into a high-dimensional lattice with a relatively small number of parameters. We show that these priors improve Fr\\'echet Inception Distance for GANs and Evidence Lower Bound for VAEs. We also study generative models with TRIP in the conditional generation setup with missing conditions. Altogether, we propose a novel plug-and-play framework for generative models that can be utilized in any GAN and VAE-like architectures.",
                    "Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.",
                    "Recent advancements have integrated Language Models (LMs) into a drug discovery pipeline. However, existing models mostly work with SMILES and SELFIES chemical string representations, which lack spatial features vital for drug discovery. Additionally, attempts to translate chemical 3D structures into text format encounter issues such as excessive length and insufficient atom connectivity information. To address these issues, we introduce nach0-pc, a model combining domain-specific encoder and textual representation to handle spatial arrangement of atoms effectively. Our approach utilizes a molecular point cloud encoder for concise and order-invariant structure representation. We introduce a novel pre-training scheme for molecular point clouds to distillate the knowledge from spatial molecular structures datasets. After fine-tuning within both single-task and multi-task frameworks, nach0-pc demonstrates performance comparable with other diffusion models in terms of generated samples quality across several established spatial molecular generation tasks. Notably, our model is a multi-task approach, in contrast to diffusion models being limited to single tasks. Additionally, it is capable of processing point cloud-related data, which language models are not capable of handling due to memory limitations. These lead to our model having reduced training and inference time while maintaining on par performance.",
                    "Chemistry42 is a software platform for de novo small molecule design that integrates Artificial Intelligence (AI) techniques with computational and medicinal chemistry methods. Chemistry42 is unique in its ability to generate novel molecular structures with predefined properties validated through in vitro and in vivo studies. Chemistry42 is a core component of Insilico Medicine Pharma.ai drug discovery suite that also includes target discovery and multi-omics data analysis (PandaOmics) and clinical trial outcomes predictions (InClinico).",
                    "Generative models are becoming a tool of choice for exploring the molecular space. These models learn on a large training dataset and produce novel molecular structures with similar properties. Generated structures can be utilized for virtual screening or training semi-supervised predictive models in the downstream tasks. While there are plenty of generative models, it is unclear how to compare and rank them. In this work, we introduce a benchmarking platform called Molecular Sets (MOSES) to standardize training and comparison of molecular generative models. MOSES provides a training and testing datasets, and a set of metrics to evaluate the quality and diversity of generated structures. We have implemented and compared several molecular generation models and suggest to use our results as reference points for further advancements in generative chemistry research. The platform and source code are available at https://github.com/molecularsets/moses."
                ],
                "domain": [
                    "Generative Models",
                    "Molecular Graphs",
                    "Machine Learning",
                    "Drug Discovery"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "4d4f1637-6eb5-4168-a049-854fbe22c308": {
                "pk": "4d4f1637-6eb5-4168-a049-854fbe22c308",
                "project_name": null,
                "name": "Annika Brundyn",
                "bio": "I am a researcher dedicated to advancing the intersection of deep learning and medical technology, particularly in the realm of minimally invasive surgery. My recent work focuses on stereo video reconstruction, where I have developed innovative U-Net-based solutions for converting 2D surgical videos into immersive 3D experiences. By experimenting with various input types, loss functions, and network architectures, I have demonstrated the critical role of motion information from multiple frames in enhancing depth perception. The positive feedback from expert surgeons in my reader studies validates the effectiveness of my approach and highlights the potential of deep learning in surgical applications.\n\nAdditionally, I have contributed to the development of Nemotron-4 15B, a large multilingual language model that excels across various tasks, including coding and multilingual capabilities. This model not only outperforms existing models of similar size but also competes with larger, specialized models, showcasing my commitment to pushing the boundaries of language processing technologies. My research aims to bridge the gap between advanced machine learning techniques and practical applications in healthcare, ultimately improving surgical outcomes and enhancing the capabilities of language models in diverse contexts.",
                "collaborators": [
                    "Jesse Swanson",
                    "Kyunghyun Cho",
                    "Doug Kondziolka",
                    "Eric Oermann",
                    "Jupinder Parmar",
                    "Shrimai Prabhumoye",
                    "Joseph Jennings",
                    "Mostofa Patwary",
                    "Sandeep Subramanian",
                    "Dan Su"
                ],
                "pub_titles": [
                    "Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery",
                    "Nemotron-4 15B Technical Report"
                ],
                "pub_abstracts": [
                    "We introduce the task of stereo video reconstruction or, equivalently, 2D-to-3D video conversion for minimally invasive surgical video. We design and implement a series of end-to-end U-Net-based solutions for this task by varying the input (single frame vs. multiple consecutive frames), loss function (MSE, MAE, or perceptual losses), and network architecture. We evaluate these solutions by surveying ten experts - surgeons who routinely perform endoscopic surgery. We run two separate reader studies: one evaluating individual frames and the other evaluating fully reconstructed 3D video played on a VR headset. In the first reader study, a variant of the U-Net that takes as input multiple consecutive video frames and outputs the missing view performs best. We draw two conclusions from this outcome. First, motion information coming from multiple past frames is crucial in recreating stereo vision. Second, the proposed U-Net variant can indeed exploit such motion information for solving this task. The result from the second study further confirms the effectiveness of the proposed U-Net variant. The surgeons reported that they could successfully perceive depth from the reconstructed 3D video clips. They also expressed a clear preference for the reconstructed 3D video over the original 2D video. These two reader studies strongly support the usefulness of the proposed task of stereo reconstruction for minimally invasive surgical video and indicate that deep learning is a promising approach to this task. Finally, we identify two automatic metrics, LPIPS and DISTS, that are strongly correlated with expert judgement and that could serve as proxies for the latter in future studies.",
                    "We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks."
                ],
                "domain": [
                    "Deep Learning",
                    "Video Processing",
                    "Natural Language Processing",
                    "Multilingual Models"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "288d63e2-a859-4169-b6db-22d07165edac": {
                "pk": "288d63e2-a859-4169-b6db-22d07165edac",
                "project_name": null,
                "name": "Aastha Jhunjhunwala",
                "bio": "I am a researcher dedicated to enhancing the capabilities of language models through systematic exploration of pretraining datasets. My recent work focuses on the construction methodologies of these datasets, which are crucial for the impressive performance of large language models. I conducted the first comprehensive study of the entire pretraining set construction pipeline, identifying effective techniques that lead to significant gains in model accuracy on downstream tasks. \n\nIn my research, I categorized widely used data sources, particularly web crawl snapshots, by attributes such as toxicity, quality, and domain. This categorization not only sheds light on the intricacies of dataset quality but also provides actionable insights for practitioners aiming to develop high-quality pretraining sets. \n\nAdditionally, I introduced Nemotron-4 15B, a 15-billion-parameter multilingual language model trained on an extensive 8 trillion text tokens. This model showcases exceptional performance across various tasks, particularly in multilingual capabilities, outperforming both similarly-sized models and those specifically designed for multilingual tasks. My work aims to bridge the gap between model development and dataset construction, ultimately contributing to the advancement of language understanding and generation technologies.",
                "collaborators": [
                    "Jupinder Parmar",
                    "Shrimai Prabhumoye",
                    "Joseph Jennings",
                    "Mostofa Patwary",
                    "Mohammad Shoeybi",
                    "Bryan Catanzaro",
                    "Bo Liu",
                    "Zhilin Wang",
                    "Sandeep Subramanian",
                    "Dan Su"
                ],
                "pub_titles": [
                    "Data, Data Everywhere: A Guide for Pretraining Dataset Construction",
                    "Nemotron-4 15B Technical Report"
                ],
                "pub_abstracts": [
                    "The impressive capabilities of recent language models can be largely attributed to the multi-trillion token pretraining datasets that they are trained on. However, model developers fail to disclose their construction methodology which has lead to a lack of open information on how to develop effective pretraining sets. To address this issue, we perform the first systematic study across the entire pipeline of pretraining set construction. First, we run ablations on existing techniques for pretraining set development to identify which methods translate to the largest gains in model accuracy on downstream evaluations. Then, we categorize the most widely used data source, web crawl snapshots, across the attributes of toxicity, quality, type of speech, and domain. Finally, we show how such attribute information can be used to further refine and improve the quality of a pretraining set. These findings constitute an actionable set of steps that practitioners can use to develop high quality pretraining sets.",
                    "We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Large Language Models",
                    "Pretraining Techniques"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "39cc7fc6-4f4b-4536-886a-1044efa606bc": {
                "pk": "39cc7fc6-4f4b-4536-886a-1044efa606bc",
                "project_name": null,
                "name": "Anthony Costa",
                "bio": "I am a researcher dedicated to the field of 3D object classification, with a particular focus on optimizing neural network architectures for improved performance. My recent work centers around the implementation of residual neural networks, which I have found to be more efficient and easier to optimize compared to traditional deep convolutional models. By leveraging the advantages of residual connections, I have successfully tackled the challenges of vanishing and exploding gradients that often plague deeper networks.\n\nIn my studies, I have explored the impact of widening network layers on classification accuracy, demonstrating that even shallow residual networks can achieve performance levels comparable to state-of-the-art 3D shape models. My research not only contributes to the understanding of network architectures suitable for 3D object classification but also provides extensive training and architectural parameters that can guide future implementations. I am passionate about advancing the capabilities of 3D shape recognition and making these models more accessible and efficient for practical applications.",
                "collaborators": [
                    "Varun Arvind",
                    "Marcus Badgeley",
                    "Samuel Cho",
                    "Eric Oermann"
                ],
                "pub_titles": [
                    "Wide and deep volumetric residual networks for volumetric image classification"
                ],
                "pub_abstracts": [
                    "3D shape models that directly classify objects from 3D information have become more widely implementable. Current state of the art models rely on deep convolutional and inception models that are resource intensive. Residual neural networks have been demonstrated to be easier to optimize and do not suffer from vanishing/exploding gradients observed in deep networks. Here we implement a residual neural network for 3D object classification of the 3D Princeton ModelNet dataset. Further, we show that widening network layers dramatically improves accuracy in shallow residual nets, and residual neural networks perform comparable to state-of-the-art 3D shape net models, and we show that widening network layers improves classification accuracy. We provide extensive training and architecture parameters providing a better understanding of available network architectures for use in 3D object classification."
                ],
                "domain": [
                    "3D Object Classification",
                    "Deep Learning",
                    "Residual Neural Networks"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "fcd0e436-ac03-42b5-9b63-51c690796f73": {
                "pk": "fcd0e436-ac03-42b5-9b63-51c690796f73",
                "project_name": null,
                "name": "Alex Aliper",
                "bio": "I am a researcher at the intersection of artificial intelligence, quantum computing, and drug discovery. My recent work focuses on enhancing the drug development pipeline by integrating advanced machine learning techniques and quantum algorithms. I have developed nach0-pc, a model that effectively captures the spatial arrangement of atoms in molecular structures, addressing the limitations of traditional chemical string representations. This model not only excels in generating high-quality molecular samples but also operates efficiently in multi-task settings.\n\nIn my exploration of hybrid quantum-classical generative adversarial networks (GANs), I have demonstrated the potential of variational quantum circuits to improve small molecule discovery, achieving superior physicochemical properties with fewer learnable parameters compared to classical models. My research also leverages AlphaFold's groundbreaking protein structure predictions to streamline the identification of novel drug candidates, successfully discovering small molecule hits for previously uncharacterized targets.\n\nThrough my work, I aim to bridge the gap between computational techniques and practical applications in drug discovery, showcasing the transformative potential of quantum-assisted methods. I am committed to advancing the field by developing innovative models that not only enhance the efficiency of drug design but also contribute to the discovery of therapeutics with significant clinical impact.",
                "collaborators": [
                    "Feng Ren",
                    "Alex Zhavoronkov",
                    "Yudong Cao",
                    "Alan Aspuru-Guzik",
                    "Xiao Ding",
                    "Maksim Kuznetsov",
                    "Airat Valiev",
                    "Daniil Polykovskiy",
                    "Elena Tutubalina",
                    "Rim Shayakhmetov"
                ],
                "pub_titles": [
                    "nach0-pc: Multi-task Language Model with Molecular Point Cloud Encoder",
                    "Exploring the Advantages of Quantum Generative Adversarial Networks in Generative Chemistry",
                    "AlphaFold Accelerates Artificial Intelligence Powered Drug Discovery: Efficient Discovery of a Novel Cyclin-dependent Kinase 20 (CDK20) Small Molecule Inhibitor",
                    "Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS"
                ],
                "pub_abstracts": [
                    "Recent advancements have integrated Language Models (LMs) into a drug discovery pipeline. However, existing models mostly work with SMILES and SELFIES chemical string representations, which lack spatial features vital for drug discovery. Additionally, attempts to translate chemical 3D structures into text format encounter issues such as excessive length and insufficient atom connectivity information. To address these issues, we introduce nach0-pc, a model combining domain-specific encoder and textual representation to handle spatial arrangement of atoms effectively. Our approach utilizes a molecular point cloud encoder for concise and order-invariant structure representation. We introduce a novel pre-training scheme for molecular point clouds to distillate the knowledge from spatial molecular structures datasets. After fine-tuning within both single-task and multi-task frameworks, nach0-pc demonstrates performance comparable with other diffusion models in terms of generated samples quality across several established spatial molecular generation tasks. Notably, our model is a multi-task approach, in contrast to diffusion models being limited to single tasks. Additionally, it is capable of processing point cloud-related data, which language models are not capable of handling due to memory limitations. These lead to our model having reduced training and inference time while maintaining on par performance.",
                    "De novo drug design with desired biological activities is crucial for developing novel therapeutics for patients. The drug development process is time and resource-consuming, and it has a low probability of success. Recent advances in machine learning and deep learning technology have reduced the time and cost of the discovery process and therefore, improved pharmaceutical research and development. In this paper, we explore the combination of two rapidly-developing fields with lead candidate discovery in the drug development process. First, Artificial intelligence has already been demonstrated to successfully accelerate conventional drug design approaches. Second, quantum computing has demonstrated promising potential in different applications, such as quantum chemistry, combinatorial optimizations, and machine learning. This manuscript explores hybrid quantum-classical generative adversarial networks (GAN) for small molecule discovery. We substituted each element of GAN with a variational quantum circuit (VQC) and demonstrated the quantum advantages in the small drug discovery. Utilizing a VQC in the noise generator of a GAN to generate small molecules achieves better physicochemical properties and performance in the goal-directed benchmark than the classical counterpart. Moreover, we demonstrate the potential of a VQC with only tens of learnable parameters in the generator of GAN to generate small molecules. We also demonstrate the quantum advantage of a VQC in the discriminator of GAN. In this hybrid model, the number of learnable parameters is significantly less than the classical ones, and it can still generate valid molecules. The hybrid model with only tens of training parameters in the quantum discriminator outperforms the MLP-based one in terms of both generated molecule properties and the achieved KL divergence.",
                    "The AlphaFold computer program predicted protein structures for the whole human genome, which has been considered as a remarkable breakthrough both in artificial intelligence (AI) application and structural biology. Despite the varying confidence level, these predicted structures still could significantly contribute to structure-based drug design of novel targets, especially the ones with no or limited structural information. In this work, we successfully applied AlphaFold in our end-to-end AI-powered drug discovery engines constituted of a biocomputational platform PandaOmics and a generative chemistry platform Chemistry42, to identify a first-in-class hit molecule of a novel target without an experimental structure starting from target selection towards hit identification in a cost- and time-efficient manner. PandaOmics provided the targets of interest and Chemistry42 generated the molecules based on the AlphaFold predicted structure, and the selected molecules were synthesized and tested in biological assays. Through this approach, we identified a small molecule hit compound for CDK20 with a Kd value of 8.9 +/- 1.6 uM (n = 4) within 30 days from target selection and after only synthesizing 7 compounds. Based on the available data, the second round of AI-powered compound generation was conducted and through which, a more potent hit molecule, ISM042-2 048, was discovered with a Kd value of 210.0 +/- 42.4 nM (n = 2), within 30 days and after synthesizing 6 compounds from the discovery of the first hit ISM042-2-001. To the best of our knowledge, this is the first reported small molecule targeting CDK20 and more importantly, this work is the first demonstration of AlphaFold application in the hit identification process in early drug discovery.",
                    "The discovery of small molecules with therapeutic potential is a long-standing challenge in chemistry and biology. Researchers have increasingly leveraged novel computational techniques to streamline the drug development process to increase hit rates and reduce the costs associated with bringing a drug to market. To this end, we introduce a quantum-classical generative model that seamlessly integrates the computational power of quantum algorithms trained on a 16-qubit IBM quantum computer with the established reliability of classical methods for designing small molecules. Our hybrid generative model was applied to designing new KRAS inhibitors, a crucial target in cancer therapy. We synthesized 15 promising molecules during our investigation and subjected them to experimental testing to assess their ability to engage with the target. Notably, among these candidates, two molecules, ISM061-018-2 and ISM061-22, each featuring unique scaffolds, stood out by demonstrating effective engagement with KRAS. ISM061-018-2 was identified as a broad-spectrum KRAS inhibitor, exhibiting a binding affinity to KRAS-G12D at $1.4 \\mu M$. Concurrently, ISM061-22 exhibited specific mutant selectivity, displaying heightened activity against KRAS G12R and Q61H mutants. To our knowledge, this work shows for the first time the use of a quantum-generative model to yield experimentally confirmed biological hits, showcasing the practical potential of quantum-assisted drug discovery to produce viable therapeutics. Moreover, our findings reveal that the efficacy of distribution learning correlates with the number of qubits utilized, underlining the scalability potential of quantum computing resources. Overall, we anticipate our results to be a stepping stone towards developing more advanced quantum generative models in drug discovery."
                ],
                "domain": [
                    "Drug Discovery",
                    "Quantum Computing",
                    "Machine Learning",
                    "Generative Models"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "a193517d-9390-479a-a7f3-285f8f165742": {
                "pk": "a193517d-9390-479a-a7f3-285f8f165742",
                "project_name": null,
                "name": "Alán Aspuru-Guzik",
                "bio": "I am a researcher deeply engaged in the intersection of quantum mechanics and computational chemistry, with a focus on developing innovative methods for simulating and understanding complex quantum systems. My work spans a variety of topics, including quantum Monte Carlo methods, quantum optimization algorithms, and the dynamics of exciton transfer in photosynthetic systems. \n\nIn my recent contributions, I have explored the potential of quantum algorithms to solve chemical problems, demonstrating how they can efficiently compute molecular ground-state energies and optimize protein folding models. I have also developed frameworks to study energy transfer dynamics, revealing how quantum coherence can enhance efficiency in molecular arrays. \n\nMy research emphasizes the importance of spectral density in engineering exciton dynamics and the application of quantum adiabatic algorithms to molecular properties. I have introduced novel variational principles that extend ground-state methodologies to quantum dynamics, allowing for more efficient simulations. \n\nThrough my work, I aim to bridge theoretical insights with practical applications, paving the way for advancements in quantum information processing and materials science. I am passionate about leveraging quantum computing to tackle challenges in physical chemistry, and I strive to contribute to the growing field of quantum technologies.",
                "collaborators": [
                    "Ryan Babbush",
                    "Alejandro Perdomo",
                    "Peter J. Love",
                    "Joel Yuen-Zhou",
                    "Jarrod R. McClean",
                    "Geordie Rose",
                    "Masoud Mohseni",
                    "Bryan O'Gorman",
                    "Jacob J. Krich",
                    "Alejandro Perdomo-Ortiz"
                ],
                "pub_titles": [
                    "Quantum Monte Carlo methods for the solution of the Schroedinger equation for molecular systems",
                    "Engineering directed excitonic energy transfer",
                    "Simulated Quantum Computation of Molecular Energies",
                    "First-Principles Semiclassical Initial Value Representation Molecular Dynamics",
                    "On the construction of model Hamiltonians for adiabatic quantum computation and its application to finding low energy conformations of lattice protein models",
                    "Environment-Assisted Quantum Walks in Photosynthetic Energy Transfer",
                    "Time-Dependent Density Functional Theory for Open Quantum Systems with Unitary Propagation",
                    "On the alternatives for bath correlators and spectral densities from mixed quantum-classical simulations",
                    "Force-Field Functor Theory: Classical Force-Fields which Reproduce Equilibrium Quantum Distributions",
                    "Resource Efficient Gadgets for Compiling Adiabatic Quantum Optimization Problems",
                    "Adiabatic Quantum Simulation of Quantum Chemistry",
                    "Exploiting locality in quantum computation for quantum chemistry",
                    "Clock Quantum Monte Carlo: an imaginary-time method for real-time quantum dynamics",
                    "A witness for coherent electronic oscillations in ultrafast spectroscopy",
                    "Finding low-energy conformations of lattice protein models by quantum annealing",
                    "Feynman's clock, a new variational principle, and parallel-in-time quantum dynamics",
                    "A study of heuristic guesses for adiabatic quantum computation",
                    "Quantum State and Process Tomography of Energy Transfer Systems via Ultrafast Spectroscopy",
                    "Construction of Energy Functions for Lattice Heteropolymer Models: A Case Study in Constraint Satisfaction Programming and Adiabatic Quantum Optimization"
                ],
                "pub_abstracts": [
                    "This is a book chapter soon to appear (2002) in the \"Handbook for Numerical Analysis\" volume dedicated to \"Computational Chemistry\" edited by Claude Le Bris. The series editors are P.G. Ciarlet and J. L. Lions. [North Holland/Elservier]. This review deals with some of the methods known under the umbrella term quantum Monte Carlo (QMC), specifically those that have been most commonly used for electronic structure.",
                    "We provide an intuitive platform for engineering exciton transfer dynamics. We show that careful consideration of the spectral density, which describes the system-bath interaction, leads to opportunities to engineer the transfer of an exciton. Since excitons in nanostructures are proposed for use in quantum information processing and artificial photosynthetic designs, our approach paves the way for engineering a wide range of desired exciton dynamics. We carefully describe the validity of the model and use experimentally relevant material parameters to show counter-intuitive examples of a directed exciton transfer in a linear chain of quantum dots.",
                    "The calculation time for the energy of atoms and molecules scales exponentially with system size on a classical computer but polynomially using quantum algorithms. We demonstrate that such algorithms can be applied to problems of chemical interest using modest numbers of quantum bits. Calculations of the water and lithium hydride molecular ground-state energies have been carried out on a quantum computer simulator using a recursive phase-estimation algorithm. The recursive algorithm reduces the number of quantum bits required for the readout register from about 20 to 4. Mappings of the molecular wave function to the quantum bits are described. An adiabatic method for the preparation of a good approximate ground-state wave function is described and demonstrated for a stretched hydrogen molecule. The number of quantum bits required scales linearly with the number of basis functions, and the number of gates required grows polynomially with the number of quantum bits.",
                    "A method for carrying out semiclassical initial value representation calculations using first-principles molecular dynamics (FP-SC-IVR) is presented. This method can extract the full vibrational power spectrum of carbon dioxide from a single trajectory providing numerical results that agree with experiment even for Fermi resonant states. The computational demands of the method are comparable to those of classical single-trajectory calculations, while describing uniquely quantum features such as the zero-point energy and Fermi resonances. By propagating the nuclear degrees of freedom using first-principles Born-Oppenheimer molecular dynamics, the stability of the method presented is improved considerably when compared to dynamics carried out using fitted potential energy surfaces and numerical derivatives.",
                    "In this report, we explore the use of a quantum optimization algorithm for obtaining low energy conformations of protein models. We discuss mappings between protein models and optimization variables, which are in turn mapped to a system of coupled quantum bits. General strategies are given for constructing Hamiltonians to be used to solve optimization problems of physical/chemical/biological interest via quantum computation by adiabatic evolution. As an example, we implement the Hamiltonian corresponding to the Hydrophobic-Polar (HP) model for protein folding. Furthermore, we present an approach to reduce the resulting Hamiltonian to two-body terms gearing towards an experimental realization.",
                    "Energy transfer within photosynthetic systems can display quantum effects such as delocalized excitonic transport. Recently, direct evidence of long-lived coherence has been experimentally demonstrated for the dynamics of the Fenna-Matthews-Olson (FMO) protein complex [Engel et al., Nature 446, 782 (2007)]. However, the relevance of quantum dynamical processes to the exciton transfer efficiency is to a large extent unknown. Here, we develop a theoretical framework for studying the role of quantum interference effects in energy transfer dynamics of molecular arrays interacting with a thermal bath within the Lindblad formalism. To this end, we generalize continuous-time quantum walks to non-unitary and temperature-dependent dynamics in Liouville space derived from a microscopic Hamiltonian. Different physical effects of coherence and decoherence processes are explored via a universal measure for the energy transfer efficiency and its susceptibility. In particular, we demonstrate that for the FMO complex an effective interplay between free Hamiltonian and thermal fluctuations in the environment leads to a substantial increase in energy transfer efficiency from about 70% to 99%.",
                    "We extend the Runge-Gross theorem for a very general class of Markovian and non-Markovian open quantum systems under weak assumptions about the nature of the bath and its coupling to the system. We show that for Kohn-Sham (KS) Time-Dependent Density Functional Theory, it is possible to rigorously include the effects of the environment within a bath functional in the KS potential, thus placing the interactions between the particles of the system and the coupling to the environment on the same footing. A Markovian bath functional inspired by the theory of nonlinear Schrodinger equations is suggested, which can be readily implemented in currently existing real-time codes. Finally, calculations on a helium model system are presented.",
                    "We investigate on the procedure of extracting a \"spectral density\" from mixed QM/MM calculations and employing it in open quantum systems models. In particular, we study the connection between the energy gap correlation function extracted from ground state QM/MM and the bath spectral density used as input in open quantum system approaches. We introduce a simple model which can give intuition on when the ground state QM/MM propagation will give the correct energy gap. We also discuss the role of higher order correlators of the energy-gap fluctuations which can provide useful information on the bath. Further, various semiclassical corrections to the spectral density, are applied and investigated. Finally, we apply our considerations to the photosynthetic Fenna-Matthews-Olson complex. For this system, our results suggest the use of the Harmonic prefactor for the spectral density rather than the Standard one, which was employed in the simulations of the system carried out to date.",
                    "Feynman and Hibbs were the first to variationally determine an effective potential whose associated classical canonical ensemble approximates the exact quantum partition function. We examine the existence of a map between the local potential and an effective classical potential which matches the exact quantum equilibrium density and partition function. The usefulness of such a mapping rests in its ability to readily improve Born-Oppenheimer potentials for use with classical sampling. We show that such a map is unique and must exist. To explore the feasibility of using this result to improve classical molecular mechanics, we numerically produce a map from a library of randomly generated one-dimensional potential/effective potential pairs then evaluate its performance on independent test problems. We also apply the map to simulate liquid para-hydrogen, finding that the resulting radial pair distribution functions agree well with path integral Monte Carlo simulations. The surprising accessibility and transferability of the technique suggest a quantitative route to adapting Born-Oppenheimer potentials, with a motivation similar in spirit to the powerful ideas and approximations of density functional theory.",
                    "We develop a resource efficient method by which the ground-state of an arbitrary k-local, optimization Hamiltonian can be encoded as the ground-state of a (k-1)-local optimization Hamiltonian. This result is important because adiabatic quantum algorithms are often most easily formulated using many-body interactions but experimentally available interactions are generally 2-body. In this context, the efficiency of a reduction gadget is measured by the number of ancilla qubits required as well as the amount of control precision needed to implement the resulting Hamiltonian. First, we optimize methods of applying these gadgets to obtain 2-local Hamiltonians using the least possible number of ancilla qubits. Next, we show a novel reduction gadget which minimizes control precision and a heuristic which uses this gadget to compile 3-local problems with a significant reduction in control precision. Finally, we present numerics which indicate a substantial decrease in the resources required to implement randomly generated, 3-body optimization Hamiltonians when compared to other methods in the literature.",
                    "We show how to apply the quantum adiabatic algorithm directly to the quantum computation of molecular properties. We describe a procedure to map electronic structure Hamiltonians to 2-local qubit Hamiltonians with a small set of physically realizable couplings. By combining the Bravyi-Kitaev construction to map fermions to qubits with perturbative gadgets to reduce the Hamiltonian to 2-local, we obtain precision requirements on the coupling strengths and a number of ancilla qubits that scale polynomially in the problem size. Hence our mapping is efficient. The required set of controllable interactions includes only two types of interaction beyond the Ising interactions required to apply the quantum adiabatic algorithm to combinatorial optimization problems. Our mapping may also be of interest to chemists directly as it defines a dictionary from electronic structure to spin Hamiltonians with physical interactions.",
                    "Accurate prediction of chemical and material properties from first principles quantum chemistry is a challenging task on traditional computers. Recent developments in quantum computation offer a route towards highly accurate solutions with polynomial cost, however this solution still carries a large overhead. In this perspective, we aim to bring together known results about the locality of physical interactions from quantum chemistry with ideas from quantum computation. We show that the utilization of spatial locality combined with the Bravyi-Kitaev transformation offers an improvement in the scaling of known quantum algorithms for quantum chemistry and provide numerical examples to help illustrate this point. We combine these developments to improve the outlook for the future of quantum chemistry on quantum computers.",
                    "In quantum information theory, there is an explicit mapping between general unitary dynamics and Hermitian ground state eigenvalue problems known as the Feynman-Kitaev Clock. A prominent family of methods for the study of quantum ground states are quantum Monte Carlo methods, and recently the full configuration interaction quantum Monte Carlo (FCIQMC) method has demonstrated great promise for practical systems. We combine the Feynman-Kitaev Clock with FCIQMC to formulate a new technique for the study of quantum dynamics problems. Numerical examples using quantum circuits are provided as well as a technique to further mitigate the sign problem through time-dependent basis rotations. Moreover, this method allows one to combine the parallelism of Monte Carlo techniques with the locality of time to yield an effective parallel-in-time simulation technique.",
                    "We report a conceptually straightforward witness that isolates coherent electronic oscillations from their vibronic counterparts in nonlinear optical spectra of molecular aggregates: Coherent oscillations as a function of waiting time in broadband pump/broadband probe spectra correspond to coherent electronic oscillations. Oscillations in individual peaks of 2D electronic spectra do not necessarily yield this conclusion. Our witness is simpler to implement than quantum process tomography and potentially resolves a long-standing controversy on the character of oscillations in ultrafast spectra of photosynthetic light harvesting systems.",
                    "Lattice protein folding models are a cornerstone of computational biophysics. Although these models are a coarse grained representation, they provide useful insight into the energy landscape of natural proteins. Finding low-energy three-dimensional structures is an intractable problem even in the simplest model, the Hydrophobic-Polar (HP) model. Exhaustive search of all possible global minima is limited to sequences in the tens of amino acids. Description of protein-like properties are more accurately described by generalized models, such as the one proposed by Miyazawa and Jernigan (MJ), which explicitly take into account the unique interactions among all 20 amino acids. There is theoretical and experimental evidence of the advantage of solving classical optimization problems using quantum annealing over its classical analogue (simulated annealing). In this report, we present a benchmark implementation of quantum annealing for a biophysical problem (six different experiments up to 81 superconducting quantum bits). Although the cases presented here can be solved in a classical computer, we present the first implementation of lattice protein folding on a quantum device under the Miyazawa-Jernigan model. This paves the way towards studying optimization problems in biophysics and statistical mechanics using quantum devices.",
                    "We introduce a new discrete-time variational principle inspired by the quantum clock originally proposed by Feynman, and use it to write down quantum evolution as a ground state eigenvalue problem. The construction allows one to apply ground state quantum many-body theory to quantum dynamics, extending the reach of many highly developed tools from this fertile research area. Moreover this formalism naturally leads to an algorithm to parallelize quantum simulation over time. We draw an explicit connection between previously known time-dependent variational principles and the new time embedded variational principle presented. Sample calculations are presented applying the idea to a Hydrogen molecule and the spin degrees of freedom of a model inorganic compound demonstrating the parallel speedup of our method as well as its flexibility in applying ground-state methodologies. Finally, we take advantage of the unique perspective of the new variational principle to examine the error of basis approximations in quantum dynamics.",
                    "Adiabatic quantum computation (AQC) is a universal model for quantum computation which seeks to transform the initial ground state of a quantum system into a final ground state encoding the answer to a computational problem. AQC initial Hamiltonians conventionally have a uniform superposition as ground state. We diverge from this practice by introducing a simple form of heuristics: the ability to start the quantum evolution with a state which is a guess to the solution of the problem. With this goal in mind, we explain the viability of this approach and the needed modifications to the conventional AQC (CAQC) algorithm. By performing a numerical study on hard-to-satisfy 6 and 7 bit random instances of the satisfiability problem (3-SAT), we show how this heuristic approach is possible and we identify that the performance of the particular algorithm proposed is largely determined by the Hamming distance of the chosen initial guess state with respect to the solution. Besides the possibility of introducing educated guesses as initial states, the new strategy allows for the possibility of restarting a failed adiabatic process from the measured excited state as opposed to restarting from the full superposition of states as in CAQC. The outcome of the measurement can be used as a more refined guess state to restart the adiabatic evolution. This concatenated restart process is another heuristic that the CAQC strategy cannot capture.",
                    "The description of excited state dynamics in multichromophoric systems constitutes both a theoretical and experimental challenge in modern physical chemistry. An experimental protocol which can systematically characterize both coherent and dissipative processes at the level of the evolving quantum state of the chromophores is desired. In this article, we show that a carefully chosen set of polarization controlled two-color heterodyned photon-echo experiments can be used to reconstruct the time-evolving density matrix of the one-exciton manifold of a heterodimer. This possibility in turn allows for a complete description of the excited state dynamics via quantum process tomography (QPT). Calculations on the dimer show that QPT can reveal rich information about system-bath interactions, which otherwise appear nontrivially hidden in the polarization monitored in standard four-wave mixing experiments. Our study presents a novel method for analyzing condensed phase experiments with a quantum information processing perspective.",
                    "Optimization problems associated with the interaction of linked particles are at the heart of polymer science, protein folding and other important problems in the physical sciences. In this review we explain how to recast these problems as constraint satisfaction problems such as linear programming, maximum satisfiability, and pseudo-boolean optimization. By encoding problems this way, one can leverage substantial insight and powerful solvers from the computer science community which studies constraint programming for diverse applications such as logistics, scheduling, artificial intelligence, and circuit design. We demonstrate how to constrain and embed lattice heteropolymer problems using several strategies. Each strikes a unique balance between number of constraints, complexity of constraints, and number of variables. Finally, we show how to reduce the locality of couplings in these energy functions so they can be realized as Hamiltonians on existing quantum annealing machines. We intend that this review be used as a case study for encoding related combinatorial optimization problems in a form suitable for adiabatic quantum optimization."
                ],
                "domain": [
                    "Quantum Computing",
                    "Quantum Chemistry",
                    "Optimization",
                    "Exciton Dynamics"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "6251d2cc-1442-4980-b0ae-fc13164f9426": {
                "pk": "6251d2cc-1442-4980-b0ae-fc13164f9426",
                "project_name": null,
                "name": "Alex Zhavoronkov",
                "bio": "I am a researcher at the intersection of artificial intelligence and medicinal chemistry, dedicated to revolutionizing drug discovery through innovative computational methods. My work primarily focuses on developing advanced software platforms like Chemistry42, which integrates AI techniques with traditional chemistry to design novel small molecules with specific properties. This platform is a vital part of Insilico Medicine's Pharma.ai suite, enhancing drug discovery processes from target identification to clinical trial predictions.\n\nI have also explored the application of deep learning in assessing organizational diversity, demonstrating how automated systems can provide unbiased insights into demographic representation within companies. My research extends to generative models, where I introduced BindGPT, a novel approach for generating 3D molecular structures that significantly streamlines the drug design process.\n\nIn my recent projects, I have successfully applied AlphaFold to identify first-in-class hit molecules, showcasing the potential of AI in structural biology. Additionally, I am pioneering hybrid quantum-classical generative models to design small molecules, particularly KRAS inhibitors, which are crucial in cancer therapy. My work emphasizes the integration of quantum computing with classical methods, revealing promising results in generating therapeutically viable compounds.\n\nThrough my research, I aim to enhance the efficiency and effectiveness of drug discovery, ultimately contributing to the development of novel therapeutics that can significantly impact patient care.",
                "collaborators": [
                    "Vladimir Aladinskiy",
                    "Daniil Polykovskiy",
                    "Feng Ren",
                    "Alex Aliper",
                    "Alan Aspuru-Guzik",
                    "Dmitry Bezrukov",
                    "Xiao Ding",
                    "Yudong Cao",
                    "Yan A. Ivanenkov",
                    "Alex Zhebrak"
                ],
                "pub_titles": [
                    "Chemistry42: An AI-based platform for de novo molecular design",
                    "Evaluating race and sex diversity in the world's largest companies using deep neural networks",
                    "BindGPT: A Scalable Framework for 3D Molecular Design via Language Modeling and Reinforcement Learning",
                    "GrantMed: a new, international system for tracking grants and funding trends in the life sciences",
                    "Models of Innate Neural Attractors and Their Applications for Neural Information Processing",
                    "AlphaFold Accelerates Artificial Intelligence Powered Drug Discovery: Efficient Discovery of a Novel Cyclin-dependent Kinase 20 (CDK20) Small Molecule Inhibitor",
                    "Exploring the Advantages of Quantum Generative Adversarial Networks in Generative Chemistry",
                    "Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models",
                    "Quantum Computing-Enhanced Algorithm Unveils Novel Inhibitors for KRAS"
                ],
                "pub_abstracts": [
                    "Chemistry42 is a software platform for de novo small molecule design that integrates Artificial Intelligence (AI) techniques with computational and medicinal chemistry methods. Chemistry42 is unique in its ability to generate novel molecular structures with predefined properties validated through in vitro and in vivo studies. Chemistry42 is a core component of Insilico Medicine Pharma.ai drug discovery suite that also includes target discovery and multi-omics data analysis (PandaOmics) and clinical trial outcomes predictions (InClinico).",
                    "Diversity is one of the fundamental properties for the survival of species, populations, and organizations. Recent advances in deep learning allow for the rapid and automatic assessment of organizational diversity and possible discrimination by race, sex, age and other parameters. Automating the process of assessing the organizational diversity using the deep neural networks and eliminating the human factor may provide a set of real-time unbiased reports to all stakeholders. In this pilot study we applied the deep-learned predictors of race and sex to the executive management and board member profiles of the 500 largest companies from the 2016 Forbes Global 2000 list and compared the predicted ratios to the ratios within each company's country of origin and ranked them by the sex-, age- and race- diversity index (DI). While the study has many limitations and no claims are being made concerning the individual companies, it demonstrates a method for the rapid and impartial assessment of organizational diversity using deep neural networks.",
                    "Generating novel active molecules for a given protein is an extremely challenging task for generative models that requires an understanding of the complex physical interactions between the molecule and its environment. In this paper, we present a novel generative model, BindGPT which uses a conceptually simple but powerful approach to create 3D molecules within the protein's binding site. Our model produces molecular graphs and conformations jointly, eliminating the need for an extra graph reconstruction step. We pretrain BindGPT on a large-scale dataset and fine-tune it with reinforcement learning using scores from external simulation software. We demonstrate how a single pretrained language model can serve at the same time as a 3D molecular generative model, conformer generator conditioned on the molecular graph, and a pocket-conditioned 3D molecule generator. Notably, the model does not make any representational equivariance assumptions about the domain of generation. We show how such simple conceptual approach combined with pretraining and scaling can perform on par or better than the current best specialized diffusion models, language models, and graph neural networks while being two orders of magnitude cheaper to sample.",
                    "Despite the success of PubMed and other search engines in managing the massive volume of biomedical literature and the retrieval of individual publications, grant-related data remains scattered and relatively inaccessible. This is problematic, as project and funding data has significant analytical value and could be integral to publication retrieval. Here, we introduce GrantMed, a searchable international database of biomedical grants that integrates some 20 million publications with the nearly 1.4 million research projects and 650 billion dollars of funding that made them possible. For any given topic in the life sciences, Grantmed provides instantaneous visualization of the past 30 years of dollars spent and projects awarded, along with detailed individual project descriptions, funding amounts, and links to investigators, research organizations, and resulting publications. It summarizes trends in funding and publication rates for areas of interest and merges data from various national grant databases to create one international grant tracking system. This information will benefit the research community and funding entities alike. Users can view trends over time or current projects underway and use this information to navigate the decision-making process in moving forward. They can view projects prior to publication and records of previous projects. Convenient access to this data for analytical purposes will be beneficial in many ways, helping to prevent project overlap, reduce funding redundancy, identify areas of success, accelerate dissemination of ideas, and expose knowledge gaps in moving forward. It is our hope that this will be a central resource for international life sciences research communities and the funding organizations that support them, ultimately streamlining progress.",
                    "In this work we reveal and explore a new class of attractor neural networks, based on inborn connections provided by model molecular markers, the molecular marker based attractor neural networks (MMBANN). We have explored conditions for the existence of attractor states, critical relations between their parameters and the spectrum of single neuron models, which can implement the MMBANN. Besides, we describe functional models (perceptron and SOM) which obtain significant advantages, while using MMBANN. In particular, the perceptron based on MMBANN, gets specificity gain in orders of error probabilities values, MMBANN SOM obtains real neurophysiological meaning, the number of possible grandma cells increases 1000- fold with MMBANN. Each set of markers has a metric, which is used to make connections between neurons containing the markers. The resulting neural networks have sets of attractor states, which can serve as finite grids for representation of variables in computations. These grids may show dimensions of d = 0, 1, 2,... We work with static and dynamic attractor neural networks of dimensions d = 0 and d = 1. We also argue that the number of dimensions which can be represented by attractors of activities of neural networks with the number of elements N=104 does not exceed 8.",
                    "The AlphaFold computer program predicted protein structures for the whole human genome, which has been considered as a remarkable breakthrough both in artificial intelligence (AI) application and structural biology. Despite the varying confidence level, these predicted structures still could significantly contribute to structure-based drug design of novel targets, especially the ones with no or limited structural information. In this work, we successfully applied AlphaFold in our end-to-end AI-powered drug discovery engines constituted of a biocomputational platform PandaOmics and a generative chemistry platform Chemistry42, to identify a first-in-class hit molecule of a novel target without an experimental structure starting from target selection towards hit identification in a cost- and time-efficient manner. PandaOmics provided the targets of interest and Chemistry42 generated the molecules based on the AlphaFold predicted structure, and the selected molecules were synthesized and tested in biological assays. Through this approach, we identified a small molecule hit compound for CDK20 with a Kd value of 8.9 +/- 1.6 uM (n = 4) within 30 days from target selection and after only synthesizing 7 compounds. Based on the available data, the second round of AI-powered compound generation was conducted and through which, a more potent hit molecule, ISM042-2 048, was discovered with a Kd value of 210.0 +/- 42.4 nM (n = 2), within 30 days and after synthesizing 6 compounds from the discovery of the first hit ISM042-2-001. To the best of our knowledge, this is the first reported small molecule targeting CDK20 and more importantly, this work is the first demonstration of AlphaFold application in the hit identification process in early drug discovery.",
                    "De novo drug design with desired biological activities is crucial for developing novel therapeutics for patients. The drug development process is time and resource-consuming, and it has a low probability of success. Recent advances in machine learning and deep learning technology have reduced the time and cost of the discovery process and therefore, improved pharmaceutical research and development. In this paper, we explore the combination of two rapidly-developing fields with lead candidate discovery in the drug development process. First, Artificial intelligence has already been demonstrated to successfully accelerate conventional drug design approaches. Second, quantum computing has demonstrated promising potential in different applications, such as quantum chemistry, combinatorial optimizations, and machine learning. This manuscript explores hybrid quantum-classical generative adversarial networks (GAN) for small molecule discovery. We substituted each element of GAN with a variational quantum circuit (VQC) and demonstrated the quantum advantages in the small drug discovery. Utilizing a VQC in the noise generator of a GAN to generate small molecules achieves better physicochemical properties and performance in the goal-directed benchmark than the classical counterpart. Moreover, we demonstrate the potential of a VQC with only tens of learnable parameters in the generator of GAN to generate small molecules. We also demonstrate the quantum advantage of a VQC in the discriminator of GAN. In this hybrid model, the number of learnable parameters is significantly less than the classical ones, and it can still generate valid molecules. The hybrid model with only tens of training parameters in the quantum discriminator outperforms the MLP-based one in terms of both generated molecule properties and the achieved KL divergence.",
                    "Generative models are becoming a tool of choice for exploring the molecular space. These models learn on a large training dataset and produce novel molecular structures with similar properties. Generated structures can be utilized for virtual screening or training semi-supervised predictive models in the downstream tasks. While there are plenty of generative models, it is unclear how to compare and rank them. In this work, we introduce a benchmarking platform called Molecular Sets (MOSES) to standardize training and comparison of molecular generative models. MOSES provides a training and testing datasets, and a set of metrics to evaluate the quality and diversity of generated structures. We have implemented and compared several molecular generation models and suggest to use our results as reference points for further advancements in generative chemistry research. The platform and source code are available at https://github.com/molecularsets/moses.",
                    "The discovery of small molecules with therapeutic potential is a long-standing challenge in chemistry and biology. Researchers have increasingly leveraged novel computational techniques to streamline the drug development process to increase hit rates and reduce the costs associated with bringing a drug to market. To this end, we introduce a quantum-classical generative model that seamlessly integrates the computational power of quantum algorithms trained on a 16-qubit IBM quantum computer with the established reliability of classical methods for designing small molecules. Our hybrid generative model was applied to designing new KRAS inhibitors, a crucial target in cancer therapy. We synthesized 15 promising molecules during our investigation and subjected them to experimental testing to assess their ability to engage with the target. Notably, among these candidates, two molecules, ISM061-018-2 and ISM061-22, each featuring unique scaffolds, stood out by demonstrating effective engagement with KRAS. ISM061-018-2 was identified as a broad-spectrum KRAS inhibitor, exhibiting a binding affinity to KRAS-G12D at $1.4 \\mu M$. Concurrently, ISM061-22 exhibited specific mutant selectivity, displaying heightened activity against KRAS G12R and Q61H mutants. To our knowledge, this work shows for the first time the use of a quantum-generative model to yield experimentally confirmed biological hits, showcasing the practical potential of quantum-assisted drug discovery to produce viable therapeutics. Moreover, our findings reveal that the efficacy of distribution learning correlates with the number of qubits utilized, underlining the scalability potential of quantum computing resources. Overall, we anticipate our results to be a stepping stone towards developing more advanced quantum generative models in drug discovery."
                ],
                "domain": [
                    "Drug Discovery",
                    "Generative Models",
                    "Quantum Computing",
                    "Artificial Intelligence"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can we effectively integrate chemical structure representations with natural language processing to enhance the performance of language models in biomedical and chemical tasks?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for advancing the field of drug development and biomedical research, as it can lead to more efficient information retrieval, improved drug design, and better prediction of pharmacological properties. By creating a unified model that understands both natural language and chemical structures, we can facilitate the translation of complex chemical data into actionable insights, ultimately accelerating the drug development process and reducing costs. This research could pave the way for future studies that explore the intersection of language models and chemical informatics, leading to innovative applications in various scientific domains.\n\n**[Question 3] - Why is it hard?**  \nThe integration of chemical structure representations with natural language poses significant challenges, including the complexity of accurately encoding chemical information in a format that language models can understand. Naive approaches may fail due to the inherent differences in data types and the need for a shared representation space that captures the nuances of both languages. Additionally, the lack of diverse and comprehensive datasets that include both natural language and chemical structures complicates the training process. Overcoming these technical and theoretical obstacles requires sophisticated model architectures and training methodologies that can effectively bridge the gap between these two domains.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has primarily focused on either natural language processing or chemical data separately, leading to a lack of models that can handle both effectively. Existing models, such as BioBERT and SciFive, have not incorporated chemical structure descriptions, limiting their applicability in chemical tasks. Barriers such as the absence of large, annotated datasets that combine both types of information and the complexity of developing a unified model have hindered progress. Our approach differs by proposing a unified encoder-decoder transformer, nach0, that is pre-trained on both natural language and chemical data, addressing the limitations of prior work and enabling multi-task learning across diverse chemical and NLP tasks.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves the development of the nach0 model, a unified encoder-decoder transformer that is pre-trained using self-supervised learning on a dataset that includes both natural language and chemical data. We will evaluate the model's performance on a range of downstream tasks, including information extraction, question answering, molecular structure generation, chemical property"
    },
    "2311.10776": {
        "paper_data": {
            "title": "Chemist-X: Large Language Model-empowered Agent for Reaction Condition Recommendation in Chemical Synthesis",
            "url": "http://arxiv.org/abs/2311.10776v5",
            "arxiv_id": "2311.10776",
            "authors": [
                "Kexin Chen",
                "Junyou Li",
                "Kunyi Wang",
                "Yuyang Du",
                "Jiahui Yu",
                "Jiamin Lu",
                "Lanqing Li",
                "Jiezhong Qiu",
                "Jianzhang Pan",
                "Yi Huang",
                "Qun Fang",
                "Pheng Ann Heng",
                "Guangyong Chen"
            ],
            "abstract": "Recent AI research plots a promising future of automatic chemical reactions within the chemistry society. This study proposes Chemist-X, a transformative AI agent that automates the reaction condition recommendation (RCR) task in chemical synthesis with retrieval-augmented generation (RAG) technology. To emulate expert chemists' strategies when solving RCR tasks, Chemist-X utilizes advanced RAG schemes to interrogate online molecular databases and distill critical data from the latest literature database. Further, the agent leverages state-of-the-art computer-aided design (CAD) tools with a large language model (LLM) supervised programming interface. With the ability to utilize updated chemical knowledge and CAD tools, our agent significantly outperforms conventional synthesis AIs confined to the fixed knowledge within its training data. Chemist-X considerably reduces chemists' workload and allows them to focus on more fundamental and creative problems, thereby bringing closer computational techniques and chemical research and making a remarkable leap toward harnessing AI's full capabilities in scientific discovery.",
            "introduction": "   1 Introduction  Recent “artificial intelligence (AI) for chemistry” research aims to free human labor with AI-supervised chemical reaction platforms [1, 2, 3, 4]. These innovative platforms employ machine learning (ML) algorithms and data-driven methods to orchestrate synthetic routes and identify optimal reaction conditions for maximizing yields. In the foreseeable future, chemical robots equipped with integrated AIs will autonomously design and refine chemical reactions, thus allowing human experts to focus on more creative and foundational research. Yet, traditional AI systems fall short of human experts who possess a broad spectrum of chemical knowledge and can continually update their understanding by consulting related literature when dealing with new problems. Conventional AI models, being confined to the knowledge from their training data, exhibit limited generalization ability when encountering unknown reactions. They can provide chemists with preliminary assistance for familiar reactions but cannot realize the ambitious aim of fully automated synthesis of unprecedented products.   Recent technical breakthroughs in retrieval-augmented generative AI (RAG-AI) have shed light on this problem. RAG-AI, as its name suggests, harnesses the strengths of retrieval-based methods and the capacities of generative AIs like large language models (LLMs). RAG enables an AI to retrieve relevant information from a vast and continuously updated knowledge source [5, 6], while LLM equips the AI with close-to-human abilities in information analysis and code writing [7, 8]. This paper develops a sophisticated chemical reaction agent, namely Chemist-X, with RAG-AI technology. As a proof of concept, we focus on RAG for reaction condition recommendation (RCR), which is equally important as retrosynthesis but receives less attention in chemical synthetic research.   LLMs like ChatGPT [9] or GPT-4 [10], thanks to their advanced generalization capabilities and near-human intelligence, demonstrate remarkable proficiency in devising feasible execution plans for RCR problems. The plan proposed by the LLM can be summarized as the following methodical three-phase approach: 1) identify structurally analogous molecules, which may share similar chemical characteristics with the target products, to provide foundational references for further analysis; 2) review existing literature about these analogous molecules to narrow down the chemical space of potential reaction conditions; and 3) utilize Computer-Aided Design (CAD) technology to select a subset of promising conditions from the refined chemical space. This systematic ”search-analyze-recommend” pattern suggested by the LLM well aligns with the methodology a human expert might undertake when addressing the RCR challenge associated with an unfamiliar molecule, thereby providing strong evidence for the practicality of the LLM’s three-phase solution. Fig. 1 illustrates the three-phase solution with more technical details.   Figure 1: The three-phase RCR framework of Chemist-X. a) The agent’s problem-solving procedure presented in the form of a pipeline. All three phases are automatically executed with the control Chemist-X. b) A detailed illustration of operations involved in Chemist-X. The LLM is the kernel of the agent. It is important to note that the figure follows the recent concept of “AI agent”, in which an LLM-empowered AI system can achieve close-to-human performance in a specific task owing to its abilities in i) using additional knowledge as “memory”, ii) leveraging “tools” like pre-defined APIs and auto-generated codes, and iii) taking necessary actions so that",
            "references": [
                {
                    "title": "LLMind: Orchestrating AI and IoT with LLM for Complex Task Execution",
                    "abstract": "Task-oriented communications are an important element in future intelligent IoT systems. Existing IoT systems, however, are limited in their capacity to handle complex tasks, particularly in their interactions with humans to accomplish these tasks. In this paper, we present LLMind, an LLM-based task-oriented AI agent framework that enables effective collaboration among IoT devices, with humans communicating high-level verbal instructions, to perform complex tasks. Inspired by the functional specialization theory of the brain, our framework integrates an LLM with domain-specific AI modules, enhancing its capabilities. Complex tasks, which may involve collaborations of multiple domain-specific AI modules and IoT devices, are executed through a control script generated by the LLM using a Language-Code transformation approach, which first converts language descriptions to an intermediate finite-state machine (FSM) before final precise transformation to code. Furthermore, the framework incorporates a novel experience accumulation mechanism to enhance response speed and effectiveness, allowing the framework to evolve and become progressively sophisticated through continuing user and machine interactions."
                },
                {
                    "title": "Autonomous chemical research with large language models",
                    "abstract": null
                },
                {
                    "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine",
                    "abstract": "Generalist foundation models such as GPT-4 have displayed surprising capabilities in a wide variety of domains and tasks. Yet, there is a prevalent assumption that they cannot match specialist capabilities of fine-tuned models. For example, most explorations to date on medical competency benchmarks have leveraged domain-specific training, as exemplified by efforts on BioGPT and Med-PaLM. We build on a prior study of GPT-4's capabilities on medical challenge benchmarks in the absence of special training. Rather than using simple prompting to highlight the model's out-of-the-box capabilities, we perform a systematic exploration of prompt engineering. We find that prompting innovation can unlock deeper specialist capabilities and show that GPT-4 easily tops prior leading results for medical benchmarks. The prompting methods we explore are general purpose, and make no specific use of domain expertise, removing the need for expert-curated content. Our experimental design carefully controls for overfitting during the prompt engineering process. We introduce Medprompt, based on a composition of several prompting strategies. With Medprompt, GPT-4 achieves state-of-the-art results on all nine of the benchmark datasets in the MultiMedQA suite. The method outperforms leading specialist models such as Med-PaLM 2 by a significant margin with an order of magnitude fewer calls to the model. Steering GPT-4 with Medprompt achieves a 27% reduction in error rate on the MedQA dataset over the best methods to date achieved with specialist models and surpasses a score of 90% for the first time. Beyond medical problems, we show the power of Medprompt to generalize to other domains and provide evidence for the broad applicability of the approach via studies of the strategy on exams in electrical engineering, machine learning, philosophy, accounting, law, nursing, and clinical psychology."
                },
                {
                    "title": "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following",
                    "abstract": "We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM."
                },
                {
                    "title": "The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms",
                    "abstract": "Large language models (LLMs) have garnered significant attention across various research disciplines, including the wireless communication community. There have been several heated discussions on the intersection of LLMs and wireless technologies. While recent studies have demonstrated the ability of LLMs to generate hardware description language (HDL) code for simple computation tasks, developing wireless prototypes and products via HDL poses far greater challenges because of the more complex computation tasks involved. In this paper, we aim to address this challenge by investigating the role of LLMs in FPGA-based hardware development for advanced wireless signal processing. We begin by exploring LLM-assisted code refactoring, reuse, and validation, using an open-source software-defined radio (SDR) project as a case study. Through the case study, we find that an LLM assistant can potentially yield substantial productivity gains for researchers and developers. We then examine the feasibility of using LLMs to generate HDL code for advanced wireless signal processing, using the Fast Fourier Transform (FFT) algorithm as an example. This task presents two unique challenges: the scheduling of subtasks within the overall task and the multi-step thinking required to solve certain arithmetic problem within the task. To address these challenges, we employ in-context learning (ICL) and Chain-of-Thought (CoT) prompting techniques, culminating in the successful generation of a 64-point Verilog FFT module. Our results demonstrate the potential of LLMs for generalization and imitation, affirming their usefulness in writing HDL code for wireless communication systems. Overall, this work contributes to understanding the role of LLMs in wireless communication and motivates further exploration of their capabilities."
                },
                {
                    "title": "A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development",
                    "abstract": "ChatGPT, an artificial intelligence generated content (AIGC) model developed by OpenAI, has attracted world-wide attention for its capability of dealing with challenging language understanding and generation tasks in the form of conversations. This paper briefly provides an overview on the history, status quo and potential future development of ChatGPT, helping to provide an entry point to think about ChatGPT. Specifically, from the limited open-accessed resources, we conclude the core techniques of ChatGPT, mainly including large-scale language models, in-context learning, reinforcement learning from human feedback and the key technical steps for developing Chat-GPT. We further analyze the pros and cons of ChatGPT and we rethink the duality of ChatGPT in various fields. Although it has been widely acknowledged that ChatGPT brings plenty of opportunities for various fields, mankind should still treat and use ChatGPT properly to avoid the potential threat, e.g., academic integrity and safety challenge. Finally, we discuss several open problems as the potential development of ChatGPT."
                },
                {
                    "title": "MetaRF: attention-based random forest for reaction yield prediction with a few trails",
                    "abstract": null
                },
                {
                    "title": "PubChem 2023 update",
                    "abstract": "PubChem (https://pubchem.ncbi.nlm.nih.gov) is a popular chemical information resource that serves a wide range of use cases. In the past two years, a number of changes were made to PubChem. Data from more than 120 data sources was added to PubChem. Some major highlights include: the integration of Google Patents data into PubChem, which greatly expanded the coverage of the PubChem Patent data collection; the creation of the Cell Line and Taxonomy data collections, which provide quick and easy access to chemical information for a given cell line and taxon, respectively; and the update of the bioassay data model. In addition, new functionalities were added to the PubChem programmatic access protocols, PUG-REST and PUG-View, including support for target-centric data download for a given protein, gene, pathway, cell line, and taxon and the addition of the 'standardize' option to PUG-REST, which returns the standardized form of an input chemical structure. A significant update was also made to PubChemRDF. The present paper provides an overview of these changes."
                },
                {
                    "title": "Understanding HTML with Large Language Models",
                    "abstract": "Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl."
                },
                {
                    "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
                    "abstract": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency."
                },
                {
                    "title": "Chemistry-informed molecular graph as reaction descriptor for machine-learned retrosynthesis planning",
                    "abstract": "Significance Machine learning has achieved great success in retrosynthesis planning. We introduced chemical information, including NMR chemical shifts, bond energies, catalysts, and solvents into the descriptor of molecules and reactions and into molecular graphs to represent molecules and reactions, and constructed a retrosynthesis planning model. It was developed using five molecular graph–based neural networks and Monte Carlo tree search. Our model, trained with a dataset of 1.4 million reaction data, achieved a top 50 accuracy of 0.94 for reaction template selection, a top 10 accuracy of 0.93 for catalyst prediction, and a top 10 accuracy of 0.89 for solvent prediction. The introduction of chemical information greatly enhances the accuracy, reliability, and efficiency of both single-step and multistep path planning."
                },
                {
                    "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation",
                    "abstract": "Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency."
                },
                {
                    "title": "Text and Code Embeddings by Contrastive Pre-Training",
                    "abstract": "Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4% and 1.8% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4%, 14.7%, and 10.6% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8% relative improvement over prior best work on code search."
                },
                {
                    "title": "On the use of real-world datasets for reaction yield prediction",
                    "abstract": "The lack of publicly available, large, and unbiased datasets is a key bottleneck for the application of machine learning (ML) methods in synthetic chemistry. Data from electronic laboratory notebooks (ELNs) could provide less biased, large datasets, but no such datasets have been made publicly available. The first real-world dataset from the ELNs of a large pharmaceutical company is disclosed and its relationship to high-throughput experimentation (HTE) datasets is described. For chemical yield predictions, a key task in chemical synthesis, an attributed graph neural network (AGNN) performs as well as or better than the best previous models on two HTE datasets for the Suzuki–Miyaura and Buchwald–Hartwig reactions. However, training the AGNN on an ELN dataset does not lead to a predictive model. The implications of using ELN data for training ML-based models are discussed in the context of yield predictions."
                },
                {
                    "title": "Artificial Intelligence in Chemistry: Current Trends and Future Directions",
                    "abstract": "The application of artificial intelligence (AI) to chemistry has grown tremendously in recent years. In this Review, we studied the growth and distribution of AI-related chemistry publications in the last two decades using the CAS Content Collection. The volume of both journal and patent publications have increased dramatically, especially since 2015. Study of the distribution of publications over various chemistry research areas revealed that analytical chemistry and biochemistry are integrating AI to the greatest extent and with the highest growth rates. We also investigated trends in interdisciplinary research and identified frequently occurring combinations of research areas in publications. Furthermore, topic analyses were conducted for journal and patent publications to illustrate emerging associations of AI with certain chemistry research topics. Notable publications in various chemistry disciplines were then evaluated and presented to highlight emerging use cases. Finally, the occurrence of different classes of substances and their roles in AI-related chemistry research were quantified, further detailing the popularity of AI adoption in the life sciences and analytical chemistry. In summary, this Review offers a broad overview of how AI has progressed in various fields of chemistry and aims to provide an understanding of its future directions."
                },
                {
                    "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
                    "abstract": "We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling title tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research."
                },
                {
                    "title": "Reaction classification and yield prediction using the differential reaction fingerprint DRFP",
                    "abstract": "Predicting the nature and outcome of reactions using computational methods is a crucial tool to accelerate chemical research. The recent application of deep learning-based learned fingerprints to reaction classification and reaction yield prediction has shown an impressive increase in performance compared to previous methods such as DFT- and structure-based fingerprints. However, learned fingerprints require large training data sets, are inherently biased, and are based on complex deep learning architectures. Here we present the differential reaction fingerprint DRFP. The DRFP algorithm takes a reaction SMILES as an input and creates a binary fingerprint based on the symmetric difference of two sets containing the circular molecular n-grams generated from the molecules listed left and right from the reaction arrow, respectively, without the need for distinguishing between reactants and reagents. We show that DRFP performs better than DFT-based fingerprints in reaction yield prediction and other structure-based fingerprints in reaction classification, reaching the performance of state-of-the-art learned fingerprints in both tasks while being data-independent."
                },
                {
                    "title": "Bayesian reaction optimization as a tool for chemical synthesis",
                    "abstract": null
                },
                {
                    "title": "Learning the Best Pooling Strategy for Visual Semantic Embedding",
                    "abstract": "Visual Semantic Embedding (VSE) is a dominant approach for vision-language retrieval, which aims at learning a deep embedding space such that visual data are embedded close to their semantic text labels or descriptions. Recent VSE models use complex methods to better contextualize and aggregate multi-modal features into holistic embeddings. However, we discover that surprisingly simple (but carefully selected) global pooling functions (e.g., max pooling) outperform those complex models, across different feature extractors. Despite its simplicity and effectiveness, seeking the best pooling function for different data modality and feature extractor is costly and tedious, especially when the size of features varies (e.g., text, video). Therefore, we propose a Generalized Pooling Operator (GPO), which learns to automatically adapt itself to the best pooling strategy for different features, requiring no manual tuning while staying effective and efficient. We extend the VSE model using this proposed GPO and denote it as VSE∞.Without bells and whistles, VSE∞ outperforms previous VSE methods significantly on image-text retrieval benchmarks across popular feature extractors. With a simple adaptation, variants of VSE∞ further demonstrate its strength by achieving the new state of the art on two video-text retrieval datasets. Comprehensive experiments and visualizations confirm that GPO always discovers the best pooling strategy and can be a plug-and-play feature aggregation module for standard VSE models. Code and pre-trained models are available at http://jcchen.me/vse_infty/"
                },
                {
                    "title": "Predicting chemical shifts with graph neural networks",
                    "abstract": "Inferring molecular structure from NMR measurements requires an accurate forward model that can predict chemical shifts from 3D structure. Current forward models are limited to specific molecules like proteins and state of the art models are not differentiable. Thus they cannot be used with gradient methods like biased molecular dynamics. Here we use graph neural networks (GNNs) for NMR chemical shift prediction. Our GNN can model chemical shifts accurately and capture important phenomena like hydrogen bonding induced downfield shift between multiple proteins, secondary structure effects, and predict shifts of organic molecules. Previous empirical NMR models of protein NMR have relied on careful feature engineering with domain expertise. These GNNs are trained from data alone with no feature engineering yet are as accurate and can work on arbitrary molecular structures. The models are also efficient, able to compute one million chemical shifts in about 5 seconds. This work enables a new category of NMR models that have multiple interacting types of macromolecules."
                },
                {
                    "title": "Language Models are Few-Shot Learners",
                    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                },
                {
                    "title": "Prediction of organic homolytic bond dissociation enthalpies at near chemical accuracy with sub-second computational cost",
                    "abstract": null
                },
                {
                    "title": "Supervised Contrastive Learning",
                    "abstract": "Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1%, setting a new state of the art number of 78.8% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations."
                },
                {
                    "title": "Synthetic organic chemistry driven by artificial intelligence",
                    "abstract": null
                },
                {
                    "title": "Graph Neural Networks: A Review of Methods and Applications",
                    "abstract": null
                },
                {
                    "title": "SciFinder",
                    "abstract": "SciFinder, a resource from the Chemical Abstracts Service (CAS), is a curated database of chemical and bibliographic information that covers several scientific and biomedical fields, with an emphasis on chemistry. SciFinder is an appropriate resource to consult for literature searches and to find background information on chemicals, drugs, and substances."
                },
                {
                    "title": "Predicting reaction performance in C–N cross-coupling using machine learning",
                    "abstract": "A guide for catalyst choice in the forest Chemists often discover reactions by applying catalysts to a series of simple compounds. Tweaking those reactions to tolerate more structural complexity in pharmaceutical research is time-consuming. Ahneman et al. report that machine learning can help. Using a high-throughput data set, they trained a random forest algorithm to predict which specific palladium catalysts would best tolerate isoxazoles (cyclic structures with an N–O bond) during C–N bond formation. The predictions also helped to guide analysis of the catalyst inhibition mechanism. Science, this issue p. 186 A random forest algorithm trained on high-throughput data predicts which catalysts best tolerate certain heterocycles. Machine learning methods are becoming integral to scientific inquiry in numerous disciplines. We demonstrated that machine learning can be used to predict the performance of a synthetic reaction in multidimensional chemical space using data obtained via high-throughput experimentation. We created scripts to compute and extract atomic, molecular, and vibrational descriptors for the components of a palladium-catalyzed Buchwald-Hartwig cross-coupling of aryl halides with 4-methylaniline in the presence of various potentially inhibitory additives. Using these descriptors as inputs and reaction yield as output, we showed that a random forest algorithm provides significantly improved predictive performance over linear regression analysis. The random forest model was also successfully applied to sparse training sets and out-of-sample prediction, suggesting its value in facilitating adoption of synthetic methodology."
                },
                {
                    "title": "Mordred: a molecular descriptor calculator",
                    "abstract": null
                },
                {
                    "title": "A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow",
                    "abstract": "A reaction screen in flowing solvent Chemists charged with manufacturing pharmaceuticals have recently been exploring the efficiency advantages of continuous flow techniques. Perera et al. now show that a flow apparatus can also accelerate reaction optimization earlier in the drug discovery process. They modified a high-performance liquid chromatography system to screen a wide variety of solvent, ligand, and base combinations to optimize carbon-carbon bond formation. Injecting stock solution aliquots of the catalyst and reactants into a carrier solvent stream let the authors vary the main solvent efficiently and scale up the optimal conditions for product isolation. Science, this issue p. 429 Chromatographic, flow-based screening of reaction conditions is demonstrated for Suzuki coupling in pharmaceutical research. The scarcity of complex intermediates in pharmaceutical research motivates the pursuit of reaction optimization protocols on submilligram scales. We report here the development of an automated flow-based synthesis platform, designed from commercially available components, that integrates both rapid nanomole-scale reaction screening and micromole-scale synthesis into a single modular unit. This system was validated by exploring a diverse range of reaction variables in a Suzuki-Miyaura coupling on nanomole scale at elevated temperatures, generating liquid chromatography–mass spectrometry data points for 5760 reactions at a rate of >1500 reactions per 24 hours. Through multiple injections of the same segment, the system directly produced micromole quantities of desired material. The optimal conditions were also replicated in traditional flow and batch mode at 50- to 200-milligram scale to provide good to excellent yields."
                },
                {
                    "title": "XGBoost: A Scalable Tree Boosting System",
                    "abstract": "Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems."
                },
                {
                    "title": "A random forest guided tour",
                    "abstract": null
                },
                {
                    "title": "Learning similarity with cosine similarity ensemble",
                    "abstract": null
                },
                {
                    "title": "PubMed: The Bibliographic Database",
                    "abstract": "PubMed comprises over 22 million citations and abstracts for biomedical literature indexed in NLM’s MEDLINE database, as well as from other life science journals and online books. PubMed citations and abstracts include the fields of biomedicine and health, and cover portions of the life sciences, behavioral sciences, chemical sciences, and bioengineering. PubMed also provides access to additional relevant websites and links to other NCBI resources, including its various molecular biology databases."
                },
                {
                    "title": "ChemSpider: The Free Chemical Database",
                    "abstract": null
                },
                {
                    "title": "Similarity between Euclidean and cosine angle distance for nearest neighbor queries",
                    "abstract": "Understanding the relationship among different distance measures is helpful in choosing a proper one for a particular application. In this paper, we compare two commonly used distance measures in vector models, namely, Euclidean distance (EUD) and cosine angle distance (CAD), for nearest neighbor (NN) queries in high dimensional data spaces. Using theoretical analysis and experimental results, we show that the retrieval results based on EUD are similar to those based on CAD when dimension is high. We have applied CAD for content based image retrieval (CBIR). Retrieval results show that CAD works no worse than EUD, which is a commonly used distance measure for CBIR, while providing other advantages, such as naturally normalized distance."
                },
                {
                    "title": "A Survey on Context Learning",
                    "abstract": "Learning semantics based on context information has been researched in many research areas for decades. Context information can not only be directly used as the input data, but also sometimes used as auxiliary knowledge to improve existing models. This survey aims at providing a structured and comprehensive overview of the research on context learning. We summarize and group the existing literature into four categories, Explicit Analysis, Implicit Analysis, Neural Network Models, and Composite Models, based on the underlying techniques adopted by them. For each category, we talk about the basic idea and techniques, and also introduce how context information is utilized as the model input or incorporated into the model to enhance the performance or extend the domain of application as auxiliary knowledge. In addition, we discuss the advantages and disadvantages of each model from both the technical and practical point of view."
                },
                {
                    "title": "A brief overview of 17",
                    "abstract": null
                }
            ]
        },
        "author_data": {
            "050a951e-eb99-4282-a5b0-fa77bbddae01": {
                "pk": "050a951e-eb99-4282-a5b0-fa77bbddae01",
                "project_name": null,
                "name": "Kexin Chen",
                "bio": "I am a researcher specializing in the intersection of communication systems, finance, and machine learning. My recent work has focused on innovative architectures for integrated sensing and communication (ISAC) using beyond-diagonal reconfigurable intelligent surfaces (BD-RIS) to enhance performance in millimeter-wave environments. I have also explored optimal consumption-investment strategies that leverage alternative data sources, such as social media and GPS data, to inform dynamic decision-making in financial markets.\n\nIn addition, I have delved into the challenges of information elicitation without verification, proposing decentralized models that incentivize crowd members to contribute effectively while solving complex tasks. My research extends to image-text matching, where I developed a step-wise hierarchical alignment network (SHAN) to improve visual-semantic alignment through multi-step reasoning.\n\nFurthermore, I have formulated robust dividend policies within a continuous-time economy, utilizing Epstein-Zin preferences to characterize investor behavior and executive signaling through dividends. My work aims to bridge theoretical insights with practical applications, providing frameworks that enhance decision-making in both communication and financial contexts. I am passionate about advancing our understanding of these fields and contributing to innovative solutions that address real-world challenges.",
                "collaborators": [
                    "Hoi Ying Wong",
                    "Yijie Mao",
                    "Chao Huang",
                    "Jianwei Huang",
                    "Zhong Ji",
                    "Haoran Wang",
                    "Kyunghyun Park"
                ],
                "pub_titles": [
                    "Transmitter Side Beyond-Diagonal RIS for mmWave Integrated Sensing and Communications",
                    "Duality in optimal consumption--investment problems with alternative data",
                    "Information Elicitation from Decentralized Crowd Without Verification",
                    "Step-Wise Hierarchical Alignment Network for Image-Text Matching",
                    "Robust dividend policy: Equivalence of Epstein-Zin and Maenhout preferences"
                ],
                "pub_abstracts": [
                    "This work initiates the study of a beyond-diagonal reconfigurable intelligent surface (BD-RIS)-aided transmitter architecture for integrated sensing and communication (ISAC) in the millimeter-wave (mmWave) frequency band. Deploying BD-RIS at the transmitter side not only alleviates the need for extensive fully digital radio frequency (RF) chains but also enhances both communication and sensing performance. These benefits are facilitated by the additional design flexibility introduced by the fully-connected scattering matrix of BD-RIS. To achieve the aforementioned benefits, in this work, we propose an efficient two-stage algorithm to design the digital beamforming of the transmitter and the scattering matrix of the BD-RIS with the aim of jointly maximizing the sum rate for multiple communication users and minimizing the largest eigenvalue of the Cramer-Rao bound (CRB) matrix for multiple sensing targets. Numerical results show that the transmitter-side BD-RIS-aided mmWave ISAC outperforms the conventional diagonal-RIS-aided ones in both communication and sensing performance.",
                    "This study investigates an optimal consumption--investment problem in which the unobserved stock trend is modulated by a hidden Markov chain that represents different economic regimes. In the classical approach, the hidden state is estimated from historical asset prices, but recent advancements in technology enable investors to consider alternative data in their decision-making. These include social media commentary, expert opinions, COVID-19 pandemic data, and GPS data, which originate outside of the standard sources of market data but are considered useful for predicting stock trends. We develop a novel duality theory for this problem and consider a jump-diffusion process for the alternative data series. This theory helps investors in identifying ``useful'' alternative data for dynamic decision-making by offering conditions to the filter equation that permit the use of a control approach based on the dynamic programming principle. We demonstrate an application for proving a unique smooth solution for a constant relative risk-averse agent once the distributions of the signals generated from alternative data satisfy a bounded likelihood ratio condition. In doing so, we obtain an explicit consumption--investment strategy that takes advantage of different types of alternative data that have not been addressed in the literature.",
                    "Information Elicitation Without Verification (IEWV) refers to the problem of eliciting high-accuracy solutions from crowd members when the ground truth is unverifiable. A high-accuracy team solution (aggregated from members' solutions) requires members' effort exertion, which should be incentivized properly. Previous research on IEWV mainly focused on scenarios where a central entity (e.g., the crowdsourcing platform) provides incentives to motivate crowd members. Still, the proposed designs do not apply to practical situations where no central entity exists. This paper studies the overlooked decentralized IEWV scenario, where crowd members act as both incentive contributors and task solvers. We model the interactions among members with heterogeneous team solution accuracy valuations as a two-stage game, where each member decides her incentive contribution strategy in Stage 1 and her effort exertion strategy in Stage 2. We analyze members' equilibrium behaviors under three incentive allocation mechanisms: Equal Allocation (EA), Output Agreement (OA), and Shapley Value (SV). We show that at an equilibrium under any allocation mechanism, a low-valuation member exerts no more effort than a high-valuation member. Counter-intuitively, a low-valuation member provides incentives to the collaboration while a high-valuation member does not at an equilibrium under SV. This is because a high-valuation member who values the aggregated team solution more needs fewer incentives to exert effort. In addition, when members' valuations are sufficiently heterogeneous, SV leads to team solution accuracy and social welfare no smaller than EA and OA.",
                    "Image-text matching plays a central role in bridging the semantic gap between vision and language. The key point to achieve precise visual-semantic alignment lies in capturing the fine-grained cross-modal correspondence between image and text. Most previous methods rely on single-step reasoning to discover the visual-semantic interactions, which lacks the ability of exploiting the multi-level information to locate the hierarchical fine-grained relevance. Different from them, in this work, we propose a step-wise hierarchical alignment network (SHAN) that decomposes image-text matching into multi-step cross-modal reasoning process. Specifically, we first achieve local-to-local alignment at fragment level, following by performing global-to-local and global-to-global alignment at context level sequentially. This progressive alignment strategy supplies our model with more complementary and sufficient semantic clues to understand the hierarchical correlations between image and text. The experimental results on two benchmark datasets demonstrate the superiority of our proposed method.",
                    "In a continuous-time economy, this study formulates the Epstein-Zin (EZ) preference for the discounted dividend (or cash payouts) of stockholders as an EZ singular control utility. We show that such a problem is well-defined and equivalent to the robust dividend policy set by the firm's executive in the sense of Maenhout's ambiguity-averse preference. While the firm's executive announces the expected future earnings in financial reports, they also signal the firm's confidence in the expected earnings through dividend or cash payouts. The robust dividend policy can then be characterized by a Hamilton-Jacobi-Bellman (HJB) variational inequality (VI). By constructing a novel shooting method for the HJB-VI, we theoretically prove that the robust dividend policy is a threshold strategy on the firm's surplus process. Therefore, dividend-caring investors can choose firms that match their preferences by examining stock's dividend policies and financial statements, whereas executives can make use of dividend to signal their confidence, in the form of ambiguity aversion, on realizing the earnings implied by their financial statements."
                ],
                "domain": [
                    "Reconfigurable Intelligent Surface",
                    "Investment Strategy",
                    "Game Theory",
                    "Image-Text Matching"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "bd5dccb8-e2d1-4761-9427-f51d21da12e1": {
                "pk": "bd5dccb8-e2d1-4761-9427-f51d21da12e1",
                "project_name": null,
                "name": "Junyou Li",
                "bio": "I am a researcher deeply engaged in the intersection of reinforcement learning (RL) and deep learning, with a focus on enhancing the efficiency and applicability of RL algorithms. My work has explored the potential of pretraining in deep RL, addressing the unique challenges it presents, and I have contributed to the development of frameworks like Affordable Generative Agents (AGA) to facilitate believable interactions in agent-based systems. \n\nI have also investigated the self-consistency of large language models (LLMs) in causal reasoning, proposing novel metrics to evaluate their performance. My research emphasizes the importance of leveraging background knowledge from LLMs to improve sample efficiency in RL tasks, and I have developed methods that utilize hierarchical RL approaches to tackle complex environments like Minecraft.\n\nMy contributions extend to the field of medicinal chemistry, where I have designed models for predicting reaction yields with minimal data, showcasing the power of AI in optimizing chemical processes. I am passionate about making AI accessible and effective in various domains, as evidenced by my efforts in organizing competitions that encourage participation and innovation in RL.\n\nOverall, my research aims to bridge theoretical advancements with practical applications, driving forward the capabilities of AI in both RL and chemistry while fostering a collaborative and inclusive research environment.",
                "collaborators": [
                    "Deheng Ye",
                    "Qiang Fu",
                    "Zichuan Lin",
                    "Wei Yang",
                    "Qin Zhang",
                    "Yangbin Yu",
                    "Jianing Shi",
                    "Kexin Chen",
                    "Guangyong Chen",
                    "Zhihui Xie"
                ],
                "pub_titles": [
                    "Pretraining in Deep Reinforcement Learning: A Survey",
                    "More Agents Is All You Need",
                    "Affordable Generative Agents",
                    "Nuance Matters: Probing Epistemic Consistency in Causal Reasoning",
                    "Improving Sample Efficiency of Reinforcement Learning with Background Knowledge from Large Language Models",
                    "Revisiting Discrete Soft Actor-Critic",
                    "Neural Entity Summarization with Joint Encoding and Weak Supervision",
                    "JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical Reinforcement Learning",
                    "MetaRF: Differentiable Random Forest for Reaction Yield Prediction with a Few Trails",
                    "An Autonomous Large Language Model Agent for Chemical Literature Data Mining",
                    "MineRL Diamond 2021 Competition: Overview, Results, and Lessons Learned"
                ],
                "pub_abstracts": [
                    "The past few years have seen rapid progress in combining reinforcement learning (RL) with deep learning. Various breakthroughs ranging from games to robotics have spurred the interest in designing sophisticated RL algorithms and systems. However, the prevailing workflow in RL is to learn tabula rasa, which may incur computational inefficiency. This precludes continuous deployment of RL algorithms and potentially excludes researchers without large-scale computing resources. In many other areas of machine learning, the pretraining paradigm has shown to be effective in acquiring transferable knowledge, which can be utilized for a variety of downstream tasks. Recently, we saw a surge of interest in Pretraining for Deep RL with promising results. However, much of the research has been based on different experimental settings. Due to the nature of RL, pretraining in this field is faced with unique challenges and hence requires new design principles. In this survey, we seek to systematically review existing works in pretraining for deep reinforcement learning, provide a taxonomy of these methods, discuss each sub-field, and bring attention to open problems and future directions.",
                    "We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method, termed as Agent Forest, is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: https://github.com/MoreAgentsIsAllYouNeed/AgentForest",
                    "The emergence of large language models (LLMs) has significantly advanced the simulation of believable interactive agents. However, the substantial cost on maintaining the prolonged agent interactions poses challenge over the deployment of believable LLM-based agents. Therefore, in this paper, we develop Affordable Generative Agents (AGA), a framework for enabling the generation of believable and low-cost interactions on both agent-environment and inter-agents levels. Specifically, for agent-environment interactions, we substitute repetitive LLM inferences with learned policies; while for inter-agent interactions, we model the social relationships between agents and compress auxiliary dialogue information. Extensive experiments on multiple environments show the effectiveness and efficiency of our proposed framework. Also, we delve into the mechanisms of emergent believable behaviors lying in LLM agents, demonstrating that agents can only generate finite behaviors in fixed environments, based upon which, we understand ways to facilitate emergent interaction behaviors. Our code is publicly available at: https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents.",
                    "To address this gap, our study introduces the concept of causal epistemic consistency, which focuses on the self-consistency of Large Language Models (LLMs) in differentiating intermediates with nuanced differences in causal reasoning. We propose a suite of novel metrics -- intensity ranking concordance, cross-group position agreement, and intra-group clustering -- to evaluate LLMs on this front. Through extensive empirical studies on 21 high-profile LLMs, including GPT-4, Claude3, and LLaMA3-70B, we have favoring evidence that current models struggle to maintain epistemic consistency in identifying the polarity and intensity of intermediates in causal reasoning. Additionally, we explore the potential of using internal token probabilities as an auxiliary tool to maintain causal epistemic consistency. In summary, our study bridges a critical gap in AI research by investigating the self-consistency over fine-grained intermediates involved in causal reasoning.",
                    "Low sample efficiency is an enduring challenge of reinforcement learning (RL). With the advent of versatile large language models (LLMs), recent works impart common-sense knowledge to accelerate policy learning for RL processes. However, we note that such guidance is often tailored for one specific task but loses generalizability. In this paper, we introduce a framework that harnesses LLMs to extract background knowledge of an environment, which contains general understandings of the entire environment, making various downstream RL tasks benefit from one-time knowledge representation. We ground LLMs by feeding a few pre-collected experiences and requesting them to delineate background knowledge of the environment. Afterward, we represent the output knowledge as potential functions for potential-based reward shaping, which has a good property for maintaining policy optimality from task rewards. We instantiate three variants to prompt LLMs for background knowledge, including writing code, annotating preferences, and assigning goals. Our experiments show that these methods achieve significant sample efficiency improvements in a spectrum of downstream tasks from Minigrid and Crafter domains.",
                    "We study the adaption of soft actor-critic (SAC) from continuous action space to discrete action space. We revisit vanilla SAC and provide an in-depth understanding of its Q value underestimation and performance instability issues when applied to discrete settings. We thereby propose entropy-penalty and double average Q-learning with Q-clip to address these issues. Extensive experiments on typical benchmarks with discrete action space, including Atari games and a large-scale MOBA game, show the efficacy of our proposed method. Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.",
                    "In a large-scale knowledge graph (KG), an entity is often described by a large number of triple-structured facts. Many applications require abridged versions of entity descriptions, called entity summaries. Existing solutions to entity summarization are mainly unsupervised. In this paper, we present a supervised approach NEST that is based on our novel neural model to jointly encode graph structure and text in KGs and generate high-quality diversified summaries. Since it is costly to obtain manually labeled summaries for training, our supervision is weak as we train with programmatically labeled data which may contain noise but is free of manual work. Evaluation results show that our approach significantly outperforms the state of the art on two public benchmarks.",
                    "Learning rational behaviors in open-world games like Minecraft remains to be challenging for Reinforcement Learning (RL) research due to the compound challenge of partial observability, high-dimensional visual perception and delayed reward. To address this, we propose JueWu-MC, a sample-efficient hierarchical RL approach equipped with representation learning and imitation learning to deal with perception and exploration. Specifically, our approach includes two levels of hierarchy, where the high-level controller learns a policy to control over options and the low-level workers learn to solve each sub-task. To boost the learning of sub-tasks, we propose a combination of techniques including 1) action-aware representation learning which captures underlying relations between action and representation, 2) discriminator-based self-imitation learning for efficient exploration, and 3) ensemble behavior cloning with consistency filtering for policy robustness. Extensive experiments show that JueWu-MC significantly improves sample efficiency and outperforms a set of baselines by a large margin. Notably, we won the championship of the NeurIPS MineRL 2021 research competition and achieved the highest performance score ever.",
                    "Artificial intelligence has deeply revolutionized the field of medicinal chemistry with many impressive applications, but the success of these applications requires a massive amount of training samples with high-quality annotations, which seriously limits the wide usage of data-driven methods. In this paper, we focus on the reaction yield prediction problem, which assists chemists in selecting high-yield reactions in a new chemical space only with a few experimental trials. To attack this challenge, we first put forth MetaRF, an attention-based differentiable random forest model specially designed for the few-shot yield prediction, where the attention weight of a random forest is automatically optimized by the meta-learning framework and can be quickly adapted to predict the performance of new reagents while given a few additional samples. To improve the few-shot learning performance, we further introduce a dimension-reduction based sampling method to determine valuable samples to be experimentally tested and then learned. Our methodology is evaluated on three different datasets and acquires satisfactory performance on few-shot prediction. In high-throughput experimentation (HTE) datasets, the average yield of our methodology's top 10 high-yield reactions is relatively close to the results of ideal yield selection.",
                    "Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency. The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry.",
                    "Reinforcement learning competitions advance the field by providing appropriate scope and support to develop solutions toward a specific problem. To promote the development of more broadly applicable methods, organizers need to enforce the use of general techniques, the use of sample-efficient methods, and the reproducibility of the results. While beneficial for the research community, these restrictions come at a cost -- increased difficulty. If the barrier for entry is too high, many potential participants are demoralized. With this in mind, we hosted the third edition of the MineRL ObtainDiamond competition, MineRL Diamond 2021, with a separate track in which we permitted any solution to promote the participation of newcomers. With this track and more extensive tutorials and support, we saw an increased number of submissions. The participants of this easier track were able to obtain a diamond, and the participants of the harder track progressed the generalizable solutions in the same task."
                ],
                "domain": [
                    "Reinforcement Learning",
                    "Large Language Models",
                    "Machine Learning",
                    "Knowledge Graphs"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "922b3c2f-981b-438c-b6e1-fb699d8e03e0": {
                "pk": "922b3c2f-981b-438c-b6e1-fb699d8e03e0",
                "project_name": null,
                "name": "Kunyi Wang",
                "bio": "I am a researcher dedicated to the field of optical engineering, with a particular focus on hybrid refractive-diffractive lenses. My work aims to bridge the gap between the light efficiency of traditional refractive lenses and the advanced information encoding capabilities of diffractive optical elements (DOEs). Recognizing the challenges in accurately simulating these hybrid designs, I developed a novel hybrid ray-tracing and wave-propagation (ray-wave) model. This fully differentiable model allows for precise simulation of optical aberrations and diffractive phase modulation, facilitating end-to-end co-design of lens optimization and image reconstruction networks.\n\nThrough rigorous validation against theoretical results and comparisons with commercial software like Zemax, I have demonstrated the model's superior accuracy. My research has significant implications for computational imaging and photography, as well as advanced optical design, and I am excited about the potential applications that can arise from this work. I am committed to sharing my findings and tools with the community, as I believe collaboration and open access are essential for driving innovation in our field.",
                "collaborators": [
                    "Xinge Yang",
                    "Matheus Souza",
                    "Praneeth Chakravarthula",
                    "Qiang Fu",
                    "Wolfgang Heidrich"
                ],
                "pub_titles": [
                    "End-to-End Hybrid Refractive-Diffractive Lens Design with Differentiable Ray-Wave Model"
                ],
                "pub_abstracts": [
                    "Hybrid refractive-diffractive lenses combine the light efficiency of refractive lenses with the information encoding power of diffractive optical elements (DOE), showing great potential as the next generation of imaging systems. However, accurately simulating such hybrid designs is generally difficult, and in particular, there are no existing differentiable image formation models for hybrid lenses with sufficient accuracy.   In this work, we propose a new hybrid ray-tracing and wave-propagation (ray-wave) model for accurate simulation of both optical aberrations and diffractive phase modulation, where the DOE is placed between the last refractive surface and the image sensor, i.e. away from the Fourier plane that is often used as a DOE position. The proposed ray-wave model is fully differentiable, enabling gradient back-propagation for end-to-end co-design of refractive-diffractive lens optimization and the image reconstruction network. We validate the accuracy of the proposed model by comparing the simulated point spread functions (PSFs) with theoretical results, as well as simulation experiments that show our model to be more accurate than solutions implemented in commercial software packages like Zemax. We demonstrate the effectiveness of the proposed model through real-world experiments and show significant improvements in both aberration correction and extended depth-of-field (EDoF) imaging. We believe the proposed model will motivate further investigation into a wide range of applications in computational imaging, computational photography, and advanced optical design. Code will be released upon publication."
                ],
                "domain": [
                    "Computational Imaging",
                    "Optical Design",
                    "Differentiable Modeling"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "97dc988f-d7a4-4ebf-997a-3fb5c62543fd": {
                "pk": "97dc988f-d7a4-4ebf-997a-3fb5c62543fd",
                "project_name": null,
                "name": "Yuyang Du",
                "bio": "I am a researcher specializing in advanced communication technologies, particularly in the realm of signal processing and machine learning applications. My work has focused on enhancing the efficiency and performance of various communication systems, including the development of innovative transceiver designs for Interleaved Frequency Division Multiple Access (IFDMA) that significantly reduce complexity while maintaining low Peak-to-Average Power Ratio (PAPR). \n\nI have also made strides in improving packet detection algorithms for wireless networks, introducing novel metrics and benchmarking methods that enhance performance in multi-antenna scenarios. My research extends to the realm of autonomous systems, where I have developed monocular 3D object detection methods that ensure consistency in object representation, achieving state-of-the-art results in benchmark evaluations.\n\nIn addition, I have explored the intersection of large language models (LLMs) and communication networks, proposing frameworks that enhance model performance in semantic communication and fine-tuning processes. My recent work investigates the application of LLMs in FPGA-based hardware development, demonstrating their potential in generating hardware description language (HDL) code for complex wireless signal processing tasks.\n\nThrough my research, I aim to bridge theoretical advancements with practical implementations, contributing to the design and optimization of next-generation communication systems. I am passionate about leveraging cutting-edge technologies to solve real-world challenges in the field of wireless communications and beyond.",
                "collaborators": [
                    "Soung Chang Liew",
                    "Yulin Shao",
                    "Kexin Chen",
                    "He Chen",
                    "Qian Ye",
                    "Ling Jiang",
                    "Wang Zhen",
                    "Liang Hao",
                    "Yiming Lei",
                    "Qun Yang"
                ],
                "pub_titles": [
                    "Efficient FFT Computation in IFDMA Transceivers",
                    "Reliable Packet Detection for Random Access Networks: Analysis, Benchmark, and Optimization",
                    "Consistency of Implicit and Explicit Features Matters for Monocular 3D Object Detection",
                    "Nonlinear Multi-Carrier System with Signal Clipping: Measurement, Analysis, and Optimization",
                    "Flow Sampling: Network Monitoring in Large-Scale Software-Defined IoT Networks",
                    "Addressing Out-of-Distribution Challenges in Image Semantic Communication Systems with Multi-modal Large Language Models",
                    "Rephrase and Contrast: Fine-Tuning Language Models for Enhanced Understanding of Communication and Computer Networks",
                    "The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms"
                ],
                "pub_abstracts": [
                    "Interleaved Frequency Division Multiple Access (IFDMA) has the salient advantage of lower Peak-to-Average Power Ratio (PAPR) than its competitors like Orthogonal FDMA (OFDMA). A recent research effort put forth a new IFDMA transceiver design significantly less complex than conventional IFDMA transceivers. The new IFDMA transceiver design reduces the complexity by exploiting a certain correspondence between the IFDMA signal processing and the Cooley-Tukey IFFT/FFT algorithmic structure so that IFDMA streams can be inserted/extracted at different stages of an IFFT/FFT module according to the sizes of the streams. Although the prior work has laid down the theoretical foundation for the new IFDMA transceiver's structure, the practical realization of the transceiver on specific hardware with resource constraints has not been carefully investigated. This paper is an attempt to fill the gap. Specifically, this paper puts forth a heuristic algorithm called multi-priority scheduling (MPS) to schedule the execution of the butterfly computations in the IFDMA transceiver with the constraint of a limited number of hardware processors. The resulting FFT computation, referred to as MPS-FFT, has a much lower computation time than conventional FFT computation when applied to the IFDMA signal processing. Importantly, we derive a lower bound for the optimal IFDMA FFT computation time to benchmark MPS-FFT. Our experimental results indicate that when the number of hardware processors is a power of two: 1) MPS-FFT has near-optimal computation time; 2) MPS-FFT incurs less than 44.13\\% of the computation time of the conventional pipelined FFT.",
                    "This paper reexamines and fundamentally improves the Schmidl-and-Cox (S&C) algorithm, which is extensively used for packet detection in wireless networks, and enhances its adaptability for multi-antenna receivers. First, we introduce a new \"compensated autocorrelation\" metric, providing a more analytically tractable solution with precise expressions for false-alarm and missed-detection probabilities. Second, this paper proposes the Pareto comparison principle for fair benchmarking packet-detection algorithms, considering both false alarms and missed detections simultaneously. Third, with the Pareto benchmarking scheme, we experimentally confirm that the performance of S&C can be greatly improved by taking only the real part and discarding the imaginary part of the autocorrelation, leading to the novel real-part S&C (RP-S&C) scheme. Fourth, and perhaps most importantly, we utilize the compensated autocorrelation metric we newly put forth to extend the single-antenna algorithm to multi-antenna scenarios through a weighted-sum approach. Two optimization problems, minimizing false-alarm and missed-detection probabilities respectively, are formulated and solutions are provided. Our experimental results reveal that the optimal weights for false alarms (WFA) scheme is more desirable than the optimal weights for missed detections (WMD) due to its simplicity, reliability, and superior performance. This study holds considerable implications for the design and deployment of packet-detection schemes in random-access networks.",
                    "Low-cost autonomous agents including autonomous driving vehicles chiefly adopt monocular 3D object detection to perceive surrounding environment. This paper studies 3D intermediate representation methods which generate intermediate 3D features for subsequent tasks. For example, the 3D features can be taken as input for not only detection, but also end-to-end prediction and/or planning that require a bird's-eye-view feature representation. In the study, we found that in generating 3D representation previous methods do not maintain the consistency between the objects' implicit poses in the latent space, especially orientations, and the explicitly observed poses in the Euclidean space, which can substantially hurt model performance. To tackle this problem, we present a novel monocular detection method, the first one being aware of the poses to purposefully guarantee that they are consistent between the implicit and explicit features. Additionally, we introduce a local ray attention mechanism to efficiently transform image features to voxels at accurate 3D locations. Thirdly, we propose a handcrafted Gaussian positional encoding function, which outperforms the sinusoidal encoding function while retaining the benefit of being continuous. Results show that our method improves the state-of-the-art 3D intermediate representation method by 3.15%. We are ranked 1st among all the reported monocular methods on both 3D and BEV detection benchmark on KITTI leaderboard as of th result's submission time.",
                    "Signal clipping is a classic technique for reducing peak-to-average power ratio (PAPR) in orthogonal frequency division multiplexing (OFDM) systems. It has been widely applied in consumer electronic devices owing to its low complexity and high efficiency. Although clipping reduces the nonlinear distortion caused by power amplifiers (PAs), it induces additional clipping distortion. Optimizing the joint system performance with consideration of both PA nonlinearity and clipping distortion remains an open problem due to the complex PA modeling. In this paper, we analyze the PA nonlinearity through the Bessel-Fourier PA (BFPA) model and simplify its power expression using inter-modulation product (IMP) analysis. We derive expressions of the receiver signal-to-noise ratio (SNR) and system symbol error rate (SER) for the nonlinear clipped OFDM system. With the derivations, we investigate the optimal system setting to achieve the SER lower bound in a practical OFDM system that considers both PA nonlinearity and clipping distortion. The methods and results presented in this paper can serve as a useful reference for the system-level optimization of clipped OFDM systems with nonlinear PA.",
                    "Software-defined Internet-of-Things networking (SDIoT) greatly simplifies the network monitoring in large-scale IoT networks by per-flow sampling, wherein the controller keeps track of all the active flows in the network and samples the IoT devices on each flow path to collect real-time flow statistics. There is a tradeoff between the controller's sampling preference and the balancing of loads among devices. On the one hand, the controller may prefer to sample some of the IoT devices on the flow path because they yield more accurate flow statistics. On the other hand, it is desirable to sample the devices uniformly so that their energy consumptions and lifespan are balanced. This paper formulates the flow sampling problem in large-scale SDIoT networks by means of a Markov decision process and devises policies that strike a good balance between these two goals. Three classes of policies are investigated: the optimal policy, the state-independent policies, and the index policies (including the Whittle index and a second-order index policies). The second-order index policy is the most desired policy among all: 1) in terms of performance, it is on an equal footing with the Whittle index policy, and outperforms the state-independent policies by much; 2) in terms of complexity, it is much simpler than the optimal policy, and is comparable to state-independent policies and the Whittle index policy; 3) in terms of realizability, it requires no prior information on the network dynamics, hence is much easier to implement in practice.",
                    "Semantic communication is a promising technology for next-generation wireless networks. However, the out-of-distribution (OOD) problem, where a pre-trained machine learning (ML) model is applied to unseen tasks that are outside the distribution of its training data, may compromise the integrity of semantic compression. This paper explores the use of multi-modal large language models (MLLMs) to address the OOD issue in image semantic communication. We propose a novel \"Plan A - Plan B\" framework that leverages the broad knowledge and strong generalization ability of an MLLM to assist a conventional ML model when the latter encounters an OOD input in the semantic encoding process. Furthermore, we propose a Bayesian optimization scheme that reshapes the probability distribution of the MLLM's inference process based on the contextual information of the image. The optimization scheme significantly enhances the MLLM's performance in semantic compression by 1) filtering out irrelevant vocabulary in the original MLLM output; and 2) using contextual similarities between prospective answers of the MLLM and the background information as prior knowledge to modify the MLLM's probability distribution during inference. Further, at the receiver side of the communication system, we put forth a \"generate-criticize\" framework that utilizes the cooperation of multiple MLLMs to enhance the reliability of image reconstruction.",
                    "Large language models (LLMs) are being widely researched across various disciplines, with significant recent efforts focusing on adapting LLMs for understanding of how communication networks operate. However, over-reliance on prompting techniques hinders the full exploitation of the generalization ability of these models, and the lack of efficient fine-tuning methods prevents the full realization of lightweight LLMs' potential. This paper addresses these challenges by introducing our Rephrase and Contrast (RaC) framework, an efficient fine-tuning framework. RaC enhances LLMs' comprehension and critical thinking abilities by incorporating question reformulation and contrastive analysis of correct and incorrect answers during the fine-tuning process. Experimental results demonstrate a 63.73% accuracy improvement over the foundational model when tested on a comprehensive networking problem set. Moreover, to efficiently construct the dataset for RaC fine-tuning, we develop a GPT-assisted data mining method for generating high-quality question-answer (QA) pairs; furthermore, we introduce ChoiceBoost, a data augmentation technique that expands dataset size while reducing answer-order bias. Apart from these technical innovations, we contribute to the networking community by open-sourcing valuable research resources, including: 1) the fine-tuned networking model referred to as RaC-Net, 2) the training dataset used for fine-tuning the model, 3) three testing problem sets of different difficulties to serve as benchmarks for future research, and 4) code associated with the above resources.",
                    "Large language models (LLMs) have garnered significant attention across various research disciplines, including the wireless communication community. There have been several heated discussions on the intersection of LLMs and wireless technologies. While recent studies have demonstrated the ability of LLMs to generate hardware description language (HDL) code for simple computation tasks, developing wireless prototypes and products via HDL poses far greater challenges because of the more complex computation tasks involved. In this paper, we aim to address this challenge by investigating the role of LLMs in FPGA-based hardware development for advanced wireless signal processing. We begin by exploring LLM-assisted code refactoring, reuse, and validation, using an open-source software-defined radio (SDR) project as a case study. Through the case study, we find that an LLM assistant can potentially yield substantial productivity gains for researchers and developers. We then examine the feasibility of using LLMs to generate HDL code for advanced wireless signal processing, using the Fast Fourier Transform (FFT) algorithm as an example. This task presents two unique challenges: the scheduling of subtasks within the overall task and the multi-step thinking required to solve certain arithmetic problem within the task. To address these challenges, we employ in-context learning (ICL) and Chain-of-Thought (CoT) prompting techniques, culminating in the successful generation of a 64-point Verilog FFT module. Our results demonstrate the potential of LLMs for generalization and imitation, affirming their usefulness in writing HDL code for wireless communication systems. Overall, this work contributes to understanding the role of LLMs in wireless communication and motivates further exploration of their capabilities."
                ],
                "domain": [
                    "Wireless Communication",
                    "Signal Processing",
                    "Machine Learning",
                    "Internet of Things"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "f1da90cc-4074-4165-90d4-c0a1de533707": {
                "pk": "f1da90cc-4074-4165-90d4-c0a1de533707",
                "project_name": null,
                "name": "Jiahui Yu",
                "bio": "I am a researcher dedicated to advancing the field of neural networks and their applications across various domains. My recent work focuses on developing universally slimmable networks (US-Nets), which allow for dynamic adjustment of network width to optimize accuracy and efficiency in real-time. This work includes innovative training techniques like the sandwich rule and inplace distillation, which have significantly improved performance on tasks such as image classification and reinforcement learning.\n\nIn addition to slimmable networks, I have introduced AutoSlim, a method that efficiently determines optimal channel configurations for neural networks under resource constraints, outperforming traditional channel pruning and architecture search methods. My research also delves into the mathematical foundations of neural networks, exploring the effects of normalization and scaling on their performance, and providing insights into hyperparameter selection for robust training.\n\nBeyond neural networks, I have engaged in diverse topics, including derived equivalences in algebraic structures and the construction of Ramanujan graphs, showcasing my versatility as a researcher. I am particularly passionate about bridging theoretical advancements with practical applications, as evidenced by my work on cross-supervised object detection, which enhances localization capabilities using weakly labeled data.\n\nMy goal is to continue pushing the boundaries of machine learning and network management, ensuring that my contributions not only advance academic knowledge but also have a meaningful impact on real-world applications.",
                "collaborators": [
                    "Thomas Huang",
                    "Konstantinos Spiliopoulos",
                    "Stephan Ramon Garcia",
                    "Shengyong Pan",
                    "Konrad Aguilar",
                    "Jonah Mendel",
                    "Gabe Udell",
                    "Ryan O'Loughlin",
                    "Zitian Chen",
                    "Zhiqiang Shen"
                ],
                "pub_titles": [
                    "Universally Slimmable Networks and Improved Training Techniques",
                    "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers",
                    "Transfer of derived equivalences from subalgebras to endomorphism algebras II",
                    "Normalization effects on deep neural networks",
                    "The Fell topology and the modular Gromov-Hausdorff propinquity",
                    "LPS-Type Ramanujan Graphs from Definite Quaternion Algebras over $\\mathbb Q$ of Class Number One",
                    "Normalization effects on shallow neural networks and related asymptotic expansions",
                    "Multidimensional Schinzel-type theorems for multiplicative functions near prime tuples",
                    "Symmetric and Antisymmetric Tensor Products for the Function-Theoretic Operator Theorist",
                    "Cross-Supervised Object Detection",
                    "A Hierarchical Approach to Encrypted Data Packet Classification in Smart Home Gateways"
                ],
                "pub_abstracts": [
                    "Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks",
                    "We study how to set channel numbers in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot solution, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods.   Notably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs). Code and models will be available at: https://github.com/JiahuiYu/slimmable_networks",
                    "We investigate derived equivalences between subalgebras of some $\\Phi$-Auslander-Yoneda algebras from a class of $n$-angles in weakly $n$-angulated categories. The derived equivalences are obtained by transferring subalgebras induced by $n$-angles to endomorphism algebras induced by approximation sequences. Then we extend our constructions \\cite{BP} to $n$-angle cases. Finally, we give an explicit example to illustrate our result.",
                    "We study the effect of normalization on the layers of deep neural networks of feed-forward type. A given layer $i$ with $N_{i}$ hidden units is allowed to be normalized by $1/N_{i}^{\\gamma_{i}}$ with $\\gamma_{i}\\in[1/2,1]$ and we study the effect of the choice of the $\\gamma_{i}$ on the statistical behavior of the neural network's output (such as variance) as well as on the test accuracy on the MNIST data set. We find that in terms of variance of the neural network's output and test accuracy the best choice is to choose the $\\gamma_{i}$'s to be equal to one, which is the mean-field scaling. We also find that this is particularly true for the outer layer, in that the neural network's behavior is more sensitive in the scaling of the outer layer as opposed to the scaling of the inner layers. The mechanism for the mathematical analysis is an asymptotic expansion for the neural network's output. An important practical consequence of the analysis is that it provides a systematic and mathematically informed way to choose the learning rate hyperparameters. Such a choice guarantees that the neural network behaves in a statistically robust way as the $N_i$ grow to infinity.",
                    "Given a unital AF-algebra $A$ equipped with a faithful tracial state, we equip each (norm-closed two-sided) ideal of $A$ with a metrized quantum vector bundle structure, when canonically viewed as a module over $A$, in the sense of Latr\\'emoli\\`ere using previous work of the first author and Latr\\'emoli\\`ere. Moreover, we show that convergence of ideals in the Fell topology implies convergence of the associated metrized quantum vector bundles in the modular Gromov-Hausdorff propinquity of Latr\\'emoli\\`ere. In a similar vein but requiring a different approach, given a compact metric space $(X,d)$, we equip each ideal of $C(X)$ with a metrized quantum vector bundle structure, and show that convergence in the Fell topology implies convergence in the modular Gromov-Hausdorff propinquity.",
                    "In this paper we construct explicit LPS-type Ramanujan graphs from each definite quaternion algebra over $\\mathbb Q$ of class number 1, extending the constructions of Lubotzky, Phillips, Sarnak, and later Chiu, and answering in the affirmative a question raised by Jo and Yamasaki. We do this by showing that for each definite quaternion algebra $\\mathcal H$ over $\\mathbb Q$ of class number 1 with maximal order $\\mathcal O$, if $G = \\mathcal H^\\times/Z(\\mathcal H^\\times)$ and $p$ is prime such that $G(\\mathbb Q_p) \\cong PGL_2(\\mathbb Q_p)$, then there exists a congruence $p$-arithmetic subgroup of $G$ which acts simply transitively on the Bruhat-Tits tree of $G(\\mathbb Q_p)$.",
                    "We consider shallow (single hidden layer) neural networks and characterize their performance when trained with stochastic gradient descent as the number of hidden units $N$ and gradient descent steps grow to infinity. In particular, we investigate the effect of different scaling schemes, which lead to different normalizations of the neural network, on the network's statistical output, closing the gap between the $1/\\sqrt{N}$ and the mean-field $1/N$ normalization. We develop an asymptotic expansion for the neural network's statistical output pointwise with respect to the scaling parameter as the number of hidden units grows to infinity. Based on this expansion, we demonstrate mathematically that to leading order in $N$, there is no bias-variance trade off, in that both bias and variance (both explicitly characterized) decrease as the number of hidden units increases and time grows. In addition, we show that to leading order in $N$, the variance of the neural network's statistical output decays as the implied normalization by the scaling parameter approaches the mean field normalization. Numerical studies on the MNIST and CIFAR10 datasets show that test and train accuracy monotonically improve as the neural network's normalization gets closer to the mean field normalization.",
                    "Assuming Dickson's conjecture, we obtain multidimensional analogues of recent results on the behavior of certain multiplicative arithmetic functions near twin-prime arguments. This is inspired by analogous unconditional theorems of Schinzel undertaken without primality assumptions.",
                    "We study symmetric and antisymmetric tensor products of Hilbert-space operators, focusing on norms and spectra for some well-known classes favored by function-theoretic operator theorists. We pose many open questions that should interest the field.",
                    "After learning a new object category from image-level annotations (with no object bounding boxes), humans are remarkably good at precisely localizing those objects. However, building good object localizers (i.e., detectors) currently requires expensive instance-level annotations. While some work has been done on learning detectors from weakly labeled samples (with only class labels), these detectors do poorly at localization. In this work, we show how to build better object detectors from weakly labeled images of new categories by leveraging knowledge learned from fully labeled base categories. We call this novel learning paradigm cross-supervised object detection. We propose a unified framework that combines a detection head trained from instance-level annotations and a recognition head learned from image-level annotations, together with a spatial correlation module that bridges the gap between detection and recognition. These contributions enable us to better detect novel objects with image-level annotations in complex multi-object scenes such as the COCO dataset.",
                    "With the pervasive network based services in smart homes, traditional network management cannot guarantee end-user quality-of-experience (QoE) for all applications. End-user QoE must be supported by efficient network quality-of-service (QoS) measurement and efficient network resource allocation. With the software-defined network technology, the core network may be controlled more efficiently by a network service provider. However, end-to-end network QoS can hardly be improved the managing the core network only. In this paper, we propose an encrypted packet classification scheme for smart home gateways to improve end-to-end QoS measurement from the network operator side. Furthermore, other services such as statistical data collecting, billing to service providers, etc., can be provided without compromising end-user privacy nor security of a network. The proposed encrypted packet classification scheme has a two-level hierarchical structure. One is the service level, which is based on applications that have the same network QoS requirements. A faster classification scheme based on deep learning is proposed to achieve real-time classification with high accuracy. The other one is the application level, which is based on fine-grained applications. A non-real-time classifier can be applied to provide high accuracy. Evaluation is conducted on both level classifiers to demonstrate the efficiency and accuracy of the two types of classifiers."
                ],
                "domain": [
                    "Neural Networks",
                    "Deep Learning",
                    "Object Detection",
                    "Network Management"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "117e3384-9a2e-430f-9688-d69ff18686bf": {
                "pk": "117e3384-9a2e-430f-9688-d69ff18686bf",
                "project_name": null,
                "name": "Jiamin Lu",
                "bio": "I am a researcher dedicated to enhancing the fields of entity recognition and relation extraction through innovative models and methodologies. My recent work has focused on developing span-based joint extraction models that leverage multitask learning to improve performance in the presence of negative samples. By introducing the Intersection over Union (IoU) concept, I have successfully integrated positional information into entity classification, leading to significant improvements in span boundary detection. My SpERT.MT model has demonstrated impressive F1 scores across multiple datasets, showcasing its effectiveness in mitigating the adverse effects of irrelevant spans.\n\nIn addition to my work in extraction models, I have also tackled the challenges of entity matching in heterogeneous data environments. My Entity Matching Model for Capturing Complex Attribute Relationships (EMM-CCAR) transforms the matching task into a sequence matching problem, allowing for a more nuanced understanding of complex attribute relationships. By employing attention mechanisms, my model emphasizes the intricate interdependencies between attributes, resulting in notable improvements over existing methodologies.\n\nThrough my research, I aim to bridge the gaps in current methodologies, providing robust solutions that address the complexities of real-world data. I am passionate about advancing the understanding of how to effectively extract and match entities across diverse formats, ultimately contributing to more accurate and efficient information retrieval systems.",
                "collaborators": [
                    "Chenguang Xue",
                    "Shitao Wang"
                ],
                "pub_titles": [
                    "Dealing with negative samples with multi-task learning on span-based joint entity-relation extraction",
                    "Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks"
                ],
                "pub_abstracts": [
                    "Recent span-based joint extraction models have demonstrated significant advantages in both entity recognition and relation extraction. These models treat text spans as candidate entities, and span pairs as candidate relationship tuples, achieving state-of-the-art results on datasets like ADE. However, these models encounter a significant number of non-entity spans or irrelevant span pairs during the tasks, impairing model performance significantly. To address this issue, this paper introduces a span-based multitask entity-relation joint extraction model. This approach employs the multitask learning to alleviate the impact of negative samples on entity and relation classifiers. Additionally, we leverage the Intersection over Union(IoU) concept to introduce the positional information into the entity classifier, achieving a span boundary detection. Furthermore, by incorporating the entity Logits predicted by the entity classifier into the embedded representation of entity pairs, the semantic input for the relation classifier is enriched. Experimental results demonstrate that our proposed SpERT.MT model can effectively mitigate the adverse effects of excessive negative samples on the model performance. Furthermore, the model demonstrated commendable F1 scores of 73.61\\%, 53.72\\%, and 83.72\\% on three widely employed public datasets, namely CoNLL04, SciERC, and ADE, respectively.",
                    "Across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the matching task into a sequence matching problem to mitigate the impact of varying data formats. Moreover, by introducing attention mechanisms, it identifies complex relationships between attributes, emphasizing the degree of matching among multiple attributes rather than one-to-one correspondences. Through the integration of the EMM-CCAR model, we adeptly surmount the challenges posed by data heterogeneity and intricate attribute interdependencies. In comparison with the prevalent DER-SSM and Ditto approaches, our model achieves improvements of approximately 4% and 1% in F1 scores, respectively. This furnishes a robust solution for addressing the intricacies of attribute complexity in entity matching."
                ],
                "domain": [
                    "Natural Language Processing",
                    "Entity Recognition",
                    "Relation Extraction",
                    "Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "ea75596b-39e4-4979-9614-d1bc83477d21": {
                "pk": "ea75596b-39e4-4979-9614-d1bc83477d21",
                "project_name": null,
                "name": "Lanqing Li",
                "bio": "I am a researcher dedicated to advancing the fields of molecular property prediction and reinforcement learning (RL). My recent work focuses on innovative methodologies that bridge the gap between chemical domain knowledge and machine learning, exemplified by my development of MolKD, which distills cross-modal knowledge from chemical reactions to enhance molecular representations. This approach has shown significant improvements in predictive performance, particularly in challenging datasets like Tox21.\n\nIn the realm of offline meta-reinforcement learning (OMRL), I have tackled critical challenges such as bootstrapping errors and efficient task inference. My contributions include a robust algorithm that minimizes the Median-of-Means objective to ensure accurate policy estimation even in the presence of corrupted demonstrations. Additionally, I have explored the integration of human supervision in RL models to enhance safety and performance during online deployment.\n\nMy research also addresses the imbalanced class distribution in graph contrastive learning (GCL), leading to the development of the ImGCL framework, which effectively balances representations and improves performance on imbalanced node classification tasks. Through these efforts, I aim to create robust, interpretable, and efficient models that can adapt to real-world complexities, ultimately contributing to advancements in drug discovery and decision-making processes in various applications.",
                "collaborators": [
                    "Dijun Luo",
                    "Liang Zeng",
                    "Jian Li",
                    "Liu Liu",
                    "Peilin Zhao",
                    "Rui Yang",
                    "Ziyang Tang",
                    "Ziqi Gao",
                    "Ziniu Li",
                    "Ke Xu"
                ],
                "pub_titles": [
                    "MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction",
                    "FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization",
                    "Robust Imitation Learning from Corrupted Demonstrations",
                    "ImGCL: Revisiting Graph Contrastive Learning on Imbalanced Node Classification",
                    "Deploying Offline Reinforcement Learning with Human Feedback",
                    "Provably Improved Context-Based Offline Meta-RL with Attention and Contrastive Learning",
                    "Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning"
                ],
                "pub_abstracts": [
                    "How to effectively represent molecules is a long-standing challenge for molecular property prediction and drug discovery. This paper studies this problem and proposes to incorporate chemical domain knowledge, specifically related to chemical reactions, for learning effective molecular representations. However, the inherent cross-modality property between chemical reactions and molecules presents a significant challenge to address. To this end, we introduce a novel method, namely MolKD, which Distills cross-modal Knowledge in chemical reactions to assist Molecular property prediction. Specifically, the reaction-to-molecule distillation model within MolKD transfers cross-modal knowledge from a pre-trained teacher network learning with one modality (i.e., reactions) into a student network learning with another modality (i.e., molecules). Moreover, MolKD learns effective molecular representations by incorporating reaction yields to measure transformation efficiency of the reactant-product pair when pre-training on reactions. Extensive experiments demonstrate that MolKD significantly outperforms various competitive baseline models, e.g., 2.1% absolute AUC-ROC gain on Tox21. Further investigations demonstrate that pre-trained molecular representations in MolKD can distinguish chemically reasonable molecular similarities, which enables molecular property prediction with high robustness and interpretability.",
                    "We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.",
                    "We consider offline Imitation Learning from corrupted demonstrations where a constant fraction of data can be noise or even arbitrary outliers. Classical approaches such as Behavior Cloning assumes that demonstrations are collected by an presumably optimal expert, hence may fail drastically when learning from corrupted demonstrations. We propose a novel robust algorithm by minimizing a Median-of-Means (MOM) objective which guarantees the accurate estimation of policy, even in the presence of constant fraction of outliers. Our theoretical analysis shows that our robust method in the corrupted setting enjoys nearly the same error scaling and sample complexity guarantees as the classical Behavior Cloning in the expert demonstration setting. Our experiments on continuous-control benchmarks validate that our method exhibits the predicted robustness and effectiveness, and achieves competitive results compared to existing imitation learning methods.",
                    "Graph contrastive learning (GCL) has attracted a surge of attention due to its superior performance for learning node/graph representations without labels. However, in practice, the underlying class distribution of unlabeled nodes for the given graph is usually imbalanced. This highly imbalanced class distribution inevitably deteriorates the quality of learned node representations in GCL. Indeed, we empirically find that most state-of-the-art GCL methods cannot obtain discriminative representations and exhibit poor performance on imbalanced node classification. Motivated by this observation, we propose a principled GCL framework on Imbalanced node classification (ImGCL), which automatically and adaptively balances the representations learned from GCL without labels. Specifically, we first introduce the online clustering based progressively balanced sampling (PBS) method with theoretical rationale, which balances the training sets based on pseudo-labels obtained from learned representations in GCL. We then develop the node centrality based PBS method to better preserve the intrinsic structure of graphs, by upweighting the important nodes of the given graph. Extensive experiments on multiple imbalanced graph datasets and imbalanced settings demonstrate the effectiveness of our proposed framework, which significantly improves the performance of the recent state-of-the-art GCL methods. Further experimental ablations and analyses show that the ImGCL framework consistently improves the representation quality of nodes in under-represented (tail) classes.",
                    "Reinforcement learning (RL) has shown promise for decision-making tasks in real-world applications. One practical framework involves training parameterized policy models from an offline dataset and subsequently deploying them in an online environment. However, this approach can be risky since the offline training may not be perfect, leading to poor performance of the RL models that may take dangerous actions. To address this issue, we propose an alternative framework that involves a human supervising the RL models and providing additional feedback in the online deployment phase. We formalize this online deployment problem and develop two approaches. The first approach uses model selection and the upper confidence bound algorithm to adaptively select a model to deploy from a candidate set of trained offline RL models. The second approach involves fine-tuning the model in the online deployment phase when a supervision signal arrives. We demonstrate the effectiveness of these approaches for robot locomotion control and traffic light control tasks through empirical validation.",
                    "Meta-learning for offline reinforcement learning (OMRL) is an understudied problem with tremendous potential impact by enabling RL algorithms in many real-world applications. A popular solution to the problem is to infer task identity as augmented state using a context-based encoder, for which efficient learning of robust task representations remains an open challenge. In this work, we provably improve upon one of the SOTA OMRL algorithms, FOCAL, by incorporating intra-task attention mechanism and inter-task contrastive learning objectives, to robustify task representation learning against sparse reward and distribution shift. Theoretical analysis and experiments are presented to demonstrate the superior performance and robustness of our end-to-end and model-free framework compared to prior algorithms across multiple meta-RL benchmarks.",
                    "As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\\boldsymbol{M}$ and its latent representation $\\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of RL benchmarks, context shift scenarios, data qualities and deep learning architectures, attaining the new state-of-the-art. We believe that our framework could open up avenues for new optimality bounds and COMRL algorithms."
                ],
                "domain": [
                    "Reinforcement Learning",
                    "Graph Neural Network",
                    "Meta-Learning",
                    "Molecular Representation"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "c1a1d8c4-c305-401d-983e-6e7e778307e3": {
                "pk": "c1a1d8c4-c305-401d-983e-6e7e778307e3",
                "project_name": null,
                "name": "Jiezhong Qiu",
                "bio": "I am a researcher deeply engaged in the intersection of machine learning, graph representation, and network embedding. My work spans a variety of domains, from developing novel algorithms for large-scale network embedding to enhancing molecular generative models for drug discovery. Recently, I have focused on creating efficient frameworks like LIGHTNE 2.0, which demonstrates that high-quality network embeddings can be achieved on CPU-only architectures, challenging the conventional reliance on distributed systems.\n\nMy research also delves into the theoretical underpinnings of graph neural networks (GNNs) and their applications in social influence prediction and session-based recommendations. I have pioneered methods such as Graph Contrastive Coding (GCC) to enable self-supervised learning across diverse graph datasets, showcasing the potential for transferability in graph representation tasks.\n\nIn addition, I have explored the use of deep learning in protein-ligand binding affinity prediction, addressing challenges posed by noisy data and varying bioassay labels through innovative pre-training frameworks. My interdisciplinary approach combines insights from natural language processing and computational biology, leading to significant advancements in both fields.\n\nOverall, my research aims to bridge theoretical foundations with practical applications, driving forward the capabilities of machine learning in understanding complex systems and improving real-world outcomes.",
                "collaborators": [
                    "Jie Tang",
                    "Shengyu Zhang",
                    "Yuxiao Dong",
                    "Hao Ma",
                    "Kuansan Wang",
                    "Chi Wang",
                    "Ben Liao",
                    "Chang-Yu Hsieh",
                    "Richard Peng",
                    "Jonathan P. Mailoa"
                ],
                "pub_titles": [
                    "A Matrix Chernoff Bound for Markov Chains and Its Application to Co-occurrence Matrices",
                    "Multi-Constraint Molecular Generation using Sparsely Labelled Training Data for Localized High-Concentration Electrolyte Diluent Screening",
                    "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",
                    "DeepInf: Social Influence Prediction with Deep Learning",
                    "Modeling Protein Using Large-scale Pretrain Language Model",
                    "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
                    "FastMoE: A Fast Mixture-of-Expert Training System",
                    "Protein-Ligand Complex Generator & Drug Screening via Tiered Tensor Transform",
                    "SketchNE: Embedding Billion-Scale Networks Accurately in One Hour",
                    "Multi-task Bioassay Pre-training for Protein-ligand Binding Affinity Prediction",
                    "Blockwise Self-Attention for Long Document Understanding",
                    "NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization",
                    "GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training",
                    "The Lifecycle and Cascade of WeChat Social Messaging Groups",
                    "Fast Extraction of Word Embedding from Q-contexts",
                    "Learning Large-scale Network Embedding from Representative Subgraph",
                    "Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries",
                    "Stable Prediction on Graphs with Agnostic Distribution Shift",
                    "Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation",
                    "Towards Lightweight and Automated Representation Learning System for Networks"
                ],
                "pub_abstracts": [
                    "We prove a Chernoff-type bound for sums of matrix-valued random variables sampled via a regular (aperiodic and irreducible) finite Markov chain. Specially, consider a random walk on a regular Markov chain and a Hermitian matrix-valued function on its state space. Our result gives exponentially decreasing bounds on the tail distributions of the extreme eigenvalues of the sample mean matrix. Our proof is based on the matrix expander (regular undirected graph) Chernoff bound [Garg et al. STOC '18] and scalar Chernoff-Hoeffding bounds for Markov chains [Chung et al. STACS '12].   Our matrix Chernoff bound for Markov chains can be applied to analyze the behavior of co-occurrence statistics for sequential data, which have been common and important data signals in machine learning. We show that given a regular Markov chain with $n$ states and mixing time $\\tau$, we need a trajectory of length $O(\\tau (\\log{(n)}+\\log{(\\tau)})/\\epsilon^2)$ to achieve an estimator of the co-occurrence matrix with error bound $\\epsilon$. We conduct several experiments and the experimental results are consistent with the exponentially fast convergence rate from theoretical analysis. Our result gives the first bound on the convergence rate of the co-occurrence matrix and the first sample complexity analysis in graph representation learning.",
                    "Recently, machine learning methods have been used to propose molecules with desired properties, which is especially useful for exploring large chemical spaces efficiently. However, these methods rely on fully labelled training data, and are not practical in situations where molecules with multiple property constraints are required. There is often insufficient training data for all those properties from publicly available databases, especially when ab-initio simulation or experimental property data is also desired for training the conditional molecular generative model. In this work, we show how to modify a semi-supervised variational auto-encoder (SSVAE) model which only works with fully labelled and fully unlabelled molecular property training data into the ConGen model, which also works on training data that have sparsely populated labels. We evaluate ConGen's performance in generating molecules with multiple constraints when trained on a dataset combined from multiple publicly available molecule property databases, and demonstrate an example application of building the virtual chemical space for potential Lithium-ion battery localized high-concentration electrolyte (LHCE) diluents.",
                    "Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.",
                    "Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friends' behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising.   Conventional social influence prediction approaches typically design various hand-crafted rules to extract user- and network-specific features. However, their effectiveness heavily relies on the knowledge of domain experts. As a result, it is usually difficult to generalize them into different domains. Inspired by the recent success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf, to learn users' latent feature representation for predicting social influence. In general, DeepInf takes a user's local network as the input to a graph neural network for learning her latent social representation. We design strategies to incorporate both network structures and user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter, Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model, DeepInf, significantly outperforms traditional feature engineering-based approaches, suggesting the effectiveness of representation learning for social applications.",
                    "Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",
                    "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
                    "Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of language model to trillions of parameters. However, training trillion-scale MoE requires algorithm and system co-design for a well-tuned high performance distributed training system. Unfortunately, the only existing platform that meets the requirements strongly depends on Google's hardware (TPU) and software (Mesh Tensorflow) stack, and is not open and available to the public, especially GPU and PyTorch communities.   In this paper, we present FastMoE, a distributed MoE training system based on PyTorch with common accelerators. The system provides a hierarchical interface for both flexible model design and easy adaption to different applications, such as Transformer-XL and Megatron-LM. Different from direct implementation of MoE models using PyTorch, the training speed is highly optimized in FastMoE by sophisticated high-performance acceleration skills. The system supports placing different experts on multiple GPUs across multiple nodes, enabling enlarging the number of experts linearly against the number of GPUs. The source of FastMoE is available at https://github.com/laekov/fastmoe under Apache-2 license.",
                    "The generation of small molecule candidate (ligand) binding poses in its target protein pocket is important for computer-aided drug discovery. Typical rigid-body docking methods ignore the pocket flexibility of protein, while the more accurate pose generation using molecular dynamics is hindered by slow protein dynamics. We develop a tiered tensor transform (3T) algorithm to rapidly generate diverse protein-ligand complex conformations for both pose and affinity estimation in drug screening, requiring neither machine learning training nor lengthy dynamics computation, while maintaining both coarse-grain-like coordinated protein dynamics and atomistic-level details of the complex pocket. The 3T conformation structures we generate achieve significantly higher accuracy in active ligand classification than traditional ensemble docking using hundreds of experimental protein conformations. Furthermore, we demonstrate that 3T can be used to explore distant protein-ligand binding poses within the protein pocket. 3T structure transformation is decoupled from the system physics, making future usage in other computational scientific domains possible.",
                    "We study large-scale network embedding with the goal of generating high-quality embeddings for networks with more than 1 billion vertices and 100 billion edges. Recent attempts LightNE and NetSMF propose to sparsify and factorize the (dense) NetMF matrix for embedding large networks, where NetMF is a theoretically-grounded network embedding method. However, there is a trade-off between their embeddings' quality and scalability due to their expensive memory requirements, making embeddings less effective under real-world memory constraints. Therefore, we present the SketchNE model, a scalable, effective, and memory-efficient network embedding solution developed for a single machine with CPU only. The main idea of SketchNE is to avoid the explicit construction and factorization of the NetMF matrix either sparsely or densely when producing the embeddings through the proposed sparse-sign randomized single-pass SVD algorithm. We conduct extensive experiments on nine datasets of various sizes for vertex classification and link prediction, demonstrating the consistent outperformance of SketchNE over state-of-the-art baselines in terms of both effectiveness and efficiency. SketchNE costs only 1.0 hours to embed the Hyperlink2012 network with 3.5 billion vertices and 225 billion edges on a CPU-only single machine with embedding superiority (e.g., a 282% relative HITS@10 gain over LightNE).",
                    "Protein-ligand binding affinity (PLBA) prediction is the fundamental task in drug discovery. Recently, various deep learning-based models predict binding affinity by incorporating the three-dimensional structure of protein-ligand complexes as input and achieving astounding progress. However, due to the scarcity of high-quality training data, the generalization ability of current models is still limited. In addition, different bioassays use varying affinity measurement labels (i.e., IC50, Ki, Kd), and different experimental conditions inevitably introduce systematic noise, which poses a significant challenge to constructing high-precision affinity prediction models. To address these issues, we (1) propose Multi-task Bioassay Pre-training (MBP), a pre-training framework for structure-based PLBA prediction; (2) construct a pre-training dataset called ChEMBL-Dock with more than 300k experimentally measured affinity labels and about 2.8M docked three-dimensional structures. By introducing multi-task pre-training to treat the prediction of different affinity labels as different tasks and classifying relative rankings between samples from the same bioassay, MBP learns robust and transferrable structural knowledge from our new ChEMBL-Dock dataset with varied and noisy labels. Experiments substantiate the capability of MBP as a general framework that can improve and be tailored to mainstream structure-based PLBA prediction tasks. To the best of our knowledge, MBP is the first affinity pre-training model and shows great potential for future development.",
                    "We present BlockBERT, a lightweight and efficient BERT model for better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.",
                    "We study the problem of large-scale network embedding, which aims to learn latent representations for network mining applications. Previous research shows that 1) popular network embedding benchmarks, such as DeepWalk, are in essence implicitly factorizing a matrix with a closed form, and 2)the explicit factorization of such matrix generates more powerful embeddings than existing methods. However, directly constructing and factorizing this matrix---which is dense---is prohibitively expensive in terms of both time and space, making it not scalable for large networks.   In this work, we present the algorithm of large-scale network embedding as sparse matrix factorization (NetSMF). NetSMF leverages theories from spectral sparsification to efficiently sparsify the aforementioned dense matrix, enabling significantly improved efficiency in embedding learning. The sparsified matrix is spectrally close to the original dense one with a theoretically bounded approximation error, which helps maintain the representation power of the learned embeddings. We conduct experiments on networks of various scales and types. Results show that among both popular benchmarks and factorization based methods, NetSMF is the only method that achieves both high efficiency and effectiveness. We show that NetSMF requires only 24 hours to generate effective embeddings for a large-scale academic collaboration network with tens of millions of nodes, while it would cost DeepWalk months and is computationally infeasible for the dense matrix factorization solution. The source code of NetSMF is publicly available (https://github.com/xptree/NetSMF).",
                    "Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.",
                    "Social instant messaging services are emerging as a transformative form with which people connect, communicate with friends in their daily life - they catalyze the formation of social groups, and they bring people stronger sense of community and connection. However, research community still knows little about the formation and evolution of groups in the context of social messaging - their lifecycles, the change in their underlying structures over time, and the diffusion processes by which they develop new members. In this paper, we analyze the daily usage logs from WeChat group messaging platform - the largest standalone messaging communication service in China - with the goal of understanding the processes by which social messaging groups come together, grow new members, and evolve over time. Specifically, we discover a strong dichotomy among groups in terms of their lifecycle, and develop a separability model by taking into account a broad range of group-level features, showing that long-term and short-term groups are inherently distinct. We also found that the lifecycle of messaging groups is largely dependent on their social roles and functions in users' daily social experiences and specific purposes. Given the strong separability between the long-term and short-term groups, we further address the problem concerning the early prediction of successful communities. In addition to modeling the growth and evolution from group-level perspective, we investigate the individual-level attributes of group members and study the diffusion process by which groups gain new members. By considering members' historical engagement behavior as well as the local social network structure that they embedded in, we develop a membership cascade model and demonstrate the effectiveness by achieving AUC of 95.31% in predicting inviter, and an AUC of 98.66% in predicting invitee.",
                    "The notion of word embedding plays a fundamental role in natural language processing (NLP). However, pre-training word embedding for very large-scale vocabulary is computationally challenging for most existing methods. In this work, we show that with merely a small fraction of contexts (Q-contexts)which are typical in the whole corpus (and their mutual information with words), one can construct high-quality word embedding with negligible errors. Mutual information between contexts and words can be encoded canonically as a sampling state, thus, Q-contexts can be fast constructed. Furthermore, we present an efficient and effective WEQ method, which is capable of extracting word embedding directly from these typical contexts. In practical scenarios, our algorithm runs 11$\\sim$13 times faster than well-established methods. By comparing with well-known methods such as matrix factorization, word2vec, GloVeand fasttext, we demonstrate that our method achieves comparable performance on a variety of downstream NLP tasks, and in the meanwhile maintains run-time and resource advantages over all these baselines.",
                    "We study the problem of large-scale network embedding, which aims to learn low-dimensional latent representations for network mining applications. Recent research in the field of network embedding has led to significant progress such as DeepWalk, LINE, NetMF, NetSMF. However, the huge size of many real-world networks makes it computationally expensive to learn network embedding from the entire network. In this work, we present a novel network embedding method called \"NES\", which learns network embedding from a small representative subgraph. NES leverages theories from graph sampling to efficiently construct representative subgraph with smaller size which can be used to make inferences about the full network, enabling significantly improved efficiency in embedding learning. Then, NES computes the network embedding from this representative subgraph, efficiently. Compared with well-known methods, extensive experiments on networks of various scales and types demonstrate that NES achieves comparable performance and significant efficiency superiority.",
                    "Knowledge graph (KG) embeddings have been a mainstream approach for reasoning over incomplete KGs. However, limited by their inherently shallow and static architectures, they can hardly deal with the rising focus on complex logical queries, which comprise logical operators, imputed edges, multiple source entities, and unknown intermediate entities. In this work, we present the Knowledge Graph Transformer (kgTransformer) with masked pre-training and fine-tuning strategies. We design a KG triple transformation method to enable Transformer to handle KGs, which is further strengthened by the Mixture-of-Experts (MoE) sparse activation. We then formulate the complex logical queries as masked prediction and introduce a two-stage masked pre-training strategy to improve transferability and generalizability. Extensive experiments on two benchmarks demonstrate that kgTransformer can consistently outperform both KG embedding-based baselines and advanced encoders on nine in-domain and out-of-domain reasoning tasks. Additionally, kgTransformer can reason with explainability via providing the full reasoning paths to interpret given answers.",
                    "Graph is a flexible and effective tool to represent complex structures in practice and graph neural networks (GNNs) have been shown to be effective on various graph tasks with randomly separated training and testing data. In real applications, however, the distribution of training graph might be different from that of the test one (e.g., users' interactions on the user-item training graph and their actual preference on items, i.e., testing environment, are known to have inconsistencies in recommender systems). Moreover, the distribution of test data is always agnostic when GNNs are trained. Hence, we are facing the agnostic distribution shift between training and testing on graph learning, which would lead to unstable inference of traditional GNNs across different test environments. To address this problem, we propose a novel stable prediction framework for GNNs, which permits both locally and globally stable learning and prediction on graphs. In particular, since each node is partially represented by its neighbors in GNNs, we propose to capture the stable properties for each node (locally stable) by re-weighting the information propagation/aggregation processes. For global stability, we propose a stable regularizer that reduces the training losses on heterogeneous environments and thus warping the GNNs to generalize well. We conduct extensive experiments on several graph benchmarks and a noisy industrial recommendation dataset that is collected from 5 consecutive days during a product promotion festival. The results demonstrate that our method outperforms various SOTA GNNs for stable prediction on graphs with agnostic distribution shift, including shift caused by node labels and attributes.",
                    "Session-based recommendation (SBR) systems aim to utilize the user's short-term behavior sequence to predict the next item without the detailed user profile. Most recent works try to model the user preference by treating the sessions as between-item transition graphs and utilize various graph neural networks (GNNs) to encode the representations of pair-wise relations among items and their neighbors. Some of the existing GNN-based models mainly focus on aggregating information from the view of spatial graph structure, which ignores the temporal relations within neighbors of an item during message passing and the information loss results in a sub-optimal problem. Other works embrace this challenge by incorporating additional temporal information but lack sufficient interaction between the spatial and temporal patterns. To address this issue, inspired by the uniformity and alignment properties of contrastive learning techniques, we propose a novel framework called Session-based Recommendation with Spatio-Temporal Contrastive Learning Enhanced GNNs (RESTC). The idea is to supplement the GNN-based main supervised recommendation task with the temporal representation via an auxiliary cross-view contrastive learning mechanism. Furthermore, a novel global collaborative filtering graph (CFG) embedding is leveraged to enhance the spatial view in the main task. Extensive experiments demonstrate the significant performance of RESTC compared with the state-of-the-art baselines e.g., with an improvement as much as 27.08% gain on HR@20 and 20.10% gain on MRR@20.",
                    "We propose LIGHTNE 2.0, a cost-effective, scalable, automated, and high-quality network embedding system that scales to graphs with hundreds of billions of edges on a single machine. In contrast to the mainstream belief that distributed architecture and GPUs are needed for large-scale network embedding with good quality, we prove that we can achieve higher quality, better scalability, lower cost, and faster runtime with shared-memory, CPU-only architecture. LIGHTNE 2.0 combines two theoretically grounded embedding methods NetSMF and ProNE. We introduce the following techniques to network embedding for the first time: (1) a newly proposed downsampling method to reduce the sample complexity of NetSMF while preserving its theoretical advantages; (2) a high-performance parallel graph processing stack GBBS to achieve high memory efficiency and scalability; (3) sparse parallel hash table to aggregate and maintain the matrix sparsifier in memory; (4) a fast randomized singular value decomposition (SVD) enhanced by power iteration and fast orthonormalization to improve vanilla randomized SVD in terms of both efficiency and effectiveness; (5) Intel MKL for proposed fast randomized SVD and spectral propagation; and (6) a fast and lightweight AutoML library FLAML for automated hyperparameter tuning. Experimental results show that LIGHTNE 2.0 can be up to 84X faster than GraphVite, 30X faster than PBG and 9X faster than NetSMF while delivering better performance. LIGHTNE 2.0 can embed very large graph with 1.7 billion nodes and 124 billion edges in half an hour on a CPU server, while other baselines cannot handle very large graphs of this scale."
                ],
                "domain": [
                    "Graph Neural Network",
                    "Network Embedding",
                    "Machine Learning",
                    "Drug Discovery"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "61ab947e-cc86-40d2-b003-94309cc5d981": {
                "pk": "61ab947e-cc86-40d2-b003-94309cc5d981",
                "project_name": null,
                "name": "Jianzhang Pan",
                "bio": "As a researcher deeply immersed in the field of graph neural networks (GNNs) and their applications, my work primarily revolves around enhancing the capabilities and understanding of these powerful models. My recent publications reflect a commitment to addressing the limitations of existing GNN architectures and exploring innovative solutions.\n\nOne of my notable contributions is the development of Position-aware GNNs (P-GNNs), which effectively capture the positional context of nodes within a graph, significantly improving performance in tasks like link prediction and community detection. I also introduced Identity-aware GNNs (ID-GNNs), which enhance the expressive power of message-passing frameworks by incorporating node identities, leading to substantial accuracy improvements across various prediction tasks.\n\nRecognizing the challenges posed by dynamic graphs, I proposed the ROLAND framework, which allows for the seamless adaptation of static GNNs to dynamic environments, ensuring scalability and efficiency. My research also delves into the architectural design space of GNNs, where I systematically explored over 315,000 designs to provide guidelines for optimizing performance across different tasks.\n\nIn addition to my work on GNNs, I have ventured into automated machine learning (AutoML) with methods like FALCON and AutoTransfer, which aim to streamline the search for optimal model designs while leveraging prior knowledge to enhance efficiency.\n\nOverall, my research is driven by a passion for pushing the boundaries of what GNNs can achieve, fostering a deeper understanding of their structures, and developing tools that empower researchers and practitioners alike.",
                "collaborators": [],
                "pub_titles": [],
                "pub_abstracts": [],
                "domain": [
                    "Graph Neural Network",
                    "Machine Learning",
                    "Multi-task Learning",
                    "AutoML"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "8bb959c7-c138-4aae-a76a-a5c5e37c0aa3": {
                "pk": "8bb959c7-c138-4aae-a76a-a5c5e37c0aa3",
                "project_name": null,
                "name": "Yi Huang",
                "bio": "I am a researcher deeply engaged in the interplay between geometry, topology, and dynamical systems, with a particular focus on hyperbolic surfaces and their moduli spaces. My recent work has explored Mirzakhani's volume recursion, which has significant implications for understanding the Weil-Petersson volumes and their connections to Witten's conjecture. I have also generalized McShane identities to quasifuchsian representations, contributing to the understanding of geodesic length spectra in nonorientable surfaces.\n\nIn addition to my work on hyperbolic geometry, I have developed a new scale of tent spaces that unifies various existing frameworks, enhancing our understanding of elliptic equations and interpolation theory. My research extends to the action of mapping class groups, where I have devised algorithms to generate simple length spectra for quasi-Fuchsian thrice-punctured projective planes, revealing surprising growth behaviors in geodesic lengths.\n\nI am also interested in permutation groups and their base sizes, having computed the exact base sizes for finite primitive groups of diagonal type, a significant advancement in the field. My exploration of Thurston's metrics on Teichmüller spaces has led to novel insights into the relationships between different metric structures.\n\nRecently, I have ventured into time series analysis, proposing a universal metric based on sequence likelihood divergence. This work allows for efficient comparisons of data streams, enhancing our ability to model and forecast based on historical patterns. My research is characterized by a commitment to bridging theoretical insights with practical applications across diverse mathematical landscapes.",
                "collaborators": [
                    "Paul Norbury",
                    "Hong Yi Huang",
                    "Athanase Papadopoulos",
                    "Ishanu Chattopadhyay"
                ],
                "pub_titles": [
                    "Mirzakhani's recursion formula on Weil-Petersson volume and applications",
                    "McShane-type identities for quasifuchsian representations of nonorientable surfaces",
                    "A McShane-type identity for closed surfaces",
                    "Weighted tent spaces with Whitney averages: factorization, interpolation and duality",
                    "Simple geodesics and Markoff quads",
                    "Base sizes of primitive groups of diagonal type",
                    "Optimal Lipschitz Maps on One-holed Tori and the Thurston Metric Theory of Teichmueller Space",
                    "Data Smashing 2.0: Sequence Likelihood (SL) Divergence For Fast Time Series Comparison"
                ],
                "pub_abstracts": [
                    "We give an overview of the proof for Mirzakhani's volume recursion for the Weil-Petersson volumes of the moduli spaces of genus $g$ hyperbolic surfaces with $n$ labeled geodesic boundary components, and her application of this recursion to Witten's conjecture and the study of simple geodesic length spectrum growth rates.",
                    "We show that Norbury's McShane identity for nonorientable cusped hyperbolic surfaces N generalizes to quasifuchsian representations of pi_1(N) as well as pseudo-Anosov mapping Klein bottles with singular fibers given by N.",
                    "We prove a McShane-type identity - a series, expressed in terms of geodesic lengths, that sums to 2\\pi for any closed hyperbolic surface with one distinguished point. To do so, we prove a generalized Birman-Series theorem showing that the set of complete geodesics on a hyperbolic surface with large cone angles is sparse.",
                    "In this paper, we introduce a new scale of tent spaces which covers, the (weighted) tent spaces of Coifman-Meyer-Stein and of Hofmann-Mayboroda-McIntosh, and some other tent spaces considered by Dahlberg, Kenig-Pipher and Auscher-Axelsson in elliptic equations. The strong factorizations within our tent spaces, with applications to quasi-Banach complex interpolation and to multiplier-duality theory, are established. This way, we unify and extend the corresponding results obtained by Coifman-Meyer-Stein, Cohn-Verbitsky and Hyt\\\"onen-Ros\\'en.",
                    "The action of the mapping class group of the thrice-punctured projective plane on its $\\mathrm{GL}(2,\\mathbb{C})$ character variety produces an algorithm for generating the simple length spectra of quasi-Fuchsian thrice-punctured projective planes. We apply this algorithm to quasi-Fuchsian representations of the corresponding fundamental group to prove: a sharp upper-bound for the length of its shortest geodesic, a McShane identity and the surprising result of non-polynomial growth for the number of simple closed geodesic lengths.",
                    "Let $G$ be a permutation group on a finite set $\\Omega$. The base size of $G$ is the minimal size of a subset of $\\Omega$ with trivial pointwise stabiliser in $G$. In this paper, we extend earlier work of Fawcett by determining the precise base size of every finite primitive permutation group of diagonal type. In particular, this is the first family of primitive groups arising in the O'Nan-Scott theorem for which the exact base size has been computed in all cases. Our methods also allow us to determine all the primitive groups of diagonal type with a unique regular suborbit.",
                    "We study Thurston's Lipschitz and curve metrics, as well as the arc metric on the Teichmueller space of one-hold tori equipped with complete hyperbolic metrics with boundary holonomy of fixed length. We construct natural Lipschitz maps between two surfaces equipped with such hyperbolic metrics that generalize Thurston's stretch maps and prove the following: (1) On the Teichmueller space of the torus with one boundary component, the Lipschitz and the curve metrics coincide and define a geodesic metric on this space. (2) On the same space, the arc and the curve metrics coincide when the length of the boundary component is =< 4 arcsinh(1), but differ when the boundary length is large. We further apply our stretch map generalization to construct novel Thurston geodesics on the Teichmueller spaces of closed hyperbolic surfaces, and use these geodesics to show that the sum-symmetrization of the Thurston metric fails to exhibit Gromov hyperbolicity.",
                    "Recognizing subtle historical patterns is central to modeling and forecasting problems in time series analysis. Here we introduce and develop a new approach to quantify deviations in the underlying hidden generators of observed data streams, resulting in a new efficiently computable universal metric for time series. The proposed metric is in the sense that we can compare and contrast data streams regardless of where and how they are generated and without any feature engineering step. The approach proposed in this paper is conceptually distinct from our previous work on data smashing, and vastly improves discrimination performance and computing speed. The core idea here is the generalization of the notion of KL divergence often used to compare probability distributions to a notion of divergence in time series. We call this the sequence likelihood (SL) divergence, which may be used to measure deviations within a well-defined class of discrete-valued stochastic processes. We devise efficient estimators of SL divergence from finite sample paths and subsequently formulate a universal metric useful for computing distance between time series produced by hidden stochastic generators."
                ],
                "domain": [
                    "Geometric Topology",
                    "Hyperbolic Geometry",
                    "Time Series Analysis",
                    "Metric Spaces"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "b03a0fad-cf72-4d3b-8b35-570e475affb3": {
                "pk": "b03a0fad-cf72-4d3b-8b35-570e475affb3",
                "project_name": null,
                "name": "Qun Fang",
                "bio": "I am a researcher specializing in gesture recognition technology, particularly within the context of intelligent education. My recent work focuses on leveraging millimeter-wave (mmWave) radar signals, which offer high resolution and strong penetration capabilities, to develop a highly accurate and robust gesture recognition method. \n\nIn my approach, I capture raw signals of hand movements using a mmWave radar module and employ a series of preprocessing techniques, including Fourier transformation, distance compression, Doppler processing, and noise reduction through moving target indication (MTI). These preprocessed signals are then analyzed using a Convolutional Neural Network-Time Domain Convolutional Network (CNN-TCN) model to extract spatio-temporal features essential for accurate gesture classification.\n\nThe results of my experiments are promising, achieving an impressive accuracy rate of 98.2% in domain-specific recognition tasks. Moreover, my method demonstrates exceptional robustness, maintaining high recognition rates across various neural network architectures. I am passionate about advancing gesture recognition technologies and their applications in enhancing interactive learning experiences.",
                "collaborators": [
                    "YiHui Yan",
                    "GuoQing Ma"
                ],
                "pub_titles": [
                    "Gesture Recognition in Millimeter-Wave Radar Based on Spatio-Temporal Feature Sequences"
                ],
                "pub_abstracts": [
                    "Gesture recognition is a pivotal technology in the realm of intelligent education, and millimeter-wave (mmWave) signals possess advantages such as high resolution and strong penetration capability. This paper introduces a highly accurate and robust gesture recognition method using mmWave radar. The method involves capturing the raw signals of hand movements with the mmWave radar module and preprocessing the received radar signals, including Fourier transformation, distance compression, Doppler processing, and noise reduction through moving target indication (MTI). The preprocessed signals are then fed into the Convolutional Neural Network-Time Domain Convolutional Network (CNN-TCN) model to extract spatio-temporal features, with recognition performance evaluated through classification. Experimental results demonstrate that this method achieves an accuracy rate of 98.2% in domain-specific recognition and maintains a consistently high recognition rate across different neural networks, showcasing exceptional recognition performance and robustness."
                ],
                "domain": [
                    "Gesture Recognition",
                    "Millimeter-Wave Radar",
                    "Convolutional Neural Networks",
                    "Intelligent Education"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "3f29275b-7020-4358-955b-04d1eb0531d2": {
                "pk": "3f29275b-7020-4358-955b-04d1eb0531d2",
                "project_name": null,
                "name": "Pheng Ann Heng",
                "bio": "I am a researcher dedicated to advancing the fields of medical image analysis, machine learning, and artificial intelligence. My work spans a variety of innovative approaches, including the development of a fully Bayesian nonparametric framework for dyadic data prediction, which integrates discrete mixed membership and continuous latent factor modeling. I have also pioneered methods for unpaired cross-modality image segmentation, leveraging shared network parameters to enhance segmentation accuracy across different imaging modalities.\n\nMy research addresses critical challenges in medical imaging, such as the scarcity of high-quality annotated data. I have proposed frameworks like the Multi-site Network (MS-Net) to improve prostate segmentation by effectively aggregating data from multiple sites while managing inter-site heterogeneity. Additionally, I have developed robust learning frameworks to tackle the issues of noisy labels in medical image classification, ensuring that our models can learn effectively even from imperfect data.\n\nI am particularly interested in the intersection of AI and drug discovery, where I introduced the Multimodal Pretraining DEL-Fusion model to enhance compound feature extraction from DNA-encoded libraries. My recent work also explores the potential of large language models in reasoning tasks, aiming to contribute to the development of artificial general intelligence.\n\nThrough my research, I strive to create impactful solutions that not only advance theoretical understanding but also translate into practical applications in healthcare and beyond. I am passionate about leveraging AI to improve diagnostic processes and enhance the quality of medical care.",
                "collaborators": [
                    "Qi Dou",
                    "Guangyong Chen",
                    "Quande Liu",
                    "Cheng Xue",
                    "Hao Chen",
                    "Xiaomeng Li",
                    "Lequan Yu",
                    "Cheng Chen",
                    "Jiaqi Xu",
                    "Jianye Hao"
                ],
                "pub_titles": [
                    "Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization",
                    "Unpaired Multi-modal Segmentation via Knowledge Distillation",
                    "MS-Net: Multi-Site Network for Improving Prostate Segmentation with Heterogeneous MRI Data",
                    "Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification",
                    "Unsupervised Bidirectional Cross-Modality Adaptation via Deeply Synergistic Image and Feature Alignment for Medical Image Segmentation",
                    "Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning",
                    "Cascaded Robust Learning at Imperfect Labels for Chest X-ray Segmentation",
                    "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
                    "Semi-supervised Medical Image Classification with Relation-driven Self-ensembling Model",
                    "SeaDAG: Semi-autoregressive Diffusion for Conditional Directed Acyclic Graph Generation",
                    "H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes",
                    "Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images",
                    "DLTTA: Dynamic Learning Rate for Test-time Adaptation on Cross-domain Medical Images",
                    "Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries",
                    "LHNN: Lattice Hypergraph Neural Network for VLSI Congestion Prediction",
                    "Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery",
                    "An Autonomous Large Language Model Agent for Chemical Literature Data Mining",
                    "Weakly-supervised Medical Image Segmentation with Gaze Annotations",
                    "A Survey of Reasoning with Foundation Models"
                ],
                "pub_abstracts": [
                    "Dyadic Data Prediction (DDP) is an important problem in many research areas. This paper develops a novel fully Bayesian nonparametric framework which integrates two popular and complementary approaches, discrete mixed membership modeling and continuous latent factor modeling into a unified Heterogeneous Matrix Factorization~(HeMF) model, which can predict the unobserved dyadics accurately. The HeMF can determine the number of communities automatically and exploit the latent linear structure for each bicluster efficiently. We propose a Variational Bayesian method to estimate the parameters and missing data. We further develop a novel online learning approach for Variational inference and use it for the online learning of HeMF, which can efficiently cope with the important large-scale DDP problem. We evaluate the performance of our method on the EachMoive, MovieLens and Netflix Prize collaborative filtering datasets. The experiment shows that, our model outperforms state-of-the-art methods on all benchmarks. Compared with Stochastic Gradient Method (SGD), our online learning approach achieves significant improvement on the estimation accuracy and robustness.",
                    "Multi-modal learning is typically performed with network architectures containing modality-specific layers and shared layers, utilizing co-registered images of different modalities. We propose a novel learning scheme for unpaired cross-modality image segmentation, with a highly compact architecture achieving superior segmentation accuracy. In our method, we heavily reuse network parameters, by sharing all convolutional kernels across CT and MRI, and only employ modality-specific internal normalization layers which compute respective statistics. To effectively train such a highly compact model, we introduce a novel loss term inspired by knowledge distillation, by explicitly constraining the KL-divergence of our derived prediction distributions between modalities. We have extensively validated our approach on two multi-class segmentation problems: i) cardiac structure segmentation, and ii) abdominal organ segmentation. Different network settings, i.e., 2D dilated network and 3D U-net, are utilized to investigate our method's general efficacy. Experimental results on both tasks demonstrate that our novel multi-modal learning scheme consistently outperforms single-modal training and previous multi-modal approaches.",
                    "Automated prostate segmentation in MRI is highly demanded for computer-assisted diagnosis. Recently, a variety of deep learning methods have achieved remarkable progress in this task, usually relying on large amounts of training data. Due to the nature of scarcity for medical images, it is important to effectively aggregate data from multiple sites for robust model training, to alleviate the insufficiency of single-site samples. However, the prostate MRIs from different sites present heterogeneity due to the differences in scanners and imaging protocols, raising challenges for effective ways of aggregating multi-site data for network training. In this paper, we propose a novel multi-site network (MS-Net) for improving prostate segmentation by learning robust representations, leveraging multiple sources of data. To compensate for the inter-site heterogeneity of different MRI datasets, we develop Domain-Specific Batch Normalization layers in the network backbone, enabling the network to estimate statistics and perform feature normalization for each site separately. Considering the difficulty of capturing the shared knowledge from multiple datasets, a novel learning paradigm, i.e., Multi-site-guided Knowledge Transfer, is proposed to enhance the kernels to extract more generic representations from multi-site data. Extensive experiments on three heterogeneous prostate MRI datasets demonstrate that our MS-Net improves the performance across all datasets consistently, and outperforms state-of-the-art methods for multi-site learning.",
                    "Deep neural networks (DNNs) have achieved great success in a wide variety of medical image analysis tasks. However, these achievements indispensably rely on the accurately-annotated datasets. If with the noisy-labeled images, the training procedure will immediately encounter difficulties, leading to a suboptimal classifier. This problem is even more crucial in the medical field, given that the annotation quality requires great expertise. In this paper, we propose an effective iterative learning framework for noisy-labeled medical image classification, to combat the lacking of high quality annotated medical data. Specifically, an online uncertainty sample mining method is proposed to eliminate the disturbance from noisy-labeled images. Next, we design a sample re-weighting strategy to preserve the usefulness of correctly-labeled hard samples. Our proposed method is validated on skin lesion classification task, and achieved very promising results.",
                    "Unsupervised domain adaptation has increasingly gained interest in medical image computing, aiming to tackle the performance degradation of deep neural networks when being deployed to unseen data with heterogeneous characteristics. In this work, we present a novel unsupervised domain adaptation framework, named as Synergistic Image and Feature Alignment (SIFA), to effectively adapt a segmentation network to an unlabeled target domain. Our proposed SIFA conducts synergistic alignment of domains from both image and feature perspectives. In particular, we simultaneously transform the appearance of images across domains and enhance domain-invariance of the extracted features by leveraging adversarial learning in multiple aspects and with a deeply supervised mechanism. The feature encoder is shared between both adaptive perspectives to leverage their mutual benefits via end-to-end learning. We have extensively evaluated our method with cardiac substructure segmentation and abdominal multi-organ segmentation for bidirectional cross-modality adaptation between MRI and CT images. Experimental results on two different tasks demonstrate that our SIFA method is effective in improving segmentation performance on unlabeled target images, and outperforms the state-of-the-art domain adaptation approaches by a large margin.",
                    "Fairness in recommendation has attracted increasing attention due to bias and discrimination possibly caused by traditional recommenders. In Interactive Recommender Systems (IRS), user preferences and the system's fairness status are constantly changing over time. Existing fairness-aware recommenders mainly consider fairness in static settings. Directly applying existing methods to IRS will result in poor recommendation. To resolve this problem, we propose a reinforcement learning based framework, FairRec, to dynamically maintain a long-term balance between accuracy and fairness in IRS. User preferences and the system's fairness status are jointly compressed into the state representation to generate recommendations. FairRec aims at maximizing our designed cumulative reward that combines accuracy and fairness. Extensive experiments validate that FairRec can improve fairness, while preserving good recommendation quality.",
                    "The superior performance of CNN on medical image analysis heavily depends on the annotation quality, such as the number of labeled image, the source of image, and the expert experience. The annotation requires great expertise and labour. To deal with the high inter-rater variability, the study of imperfect label has great significance in medical image segmentation tasks. In this paper, we present a novel cascaded robust learning framework for chest X-ray segmentation with imperfect annotation. Our model consists of three independent network, which can effectively learn useful information from the peer networks. The framework includes two stages. In the first stage, we select the clean annotated samples via a model committee setting, the networks are trained by minimizing a segmentation loss using the selected clean samples. In the second stage, we design a joint optimization framework with label correction to gradually correct the wrong annotation and improve the network performance. We conduct experiments on the public chest X-ray image datasets collected by Shenzhen Hospital. The results show that our methods could achieve a significant improvement on the accuracy in segmentation tasks compared to the previous methods.",
                    "While Large Language Models (LLMs) have demonstrated exceptional multitasking abilities, fine-tuning these models on downstream, domain-specific datasets is often necessary to yield superior performance on test sets compared to their counterparts without fine-tuning. However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood. This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants. Our primary investigation centers on whether fine-tuning affects the generalization ability intrinsic to LLMs. To elaborate on this, we conduct extensive experiments across five distinct language tasks on various datasets. Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks. Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability. Through this systematic investigation, we aim to contribute valuable insights into the evolving landscape of fine-tuning practices for LLMs.",
                    "Training deep neural networks usually requires a large amount of labeled data to obtain good performance. However, in medical image analysis, obtaining high-quality labels for the data is laborious and expensive, as accurately annotating medical images demands expertise knowledge of the clinicians. In this paper, we present a novel relation-driven semi-supervised framework for medical image classification. It is a consistency-based method which exploits the unlabeled data by encouraging the prediction consistency of given input under perturbations, and leverages a self-ensembling model to produce high-quality consistency targets for the unlabeled data. Considering that human diagnosis often refers to previous analogous cases to make reliable decisions, we introduce a novel sample relation consistency (SRC) paradigm to effectively exploit unlabeled data by modeling the relationship information among different samples. Superior to existing consistency-based methods which simply enforce consistency of individual predictions, our framework explicitly enforces the consistency of semantic relation among different samples under perturbations, encouraging the model to explore extra semantic information from unlabeled data. We have conducted extensive experiments to evaluate our method on two public benchmark medical image classification datasets, i.e.,skin lesion diagnosis with ISIC 2018 challenge and thorax disease classification with ChestX-ray14. Our method outperforms many state-of-the-art semi-supervised learning methods on both single-label and multi-label image classification scenarios.",
                    "We introduce SeaDAG, a semi-autoregressive diffusion model for conditional generation of Directed Acyclic Graphs (DAGs). Considering their inherent layer-wise structure, we simulate layer-wise autoregressive generation by designing different denoising speed for different layers. Unlike conventional autoregressive generation that lacks a global graph structure view, our method maintains a complete graph structure at each diffusion step, enabling operations such as property control that require the full graph structure. Leveraging this capability, we evaluate the DAG properties during training by employing a graph property decoder. We explicitly train the model to learn graph conditioning with a condition loss, which enhances the diffusion model's capacity to generate graphs that are both realistic and aligned with specified properties. We evaluate our method on two representative conditional DAG generation tasks: (1) circuit generation from truth tables, where precise DAG structures are crucial for realizing circuit functionality, and (2) molecule generation based on quantum properties. Our approach demonstrates promising results, generating high-quality and realistic DAGs that closely align with given conditions.",
                    "Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2D and 3D FCNs, serve as the back-bone in many volumetric image segmentation. However, 2D convolutions can not fully leverage the spatial information along the third dimension while 3D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2D DenseUNet for efficiently extracting intra-slice features and a 3D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of H-DenseUNet in an end-to-end manner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion (HFF) layer. We extensively evaluated our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.",
                    "Automatic breast lesion segmentation in ultrasound helps to diagnose breast cancer, which is one of the dreadful diseases that affect women globally. Segmenting breast regions accurately from ultrasound image is a challenging task due to the inherent speckle artifacts, blurry breast lesion boundaries, and inhomogeneous intensity distributions inside the breast lesion regions. Recently, convolutional neural networks (CNNs) have demonstrated remarkable results in medical image segmentation tasks. However, the convolutional operations in a CNN often focus on local regions, which suffer from limited capabilities in capturing long-range dependencies of the input ultrasound image, resulting in degraded breast lesion segmentation accuracy. In this paper, we develop a deep convolutional neural network equipped with a global guidance block (GGB) and breast lesion boundary detection (BD) modules for boosting the breast ultrasound lesion segmentation. The GGB utilizes the multi-layer integrated feature map as a guidance information to learn the long-range non-local dependencies from both spatial and channel domains. The BD modules learn additional breast lesion boundary map to enhance the boundary quality of a segmentation result refinement. Experimental results on a public dataset and a collected dataset show that our network outperforms other medical image segmentation methods and the recent semantic segmentation methods on breast ultrasound lesion segmentation. Moreover, we also show the application of our network on the ultrasound prostate segmentation, in which our method better identifies prostate regions than state-of-the-art networks.",
                    "Test-time adaptation (TTA) has increasingly been an important topic to efficiently tackle the cross-domain distribution shift at test time for medical images from different institutions. Previous TTA methods have a common limitation of using a fixed learning rate for all the test samples. Such a practice would be sub-optimal for TTA, because test data may arrive sequentially therefore the scale of distribution shift would change frequently. To address this problem, we propose a novel dynamic learning rate adjustment method for test-time adaptation, called DLTTA, which dynamically modulates the amount of weights update for each test image to account for the differences in their distribution shift. Specifically, our DLTTA is equipped with a memory bank based estimation scheme to effectively measure the discrepancy of a given test sample. Based on this estimated discrepancy, a dynamic learning rate adjustment strategy is then developed to achieve a suitable degree of adaptation for each test sample. The effectiveness and general applicability of our DLTTA is extensively demonstrated on three tasks including retinal optical coherence tomography (OCT) segmentation, histopathological image classification, and prostate 3D MRI segmentation. Our method achieves effective and fast test-time adaptation with consistent performance improvement over current state-of-the-art test-time adaptation methods. Code is available at: https://github.com/med-air/DLTTA.",
                    "In the realm of drug discovery, DNA-encoded library (DEL) screening technology has emerged as an efficient method for identifying high-affinity compounds. However, DEL screening faces a significant challenge: noise arising from nonspecific interactions within complex biological systems. Neural networks trained on DEL libraries have been employed to extract compound features, aiming to denoise the data and uncover potential binders to the desired therapeutic target. Nevertheless, the inherent structure of DEL, constrained by the limited diversity of building blocks, impacts the performance of compound encoders. Moreover, existing methods only capture compound features at a single level, further limiting the effectiveness of the denoising strategy. To mitigate these issues, we propose a Multimodal Pretraining DEL-Fusion model (MPDF) that enhances encoder capabilities through pretraining and integrates compound features across various scales. We develop pretraining tasks applying contrastive objectives between different compound representations and their text descriptions, enhancing the compound encoders' ability to acquire generic features. Furthermore, we propose a novel DEL-fusion framework that amalgamates compound information at the atomic, submolecular, and molecular levels, as captured by various compound encoders. The synergy of these innovations equips MPDF with enriched, multi-scale features, enabling comprehensive downstream denoising. Evaluated on three DEL datasets, MPDF demonstrates superior performance in data processing and analysis for validation tasks. Notably, MPDF offers novel insights into identifying high-affinity molecules, paving the way for improved DEL utility in drug discovery.",
                    "Precise congestion prediction from a placement solution plays a crucial role in circuit placement. This work proposes the lattice hypergraph (LH-graph), a novel graph formulation for circuits, which preserves netlist data during the whole learning process, and enables the congestion information propagated geometrically and topologically. Based on the formulation, we further developed a heterogeneous graph neural network architecture LHNN, jointing the routing demand regression to support the congestion spot classification. LHNN constantly achieves more than 35% improvements compared with U-nets and Pix2Pix on the F1 score. We expect our work shall highlight essential procedures using machine learning for congestion prediction.",
                    "Automatic surgical gesture recognition is fundamentally important to enable intelligent cognitive assistance in robotic surgery. With recent advancement in robot-assisted minimally invasive surgery, rich information including surgical videos and robotic kinematics can be recorded, which provide complementary knowledge for understanding surgical gestures. However, existing methods either solely adopt uni-modal data or directly concatenate multi-modal representations, which can not sufficiently exploit the informative correlations inherent in visual and kinematics data to boost gesture recognition accuracies. In this regard, we propose a novel online approach of multi-modal relational graph network (i.e., MRG-Net) to dynamically integrate visual and kinematics information through interactive message propagation in the latent feature space. In specific, we first extract embeddings from video and kinematics sequences with temporal convolutional networks and LSTM units. Next, we identify multi-relations in these multi-modal embeddings and leverage them through a hierarchical relational graph learning module. The effectiveness of our method is demonstrated with state-of-the-art results on the public JIGSAWS dataset, outperforming current uni-modal and multi-modal methods on both suturing and knot typing tasks. Furthermore, we validated our method on in-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK) platforms in two centers, with consistent promising performance achieved.",
                    "Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency. The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry.",
                    "Eye gaze that reveals human observational patterns has increasingly been incorporated into solutions for vision tasks. Despite recent explorations on leveraging gaze to aid deep networks, few studies exploit gaze as an efficient annotation approach for medical image segmentation which typically entails heavy annotating costs. In this paper, we propose to collect dense weak supervision for medical image segmentation with a gaze annotation scheme. To train with gaze, we propose a multi-level framework that trains multiple networks from discriminative human attention, simulated with a set of pseudo-masks derived by applying hierarchical thresholds on gaze heatmaps. Furthermore, to mitigate gaze noise, a cross-level consistency is exploited to regularize overfitting noisy labels, steering models toward clean patterns learned by peer networks. The proposed method is validated on two public medical datasets of polyp and prostate segmentation tasks. We contribute a high-quality gaze dataset entitled GazeMedSeg as an extension to the popular medical segmentation datasets. To the best of our knowledge, this is the first gaze dataset for medical image segmentation. Our experiments demonstrate that gaze annotation outperforms previous label-efficient annotation schemes in terms of both performance and annotation time. Our collected gaze data and code are available at: https://github.com/med-air/GazeMedSeg.",
                    "Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI."
                ],
                "domain": [
                    "Medical Image Analysis",
                    "Multi-modal Learning",
                    "Graph Neural Network",
                    "Automated Machine Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            },
            "576fafb4-fcf6-450b-8154-b2a39ab76503": {
                "pk": "576fafb4-fcf6-450b-8154-b2a39ab76503",
                "project_name": null,
                "name": "Guangyong Chen",
                "bio": "I am a researcher dedicated to advancing the fields of deep learning and machine learning, with a particular focus on addressing challenges posed by noisy labels, multi-task learning, and the robustness of neural networks. My recent work has explored the impact of label noise on model performance, leading to the development of innovative strategies such as Co-teaching and Paired Softmax Divergence Regularization (PSDR) to enhance the robustness of deep neural networks against noisy labels. \n\nI have also delved into the intricacies of multi-class classification under class-conditional label noise, demonstrating that noisy validation sets can still yield reliable model selection. My research extends to the realm of graph neural networks (GNNs), where I have proposed Information Maximizing Graph Neural Networks (IGNN) to optimize the mutual information between edge states and transformation parameters, achieving state-of-the-art results in various tasks.\n\nAdditionally, I have tackled the item cold-start problem in collaborative filtering using Wasserstein distance, and I have developed a novel blind image denoising algorithm that adapts to unknown noise models. My work on few-shot learning in medicinal chemistry through the MetaRF model showcases my commitment to applying machine learning techniques to real-world problems.\n\nOverall, my research aims to bridge theoretical insights with practical applications, enhancing the performance and reliability of machine learning models across diverse domains. I am passionate about pushing the boundaries of what is possible in AI and contributing to the development of robust, efficient, and fair machine learning systems.",
                "collaborators": [
                    "Pengfei Chen",
                    "Pheng-Ann Heng",
                    "Benben Liao",
                    "Shengyu Zhang",
                    "Jianye Hao",
                    "Weiwen Liu",
                    "Fengyuan Zhu",
                    "Pheng Ann Heng",
                    "Chang-Yu Hsieh",
                    "Junjie Ye"
                ],
                "pub_titles": [
                    "Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels",
                    "A Meta Approach to Defend Noisy Labels by the Manifold Regularizer PSDR",
                    "Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization",
                    "Blind Image Denoising via Dependent Dirichlet Process Tree",
                    "Wasserstein Collaborative Filtering for Item Cold-start Recommendation",
                    "Variable Projection Algorithms: Theoretical Insights and A Novel Approach for Problems with Large Residual",
                    "Utilizing Edge Features in Graph Neural Networks via Variational Information Maximization",
                    "Robustness of Accuracy Metric and its Inspirations in Learning with Noisy Labels",
                    "Multi-Task Mixture Density Graph Neural Networks for Predicting Cu-based Single-Atom Alloy Catalysts for CO2 Reduction Reaction",
                    "Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks",
                    "Beyond Class-Conditional Assumption: A Primary Attempt to Combat Instance-Dependent Label Noise",
                    "MetaRF: Differentiable Random Forest for Reaction Yield Prediction with a Few Trails",
                    "Disentangling Dynamics and Returns: Value Function Decomposition with Future Prediction",
                    "Acknowledging the Unknown for Multi-label Learning with Single Positive Labels",
                    "Text-Anchored Score Composition: Tackling Condition Misalignment in Text-to-Image Diffusion Models",
                    "Understanding Adversarial Behavior of DNNs by Disentangling Non-Robust and Robust Components in Performance Metric",
                    "Spectral-based Graph Convolutional Network for Directed Graphs",
                    "Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning",
                    "Flattening Sharpness for Dynamic Gradient Projection Memory Benefits Continual Learning"
                ],
                "pub_abstracts": [
                    "Noisy labels are ubiquitous in real-world datasets, which poses a challenge for robustly training deep neural networks (DNNs) as DNNs usually have the high capacity to memorize the noisy labels. In this paper, we find that the test accuracy can be quantitatively characterized in terms of the noise ratio in datasets. In particular, the test accuracy is a quadratic function of the noise ratio in the case of symmetric noise, which explains the experimental findings previously published. Based on our analysis, we apply cross-validation to randomly split noisy datasets, which identifies most samples that have correct labels. Then we adopt the Co-teaching strategy which takes full advantage of the identified samples to train DNNs robustly against noisy labels. Compared with extensive state-of-the-art methods, our strategy consistently improves the generalization performance of DNNs under both synthetic and real-world training noise.",
                    "Noisy labels are ubiquitous in real-world datasets, which poses a challenge for robustly training deep neural networks (DNNs) since DNNs can easily overfit to the noisy labels. Most recent efforts have been devoted to defending noisy labels by discarding noisy samples from the training set or assigning weights to training samples, where the weight associated with a noisy sample is expected to be small. Thereby, these previous efforts result in a waste of samples, especially those assigned with small weights. The input $x$ is always useful regardless of whether its observed label $y$ is clean. To make full use of all samples, we introduce a manifold regularizer, named as Paired Softmax Divergence Regularization (PSDR), to penalize the Kullback-Leibler (KL) divergence between softmax outputs of similar inputs. In particular, similar inputs can be effectively generated by data augmentation. PSDR can be easily implemented on any type of DNNs to improve the robustness against noisy labels. As empirically demonstrated on benchmark datasets, our PSDR impressively improve state-of-the-art results by a significant margin.",
                    "Dyadic Data Prediction (DDP) is an important problem in many research areas. This paper develops a novel fully Bayesian nonparametric framework which integrates two popular and complementary approaches, discrete mixed membership modeling and continuous latent factor modeling into a unified Heterogeneous Matrix Factorization~(HeMF) model, which can predict the unobserved dyadics accurately. The HeMF can determine the number of communities automatically and exploit the latent linear structure for each bicluster efficiently. We propose a Variational Bayesian method to estimate the parameters and missing data. We further develop a novel online learning approach for Variational inference and use it for the online learning of HeMF, which can efficiently cope with the important large-scale DDP problem. We evaluate the performance of our method on the EachMoive, MovieLens and Netflix Prize collaborative filtering datasets. The experiment shows that, our model outperforms state-of-the-art methods on all benchmarks. Compared with Stochastic Gradient Method (SGD), our online learning approach achieves significant improvement on the estimation accuracy and robustness.",
                    "Most existing image denoising approaches assumed the noise to be homogeneous white Gaussian distributed with known intensity. However, in real noisy images, the noise models are usually unknown beforehand and can be much more complex. This paper addresses this problem and proposes a novel blind image denoising algorithm to recover the clean image from noisy one with the unknown noise model. To model the empirical noise of an image, our method introduces the mixture of Gaussian distribution, which is flexible enough to approximate different continuous distributions. The problem of blind image denoising is reformulated as a learning problem. The procedure is to first build a two-layer structural model for noisy patches and consider the clean ones as latent variable. To control the complexity of the noisy patch model, this work proposes a novel Bayesian nonparametric prior called \"Dependent Dirichlet Process Tree\" to build the model. Then, this study derives a variational inference algorithm to estimate model parameters and recover clean patches. We apply our method on synthesis and real noisy images with different noise models. Comparing with previous approaches, ours achieves better performance. The experimental results indicate the efficiency of the proposed algorithm to cope with practical image denoising tasks.",
                    "The item cold-start problem seriously limits the recommendation performance of Collaborative Filtering (CF) methods when new items have either none or very little interactions. To solve this issue, many modern Internet applications propose to predict a new item's interaction from the possessing contents. However, it is difficult to design and learn a map between the item's interaction history and the corresponding contents. In this paper, we apply the Wasserstein distance to address the item cold-start problem. Given item content information, we can calculate the similarity between the interacted items and cold-start ones, so that a user's preference on cold-start items can be inferred by minimizing the Wasserstein distance between the distributions over these two types of items. We further adopt the idea of CF and propose Wasserstein CF (WCF) to improve the recommendation performance on cold-start items. Experimental results demonstrate the superiority of WCF over state-of-the-art approaches.",
                    "This paper delves into an in-depth exploration of the Variable Projection (VP) algorithm, a powerful tool for solving separable nonlinear optimization problems across multiple domains, including system identification, image processing, and machine learning. We first establish a theoretical framework to examine the effect of the approximate treatment of the coupling relationship among parameters on the local convergence of the VP algorithm and theoretically prove that the Kaufman's VP algorithm can achieve a similar convergence rate as the Golub \\& Pereyra's form. These studies fill the gap in the existing convergence theory analysis, and provide a solid foundation for understanding the mechanism of VP algorithm and broadening its application horizons. Furthermore, drawing inspiration from these theoretical revelations, we design a refined VP algorithm for handling separable nonlinear optimization problems characterized by large residual, called VPLR, which boosts the convergence performance by addressing the interdependence of parameters within the separable model and by continually correcting the approximated Hessian matrix to counteract the influence of large residual during the iterative process. The effectiveness of this refined algorithm is corroborated through numerical experimentation.",
                    "Graph Neural Networks (GNNs) achieve an impressive performance on structured graphs by recursively updating the representation vector of each node based on its neighbors, during which parameterized transformation matrices should be learned for the node feature updating. However, existing propagation schemes are far from being optimal since they do not fully utilize the relational information between nodes. We propose the information maximizing graph neural networks (IGNN), which maximizes the mutual information between edge states and transform parameters. We reformulate the mutual information as a differentiable objective via a variational approach. We compare our model against several recent variants of GNNs and show that our model achieves the state-of-the-art performance on multiple tasks including quantum chemistry regression on QM9 dataset, generalization capability from QM9 to larger molecular graphs, and prediction of molecular bioactivities relevant for drug discovery. The IGNN model is based on an elegant and fundamental idea in information theory as explained in the main text, and it could be easily generalized beyond the contexts of molecular graphs considered in this work. To encourage more future work in this area, all datasets and codes used in this paper will be released for public access.",
                    "For multi-class classification under class-conditional label noise, we prove that the accuracy metric itself can be robust. We concretize this finding's inspiration in two essential aspects: training and validation, with which we address critical issues in learning with noisy labels. For training, we show that maximizing training accuracy on sufficiently many noisy samples yields an approximately optimal classifier. For validation, we prove that a noisy validation set is reliable, addressing the critical demand of model selection in scenarios like hyperparameter-tuning and early stopping. Previously, model selection using noisy validation samples has not been theoretically justified. We verify our theoretical results and additional claims with extensive experiments. We show characterizations of models trained with noisy labels, motivated by our theoretical results, and verify the utility of a noisy validation set by showing the impressive performance of a framework termed noisy best teacher and student (NTS). Our code is released.",
                    "Graph neural networks (GNNs) have drawn more and more attention from material scientists and demonstrated a high capacity to establish connections between the structure and properties. However, with only unrelaxed structures provided as input, few GNN models can predict the thermodynamic properties of relaxed configurations with an acceptable level of error. In this work, we develop a multi-task (MT) architecture based on DimeNet++ and mixture density networks to improve the performance of such task. Taking CO adsorption on Cu-based single-atom alloy catalysts as an illustration, we show that our method can reliably estimate CO adsorption energy with a mean absolute error of 0.087 eV from the initial CO adsorption structures without costly first-principles calculations. Further, compared to other state-of-the-art GNN methods, our model exhibits improved generalization ability when predicting catalytic performance of out-of-domain configurations, built with either unseen substrate surfaces or doping species. We show that the proposed MT GNN strategy can facilitate catalyst discovery.",
                    "In this work, we propose a novel technique to boost training efficiency of a neural network. Our work is based on an excellent idea that whitening the inputs of neural networks can achieve a fast convergence speed. Given the well-known fact that independent components must be whitened, we introduce a novel Independent-Component (IC) layer before each weight layer, whose inputs would be made more independent. However, determining independent components is a computationally intensive task. To overcome this challenge, we propose to implement an IC layer by combining two popular techniques, Batch Normalization and Dropout, in a new manner that we can rigorously prove that Dropout can quadratically reduce the mutual information and linearly reduce the correlation between any pair of neurons with respect to the dropout layer parameter $p$. As demonstrated experimentally, the IC layer consistently outperforms the baseline approaches with more stable training process, faster convergence speed and better convergence limit on CIFAR10/100 and ILSVRC2012 datasets. The implementation of our IC layer makes us rethink the common practices in the design of neural networks. For example, we should not place Batch Normalization before ReLU since the non-negative responses of ReLU will make the weight layer updated in a suboptimal way, and we can achieve better performance by combining Batch Normalization and Dropout together as an IC layer.",
                    "Supervised learning under label noise has seen numerous advances recently, while existing theoretical findings and empirical results broadly build up on the class-conditional noise (CCN) assumption that the noise is independent of input features given the true label. In this work, we present a theoretical hypothesis testing and prove that noise in real-world dataset is unlikely to be CCN, which confirms that label noise should depend on the instance and justifies the urgent need to go beyond the CCN assumption.The theoretical results motivate us to study the more general and practical-relevant instance-dependent noise (IDN). To stimulate the development of theory and methodology on IDN, we formalize an algorithm to generate controllable IDN and present both theoretical and empirical evidence to show that IDN is semantically meaningful and challenging. As a primary attempt to combat IDN, we present a tiny algorithm termed self-evolution average label (SEAL), which not only stands out under IDN with various noise fractions, but also improves the generalization on real-world noise benchmark Clothing1M. Our code is released. Notably, our theoretical analysis in Section 2 provides rigorous motivations for studying IDN, which is an important topic that deserves more research attention in future.",
                    "Artificial intelligence has deeply revolutionized the field of medicinal chemistry with many impressive applications, but the success of these applications requires a massive amount of training samples with high-quality annotations, which seriously limits the wide usage of data-driven methods. In this paper, we focus on the reaction yield prediction problem, which assists chemists in selecting high-yield reactions in a new chemical space only with a few experimental trials. To attack this challenge, we first put forth MetaRF, an attention-based differentiable random forest model specially designed for the few-shot yield prediction, where the attention weight of a random forest is automatically optimized by the meta-learning framework and can be quickly adapted to predict the performance of new reagents while given a few additional samples. To improve the few-shot learning performance, we further introduce a dimension-reduction based sampling method to determine valuable samples to be experimentally tested and then learned. Our methodology is evaluated on three different datasets and acquires satisfactory performance on few-shot prediction. In high-throughput experimentation (HTE) datasets, the average yield of our methodology's top 10 high-yield reactions is relatively close to the results of ideal yield selection.",
                    "Value functions are crucial for model-free Reinforcement Learning (RL) to obtain a policy implicitly or guide the policy updates. Value estimation heavily depends on the stochasticity of environmental dynamics and the quality of reward signals. In this paper, we propose a two-step understanding of value estimation from the perspective of future prediction, through decomposing the value function into a reward-independent future dynamics part and a policy-independent trajectory return part. We then derive a practical deep RL algorithm from the above decomposition, consisting of a convolutional trajectory representation model, a conditional variational dynamics model to predict the expected representation of future trajectory and a convex trajectory return model that maps a trajectory representation to its return. Our algorithm is evaluated in MuJoCo continuous control tasks and shows superior results under both common settings and delayed reward settings.",
                    "Due to the difficulty of collecting exhaustive multi-label annotations, multi-label datasets often contain partial labels. We consider an extreme of this weakly supervised learning problem, called single positive multi-label learning (SPML), where each multi-label training image has only one positive label. Traditionally, all unannotated labels are assumed as negative labels in SPML, which introduces false negative labels and causes model training to be dominated by assumed negative labels. In this work, we choose to treat all unannotated labels from an alternative perspective, i.e. acknowledging they are unknown. Hence, we propose entropy-maximization (EM) loss to attain a special gradient regime for providing proper supervision signals. Moreover, we propose asymmetric pseudo-labeling (APL), which adopts asymmetric-tolerance strategies and a self-paced procedure, to cooperate with EM loss and then provide more precise supervision. Experiments show that our method significantly improves performance and achieves state-of-the-art results on all four benchmarks. Code is available at https://github.com/Correr-Zhou/SPML-AckTheUnknown.",
                    "Text-to-image diffusion models have advanced towards more controllable generation via supporting various additional conditions (e.g.,depth map, bounding box) beyond text. However, these models are learned based on the premise of perfect alignment between the text and extra conditions. If this alignment is not satisfied, the final output could be either dominated by one condition, or ambiguity may arise, failing to meet user expectations. To address this issue, we present a training free approach called Text-Anchored Score Composition (TASC) to further improve the controllability of existing models when provided with partially aligned conditions. The TASC firstly separates conditions based on pair relationships, computing the result individually for each pair. This ensures that each pair no longer has conflicting conditions. Then we propose an attention realignment operation to realign these independently calculated results via a cross-attention mechanism to avoid new conflicts when combining them back. Both qualitative and quantitative results demonstrate the effectiveness of our approach in handling unaligned conditions, which performs favorably against recent methods and more importantly adds flexibility to the controllable image generation process. Our code will be available at: https://github.com/EnVision-Research/Decompose-and-Realign.",
                    "The vulnerability to slight input perturbations is a worrying yet intriguing property of deep neural networks (DNNs). Despite many previous works studying the reason behind such adversarial behavior, the relationship between the generalization performance and adversarial behavior of DNNs is still little understood. In this work, we reveal such relation by introducing a metric characterizing the generalization performance of a DNN. The metric can be disentangled into an information-theoretic non-robust component, responsible for adversarial behavior, and a robust component. Then, we show by experiments that current DNNs rely heavily on optimizing the non-robust component in achieving decent performance. We also demonstrate that current state-of-the-art adversarial training algorithms indeed try to robustify the DNNs by preventing them from using the non-robust component to distinguish samples from different categories. Also, based on our findings, we take a step forward and point out the possible direction for achieving decent standard performance and adversarial robustness simultaneously. We believe that our theory could further inspire the community to make more interesting discoveries about the relationship between standard generalization and adversarial generalization of deep learning models.",
                    "Graph convolutional networks(GCNs) have become the most popular approaches for graph data in these days because of their powerful ability to extract features from graph. GCNs approaches are divided into two categories, spectral-based and spatial-based. As the earliest convolutional networks for graph data, spectral-based GCNs have achieved impressive results in many graph related analytics tasks. However, spectral-based models cannot directly work on directed graphs. In this paper, we propose an improved spectral-based GCN for the directed graph by leveraging redefined Laplacians to improve its propagation model. Our approach can work directly on directed graph data in semi-supervised nodes classification tasks. Experiments on a number of directed graph datasets demonstrate that our approach outperforms the state-of-the-art methods.",
                    "Fairness in recommendation has attracted increasing attention due to bias and discrimination possibly caused by traditional recommenders. In Interactive Recommender Systems (IRS), user preferences and the system's fairness status are constantly changing over time. Existing fairness-aware recommenders mainly consider fairness in static settings. Directly applying existing methods to IRS will result in poor recommendation. To resolve this problem, we propose a reinforcement learning based framework, FairRec, to dynamically maintain a long-term balance between accuracy and fairness in IRS. User preferences and the system's fairness status are jointly compressed into the state representation to generate recommendations. FairRec aims at maximizing our designed cumulative reward that combines accuracy and fairness. Extensive experiments validate that FairRec can improve fairness, while preserving good recommendation quality.",
                    "The backpropagation networks are notably susceptible to catastrophic forgetting, where networks tend to forget previously learned skills upon learning new ones. To address such the 'sensitivity-stability' dilemma, most previous efforts have been contributed to minimizing the empirical risk with different parameter regularization terms and episodic memory, but rarely exploring the usages of the weight loss landscape. In this paper, we investigate the relationship between the weight loss landscape and sensitivity-stability in the continual learning scenario, based on which, we propose a novel method, Flattening Sharpness for Dynamic Gradient Projection Memory (FS-DGPM). In particular, we introduce a soft weight to represent the importance of each basis representing past tasks in GPM, which can be adaptively learned during the learning process, so that less important bases can be dynamically released to improve the sensitivity of new skill learning. We further introduce Flattening Sharpness (FS) to reduce the generalization gap by explicitly regulating the flatness of the weight loss landscape of all seen tasks. As demonstrated empirically, our proposed method consistently outperforms baselines with the superior ability to learn new skills while alleviating forgetting effectively."
                ],
                "domain": [
                    "Deep Learning",
                    "Noisy Label Robustness",
                    "Graph Neural Network",
                    "Reinforcement Learning"
                ],
                "institute": null,
                "embed": null,
                "is_leader_candidate": true,
                "is_member_candidate": true,
                "is_reviewer_candidate": true,
                "is_chair_candidate": true
            }
        },
        "reference_proposal": "**[Question 1] - What is the problem?**  \nHow can retrieval-augmented generative AI (RAG-AI) be effectively utilized to enhance the recommendation of reaction conditions in chemical synthesis, particularly for unfamiliar molecules?\n\n**[Question 2] - Why is it interesting and important?**  \nSolving this problem is crucial for advancing the field of AI in chemistry, as it could lead to the development of fully autonomous chemical synthesis platforms that outperform traditional AI systems. By enabling AI to access and analyze up-to-date chemical literature, researchers can significantly improve the efficiency and accuracy of reaction condition recommendations. This advancement could not only streamline the synthetic process but also foster innovation in the discovery of new compounds, ultimately impacting pharmaceuticals, materials science, and other chemical industries.\n\n**[Question 3] - Why is it hard?**  \nThe challenges in addressing this problem stem from the complexity of chemical reactions and the vastness of chemical knowledge. Traditional AI models struggle with generalization when faced with novel reactions due to their reliance on static training data. Naive approaches may fail because they cannot dynamically incorporate new information or adapt to the unique characteristics of unfamiliar molecules. Overcoming these technical obstacles requires sophisticated integration of retrieval mechanisms with generative models, as well as the ability to effectively analyze and synthesize information from diverse sources.\n\n**[Question 4] - Why hasn't it been solved before?**  \nPrevious research has largely focused on either retrieval-based methods or generative models in isolation, failing to combine their strengths effectively. Limitations in computational resources, the complexity of chemical data, and a lack of comprehensive datasets have also hindered progress. Additionally, existing solutions have not adequately addressed the need for real-time updates from the literature, which is essential for tackling unfamiliar reactions. Our approach differs by integrating RAG-AI technology to create a more dynamic and responsive system that can continuously learn and adapt.\n\n**[Question 5] - What are the key components of my approach and results?**  \nOur proposed methodology involves developing the Chemist-X agent, which utilizes a three-phase approach for reaction condition recommendation (RCR): 1) identifying structurally analogous molecules, 2) reviewing relevant literature to refine potential reaction conditions, and 3) employing Computer-Aided Design (CAD) technology to select optimal conditions. We will use a diverse dataset of chemical reactions and existing literature, measuring success through metrics such as accuracy of recommendations and efficiency in identifying optimal conditions. The expected outcome is a robust AI agent capable of providing near-human-level recommendations for reaction conditions"
    }
}