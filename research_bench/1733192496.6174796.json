{
    "prompt": [
        {
            "role": "user",
            "content": "Here is your profile:\nI am a researcher dedicated to advancing the field of robot learning, with a focus on developing generalist policies that can adapt to a wide range of tasks and robotic platforms. My work addresses the critical challenges of data efficiency, generalization, and robustness in robotic systems. I have pioneered several innovative approaches, including the design of flow matching architectures that leverage pre-trained vision-language models to enhance robot capabilities in complex tasks like laundry folding and assembly.\n\nMy recent contributions include the development of Octo, a transformer-based policy trained on the largest robot manipulation dataset to date, which demonstrates remarkable versatility across multiple robotic platforms. I have also explored the integration of categorical cross-entropy for training value functions in reinforcement learning, significantly improving scalability and performance across various domains.\n\nIn addition, I have created SIMPLER, a collection of simulated environments that facilitate reliable evaluation of manipulation policies, and I introduced Embodied Chain-of-Thought Reasoning (ECoT) to enhance the reasoning capabilities of vision-language-action models in robotics. My research emphasizes the importance of leveraging diverse datasets and the potential of large, pre-trained models to transform how robots learn and adapt to new environments.\n\nThrough my work, I aim to bridge the gap between simulation and real-world applications, enabling robots to learn from both human feedback and autonomous data collection. I am committed to open-sourcing my findings and tools to foster collaboration and innovation in the robotics community.\nHere is the paper:\n1:\n\nABSTRACT\nFleets of robots ingest massive amounts of heterogeneous streaming data silos\ngenerated by interacting with their environments, far more than what can be stored\nor transmitted with ease. At the same time, teams of robots should co-acquire\ndiverse skills through their heterogeneous experiences in varied settings. How\ncan we enable such fleet-level learning without having to transmit orcentralize\nfleet-scale data? In this paper, we investigate policy merging (PoMe) from such\ndistributed heterogeneous datasets as a potential solution. To efficiently merge\npolicies in the fleet setting, we propose FLEET -MERGE , an instantiation of dis-\ntributed learning that accounts for the permutation invariance that arises when\nparameterizing the control policies with recurrent neural networks. We show that\nFLEET -MERGE consolidates the behavior of policies trained on 50 tasks in the\nMeta-World environment, with good performance on nearly all training tasks at test\ntime. Moreover, we introduce a novel robotic tool-use benchmark, FLEET -TOOLS ,\nfor fleet policy learning in compositional and contact-rich robot manipulation tasks,\nto validate the efficacy of F LEET -MERGE on the benchmark.1\n1 I NTRODUCTION\nWith the fast-growing scale of robot fleets deployed in the real world, learning policies from the\ndiverse datasets collected by the fleet (Osa et al., 2018; Bagnell, 2015; Kumar et al., 2020; 2022)\nbecomes an increasingly promising approach to training sophisticated and generalizable robotic\nagents (Jang et al., 2022; Levine et al., 2020). We hope that both the magnitude of the data \u2014\nstreamed and actively generated by robots interacting with their surroundings \u2014 and the diversity of\nthe environments and tasks around which the data are collected, will allow robot fleets to acquire rich\nand varied sets of skills. However, the data heterogeneity and total amount of the data are becoming\nas much of a challenge as a benefit. Real-world robot deployments often run on devices with real-time\nconstraints and limited network bandwidth, while generating inordinate volumes of data such as video\nstreams. Hence, a \u201ctop-down\u201d scheme of centralizing these data (Grauman et al., 2022; Collaboration,\n2023), and training a single policy to handle all the diverse tasks, can be computationally prohibitive\nand violate real-world communication constraints. At the same time, we wish to consolidate the skills\neach robot acquires after being trained on its local datasets via various off-the-shelf robot learning\napproaches. Thus, it is natural to ask: How can the entire fleet efficiently acquire diverse skills,\nwithout having to transmit the massive amount of heterogeneous data that is generated constantly in\nsilos, when each one of the robots has learned some skills from its own interactions?\nTo answer this question, we propose policy merging (Figure 1), PoMe , a \u201cbottom-up\u201d approach for\nfleet policy learning from multiple datasets. Specifically, we consider neural-network-parameterized\npolicies that are already trained separately on different datasets and tasks, and seek to merge their\nweights to form one single policy, while preserving the learned skills of the original policies. Policy\nmerging acquires skills efficiently with drastically reduced communication costs, by transmitting\nonly the trained weights of neural networks but not the training data. Such a bottom-up merging\nscheme is agnostic to and thus compatible with any local training approaches used in practice.\nMerging the weights of neural networks has been studied in various contexts, including finetuning\nfoundation models (Wortsman et al., 2022a;b), multi-task learning (He et al., 2018; Stoica et al.,\n2023), and the investigation of linear mode connectivity hypothesis for (feedforward) neural networks\n(Frankle et al., 2020; Entezari et al., 2021; Ainsworth et al., 2022). Distributed and federated learning\n1Code is available at https://github.com/liruiw/Fleet-Tools.\n1arXiv:2310.01362v3  [cs.RO]  23 Feb 2024Published as a conference paper at ICLR 2024\n\ud835\udc37!\nLocal Training and Policy Merging\n\u2026.\ud835\udc37\"#!\ud835\udc37\"Data Pooling and Centralized Training\ud835\udc37$~MB/day~TB/day\n\ud835\udf0b!\ud835\udf0b$\ud835\udf0b\"#!\ud835\udf0b\"\nFigure 1: We consider the problem of merging multiple policies trained on potentially distinct and\ndiverse tasks, which can be more computation and communication efficient than pooling all data\ntogether for joint training. Instead of acquiring astronomical size of data from the top-down (red arrow,\nrequiring terabytes-per-day worth of data transfer), we demonstrate that the bottom-up approach\n(green arrow, megabytes-per-day): merging from locally trained policies, can also produce general\npolicies that incorporate skills learned by the individual constituent policies. Moreover, local training\nand sharing weights are more suitable for agents that actively generate data, which is especially the\ncase in robotic control, and are more efficient in communicating with the other agents. We aim to\nachieve the following objective in fleet learning: One robot learns, the entire fleet learns .\n(McMahan et al., 2017; Wang et al., 2020a; Kone \u02c7cn`y et al., 2016), which iteratively update central\nmodel weights from decentralized updates, can be viewed as an iterative form of model merging;\nthese approaches have achieved tremendous success in learning from diverse datasets, especially in\nsolving supervised learning tasks (McMahan et al., 2017; Wang et al., 2020a; Mansour et al., 2020).\nHowever, such an approach has not yet demonstrated its power in solving robot learning and control\ntasks, which are generally more challenging due to their dynamic andsequential nature, and the\nrichness of the tasks and environments. Notably, with the commonly used sensors in robot learning,\ne.g., cameras, one has to handle the partial observability in learning such visuomotor policies (Levine\net al., 2016), which necessitates the use of latent state in parameterizing the policies, and is usually\ninstantiated by recurrent neural networks (RNNs) (Elman, 1990). Indeed, policies with latent state\ndynamics are known to be theoretically necessary for partially-observed linear control problems\n(Bertsekas, 2012), and outperform policies parameterized by stateless policies in practice, i.e., those\nusing feedforward neural networks, in perception-based robotic tasks (Andrychowicz et al., 2020).\nIn this work, we address policy merging in robotic fleet learning, with a focus on RNN-parameterized\npolicies. A naive approach of averaging the weights would easily fail because multiple configurations\nof network weights parametrize the same function. One reason behind this is the known permutation\ninvariance of neural networks, i.e., one can swap any two units of a hidden layer in a network,\nwithout changing its functionality (Hecht-Nielsen, 1990; Entezari et al., 2021). We have to account\nfor such invariance in merging multiple policies. Indeed, such a fact has been accounted for recently\nin merging the weights of trained neural networks, with extensive focuses on aligning the weights of\nfeedforward neural networks, and solving supervised learning tasks (Entezari et al., 2021; Ainsworth\net al., 2022; Pe\u00f1a et al., 2022). We generalize these insights to the RNN setting, where permutation\nsymmetries not only appear between layers , but also between timesteps , in solving robotic control\ntasks. Compared with one of the few federated learning\n\n2:\n\nmethods, by accounting\nfor the similarity invariance in merging, outperform the naive averaging method ( average ). They\nalso outperform single-task , where policies are only trained with local datasets, since more data has\nbeen (implicitly) used by merging multiple policies. More interestingly, when there are disconnected\ncomponents in the landscape (as constructed in odd-dimensional systems, see more details in the next\nsubsections), we observe that only searching for permutation will not mitigate the issues.\nG.6.2 G RADIENT -BASED APPROACH V .S. LQG L ANDSCAPE\nIn this subsection, we reconcile the effectiveness of gradient-based policy alignment with the topo-\nlogical properties of the the LQG cost landscape. First, recall that we parameterize controllers by\n\u03c0= (A\u03b8,B\u03b8,C\u03b8), where A\u03b8maintains a latent state \u02c6xof the same dimension as the system state.\nLetCndenote the set of policies \u03c0= (A\u03b8,B\u03b8,C\u03b8)such that the cost J(\u03c0)is finite, or equivalently,\nthe set of controllers which stabilize the dynamics (Zhou et al., 1996). Zheng et al. (2021) show that\nthis set is either (a) fully connected, or (b) disconnected, but has only two connected components\nC(1)\nn,C(2)\nn. Case (a) occurs if either\n(i)theopen loop system is stable, i.e. the spectral radius \u03c1(A)<1, that is i.e. maximum\neigenvalue magnitude) of the system dynamic matrix is strictly less than 1. This is equivalent\nto saying that JT(0)is finite for the zero controller (A\u03b8,B\u03b8,C\u03b8) = (0,0,0).\n(ii) there exists a policy \u03c0with rank (A\u03b8)< n such that \u03c0\u2208 Cn.\nAnd, in case (b), Zheng et al. (2021, Theorem 3.2) shows that any two connected components can be\nrealized by any mapping of the form\n(A\u03b8,B\u03b8,C\u03b8)7\u2192(TA\u03b8T\u22121,TB\u03b8,C\u03b8T\u22121) : det( T)<0. (G.15)\nFor example, in odd dimensions, the matrix T=\u2212Ihasdet(T) =\u22121. Thus, we find a simple\ncorollary:\nProposition G.1. Let the state dimension nbe odd. Then, if Cnis disconnected and \u03b8=\n(A\u03b8,B\u03b8,C\u03b8)is stabilizing, then the policies (A\u03b8,B\u03b8,C\u03b8)and(A\u03b8,\u2212B\u03b8,\u2212C\u03b8)lie in different\nconnected components of Cn. More generally, consider any block diagonalization of a\"\nA(1,1)\n\u03b8A(1,2)\n\u03b8\nA(2,1)\n\u03b8A(2,2)\n\u03b8#\n,\"\nB(1,1)\n\u03b8\nB(2,1)\n\u03b8#\n,h\nC(1,1)\n\u03b8C(1,2)\n\u03b8i\n, (G.16)\nwhere the block dimensions are chose such that \u201c 1\u201d dimension has odd dimension. Then, if the\nabove is stabilizing and Cnis disconnected, then the following equivalent controller lies in the other\nconnected component of Cn:\"\nA(1,1)\n\u03b8\u2212A(1,2)\n\u03b8\n\u2212A(2,1)\n\u03b8A(2,2)\n\u03b8#\n,\"\n\u2212B(1,1)\n\u03b8\nB(2,1)\n\u03b8#\n,h\n\u2212C(1,1)\n\u03b8C(1,2)\n\u03b8i\n. (G.17)\nAs a second example, note that when Tis an \u201codd\u201d permutation, det(T) =\u22121, placing the\ntransformed parameters into separate components.\nG.6.3 I NSUFFICIENCY OF NAIVE AND PERMUTATION BASED MERGING\nImportantly, path connectedness does not mean connectedness by linear paths , the latter being pre-\ncisely the definition of convexity. Indeed, consider the equivalence transformation (A\u03b8,B\u03b8,C\u03b8)7\u2192\n(A\u03b8,\u2212B\u03b8,\u2212C\u03b8). The mid point between these controllers if (A\u03b8,0,0)which, one can check, is\nequivalent to the zero controller (0,0,0). Not only is the zero controller essentially never optimal,\nifAis not stable, then this controller is not stabilizing. Similarly, permutations can be insufficient\nto merge as well: indeed, permutation based mergin coincides with naive merging in dimension\nn= 1one, which the above shows will fail if Ais unstable (i.e., in dimension one, |A| \u22651). These\nexamples highlight the important of aligning using allinvertible matrices, and not simply permutation\nmatrices.\n34Published as a conference paper at ICLR 2024\nG.6.4 A VOIDING PATH-DISCONNECTNESS OF STABILIZING CONTROLLERS\nIn addition to the insufficiency of linear and permutation-based merging, it may seem that the\npossible path disconnectedness of the set Cnmay pose a challenge to weight merging. However,\nare\n\n3:\n\nintroduction) of the neurons, allowing a more efficient implementation and\nfocus on robotic control tasks.\nMulti-task policy learning. Multi-task learning and models that exhibit multi-task behaviors have\nshown impressive successes in computer vision (Zhang & Yang, 2018; Standley et al., 2020) and\nnatural language processing (Radford et al., 2019; Collobert & Weston, 2008; Bubeck et al., 2023).\nIndeed, multi-task learning (Ruder, 2017; Yu et al., 2020), meta-learning (Vilalta & Drissi, 2002;\nNichol et al., 2018; Finn et al., 2017), and few-shot learning (Wang et al., 2020b) have long lines of\nliterature. Despite some promising recent attempts in robotics (Brohan et al., 2022; Shridhar et al.,\n2023), the dominant paradigm in robotics is still to conduct single-task data collection and training\non a single domain. Recently, such multi-task paradigms have also been extended to linear control\nsettings, see e.g., Zhang et al. (2022); Wang et al. (2022a). Our work is also related to the studies on\nthe effects of distribution shifts and diverse data distributions (Agarwal et al., 2021; Koh et al., 2021).\nInspired by the increasing scale of robot fleets deployed in the real-world, building a policy learning\nframework at scale requires us to handle distribution heterogeneity and communication efficiency\n(Kalashnikov et al., 2021; Herzog et al., 2023; Driess et al., 2023). Also note that the setting we\nconsider, with multiple datasets, include but are not limited to multi-tasks settings, as the datasets\nmay come from different experts in imitation learning, or different time or operating conditions of the\n18Published as a conference paper at ICLR 2024\nsame agent/policy in the same task. While provable guarantees exist for imitation in the single-task\nsetting Block et al. (2023), understanding the implications of behavior cloning in multi-task settings\nfrom a theoretical angle is an exciting direction for future work.\nB P ERMUTATION INVARIANCE FOR RNN S\nB.1 P ROOF OF PROPOSITION 3.1\nIn this section, we prove Proposition 3.1 in the main paper. Recall that when P=\n(P0,P1, . . . ,PL)\u2208 Gperm, it acts on RNN models via\n(W\u2113\nrec,W\u2113\u22121\nff,b\u2113\u22121)7\u2192\u0000\nP\u2113W\u2113\nrec(P\u2113)\u22a4,P\u2113W\u2113\u22121\nff(P\u2113\u22121)\u22a4,P\u2113b\u2113\u22121\u0001\n, (B.1)\nwhere we note that we replace the matrix inverse in Eq. (B.1) with matrix transpose because the two\ncoincide for permutation matrices. We now note that the group operation of Pon weights can be\ndecomposed as the computation of applying operators as follows\n(P0,P1, . . . ,PL)\u25e6(\u03b8) = (P0,I, . . . ,I)\u25e6(I,P1, . . . ,I)\u25e6 \u00b7\u00b7\u00b7 \u25e6 (I,I, . . . ,PL)\u25e6\u03b8\n= (I,P1, . . . ,I)\u25e6(I,P2, . . . ,I)\u25e6 \u00b7\u00b7\u00b7 \u25e6 (I,I, . . . ,PL\u22121)\u25e6\u03b8, (B.2)\nwhere for the second equality, we recall that Gperm is defined so that its elements P=\n(P0,P1, . . . ,PL)\u2208 G perm, have P0andPLequal to the identity (i.e., we do not permute in-\nputs or outputs). It suffices to show that the RNN model is invariant to applying this operation on a\nspecific layer, i.e.,\nP= (I,I, . . . ,I,P\u2113,I, . . . ,I). (B.3)\nWe now prove the following claim.\nClaim B.1. Let\u2113be such that 0< \u2113 < L , and let Pbe of the form Eq. (B.3), i.e., P\u2113is the only\nnon-identity. Let h\u2113\u2032\ntbe the hidden state at time tand layer 0\u2264\u2113\u2032\u2264Lof model \u03b8, starting at\nh\u2113\u2032\n0= 0.3Let\u02dcht\n\u2113denote the same, but for the permuted model \u02dc\u03b8=P(\u03b8). Then, for all t,\n\u02dch\u2113\u2032\nt=\u001ah\u2113\u2032\nt \u2113\u2032\u0338=\u2113\nP\u2113h\u2113\nt\u2113\u2032=\u2113. (B.4)\nNote that this claim implies Proposition 3.1 because (B.2) shows that any Pcan be decomposed\ninto permutations of the form (B.3) for0< \u2113 < L . In particular, because \u2113\u0338=L,(B.4) means that\nhL\nt=\u02dchL\nt, as needed.\nProof of Claim B.1. It is clear that \u02dch\u2113\u2032\nt=h\u2113\u2032\ntfor layers \u2113\u2032< \u2113andt\u22650. It remains to consider\nlayers \u2113\u2032=\u2113and\u2113\u2032> \u2113.\nLet\u2019s start with \u2113\u2032=\u2113. We argue by induction on tthat\u02dch\u2113\nt=P\u2113h\u2113\nt. Let\u2019s start with t= 0. By\nassumption we have that \u02dch\u2113\nt= 0 = P\u21130 =P\u2113\nth\u2113\nt. This proves the base case. For general t >0, by\nspecializing (B.1) to permutation of the form (B.3), we have\n\u02dch\u2113\nt=\u03c3(P\u2113W\u2113\nrec(P\u2113)\u22a4\u02dch\u2113\nt\u22121+P\u2113W\u2113\u22121\nff\u02dch\u2113\u22121\nt+P\u2113b\u2113\u22121)\n=P\u2113\u03c3(W\u2113\nrec(P\u2113)\u22a4\u02dch\u2113\nt\u22121+W\u2113\u22121\nff\u02dch\u2113\u22121\nt+b\u2113\u22121)\nbecause any entry-wise activation \u03c3(\u00b7)commutes with P\u2113. As noted above, we have \u02dch\u2113\u22121\nt=h\u2113\u22121\nt.\nMoreover, by the inductive hypothesis, \u02dch\u2113\nt\u22121=P\u2113h\u2113\nt\u22121. Thus, we have\n\u02dch\u2113\nt=P\u2113\u03c3(W\u2113\nrec(P\u2113)\u22a4P\u2113h\u2113\nt\u22121+W\u2113\u22121\nff\u02dch\u2113\u22121\nt+b\u2113\u22121)\n=P\u2113\u03c3(W\u2113\nrech\u2113\nt\u22121+W\u2113\u22121\nff\u02dch\u2113\u22121\nt+b\u2113\u22121)\n=P\u2113h\u2113\nt.\nThis proves what we need for \u2113\u2032=\u2113.\n3This condition can be generalized to requiring that \u02dch\u2113\u2032\n0=P\u2113h\u2113\u2032\n0for0< \u2113\u2032< L and\u02dch\u2113\u2032\u2032\n0=h\u2113\u2032\u2032\n0for\n\u2113\u2032\u2032\u2208 {0, L}.\n19Published as a conference paper at ICLR 2024\nFor\u2113 > \u2113\u2032, it suffices to prove the case when \u2113\u2032=\u2113+ 1, because the weights for all subsequent layers\nare not changed by (B.3) and all layers above \u2113+ 1depend only on the \u2113+ 1\u2019s input. Again, we\ninduct over t. By assumption h\u2113+1\nt= 0 = \u02dch\u2113+1\nt. Moreover, permutations of the form (B.3) mean that\n\u02dch\u2113+1\nt=\u03c3(W\u2113+1\nrec\u02dch\u2113+1\nt\u22121+W\u2113\nff(P\u2113)\u22a4\u02dch\u2113\nt+b\u2113).\nUsing the inductive hypothesis that \u02dch\u2113+1\nt\u22121=h\u2113+1\ntand the fact that \u02dch\u2113\nt=P\u2113h\u2113\ntproven above, we have\n\u02dch\u2113+1\nt=\u03c3(W\u2113+1\nrech\u2113+1\nt\u22121+W\u2113\nff(P\u2113)\u22a4P\u2113h\u2113\nt+b\u2113)\n=\u03c3(W\u2113+1\nrech\u2113+1\nt\u22121+W\u2113\nffh\u2113\nt+b\u2113) =h\u2113+1\nt,\nconcluding the proof of the claim.\nB.2 I NVARIANCE FOR RELU A CTIVATIONS\nFor RNNs with common activation functions, we can study a more general group of symmetries\nbeyond just the permutations of Proposition 3.1. Again, consider the action of a general P=\n(I,P1,P2, . . . ,PL\u22121,I)\u2208 Glin, acting as\n(W\u2113\nrec,W\u2113\u22121\nff,b\u2113\u22121)7\u2192\u0000\nP\u2113W\u2113\nrec(P\u2113)\u22121,P\u2113W\u2113\u22121\nff(P\u2113\u22121)\u22121,P\u2113b\u2113\u22121\u0001\n. (B.5)\nProposition B.1. Let\u03c3(\u00b7)be any activation function, and consider P \u2208 G linacting as above. Suppose\nthat, for each \u2113,P\u2113commutes with \u03c3, in the sense that P\u2113\u03c3(\u00b7) =\u03c3(P\u2113(\u00b7)). Then, the output of the\nRNN is invariant to the transformation induced by P. In particular, if \u03c3(\u00b7) = ReLU( \u00b7), then the RNN\noutput is invariant to the set P \u2208 G scaled , where Gscaled\u2282 Glinis the set of transformations such that\nP\u2113=\u02dcP\u2113D\u2113are scaled permutations, with \u02dcP\u2113being a permutation matrix and D\u2113being a diagonal\nmatrix with strictly positive elements.4\nThe proof of Proposition B.1 follows exactly as in the proof of Proposition 3.1 above, by replacing\ntransposes with inverses for the general case. The special case of the ReLU network follows from\nchecking that the ReLU operation commutes with scaled permutations.\nEven though RNNs are invariant to scaled permutations of their weights, we follow previous model\nmerging work and focus only on the permutations (i.e., ignoring the scaling) (Ainsworth et al.,\n2022; Cuturi, 2013). As argued by the linear mode connectivity hypothesis (Entezari et al., 2021),\nscaling symmetries should not appear in practice because the implicit regularization of the training\nalgorithms, e.g., stochastic gradient descent (Neyshabur et al., 2014; Smith et al., 2021), controls the\nscale of the resulting weights. Here, we formalize the argument by proving that implicit regularization\nuniquely determines the scaling matrix.\nProposition B.2. For an RNN \u03b8, define the norm\n\u2225\u03b8\u22252\n2=\u2225WL\u22121\nff\u22252\nF+\u2225bL\u22121\u22252+X\n0<\u2113<L\u2225W\u2113\nrec\u22252\nF+\u2225W\u2113\u22121\nff\u22252\nF+\u2225b\u2113\u22121\u22252. (B.6)\nLet\u03b8be any model for which every bias term b\u2113has strictly nonzero entries. Define the set\nX(\u03b8) = argmin\n\u03b8\u2032,P\u2208G scaled\u2225\u03b8\u2032\u22252s.t.\u03b8\u2032=P(\u03b8), (B.7)\nof minimal norm models which correspond to transforming \u03b8by a scaled permutation. Then, for every\n\u03b8\u2032, \u03b8\u2032\u2032\u2208 X(\u03b8), there exists a transformation consisting only of (non-scaled) permutations P \u2208 G perm\nsuch that \u03b8\u2032=P(\u03b8\u2032\u2032).\nProof. Note that if \u03b8\u2032\u2208 X(\u03b8),X(\u03b8\u2032) =X(\u03b8)because Gscaled is a group. Thus, it suffices to prove a\nslightly simpler claim:\nClaim B.2. Suppose \u03b8\u2208 X (\u03b8)and \u03b8\u2032\u2208 X (\u03b8), and let Pbe such that P=\n(I,\u02dcP1D1, . . . , \u02dcPL\u22121DL\u22121,I), where \u02dcP-matrices are permutation matrices and Dmatrices are\ndiagonal matrices with positive diagonal elements. Then, we must have\nD\u2113=Id\u2113,0< \u2113 < L, (B.8)\nwhere d\u2113is the dimension of the D\u2113.\n4Note that this is a group because \u02dcP\u2113D\u2113=\u02dcP\u2113\u02dcD\u2113for some other \u02dcD\u2113.\n20Published as a conference paper at ICLR 2024\nWe now prove the claim. We expand\n\u2225P(\u03b8)\u22252\n2\n=\u2225WL\u22121\nff(\u02dcPL\u22121DL\u22121)\u22121\u22252\nF+\u2225bL\u22252\n+X\n0<\u2113<L\u2225(\u02dcPlDl)W\u2113\nrec(D\u2113)\u22121(\u02dcPlD\u2113)\u22121\u22252\nF+\u2225(\u02dcPlD\u2113)W\u2113\u22121\nff(\u02dcPlD\u2113\u22121)\u22121\u22252\nF+\u2225(\u02dcP\u2113Dl)b\u2113\u22121\u22252\n=\u2225WL\u22121\nff(DL\u22121)\u22121(\u02dcPL\u22121)\u22121\u22252\nF+\u2225bL\u22121\u22252\n+X\n0<\u2113<L\u2225(\u02dcPlDl)W\u2113\nrec(D\u2113)\u22121(\u02dcPl)\u22121\u22252\nF+\u2225(\u02dcPlD\u2113)W\u2113\u22121\nff(D\u2113\u22121)\u22121(\u02dcPl)\u22121\u22252\nF+\u2225(\u02dcP\u2113Dl)b\u2113\u22121\u22252\n=\u2225WL\u22121\nff(DL\u22121)\u22121\u22252\nF+\u2225bL\u22121\u22252\n+X\n0<\u2113<L\u2225DlW\u2113\nrec(D\u2113)\u22121\u22252\nF+\u2225D\u2113W\u2113\u22121\nff(D\u2113\u22121)\u22121\u22252\nF+\u2225Dlb\u2113\u22121\u22252\n=\u2225bL\u22121\u22252+dLX\ni=1dL\u22121X\nj=1WL\u22121\nff[i, j]2DL\u22121[j, j]\u22122\n+X\n0<\u2113<Ld\u2113X\ni,j=1Dl[i, i]2W\u2113\nrec[i, j]2Dl[j, j]\u22122\n+X\n0<\u2113<Ld\u2113X\ni=1d\u2113\u22121X\nj=1D\u2113[i, i]2W\u2113\u22121\nff[i, j]2D\u2113\u22121[j, j]\u22122+X\n0<\u2113<Ld\u2113X\ni=1Dl[i, i]2b\u2113\u22121[i]2,\nwhere [i, j]indexes matrices and [i]indexes vectors, and where we use that D\u2113are diagonal matrices.\nNext, let us represent the diagonals of the matrices D\u2113[i, i] = exp( \u03c4\u2113,i)as exponentials, which is\nvalid because they are strictly positive. In this representation, we have\n\u2225P(\u03b8)\u22252\n2=\u2225bL\u22121\u22252+dLX\ni=1dL\u22121X\nj=1WL\u22121\nff[i, j]2e\u22122\u03c4L\u22121,j+X\n0<\u2113<Ld\u2113X\ni,j=1W\u2113\nrec[i, j]2e2(\u03c4\u2113,i\u2212\u03c4\u2113,j)\n+X\n0<\u2113<Ld\u2113X\ni=1d\u2113\u22121X\nj=1W\u2113\u22121\nff[i, j]2e2(\u03c4\u2113,i\u2212\u03c4\u2113\u22121,j)+X\n0<\u2113<Ld\u2113X\ni=1e2\u03c4\u2113,ib\u2113\u22121[i]2\n:=F({\u03c4\u2113,i}1\u2264i\u2264d\u2113,0<\u2113<L ).\nWe now observe that the function F(\u00b7)is convex in its arguments, because it is the sum over\nexponentials of linear functions of {\u03c4\u2113,i}1\u2264i\u2264d\u2113,0<\u2113<L with positive coefficients. In fact, it is strictly\nconvex, because all but the last terms are convex, and the termP\n0<\u2113<LPd\u2113\ni=1e2\u03c4\u2113,ib\u2113\u22121[i]2is\nstrictly convex under our assumption that none of the bias weights are zero.\nTherefore, there is a unique {\u03c4\u22c6\n\u2113,i}1\u2264i\u2264d\u2113,0<\u2113<L which minimizes F(\u00b7). Thus, by assumption that\n\u03b8\u2032\u2208 X(\u03b8), we must have that\nD\u2113[i, i] = exp( \u03c4\u22c6\n\u2113,i),1\u2264i\u2264d\u2113,0< \u2113 < L.\nBut similarly, since \u03b8\u2208 X(\u03b8), then by writing and \u03b8= (I,I, . . .I)\u25e6\u03b8as the identity transformation\napplied to itself, we must have that\nexp(\u03c4\u22c6\n\u2113,i) = (Id\u2113)[i, i] = 1.\nThus, we conclude that\nD\u2113[i, i] = 1,\nas needed.\n21Published as a conference paper at ICLR 2024\nC F LEET -TOOLS BENCHMARK\nC.1 I MPLEMENTATION DETAILS\nIn this section, we introduce more details of our newly developed tool-use benchmark for fleet policy\nlearning, based on the Drake simulator (Tedrake & the Drake Development Team, 2019).\nExpert tasks. To scale the generation of expert demonstrations for the tool-use tasks, we specify\nthe tasks via key points (Manuelli et al., 2022) when generating the expert trajectories. We mainly\nconsider four skills as examples (or task families): { wrench ,hammer ,spatula , and knife }\n\u2013 consisting of tasks involving the eponymous tool-type. Specifically, we use use a spanner\nto apply wrenches ,use a hammer to hit ,use a spatula to scoop , anduse\na knife to split . All of these tasks have a common feature that the skills can be reasonably,\ncertainly not perfectly, solved by specifying a few sparse keypoints. For instance, in the wrench tasks\nas shown in Figure 5, the wrench needs to reach the nut first and then rotate by 45 degrees while\nmaintaining contact. The same strategy can be applied to a set of nuts and wrenches. We pick these\ntasks because they provide a balance of common robotic tasks that require precision, dexterity, and\ngenerality in object-object affordance.\nKeypoint transformation optimization. We denote the end effector frame and world frame as E\nandW, respectively. For each tool, we specify nkeypoints, for example, when n= 3, we can specify\none point on the tool head, one point on the tool tail, and one point on the side. We define a rigid\nbody transformation X\u2208SE(3) \u2282R4\u00d74, the current locations of the nkeypoints on the tool in the\nworld frame WasWptool=\u0002Wptool,1,\u00b7\u00b7\u00b7,Wptool,n\u0003\n, and the poses in the end effector frame as\nEptool=\u0002Eptool,1,\u00b7\u00b7\u00b7,Eptool,n\u0003\n, respectively. For the object, we specify mkeypoints in the world\nframe asWpobj=\u0002Wpobj,1,\u00b7\u00b7\u00b7,Wpobj,m\u0003\n. Each point p(either p=Eptool,iorp=Wpobj,j) lies\ninR4, and is expressed in (x, y, z, 1)-homogeneous coordinates. Then, one can solve the following\noptimization problem, referred to as kPAM-Opt (where kPAM stands for KeyPoint Affordance-based\nManipulation ) (Manuelli et al., 2022), to find the transformation X:\nkPAM-Opt min\nX\r\rXEptool\u2212Wptool\r\r2\nF(C.1)\ns.t.\r\rXEptool,i\u2212Wpobj,j\r\r\n2\u2264\u03f5,\u2200i, j (C.2)\n\u03b1i,j\u2212\u03f5\u2264\u03b2\u22a4\ni,jX(Eptool,i\u2212Eptool,j)\u2264\u03b1i,j+\u03f5,\u2200i, j (C.3)\nwhere we choose i\u2208 {1,\u00b7\u00b7\u00b7, n}andj\u2208 {1,\u00b7\u00b7\u00b7, m}, and \u03b2i,j\u2208R4is a vector on the unit sphere\nwith the fourth dimension \u03b2i,j(4) = 0 ,\u03b1i,jis some constant that represents the target angle alignment,\nand\u03f5 >0is some relaxation level. Constraint Eq. (C.5) restricts the distance between the keypoints\non the tool to be close to some target ones on the object to be \u03f5-small. For example, when i= 1and\nm= 1, where we useEptool,1to denote the keypoint corresponds to the head of the tool, then this\nconstraint ensures the head of the tool, e.g., the hammer head, to be close to the object, e.g., the pin.\nThe composition of (\u03b1i,j, \u03b2i,j, \u03f5)in Eq. (C.6) describes some constraints related to the orientations of\nthe tool (e.g., the hammer head needs to be vertical to the world and stay flat), up to some \u03f5-relaxation.\nThe overall formulation in Eq. (C.1)-Eq. (C.3) instantiates the general one in Manuelli et al. (2022).\nAlthough the kPAM procedure above can solve for the desired transformation X, it can be computa-\ntionally challenging to recover the joint angle from Xvia inverse kinematics. Hence, we propose to\nsolve kPAM optimization in the joint space, i.e., optimize over the joint angles qthat parameterize\nthe transformation through forward kinematics (i.e., forward-kinematics (q)), and only add\nconstraints to the keypoints in the transformed frame. Specifically, we solve the following joint-space\noptimization problem:\nJoint-Space-kPAM-Opt min\nq,X\r\rXEptool\u2212Wptool\r\r2\nF(C.4)\ns.t.\r\rXEptool,i\u2212Wpobj,j\r\r\n2\u2264\u03f5,\u2200i, j (C.5)\n\u03b1i,j\u2212\u03f5\u2264\u03b2\u22a4\ni,jX(Eptool,i\u2212Eptool,j)\u2264\u03b1i,j+\u03f5,\u2200i, j (C.6)\nX=forward-kinematics (q), (C.7)\n22Published as a conference paper at ICLR 2024\nPre-actuation TrajectoryActuationTrajectorySpecify keypoints\nFigure 5: Visualization of the kPAM-based (Manuelli et al., 2022) expert generation pipeline. There\nare three keypoints defined on the tool and one keypoint defined on the object. The kPAM objective\nand constraints define an actuation pose for each specific tool-use task. We then optimize for the\npre-actuation trajectory and the post-actuation trajectory jointly.\nwhere constraint Eq. (4.4) relates the rigid body transformation Xand the output of\nforward-kinematics (q)under a joint angle q, to better contrast the formulation in Eq. (C.1)-\nEq. (C.3).\nWe create the optimization problem Eq. (C.4)-Eq. (C.6) in Drake, which is then solved using nonlinear\nprogramming solvers as SNOPT (Gill et al., 2005). The intuition of this joint-space optimization is\nto leverage the multiple solutions in the manipulation tasks to simplify the constraint satisfaction\nproblem in inverse kinematics. For instance, when solving the wrench tasks, the entire rotation space\naround the zaxis in the world frame is free, so solving for joint angle directly on these constraints\ndefined on keypoints can be easier than solving the pose-space KPAM problem and then solve inverse\nkinematics.\nPre-/Post-actuation optimization. After finding the joint angle solution q\u22c6, we can work out the\npre-actuation and the post-actuation poses (Figure 5). For instance, for wrench tasks, there is a\nstandoff pose that is a few centimeters in the negative zaxis of the end effector, which are treated as\nkeyframes. Once we construct the trajectory of pose keyframes in the pre-actuation and post-actuation\ntrajectories, we can solve the joint-space trajectory as an optimization problem that further improves\nthe smoothness (by minimizing the norm of joint difference with fixed endpoints), while satisfying\nthe keyframe pose constraints.\nSpecifically, let \u03be= [\u03be[1], \u03be[2], ..., \u03be [T]]denote a joint-space trajectory of length T, including\ntrajectory before actuation at some timestep t\u22c6< T, and the post-actuation trajectory from t\u22c6+ 1\nuntil T. Choose m\u2264Tindices {I1,\u00b7\u00b7\u00b7, Im} \u2286 { 1,2,\u00b7\u00b7\u00b7, T}, and specify the corresponding\npose keyframe trajectory as [pI1, pI2, ..., p Im]. Let q1be the current joint position, and q\u22c6be the\nsolution to the previous joint-space KPAM problem Eq. (C.4)-Eq. (C.7). We then solve the following\noptimization problem:\nmin\n\u03beT\u22121X\nt=1\u2225\u03be[t+ 1]\u2212\u03be[t]\u22252\n2 (C.8)\ns.t. \u03be[1] = q1, \u03be [t\u22c6] =q\u22c6(C.9)\nforward-kinematics (\u03beIk) =pIk,\u2200k= 1,2,\u00b7\u00b7\u00b7, m, (C.10)\nwhich is also solved by nonlinear solvers as SNOPT (Gill et al., 2005) in Drake. This final solution\ngives a planned trajectory \u03be\u22c6and we can then track this trajectory with low-level controllers to provide\ndemonstrations.\nDiscussions. Our tool-use benchmark and the pipeline above provide a natural and easy way to\ncreate (new) tasks in Drake, with only a configuration file of the actuation pose optimization and a\nfew keypoints labels that can be easily obtained via a 3D visualizer. The overall pipeline, which is\ndeterministic by design, is also general enough to act as the expert in the imitation learning settings\nto represent \u201ccategory-level\u201d composition and generalization. The convenience of category-level\ngeneralization can be an important application domain to study out-of-support distribution shift,\n23Published as a conference paper at ICLR 2024\nFigure 6: The real-world setup that the benchmark recreates in the simulation with segmented point\ncloud visualizations.\nwhich can be formulated as combinatorial extrapolation/generalization problems, i.e., generalizing to\nthe unseen tool-object (or tool-task) combinations at test time, despite only being able to see specific\ntools and objects separately (while not such joint combinations) at training time. See a more formal\ntreatment in Simchowitz et al. (2023).\nThere are certain limitations of our pipeline. For example, it creates only hand-scripted experts, and\nthere is no feedback in the execution loop, which can be necessary for some dynamic and contact-rich\nmotions (Gao & Tedrake, 2021). Moreover, tool-object and tool-hand contact dynamics modeled\nin the simulation but are not considered in the current expert formulation. These are all important\ndirections we will address in the future. We believe tool-use tasks, which often involve rich force and\ncontact interactions ( ?Holladay et al., 2019; Toussaint, 2009), represent one of the key challenging\ntasks in dexterous manipulation.\nC.2 E NVIRONMENT SETUP AND IMPLEMENTATION DETAILS\nWe set up two cameras to mimic the real-world setting of Panda robotic arms (see Figure 6). One\noverhead camera is mounted on the table and the other camera is mounted on the wrist. We have 4\ntool-object pairs for each tool in the benchmark (more assets available). At each scene initialization,\nwe randomize the camera extrinsics, robot initial joints, object frictions, mass, and the tool-in-hand\npose. We also randomize object texture when using colors and support offline blender rendering. We\nweld the tool to the robot end effector which forms a kinematic chain. We assume RGB-D images\nwith masks as inputs to the robots. Specifically, we can use this information to fuse pointclouds (Seita\net al., 2023; Wang et al., 2022b) for both the tools and the objects, which we resample to contain\n1024 points. We also measure joint torques and end effector wrench in the environment. We use\nthe bounded relative end effector motion as the action output. Notably, in the simulation we have\na perfect dynamics model that can potentially improve the contact-rich motions in tool-use, which\nenables us to use a customized operation-space controller (OSC) (Khatib, 1987) with tuned gains to\nachieve high success rates in solving the tasks. We use hydroelastic contact model with SAP contact\nsolver (Masterjohn et al., 2022) in drake. The environment timesteps are 6Hz and the simulation\ndynamics step is 250Hz. We use the Ray library to support multiple rollouts in parallel.\nWe combine the segmented point cloud from a wrist camera and an overhead camera into the tool\nand object pointcloud, and feed them as observations to the policy. We train the policy using Adam\noptimizer (Kingma & Ba, 2014) with 200 epochs. The dataset contains 50000 data points for each\ntool instance, and the batch size is 512. We use 5 users across the merging\n\n4:\n\nrelated work\nWang et al. (2020a), we also develop a new merging approach based on \u201csoft\u201d permutations (see\nSection 3 for a formal\n\n5:\n\nAppendix G.4.2. The reason for this is our choice of merging cost in (G.14) . This\ncost penalizes only differences between the controller parameters . It does not, as in FLEET -MERGE\n(Algorithm 1), use a cost depending on the policy error. Therefore, even if one of the iterates lies\noutside of the stabilizing set Cn(and therefore has infinitely large infinite-horizon LQR cost J), we\ncan still compute gradients with respect to the parameter error (G.14) . In other words, merging in\nparameter loss allows us to pass through disconnectedness in the landscape of control cost J.\nThis can be seen in the following simplified variant of (G.14) for the merging of two LQG policies\n\u03b81= (A1,B1,C1)and\u03b82= (A2,B2,C2). Define\nL(P) =\u2225PA1\u2212A2P\u22252\nF+\u2225B1P\u2212B2\u22252\nF+\u2225C1\u2212C2P\u22252\nF. (G.18)\nThen note that this loss is convex in P. Under natural conditions, it is in fact strongly convex ,\nand thus admits a unique global minimizer P. If\u03b81and\u03b82are equivalent, i.e., (A2,B2,C2) =\n(TA1T\u22121,TB1,TC1)for some transformation T, then this unique minimizer must be P=T, as\nthis has zero loss in (G.18) . This is true regardless of whether or not \u03b81, \u03b82lie in separate connected\ncomponents of Cn. Hence, we see how merging based on parameters alone circumvents possible loss\ndisconnectedness.\n35\n\n6:\n\nexperiments do not seem to suggest that this is an issue for the unconstrained gradient descent\nmethod proposed in\n\n7:\n\nresults to\n\n8:\n\nResults. (Left) We observe that even averaging linear policies can\noften improve performance to almost match joint training on the pooled data for single-task settings,\nand match task-specific training for multi-task settings. (Right) We observe that gradient-based\n\n9:\n\nREFERENCES\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy\ngradient\n\n10:\n\nIntroduction 1\n2 Preliminaries 3\n3 Methodology 4\n3.1 Merging Many Recurrent Policies . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n4 F LEET -TOOLS : A Robotic Tool-Use Simulation Benchmark 6\n5\n\n11:\n\nExperiments 25\nD.1 Further Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nE Online Reinforcement Learning Ablation 28\nF Algorithm Ablation on Supervised Learning Tasks 28\nG Linear Policy Merging 29\nG.1 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nG.2 Symmetries in Nonlinear Control . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nG.3 Details for Training Imitation Learning . . . . . . . . . . . . . . . . . . . . . . . . 31\nG.4\n\n12:\n\nConclusion 9\n7 Acknowledgement 10\nA Detailed\n\n13:\n\nRelated Work 18\nB Permutation Invariance for RNNs 19\nB.1 Proof of Proposition 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB.2 Invariance for ReLU Activations . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nC F LEET -TOOLS Benchmark 22\nC.1 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nC.2 Environment Setup and Implementation Details . . . . . . . . . . . . . . . . . . . 24\nD Meta-World\n\n14:\n\nMethods for Linear Policy Merging . . . . . . . . . . . . . . . . . . . . . . . . . 32\nG.4.1 Method 1: Permutation-Based Alignment . . . . . . . . . . . . . . . . . . 32\nG.4.2 Method 2: Unconstrained Gradient Descent . . . . . . . . . . . . . . . . . 33\nG.5 Experiment Setup Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n16Published as a conference paper at ICLR 2024\nG.6\n\n15:\n\nRELATED WORK\nParameter invariance in neural networks and control. It has long been known that neural\nnetworks are invariant under certain transformations or symmetries, of their weights (Hecht-Nielsen,\n1990). In particular, permutation symmetries arise because the ordering of hidden neurons in neural\nnetworks does not affect their input-output behavior. This has been an important topic of recent\nstudies of neural networks loss landscapes (Brea et al., 2019; Entezari et al., 2021; Tatro et al., 2020;\nAinsworth et al., 2022) and of\n\n16:\n\nEXPERIMENTS\nD.1 F URTHER EXPERIMENTAL DETAILS\nIn this section, we provide the experiment details on the Meta-World benchmark (Yu et al., 2020).\nImplementation. For state-based input, we use a 4-layer network architecture with 512hidden\ndimensions, including multilayer perceptron (MLP), RNN, and Transformer, to parameterize the\npolicies. For image-based input, we use ResNet18 (He et al., 2016) as the default vision encoder and\na similar 4-layer MLP for the policy head. We use Adam optimizer with a learning rate of 1e\u22123\nand batch size of 256. We use 3, 5, and 10 users respectively for the single-shot merging, merging\nwhile training, and merging with participation ratio\n\n17:\n\nRESULTS\nAs shown in Figure 13, with static feedback policies (i.e., \u201cfeedforward\u201d policies), average can\nsometimes match the performance of policies that are trained with shared data ( multi-task ) or task-\n33Published as a conference paper at ICLR 2024\nspecific data ( task-specific ). We then compare the performance in cases with dynamic policies (i.e.,\n\u201crecurrent\u201d policies), and observe that the gradient-based method can outperform the alternation-based\nmethod that searches in only the space of permutation matrices. These two\n\n\nPlease write a review of the paper based on the introduction and authors' profiles. You should write a paragraph of approximately 200 words.\nYou should only write about the strengths of the paper. It should be 200 words long.\n"
        }
    ],
    "bio": "I am a researcher dedicated to advancing the field of robot learning, with a focus on developing generalist policies that can adapt to a wide range of tasks and robotic platforms. My work addresses the critical challenges of data efficiency, generalization, and robustness in robotic systems. I have pioneered several innovative approaches, including the design of flow matching architectures that leverage pre-trained vision-language models to enhance robot capabilities in complex tasks like laundry folding and assembly.\n\nMy recent contributions include the development of Octo, a transformer-based policy trained on the largest robot manipulation dataset to date, which demonstrates remarkable versatility across multiple robotic platforms. I have also explored the integration of categorical cross-entropy for training value functions in reinforcement learning, significantly improving scalability and performance across various domains.\n\nIn addition, I have created SIMPLER, a collection of simulated environments that facilitate reliable evaluation of manipulation policies, and I introduced Embodied Chain-of-Thought Reasoning (ECoT) to enhance the reasoning capabilities of vision-language-action models in robotics. My research emphasizes the importance of leveraging diverse datasets and the potential of large, pre-trained models to transform how robots learn and adapt to new environments.\n\nThrough my work, I aim to bridge the gap between simulation and real-world applications, enabling robots to learn from both human feedback and autonomous data collection. I am committed to open-sourcing my findings and tools to foster collaboration and innovation in the robotics community."
}