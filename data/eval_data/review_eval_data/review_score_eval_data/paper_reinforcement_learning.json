{
  "Thresholded Lexicographic Ordered Multi-Objective Reinforcement Learning": {
    "paper_pk": null,
    "title": "Thresholded Lexicographic Ordered Multi-Objective Reinforcement Learning",
    "abstract": "Lexicographic multi-objective problems, which impose  a  lexicographic importance order over the objectives, arise in many real-life scenarios. Existing Reinforcement Learning work  directly addressing lexicographic tasks has been scarce. The few proposed approaches were all noted to be heuristics without theoretical guarantees as the Bellman equation is not applicable to them. Additionally, the practical applicability of these prior approaches also  suffers from various issues such as not being able to reach the goal state.  While some of these issues have been known before, in this work we investigate further shortcomings, and propose fixes for improving practical performance in many cases. We also present a  policy optimization approach using our Lexicographic Projection Optimization (LPO) algorithm that has the potential to address these theoretical and practical concerns. Finally, we demonstrate our proposed algorithms on benchmark problems.",
    "authors": [
      "Alperen Tercan",
      "Vinayak Prabhu"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Lexicographic Ordered Multi-Objectives"
    ],
    "real_all_scores": [
      5,
      3,
      6,
      5
    ],
    "real_confidences": [
      2,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering": {
    "paper_pk": null,
    "title": "Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering",
    "abstract": "Feature engineering is widely acknowledged to be pivotal in tabular data analysis and prediction. Automated feature engineering (AutoFE) emerged to automate this process managed by experienced data scientists and engineers conventionally. In this area, most \u2014 if not all \u2014 prior work adopted an identical framework from the neural architecture search (NAS) method. While feasible, we posit that the NAS framework very much contradicts the way how human experts cope with the data since the inherent Markov decision process (MDP) setup differs. We point out that its data-unobserved setup consequentially results in an incapability to generalize across different datasets as well as also high computational cost. This paper proposes a novel AutoFE framework Feature Set Data-Driven Search (FETCH), a pipeline mainly for feature generation and selection. Notably, FETCH is built on a brand-new data-driven MDP setup using the tabular dataset as the state fed into the policy network. Further, we posit that the crucial merit of FETCH is its transferability where the yielded policy network trained on a variety of datasets is indeed capable to enact feature engineering on unseen data, without requiring additional exploration. To the best of our knowledge, this is a pioneer attempt to build a tabular data pre-training paradigm via AutoFE. Extensive experiments show that FETCH systematically surpasses the current state-of-the-art AutoFE methods and validates the transferability of AutoFE pre-training.",
    "authors": [
      "Liyao Li",
      "Haobo Wang",
      "Liangyu Zha",
      "Qingyi Huang",
      "Sai Wu",
      "Gang Chen",
      "Junbo Zhao"
    ],
    "keywords": [
      "Automated Feature Engineering",
      "Reinforcement Learning",
      "Tabular Data",
      "Data-Driven",
      "Pre-Training"
    ],
    "real_all_scores": [
      5,
      6,
      5,
      3
    ],
    "real_confidences": [
      3,
      2,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Towards biologically plausible Dreaming and Planning": {
    "paper_pk": null,
    "title": "Towards biologically plausible Dreaming and Planning",
    "abstract": "Humans and animals can learn new skills after practicing for a few hours, while current reinforcement learning algorithms require a large amount of data to achieve good performances. \nRecent model-based approaches show promising results by reducing the number of necessary interactions with the environment to learn a desirable policy. However, these methods require biological implausible ingredients, such as the detailed storage of older experiences, and long periods of offline learning. The optimal way to learn and exploit word-models is still an open question.\nTaking inspiration from biology, we suggest that dreaming might be an efficient expedient to use an inner model. We propose a two-module (agent and model) neural network in which \"dreaming\" (living new experiences in a model-based simulated environment) significantly boosts learning. We also explore \"planning\", an online alternative to dreaming, that shows comparable performances. Importantly, our model does not require the detailed storage of experiences, and learns online the world-model. This is a key ingredient for biological plausibility and implementability (e.g., in neuromorphic hardware).",
    "authors": [
      "Cristiano Capone",
      "Pier Stanislao Paolucci"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Model based",
      "Biologically Plausible"
    ],
    "real_all_scores": [
      6,
      3,
      3,
      8
    ],
    "real_confidences": [
      2,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Imitating Graph-Based Planning with Goal-Conditioned Policies": {
    "paper_pk": null,
    "title": "Imitating Graph-Based Planning with Goal-Conditioned Policies",
    "abstract": "Recently, graph-based planning algorithms have gained much attention to solve goal-conditioned reinforcement learning (RL) tasks: they provide a sequence of subgoals to reach the target-goal, and the agents learn to execute subgoal-conditioned policies. However, the sample-efficiency of such RL schemes still remains a challenge, particularly for long-horizon tasks.  To address this issue, we present a simple yet effective self-imitation scheme which distills a subgoal-conditioned policy into the target-goal-conditioned policy. Our intuition here is that to reach a target-goal, an agent should pass through a subgoal, so target-goal- and subgoal- conditioned policies should be similar to each other. We also propose a novel scheme of stochastically skipping executed subgoals in a planned path, which further improves performance. Unlike prior methods that only utilize graph-based planning in an execution phase, our method transfers knowledge from a planner along with a graph into policy learning. We empirically show that our method can significantly boost the sample-efficiency of the existing goal-conditioned RL methods under various long-horizon control tasks.",
    "authors": [
      "Junsu Kim",
      "Younggyo Seo",
      "Sungsoo Ahn",
      "Kyunghwan Son",
      "Jinwoo Shin"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Goal-Conditioned Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      6
    ],
    "real_confidences": [
      4,
      3,
      3,
      2
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories": {
    "paper_pk": null,
    "title": "Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories",
    "abstract": "In this paper, we evaluate and improve the generalization performance for reinforcement learning (RL) agents on the set of ``controllable'' states, where good policies exist on these states to achieve the goal. An RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. To practically evaluate this type of generalization, we propose relay evaluation, which starts the test agent from the middle of other independently well-trained stranger agents' trajectories. With extensive experimental evaluation, we show the prevalence of generalization failure on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\\% failure rate during regular testing, failed on 81.6\\% of the states generated by well-trained stranger PPO agents. To improve \"relay generalization,\" we propose a novel method called Self-Trajectory Augmentation (STA), which will reset the environment to the agent's old states according to the Q function during training. After applying STA to the Soft Actor Critic's (SAC) training procedure, we reduced the failure rate of SAC under relay-evaluation by more than three times in most settings without impacting agent performance and increasing the needed number of environment interactions. Our code is available at https://github.com/lan-lc/STA.",
    "authors": [
      "Li-Cheng Lan",
      "Huan Zhang",
      "Cho-Jui Hsieh"
    ],
    "keywords": [
      "Genralization",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      3,
      6
    ],
    "real_confidences": [
      2,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning Rewards and Skills to Follow Commands with a Data Efficient Visual-Audio Representation": {
    "paper_pk": null,
    "title": "Learning Rewards and Skills to Follow Commands with a Data Efficient Visual-Audio Representation",
    "abstract": "Based on the recent advancements in representation learning, we propose a novel framework for command-following robots with raw sensor inputs. Previous RL-based methods are either difficult to continuously improve after the deployment or require a large number of new labels during the fine-tuning. Motivated by (self-)supervised contrastive learning literature, we propose a novel representation, named VAR++, that generates an intrinsic reward function for command-following robot tasks by associating images with sound commands. After the robot is deployed in a new domain, the representation can be updated intuitively and data-efficiently by non-expert, and the robots are able to fulfill sound commands without any hand-crafted reward functions. We demonstrate our approach to various sound types and robotic tasks, including navigation and manipulation with raw sensor inputs. In the simulated experiments, we show that our system can continually self-improve in previously unseen scenarios given fewer new labeled data, yet achieves better performance, compared with previous methods.\n\n",
    "authors": [
      "Peixin Chang",
      "Shuijing Liu",
      "Tianchen Ji",
      "Neeloy Chakraborty",
      "D Livingston McPherson",
      "Katherine Rose Driggs-Campbell"
    ],
    "keywords": [
      "Robotics",
      "Representation Learning",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      8,
      6,
      8,
      3
    ],
    "real_confidences": [
      4,
      3,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Recursive Time Series Data Augmentation": {
    "paper_pk": null,
    "title": "Recursive Time Series Data Augmentation",
    "abstract": "Time series observations can be seen as realizations of an underlying dynamical system governed by rules that we typically do not know. For time series learning tasks we create our model using available data. Training on available realizations, where data is limited, often induces severe over-fitting thereby preventing generalization. To address this issue, we introduce a general recursive framework for time series augmentation, which we call the Recursive Interpolation Method (RIM). New augmented time series are generated using a recursive interpolation function from the original time series for use in training. We perform theoretical analysis to characterize the proposed RIM and to guarantee its performance under certain conditions. We apply RIM to diverse synthetic and real-world time series cases to achieve strong performance over non-augmented data on a variety of learning tasks. Our method is also computationally more efficient and leads to better performance when compared to state of the art time series data augmentation.\n",
    "authors": [
      "Amine Mohamed Aboussalah",
      "Minjae Kwon",
      "Raj G Patel",
      "Cheng Chi",
      "Chi-Guhn Lee"
    ],
    "keywords": [
      "Time Series",
      "Data augmentation",
      "Representation Learning",
      "Deep Learning",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "LEARNING DYNAMIC ABSTRACT REPRESENTATIONS FOR SAMPLE-EFFICIENT REINFORCEMENT LEARNING": {
    "paper_pk": null,
    "title": "LEARNING DYNAMIC ABSTRACT REPRESENTATIONS FOR SAMPLE-EFFICIENT REINFORCEMENT LEARNING",
    "abstract": "In many real-world problems, the learning agent needs to learn a problem\u2019s abstractions and solution simultaneously. However, most such abstractions need to be designed and refined by hand for different problems and domains of application. This paper presents a novel top-down approach for constructing state abstractions while carrying out reinforcement learning. Starting with state variables and a simulator, it presents a novel domain-independent approach for dynamically computing an abstraction based on the dispersion of Q-values in abstract states as the agent continues acting and learning. Extensive empirical evaluation on multiple domains and problems shows that this approach automatically learns abstractions that are finely-tuned to the problem, yield powerful sample efficiency, and result in the RL agent significantly outperforming existing approaches.",
    "authors": [
      "Mehdi Dadvar",
      "Rashmeet Kaur Nayyar",
      "Siddharth Srivastava"
    ],
    "keywords": [
      "Sequential Decision-Making",
      "Reinforcement Learning",
      "Learning Abstract Representations"
    ],
    "real_all_scores": [
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness": {
    "paper_pk": null,
    "title": "On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness",
    "abstract": "Generalization in Reinforcement Learning (RL) aims to train an agent during training that generalizes to the target environment. In this work, we first point out that RL generalization is fundamentally different from the generalization in supervised learning, and fine-tuning on the target environment is necessary for good test performance. Therefore, we seek to answer the following question: how much can we expect pre-training over training environments to be helpful for efficient and effective fine-tuning? On one hand, we give a surprising result showing that asymptotically, the improvement from pre-training is at most a constant factor. On the other hand, we show that pre-training can be indeed helpful in the non-asymptotic regime by designing a policy collection-elimination (PCE) algorithm and proving a distribution-dependent regret bound that is independent of the state-action space. We hope our theoretical results can provide insight towards understanding pre-training and generalization in RL.",
    "authors": [
      "Haotian Ye",
      "Xiaoyu Chen",
      "Liwei Wang",
      "Simon Shaolei Du"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Generalization",
      "Learning Theory"
    ],
    "real_all_scores": [
      8,
      6,
      8,
      6
    ],
    "real_confidences": [
      3,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Model-free Reinforcement Learning that Transfers Using Random Reward Features": {
    "paper_pk": null,
    "title": "Model-free Reinforcement Learning that Transfers Using Random Reward Features",
    "abstract": "Favorable reinforcement learning (RL) algorithms should not only be able to synthesize controller for complex tasks, but also transfer across various such tasks. Classical model-free RL algorithms like Q-learning can be made stable, and has the potential to solve complicated tasks individually. However, rewards are key supervision signals in model-free approaches, making it challenging in general to transfer across multiple tasks with different reward functions. On the other hand, model-based RL algorithms, naturally transfers to various reward functions if the transition dynamics are learned well. Unfortunately, model-learning usually suffers from high dimensional observations and/or long horizons due to the challenges of compounding error. In this work, we propose a new way to transfer behaviors across problems with different reward functions that enjoy the best of both worlds. Specifically, we develop a model-free approach that implicitly learns the model without constructing the transition dynamics. This is achieved by using random features to generate reward functions in training, and incorporating model predictive control with open-loop policies in online planning. We show that the approach enables fast adaptation to problems with completely new reward functions, while scaling to high dimensional observations and long horizons. Moreover, our method can easily be trained on large offline datasets, and be quickly deployed on new tasks with good performance, making it more widely applicable than typical model-free and model-based RL methods. We evaluate the superior performance of our algorithm in a variety of RL and robotics domains.",
    "authors": [
      "Boyuan Chen",
      "Chuning Zhu",
      "Pulkit Agrawal",
      "Kaiqing Zhang",
      "Abhishek Gupta"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Model-free",
      "Transfer",
      "Random Features"
    ],
    "real_all_scores": [
      8,
      5,
      6,
      3
    ],
    "real_confidences": [
      3,
      3,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Temporal Change Sensitive Representation for Reinforcement Learing": {
    "paper_pk": null,
    "title": "Temporal Change Sensitive Representation for Reinforcement Learing",
    "abstract": "Image-based deep reinforcement learning has made a great improvement recently by combining state-of-the-art reinforcement learning algorithms with self-supervised representation learning algorithms. However, these self-supervised representation learning algorithms are designed to preserve global visual information, which may miss changes in visual information that are important for performing the task. To resolve this problem, self-supervised representation learning specifically designed for better preserving task relevant information is necessary. Following this idea, we introduce Temporal Change Sensitive Representation (TCSR), which is designed for reinforcement learning algorithms that have a latent dynamic model. TCSR enforces the latent state representation of the reinforcement agent to put more emphasis on the part of observation that could potentially change in the future. Our method achieves SoTA performance in Atari100K benchmark.",
    "authors": [
      "Qi Gao",
      "Wei Xu"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Representation Learning"
    ],
    "real_all_scores": [
      6,
      6,
      3,
      5
    ],
    "real_confidences": [
      2,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "General Policy Evaluation and Improvement by Learning to Identify Few But Crucial States": {
    "paper_pk": null,
    "title": "General Policy Evaluation and Improvement by Learning to Identify Few But Crucial States",
    "abstract": "Learning to evaluate and improve policies is a core problem of Reinforcement Learning (RL). Traditional RL algorithms learn a value function defined for a single policy. A recently explored competitive alternative is to learn a single value function for many policies. Here we combine the actor-critic architecture of Parameter-Based Value Functions and the policy embedding of Policy Evaluation Networks to learn a single value function for evaluating (and thus helping to improve) any policy represented by a deep neural network (NN). The method yields competitive experimental results. In continuous control problems with infinitely many states, our value function minimizes its prediction error by simultaneously learning a small set of  `probing states' and a mapping from actions produced in probing states to the policy's return. The method extracts crucial abstract knowledge about the environment in form of very few states sufficient to fully specify the behavior of many policies. A policy improves solely by changing actions in probing states, following the gradient of the value function's predictions. Surprisingly, it is possible to clone the behavior of a near-optimal policy in Swimmer-v3 and Hopper-v3 environments only by knowing how to act in 3 and 5 such learned states, respectively. Remarkably, our value function trained to evaluate NN policies is also invariant to changes of the policy architecture: we show that it allows for zero-shot learning of linear policies competitive with the best policy seen during training.",
    "authors": [
      "Francesco Faccio",
      "Aditya Ramesh",
      "Vincent Herrmann",
      "Jean Harb",
      "J\u00fcrgen Schmidhuber"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Off-Policy Reinforcement Learning"
    ],
    "real_all_scores": [
      3,
      5,
      3
    ],
    "real_confidences": [
      4,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Decision Transformer under Random Frame Dropping": {
    "paper_pk": null,
    "title": "Decision Transformer under Random Frame Dropping",
    "abstract": "Controlling agents remotely with deep reinforcement learning~(DRL) in the real world is yet to come. One crucial stepping stone is to devise RL algorithms that are robust in the face of dropped information from corrupted communication or malfunctioning sensors. Typical RL methods usually require considerable online interaction data that are costly and unsafe to collect in the real world. Furthermore, when applying to the frame dropping scenarios, they perform unsatisfactorily even with moderate drop rates. To address these issues, we propose  Decision Transformer under Random Frame Dropping~(DeFog), an offline RL algorithm that enables agents to act robustly in frame dropping scenarios without online interaction. DeFog first randomly masks out data in the offline datasets and explicitly adds the time span of frame dropping as inputs. After that, a finetuning stage on the same offline dataset with a higher mask rate would further boost the performance. Empirical results show that DeFog outperforms strong baselines under severe frame drop rates like 90\\%, while maintaining similar returns under non-frame-dropping conditions in the regular MuJoCo control benchmarks and the Atari environments. Our approach offers a robust and deployable solution for controlling agents in real-world environments with limited or unreliable data.",
    "authors": [
      "Kaizhe Hu",
      "Ray Chen Zheng",
      "Yang Gao",
      "Huazhe Xu"
    ],
    "keywords": [
      "Decision Transformer",
      "Reinforcement Learning",
      "Frame Dropping"
    ],
    "real_all_scores": [
      3,
      3,
      5
    ],
    "real_confidences": [
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "MAD for Robust Reinforcement Learning in Machine Translation": {
    "paper_pk": null,
    "title": "MAD for Robust Reinforcement Learning in Machine Translation",
    "abstract": "We introduce a new distributed policy gradient algorithm and show that it outperforms existing reward-aware training procedures such as REINFORCE, minimum risk training (MRT) and proximal policy optimization (PPO) in terms of training stability and generalization performance when optimizing machine translation models. Our algorithm, which we call MAD (on account of using the mean absolute deviation in the importance weighting calculation), has distributed data generators sampling multiple candidates per source sentence on worker nodes, while a central learner updates the policy. MAD depends crucially on two variance reduction strategies: (1) a conditional reward normalization method that ensures each source sentence has both positive and negative reward translation examples and (2) a new robust importance weighting scheme that acts as a conditional entropy regularizer. Experiments on a variety of translation tasks show that policies learned using the MAD algorithm perform very well when using both greedy decoding and beam search, and that the learned policies are sensitive to the specific reward used during training.",
    "authors": [
      "Domenic Donato",
      "Lei Yu",
      "Wang Ling",
      "Chris Dyer"
    ],
    "keywords": [
      "Machine Translation",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      6
    ],
    "real_confidences": [
      3,
      3,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry": {
    "paper_pk": null,
    "title": "The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry",
    "abstract": "Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model's performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems.",
    "authors": [
      "Dian Wang",
      "Jung Yeon Park",
      "Neel Sortur",
      "Lawson L.S. Wong",
      "Robin Walters",
      "Robert Platt"
    ],
    "keywords": [
      "Equivariant Learning",
      "Reinforcement Learning",
      "Robotics"
    ],
    "real_all_scores": [
      6,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "How to Enable Uncertainty Estimation in Proximal Policy Optimization": {
    "paper_pk": null,
    "title": "How to Enable Uncertainty Estimation in Proximal Policy Optimization",
    "abstract": "  While deep reinforcement learning (RL) agents have showcased strong results across many domains, a major concern is their inherent opaqueness and the safety of such systems in real-world use cases. To overcome these issues, we need agents that can quantify their uncertainty and detect out-of-distribution (OOD) states. Existing uncertainty estimation techniques, like Monte-Carlo Dropout or Deep Ensembles, have not seen widespread adoption in on-policy deep RL. We posit that this is due to two reasons: concepts like uncertainty and OOD states are not well defined compared to supervised learning, especially for on-policy RL methods.\n Secondly, available implementations and comparative studies for uncertainty estimation methods in RL have been limited. To overcome the first gap, we propose definitions of uncertainty and OOD for Actor-Critic RL algorithms, namely, proximal policy optimization (PPO), and present possible applicable measures. In particular, we discuss the concepts of value and policy uncertainty. The second point is addressed by implementing different uncertainty estimation methods and comparing them across a number of environments. The OOD detection performance is evaluated via a custom evaluation benchmark of in-distribution (ID) and OOD states for various RL environments. We identify a trade-off between reward and OOD detection performance. To overcome this, we formulate a Pareto optimization problem in which we simultaneously optimize for reward and OOD detection performance. We show experimentally that the recently proposed method of Masksembles strikes a favourable balance among the survey methods, enabling high-quality uncertainty estimation and OOD detection while matching the performance of original RL agents.",
    "authors": [
      "Eugene Bykovets",
      "Yannick Metz",
      "Mennatallah El-Assady",
      "Daniel A. Keim",
      "Joachim M. Buhmann"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Uncertainty Estimation",
      "Out-of-distribution detection",
      "Proximal Policy Optimizaiton"
    ],
    "real_all_scores": [
      6,
      8,
      8,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Making Better Decision by Directly Planning in Continuous Control": {
    "paper_pk": null,
    "title": "Making Better Decision by Directly Planning in Continuous Control",
    "abstract": "By properly utilizing the learned environment model, model-based reinforcement learning methods can improve the sample efficiency for decision-making problems. Beyond using the learned environment model to train a policy, the success of MCTS-based methods shows that directly incorporating the learned environment model as a planner to make decisions might be more effective. However, when action space is of high dimension and continuous, directly planning according to the learned model is costly and non-trivial. Because of two challenges: (1) the infinite number of candidate actions and (2) the temporal dependency between actions in different timesteps. To address these challenges, inspired by Differential Dynamic Programming (DDP) in optimal control theory, we design a novel Policy Optimization with Model Planning (POMP) algorithm, which incorporates a carefully designed Deep Differential Dynamic Programming (D3P) planner into the model-based RL framework. In D3P planner, (1) to effectively plan in the continuous action space, we construct a locally quadratic programming problem that uses a gradient-based optimization process to replace search. (2) To take the temporal dependency of actions at different timesteps into account, we leverage the updated and latest actions of previous timesteps (i.e., step $1, \\cdots, h-1$) to update the action of the current step (i.e., step $h$), instead of updating all actions simultaneously. We theoretically prove the convergence rate for our D3P planner and analyze the effect of the feedback term. In practice, to effectively apply the neural network based D3P planner in reinforcement learning, we leverage the policy network to initialize the action sequence and keep the action update conservative in the planning process. Experiments demonstrate that POMP consistently improves sample efficiency on widely used continuous control tasks. Our code is released at https://github.com/POMP-D3P/POMP-D3P. ",
    "authors": [
      "Jinhua Zhu",
      "Yue Wang",
      "Lijun Wu",
      "Tao Qin",
      "Wengang Zhou",
      "Tie-Yan Liu",
      "Houqiang Li"
    ],
    "keywords": [
      "Model-based Reinforcement Learning",
      "Reinforcement Learning",
      "Planning",
      "Policy Optimization"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      8
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Efficient Exploration using Model-Based Quality-Diversity with Gradients": {
    "paper_pk": null,
    "title": "Efficient Exploration using Model-Based Quality-Diversity with Gradients",
    "abstract": "Exploration is a key challenge in Reinforcement Learning, especially in long-horizon, deceptive and sparse-reward environments. For such applications, population-based approaches have proven effective. Methods such as Quality-Diversity deals with this by encouraging novel solutions and producing a diversity of behaviours. However, these methods are driven by either undirected sampling (i.e. mutations) or use approximated gradients (i.e. Evolution Strategies) in the parameter space, which makes them highly sample-inefficient. In this paper, we propose a model-based Quality-Diversity approach, relying on gradients and learning in imagination. Our approach optimizes all members of a population simultaneously to maintain both performance and diversity efficiently by leveraging the effectiveness of QD algorithms as good data generators to train deep models. We demonstrate that it maintains the divergent search capabilities of population-based approaches while significantly improving their sample efficiency (5 times faster) and quality of solutions (2 times more performant).",
    "authors": [
      "Bryan Lim",
      "Manon Flageat",
      "Antoine Cully"
    ],
    "keywords": [
      "Quality-Diversity",
      "Exploration",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      3,
      5
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "HiT-MDP: Learning the SMDP option framework on MDPs with Hidden Temporal Embeddings": {
    "paper_pk": null,
    "title": "HiT-MDP: Learning the SMDP option framework on MDPs with Hidden Temporal Embeddings",
    "abstract": "The standard option framework is developed on the Semi-Markov Decision Process (SMDP) which is unstable to optimize and sample inefficient. To this end, we propose the Hidden Temporal MDP (HiT-MDP) and prove that the option-induced HiT-MDP is homomorphic equivalent to the option-induced SMDP. A novel transformer-based framework is introduced to learn options' embedding vectors (rather than conventional option tuples) on HiT-MDPs. We then derive a stable and sample efficient option discovering method under the maximum-entropy policy gradient framework. Extensive experiments on challenging Mujoco environments demonstrate HiT-MDP's efficiency and effectiveness: under widely used configurations, HiT-MDP achieves competitive, if not better, performance compared to the state-of-the-art baselines on all finite horizon and transfer learning environments. Moreover, HiT-MDP significantly outperforms all baselines on infinite horizon environments while exhibiting smaller variance, faster convergence, and better interpretability. Our work potentially sheds light on the theoretical ground of extending the option framework into a large-scale foundation model.",
    "authors": [
      "Chang Li",
      "Dongjin Song",
      "Dacheng Tao"
    ],
    "keywords": [
      "Hiearchical Reinforcement Learning",
      "Reinforcement Learning",
      "Markov Decision Process"
    ],
    "real_all_scores": [
      6,
      8,
      6
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration": {
    "paper_pk": null,
    "title": "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration",
    "abstract": "The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. \nSkill reuse is one of the most common approaches, but current methods have considerable limitations. For example, fine-tuning an existing policy frequently fails, as the policy can degrade rapidly early in training, particularly in sparse reward tasks. In a similar vein, distillation of expert behavior can lead to poor results when given sub-optimal experts. \nWe compare several common approaches for skill transfer on multiple domains and in several different transfer settings, including under changes in task and system dynamics. We identify how existing methods can fail and introduce an alternative approach which sidesteps some of these problems. \nOur approach learns to sequence existing temporally-abstract skills for exploration but learns the final policy directly from the raw experience. This conceptual split enables rapid adaptation and thus efficient data collection but without constraining the final solution. Our approach significantly outperforms many classical methods across a suite of evaluation tasks and we use a broad set of ablations to highlight the importance of different components of our method.",
    "authors": [
      "Giulia Vezzani",
      "Dhruva Tirumala",
      "Markus Wulfmeier",
      "Dushyant Rao",
      "Abbas Abdolmaleki",
      "Ben Moran",
      "Tuomas Haarnoja",
      "Jan Humplik",
      "Roland Hafner",
      "Michael Neunert",
      "Claudio Fantacci",
      "Tim Hertweck",
      "Thomas Lampe",
      "Fereshteh Sadeghi",
      "Nicolas Heess",
      "Martin Riedmiller"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Control",
      "Skills",
      "Priors",
      "Hierarchical Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      3,
      2
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning": {
    "paper_pk": null,
    "title": "Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning",
    "abstract": "Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent's latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).",
    "authors": [
      "Mhairi Dunion",
      "Trevor McInroe",
      "Kevin Sebastian Luck",
      "Josiah P. Hanna",
      "Stefano V Albrecht"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Representation Learning",
      "Disentanglement"
    ],
    "real_all_scores": [
      8,
      8,
      10,
      8
    ],
    "real_confidences": [
      3,
      2,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Optimistic Exploration with Learned Features Provably Solves Markov Decision Processes with Neural Dynamics": {
    "paper_pk": null,
    "title": "Optimistic Exploration with Learned Features Provably Solves Markov Decision Processes with Neural Dynamics",
    "abstract": "Incorporated with the recent advances in deep learning, deep reinforcement learning (DRL) has achieved tremendous success in empirical study. However, analyzing DRL is still challenging due to the complexity of the neural network class. In this paper, we address such a challenge by analyzing the Markov decision process (MDP) with neural dynamics, which covers several existing models as special cases, including the kernelized nonlinear regulator (KNR) model and the linear MDP. We propose a novel algorithm that designs exploration incentives via learnable representations of the dynamics model by embedding the neural dynamics into a kernel space induced by the system noise. We further establish an upper bound on the sample complexity of the algorithm, which demonstrates the sample efficiency of the algorithm. We highlight that, unlike previous analyses of RL algorithms with function approximation, our bound on the sample complexity does not depend on the Eluder dimension of the neural network class, which is known to be exponentially large (Dong et al., 2021).",
    "authors": [
      "Sirui Zheng",
      "Lingxiao Wang",
      "Shuang Qiu",
      "Zuyue Fu",
      "Zhuoran Yang",
      "Csaba Szepesvari",
      "Zhaoran Wang"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Neural Network",
      "Representation Learning."
    ],
    "real_all_scores": [
      6,
      8,
      3,
      6,
      8
    ],
    "real_confidences": [
      3,
      4,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Adversarial Cheap Talk": {
    "paper_pk": null,
    "title": "Adversarial Cheap Talk",
    "abstract": "Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim\u2019s parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim\u2019s observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim\u2019s actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT can still significantly influence the Victim\u2019s training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorithms. More specifically, we show that an ACT Adversary is capable of harming performance by interfering with the learner\u2019s function approximation, or instead helping the Victim\u2019s performance by outputting useful features. Finally, we show that an ACT Adversary can manipulate messages during train-time to directly and arbitrarily control the Victim at test-time.",
    "authors": [
      "Chris Lu",
      "Timon Willi",
      "Alistair Letcher",
      "Jakob Nicolaus Foerster"
    ],
    "keywords": [
      "Meta-Learning",
      "Reinforcement Learning",
      "Meta-Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      2,
      2,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "In-context Reinforcement Learning with Algorithm Distillation": {
    "paper_pk": null,
    "title": "In-context Reinforcement Learning with Algorithm Distillation",
    "abstract": "We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.",
    "authors": [
      "Michael Laskin",
      "Luyu Wang",
      "Junhyuk Oh",
      "Emilio Parisotto",
      "Stephen Spencer",
      "Richie Steigerwald",
      "DJ Strouse",
      "Steven Stenberg Hansen",
      "Angelos Filos",
      "Ethan Brooks",
      "maxime gazeau",
      "Himanshu Sahni",
      "Satinder Singh",
      "Volodymyr Mnih"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Transformers",
      "Learning to Learn",
      "Large Language Models"
    ],
    "real_all_scores": [
      1,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "A Mutual Information Duality Algorithm for Multi-Agent Specialization": {
    "paper_pk": null,
    "title": "A Mutual Information Duality Algorithm for Multi-Agent Specialization",
    "abstract": "The social behavior change in a population has long been studied as an essential component of multi-agent learning. The learning of behavioral change not only involves reinforcement learning (RL), but also be measured against the general population with mutual information (MI). The combination of RL and MI led us to derive MI optimizations from policy gradient. With MI as multi-agent's optimization objective, we discover that the dual properties of MI can result in distinctly different population behaviors. From MI maximization that maximizes the stability of a population to MI minimization that enables specialization among the agents, the dual of MI creates a significant change in a population's behavioral properties. In this paper, we propose a minimax formulation of MI (M\\&M) that enables agents specialization with stable regularization. Empirically we evaluated M\\&M against the prior SOTA MARL framework, and analyze the social behavior change in performance, diversity, and the stability of their social graphs. ",
    "authors": [
      "Stefan Juang",
      "Qiyang Cao",
      "Yuan Zhou",
      "Ruochen Liu",
      "Nevin Zhang",
      "Elvis S. Liu"
    ],
    "keywords": [
      "Multi-agent",
      "Reinforcement Learning",
      "Mutual Information",
      "Duality",
      "Policy Gradient",
      "Social Graph"
    ],
    "real_all_scores": [
      3,
      5,
      5
    ],
    "real_confidences": [
      5,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Emergent collective intelligence from massive-agent cooperation and competition": {
    "paper_pk": null,
    "title": "Emergent collective intelligence from massive-agent cooperation and competition",
    "abstract": "Inspired by organisms evolving through cooperation and competition between different populations on Earth, we study the emergence of artificial collective intelligence through massive-agent reinforcement learning. To this end, We propose a new massive-agent reinforcement learning environment, Lux, where dynamic and massive agents in two teams scramble for limited resources and fight off the darkness. In Lux, we build our agents through the standard reinforcement learning algorithm in curriculum learning phases and leverage centralized control via a pixel-to-pixel policy network. As agents co-evolve through self-play, we observe several stages of intelligence, from the acquisition of atomic skills to the development of group strategies. Since these learned group strategies arise from individual decisions without an explicit coordination mechanism, we claim that artificial collective intelligence emerges from massive-agent cooperation and competition. We further analyze the emergence of various learned strategies through metrics and ablation studies, aiming to provide insights for reinforcement learning implementations in massive-agent environments.",
    "authors": [
      "Hanmo Chen",
      "Stone Tao",
      "Jiaxin Chen",
      "Weihan Shen",
      "Xihui Li",
      "Sikai Cheng",
      "Xiaolong Zhu",
      "Xiu Li"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Multi-agent System",
      "Emergent Behavior"
    ],
    "real_all_scores": [
      5,
      3,
      8,
      5
    ],
    "real_confidences": [
      5,
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "User-Interactive Offline Reinforcement Learning": {
    "paper_pk": null,
    "title": "User-Interactive Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning algorithms still lack trust in practice due to the risk that the learned policy performs worse than the original policy that generated the dataset or behaves in an unexpected way that is unfamiliar to the user. At the same time, offline RL algorithms are not able to tune their most important hyperparameter - the proximity of the learned policy to the original policy. We propose an algorithm that allows the user to tune this hyperparameter at runtime, thereby addressing both of the above mentioned issues simultaneously. This allows users to start with the original behavior and grant successively greater deviation, as well as stopping at any time when the policy deteriorates or the behavior is too far from the familiar one.",
    "authors": [
      "Phillip Swazinna",
      "Steffen Udluft",
      "Thomas Runkler"
    ],
    "keywords": [
      "Offline RL",
      "Reinforcement Learning",
      "User",
      "Model-based",
      "Adaptive"
    ],
    "real_all_scores": [
      6,
      5,
      6,
      8
    ],
    "real_confidences": [
      4,
      3,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning with Linear Function Approximation": {
    "paper_pk": null,
    "title": "Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning with Linear Function Approximation",
    "abstract": "We study the problem of deployment efficient reinforcement learning (RL) with linear function approximation under the \\emph{reward-free} exploration setting. This is a well-motivated problem because deploying new policies is costly in real-life RL applications. Under the linear MDP setting with feature dimension $d$ and planning horizon $H$, we propose a new algorithm that collects at most $\\widetilde{O}(\\frac{d^2H^5}{\\epsilon^2})$ trajectories within $H$ deployments to identify $\\epsilon$-optimal policy for any (possibly data-dependent) choice of reward functions. To the best of our knowledge, our approach is the first to achieve optimal deployment complexity and optimal $d$ dependence in sample complexity at the same time, even if the reward is known ahead of time. Our novel techniques include an exploration-preserving policy discretization and a generalized G-optimal experiment design, which could be of independent interest. Lastly, we analyze the related problem of regret minimization in low-adaptive RL and provide information-theoretic lower bounds for switching cost and batch complexity.",
    "authors": [
      "Dan Qiao",
      "Yu-Xiang Wang"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Deployment efficiency",
      "Reward free RL",
      "Low adaptive RL"
    ],
    "real_all_scores": [
      5,
      3,
      6,
      3
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Provably efficient multi-task Reinforcement Learning in large state spaces": {
    "paper_pk": null,
    "title": "Provably efficient multi-task Reinforcement Learning in large state spaces",
    "abstract": "We study multi-task Reinforcement Learning where shared knowledge among different environments is distilled to enable scalable generalization to a variety of problem instances. In the context of general function approximation, Markov Decision Process (MDP) with low Bilinear rank encapsulates a wide range of structural conditions that permit polynomial sample complexity in large state spaces, where the Bellman errors are related to bilinear forms of features with low intrinsic dimensions. To achieve multi-task learning in MDPs, we propose online representation learning algorithms to capture the shared features in the different task-specific bilinear forms. We show that in the presence of low-rank structures in the features of the bilinear forms, the algorithms benefit from sample complexity improvements compared to single-task learning. Therefore, we achieve the first sample efficient multi-task reinforcement learning algorithm with general function approximation.",
    "authors": [
      "Baihe Huang",
      "Jason D. Lee",
      "Zhaoran Wang",
      "Zhuoran Yang"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Multi-task Learning",
      "Function Approximation",
      "Sample Effficiency"
    ],
    "real_all_scores": [
      3,
      5,
      3
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation": {
    "paper_pk": null,
    "title": "A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation",
    "abstract": "The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.",
    "authors": [
      "Hiroki Furuta",
      "Yusuke Iwasawa",
      "Yutaka Matsuo",
      "Shixiang Shane Gu"
    ],
    "keywords": [
      "Morphology-Task Generalization",
      "Behavior Distillation",
      "Supervised RL",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      3,
      6,
      5,
      5
    ],
    "real_confidences": [
      2,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Energy-based Predictive Representation for Reinforcement Learning": {
    "paper_pk": null,
    "title": "Energy-based Predictive Representation for Reinforcement Learning",
    "abstract": "In real world applications, it is usually necessary for a reinforcement learning algorithm to handle the partial observability beyond Markov decision processes (MDPs). Although the partially observable Markov decision process (POMDP) has been precisely motivated for this requirement, such a formulation raises significant computational and statistical hardness challenges in learning and planning. In this work, we introduce the Energy-based Predictive Representation (EPR), which leads to a unified framework for practical reinforcement learning algorithm design in both MDPs and POMDPs settings, to handle the learning, exploration, and planning in a coherent way. The proposed approach relies on the powerful neural energy-based model to extract sufficient representation, from which Q-functions can be efficiently approximated. With such a representation, we develop an efficient approach for computing confidence, which allows optimism/pessimism in the face of uncertainty to be efficiently implemented in planning, hence managing the exploration versus exploitation tradeoff. An experimental investigation shows that the proposed algorithm can surpass state-of-the-art performance in both MDP and POMDP settings in comparison to existing baselines.",
    "authors": [
      "Tianjun Zhang",
      "Tongzheng Ren",
      "Chenjun Xiao",
      "Wenli Xiao",
      "Joseph E. Gonzalez",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "keywords": [
      "Energy-based Models",
      "Predictive State Representation",
      "Partially Observable Markov Decision Process",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      6,
      6,
      8
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Latent Variable Representation for Reinforcement Learning": {
    "paper_pk": null,
    "title": "Latent Variable Representation for Reinforcement Learning",
    "abstract": "Deep latent variable models have achieved significant empirical successes in model-based reinforcement learning (RL) due to their expressiveness in modeling complex transition dynamics. On the other hand, it remains unclear theoretically and empirically how latent variable models may facilitate learning, planning, and exploration to improve the sample efficiency of RL. In this paper, we provide a representation view of the latent variable models for state-action value functions, which allows both tractable variational learning algorithm and effective implementation of the optimism/pessimism principle in the face of uncertainty for exploration. In particular, we propose a computationally efficient planning algorithm with UCB exploration by incorporating kernel embeddings of latent variable models. Theoretically, we establish the sample complexity of the proposed approach in the online and offline settings. Empirically, we demonstrate superior performance over current state-of-the-art algorithms across various benchmarks.",
    "authors": [
      "Tongzheng Ren",
      "Chenjun Xiao",
      "Tianjun Zhang",
      "Na Li",
      "Zhaoran Wang",
      "sujay sanghavi",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "keywords": [
      "Latent Variable Model",
      "Markov Decision Processes",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs": {
    "paper_pk": null,
    "title": "ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs",
    "abstract": "Solving combinatorial optimization (CO) on graphs has been attracting increasing interests from the machine learning community whereby data-driven approaches were recently devised to go beyond traditional manually-designated algorithms. In this paper, we study the robustness of a combinatorial solver as a blackbox regardless it is classic or learning-based though the latter can often be more interesting to the ML community. Specifically, we develop a practically feasible robustness metric for general CO solvers. A no-worse optimal cost guarantee is developed as such the optimal solutions are not required to achieve for solvers, and we tackle the non-differentiable challenge in input instance disturbance by resorting to black-box adversarial attack methods. Extensive experiments are conducted on 14 unique combinations of solvers and CO problems, and we demonstrate that the performance of state-of-the-art solvers like Gurobi can degenerate by over 20% under the given time limit bound on the hard instances discovered by our robustness metric, raising concerns about the robustness of combinatorial optimization solvers.",
    "authors": [
      "Han Lu",
      "Zenan Li",
      "Runzhong Wang",
      "Qibing Ren",
      "Xijun Li",
      "Mingxuan Yuan",
      "Jia Zeng",
      "Xiaokang Yang",
      "Junchi Yan"
    ],
    "keywords": [
      "Combinatorial Optimization",
      "Robustness",
      "Graph Neural Networks",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      6,
      6
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Spectral Decomposition Representation for Reinforcement Learning": {
    "paper_pk": null,
    "title": "Spectral Decomposition Representation for Reinforcement Learning",
    "abstract": "Representation learning often plays a critical role in avoiding the curse of dimensionality in reinforcement learning. A representative class of algorithms exploits spectral decomposition of the stochastic transition dynamics to construct representations that enjoy strong theoretical properties in idealized settings. However, current spectral methods suffer from limited applicability because they are constructed for\nstate-only aggregation and are derived from a policy-dependent transition kernel, without considering the issue of exploration. To address these issues, we propose an alternative spectral method, Spectral Decomposition Representation (SPEDER), that extracts a state-action abstraction from the dynamics without inducing spurious dependence on the data collection policy, while also balancing the exploration-versus-exploitation trade-off during learning. A theoretical analysis establishes the sample efficiency of the proposed algorithm in both the online and offline settings. In addition, an experimental investigation demonstrates superior performance over current state-of-the-art algorithms across several RL benchmarks.",
    "authors": [
      "Tongzheng Ren",
      "Tianjun Zhang",
      "Lisa Lee",
      "Joseph E. Gonzalez",
      "Dale Schuurmans",
      "Bo Dai"
    ],
    "keywords": [
      "Spectral Representation",
      "Markov Decision Processes",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      8,
      5,
      5
    ],
    "real_confidences": [
      5,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Goal-Space Planning with Subgoal Models": {
    "paper_pk": null,
    "title": "Goal-Space Planning with Subgoal Models",
    "abstract": "This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can learn significantly faster than a Double DQN baseline in a variety of situations.",
    "authors": [
      "Chunlok Lo",
      "Gabor Mihucz",
      "Farzane Aminmansour",
      "Adam White",
      "Martha White"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Model-Based Reinforcement Learning",
      "Planning",
      "Temporal Abstraction"
    ],
    "real_all_scores": [
      8,
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Skill-Based Reinforcement Learning with Intrinsic Reward Matching": {
    "paper_pk": null,
    "title": "Skill-Based Reinforcement Learning with Intrinsic Reward Matching",
    "abstract": "While unsupervised skill discovery has shown promise in autonomously acquiring behavioral primitives, there is still a large methodological disconnect between task-agnostic skill pretraining and downstream, task-aware finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two phases of learning via the $\\textit{skill discriminator}$, a pretraining model component often discarded during finetuning. Conventional approaches finetune pretrained agents directly at the policy level, often relying on expensive environment rollouts to empirically determine the optimal skill. However, often the most concise yet complete description of a task is the reward function itself, and skill learning methods learn an $\\textit{intrinsic}$ reward function via the discriminator that corresponds to the skill policy. We propose to leverage the skill discriminator to $\\textit{match}$ the intrinsic and downstream task rewards and determine the optimal skill for an unseen task without environment samples, consequently finetuning with greater sample-efficiency. Furthermore, we generalize IRM to sequence skills and solve more complex, long-horizon tasks. We demonstrate that IRM is competitive with previous skill selection methods on the Unsupervised Reinforcement Learning Benchmark and enables us to utilize pretrained skills far more effectively on challenging tabletop manipulation tasks.",
    "authors": [
      "Ademi Adeniji",
      "Amber Xie",
      "Pieter Abbeel"
    ],
    "keywords": [
      "Unsupervised Reinforcement Learning",
      "Reinforcement Learning",
      "Deep Learning"
    ],
    "real_all_scores": [
      6,
      3,
      5,
      5
    ],
    "real_confidences": [
      5,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Neural DAG Scheduling via One-Shot Priority Sampling": {
    "paper_pk": null,
    "title": "Neural DAG Scheduling via One-Shot Priority Sampling",
    "abstract": "We consider the problem of scheduling operations/nodes, the dependency among which is characterized by a Directed Acyclic Graph (DAG). Due to its NP-hard nature, heuristic algorithms were traditionally used to acquire reasonably good solutions, and more recent works have proposed Machine Learning (ML) heuristics that can generalize to unseen graphs and outperform the non-ML heuristics. However, it is computationally costly to generate solutions using existing ML schedulers since they adopt the episodic reinforcement learning framework that necessitates multi-round neural network processing. We propose a novel ML scheduler that uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules. Since the one-shot encoder can efficiently sample the priorities in parallel, our algorithm runs significantly faster than existing ML baselines and has comparable run time with the fast traditional heuristics. We empirically show that our algorithm generates better schedules than both non-neural and neural baselines across various real-world and synthetic scheduling tasks.",
    "authors": [
      "Wonseok Jeon",
      "Mukul Gagrani",
      "Burak Bartan",
      "Weiliang Will Zeng",
      "Harris Teague",
      "Piero Zappi",
      "Christopher Lott"
    ],
    "keywords": [
      "Combinatorial Optimization",
      "Directed Acyclic Graph",
      "Scheduling",
      "Graph Neural Network",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      8,
      6,
      1,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Value-Based Membership Inference Attack on Actor-Critic Reinforcement Learning": {
    "paper_pk": null,
    "title": "Value-Based Membership Inference Attack on Actor-Critic Reinforcement Learning",
    "abstract": "In actor-critic reinforcement learning (RL), the so-called actor and critic, respectively, compute candidate policies and a value function that evaluates the candidate policies. Such RL algorithms may be vulnerable to membership inference attacks (MIAs), a privacy attack that infers the data membership, i.e., whether a specific data record belongs to the training dataset. We investigate the vulnerability of value function in actor-critic to MIAs. We develop \\textit{CriticAttack}, a new MIA that targets black-box RL agents by examining the correlation between the expected reward and the value function. We empirically show that \\textit{CriticAttack} can correctly infer approximately 90\\% of the training data membership, i.e., it achieves 90\\% attack accuracy. Such accuracy is far beyond the 50\\% random guessing accuracy, indicating a severe privacy vulnerability of the value function. To defend against \\textit{CriticAttack}, we design a method called \\textit{CriticDefense} that inserts uniform noise to the value function. \\textit{CriticDefense} can reduce the attack accuracy to 60\\% without significantly affecting the agent\u2019s performance.",
    "authors": [
      "Yunhao Yang",
      "ufuk topcu"
    ],
    "keywords": [
      "Privacy",
      "Membership Inference Attack",
      "Value Function",
      "Actor-Critic",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Variance Double-Down: The Small Batch Size Anomaly in Multistep Deep Reinforcement Learning": {
    "paper_pk": null,
    "title": "Variance Double-Down: The Small Batch Size Anomaly in Multistep Deep Reinforcement Learning",
    "abstract": "State of the art results in reinforcement learning suggest that multi-step learning is necessary. However, the increased variance that comes with it makes it difficult to increase the update horizon beyond relatively small numbers. In this paper, we report the counterintuitive finding that decreasing the batch size substantially improves performance across a large swath of deep RL agents. It is well-known that gradient variance decreases with increasing batch sizes, so obtaining improved performance by increasing variance on two fronts is a rather surprising finding. We conduct a broad set of experiments to better understand this variance double-down phenomenon.",
    "authors": [
      "Johan Samir Obando Ceron",
      "Marc G Bellemare",
      "Pablo Samuel Castro"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Deep Reinforcement Learning",
      "Value based",
      "Batch Size",
      "Multi step learning"
    ],
    "real_all_scores": [
      5,
      3,
      8,
      5
    ],
    "real_confidences": [
      4,
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Variational Latent Branching Model for Off-Policy Evaluation": {
    "paper_pk": null,
    "title": "Variational Latent Branching Model for Off-Policy Evaluation",
    "abstract": "Model-based methods have recently shown great potential for off-policy evaluation (OPE); offline trajectories induced by behavioral policies are fitted to transitions of Markov decision processes (MDPs), which are used to rollout simulated trajectories and estimate the performance of policies. Model-based OPE methods face two key challenges. First, as offline trajectories are usually fixed, they tend to cover limited state and action space. Second, the performance of model-based methods can be sensitive to the initialization of their parameters. In this work, we propose the variational latent branching model (VLBM) to learn the transition function of MDPs by formulating the environmental dynamics as a compact latent space, from which the next states and rewards are then sampled. Specifically, VLBM leverages and extends the variational inference framework with the recurrent state alignment (RSA), which is designed to capture as much information underlying the limited training data, by smoothing out the information flow between the variational (encoding) and generative (decoding) part of VLBM. Moreover, we also introduce the branching architecture to improve the model\u2019s robustness against randomly initialized model weights. The effectiveness of the VLBM is evaluated on the deep OPE (DOPE) benchmark, from which the training trajectories are designed to result in varied coverage of the state-action space. We show that the VLBM outperforms existing state-of-the-art OPE methods in general.",
    "authors": [
      "Qitong Gao",
      "Ge Gao",
      "Min Chi",
      "Miroslav Pajic"
    ],
    "keywords": [
      "Model-Based Off-policy Evaluation",
      "Reinforcement Learning",
      "Variational Inference"
    ],
    "real_all_scores": [
      5,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Human-level Atari 200x faster": {
    "paper_pk": null,
    "title": "Human-level Atari 200x faster",
    "abstract": "The task of building general agents that perform well over a wide range of tasks has been an important goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline, within our novel agent MEME. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. Our contributions aim to achieve faster propagation of learning signals related to rare events, stabilize learning under differing value scales, improve the neural network architecture, and make updates more robust under a rapidly-changing policy.",
    "authors": [
      "Steven Kapturowski",
      "V\u00edctor Campos",
      "Ray Jiang",
      "Nemanja Rakicevic",
      "Hado van Hasselt",
      "Charles Blundell",
      "Adria Puigdomenech Badia"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Data-efficiency",
      "Exploration",
      "Off-policy"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Dynamics-aware Skill Generation from Behaviourally Diverse Demonstrations": {
    "paper_pk": null,
    "title": "Dynamics-aware Skill Generation from Behaviourally Diverse Demonstrations",
    "abstract": "Learning from demonstrations (LfD) provides a data-efficient way for a robot to learn a task by observing humans performing the task, without the need for an explicit reward function. However, in many real-world scenarios (e.g., driving a car) humans often perform the same task in different ways, motivated not only by the primary objective of the task (e.g., reaching the destination safely) but also by their individual preferences (e.g., different driving behaviours), leading to a multi-modal distribution of demonstrations. In this work, we consider an LfD problem, where the reward function for the main objective of the task is known to the learning agent; however, the individual preferences leading to the variations in the demonstrations are unknown. We show that current LfD approaches learn policies that either track a single mode or the mean of the demonstration distribution. In contrast, we propose an algorithm to learn a diverse set of policies to perform the task, capturing the different modes in the demonstrations due to the diverse preferences of the individuals. We show that we can build a parameterised solution space that captures different behaviour patterns from the demonstrations. Then, a set of policies can be generated in solution space that generate a diverse range of behaviours that go beyond the provided demonstrations.",
    "authors": [
      "Shibei Zhu",
      "Rituraj Kaushik",
      "Samuel Kaski",
      "Ville Kyrki"
    ],
    "keywords": [
      "Learning from Demonstration",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      5,
      5,
      6
    ],
    "real_confidences": [
      3,
      3,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Faster Reinforcement Learning with Value Target Lower Bounding": {
    "paper_pk": null,
    "title": "Faster Reinforcement Learning with Value Target Lower Bounding",
    "abstract": "We show that an arbitrary lower bound of the maximum achievable value can be used to improve the Bellman value target during value learning.  In the tabular case, value learning using the lower bounded Bellman operator converges to the same optimal value as using the original Bellman operator, at a potentially faster speed.  In practice, discounted episodic return in episodic tasks and n-step bootstrapped return in continuing tasks can serve as lower bounds to improve the value target.  We experiment on Atari games, FetchEnv tasks and a challenging physically simulated car push and reach task.  We see large gains in sample efficiency as well as converged performance over common baselines such as TD3, SAC and Hindsight Experience Replay (HER) in most tasks, and observe a reliable and competitive performance against the stronger n-step methods such as td-lambda, Retrace and optimality tightening.  Prior works have already successfully applied a special case of lower bounding (using episodic return), but are limited to a small number of episodic tasks.  To the best of our knowledge, we are the first to propose the general method of value target lower bounding (with possibly bootstrapped return), to demonstrate its optimality in theory, and effectiveness in a wide range of tasks over many strong baselines.",
    "authors": [
      "Le Zhao",
      "Wei Xu"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Bellman equation",
      "value improvement",
      "sample efficiency"
    ],
    "real_all_scores": [
      3,
      1,
      5
    ],
    "real_confidences": [
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Context and History Aware Other-Shaping": {
    "paper_pk": null,
    "title": "Context and History Aware Other-Shaping",
    "abstract": "Cooperation failures, in which self-interested agents converge to collectively worst-case outcomes, are a common failure mode of Multi-Agent Reinforcement Learning (MARL) methods. Methods such as Model-Free Opponent Shaping (MFOS) and The Good Shepherd address this issue by shaping their co-player\u2019s learning into mutual cooperation. However, these methods fail to capture important co-player learning dynamics or do not scale to co-players parameterised by deep neural networks. To address these issues, we propose Context and History Aware Other-Shaping (CHAOS). A CHAOS agent is a meta-learner parameterised by a recurrent neural network that learns to shape its co-player over multiple trials. CHAOS considers both the context (inter-episode information), and history (intra-episode information) to shape co-players successfully. CHAOS also successfully scales to shaping co-players parameterised by deep neural networks. In a set of experiments, we show that CHAOS achieves state-of-the-art shaping in matrix games. We provide extensive ablations, motivating the importance of both context and history. CHAOS also successfully shapes on a complex grid-worldbased game, demonstrating CHAOS\u2019s scalability empirically. Finally, we provide empirical evidence that, counterintuitively, the widely-used Coin Game environment does not require history to learn shaping because states are often indicative of past actions. This suggests that the Coin Game is, in contrast to common understanding, unsuitable for investigating shaping in high-dimensional, multi-step environments.",
    "authors": [
      "Akbir Khan",
      "Newton Kwan",
      "Timon Willi",
      "Chris Lu",
      "Andrea Tacchetti",
      "Jakob Nicolaus Foerster"
    ],
    "keywords": [
      "Shaping",
      "Multi-Agent",
      "Reinforcement Learning",
      "Meta Reinforcement Learning"
    ],
    "real_all_scores": [
      3,
      1,
      3,
      1
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "On the Data-Efficiency with Contrastive Image Transformation in Reinforcement Learning": {
    "paper_pk": null,
    "title": "On the Data-Efficiency with Contrastive Image Transformation in Reinforcement Learning",
    "abstract": "Data-efficiency has always been an essential issue in pixel-based reinforcement learning (RL). As the agent not only learns decision-making but also meaningful representations from images. The line of reinforcement learning with data augmentation shows significant improvements in sample-efficiency. However, it is challenging to guarantee the optimality invariant transformation, that is, the augmented data are readily recognized as a completely different state by the agent. In the end, we propose a contrastive invariant transformation (CoIT), a simple yet promising learnable data augmentation combined with standard model-free algorithms to improve sample-efficiency. Concretely, the differentiable CoIT leverages original samples with augmented samples and hastens the state encoder for a contrastive invariant embedding. We evaluate our approach on DeepMind Control Suite and Atari100K. Empirical results verify advances using CoIT, enabling it to outperform the new state-of-the-art on various tasks. Source code is available at https://github.com/mooricAnna/CoIT.",
    "authors": [
      "Sicong Liu",
      "Xi Sheryl Zhang",
      "Yushuo Li",
      "Yifan Zhang",
      "Jian Cheng"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Data Augmentation",
      "Self-Supervised Learning",
      "Representation Learning"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "System Identification as a Reinforcement Learning Problem": {
    "paper_pk": null,
    "title": "System Identification as a Reinforcement Learning Problem",
    "abstract": "System identification, also known as learning forward models, transfer functions, system dynamics, etc., has a long tradition both in science and engineering in different fields. Particularly, it is a recurring theme in Reinforcement Learning research, where forward models approximate the state transition function of a Markov Decision Process by learning a mapping function from current state and action to the next state. This problem is commonly defined as a Supervised Learning problem in a direct way. This common approach faces several difficulties due to the inherent complexities of the dynamics to learn, for example, delayed effects, high non-linearity, non-stationarity, partial observability and, more important, error accumulation when using bootstrapped predictions (predictions based on past predictions), over large time horizons. Here we explore the use of Reinforcement Learning in this problem. We elaborate on why and how this problem fits naturally and sound as a Reinforcement Learning problem, and present some experimental results that demonstrate RL is a promising technique to solve these kind of problems.",
    "authors": [
      "Jose Antonio Martin H.",
      "Oscar Fern\u00e1ndez Vicente",
      "Sergio Perez",
      "Anas Belfadil",
      "Cristina Ibanez-Llano",
      "Freddy Perozo",
      "Jose Javier Valle",
      "Javier Arechalde Pelaz"
    ],
    "keywords": [
      "System Identification",
      "Reinforcement Learning",
      "Offline Reinforcement Learning",
      "Forward Models"
    ],
    "real_all_scores": [
      5,
      3,
      5,
      6
    ],
    "real_confidences": [
      5,
      5,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "How Predictors Affect Search Strategies in Neural Architecture Search?": {
    "paper_pk": null,
    "title": "How Predictors Affect Search Strategies in Neural Architecture Search?",
    "abstract": "Predictor-based Neural Architecture Search (NAS) is an important topic since it can efficiently reduce the computational cost of evaluating candidate architectures. Most existing predictor-based NAS algorithms aim to design different predictors to improve prediction performance. Unfortunately, even a promising performance predictor may suffer from the accuracy decline due to long-term and\ncontinuous usage, thus leading to the degraded performance of the search strategy. That naturally gives rise to the following problems: how predictors affect search strategies and how to appropriately use the predictor? In this paper, we take reinforcement learning (RL) based search strategy to study theoretically and empirically the impact of predictors on search strategies. We first formulate a\npredictor-RL-based NAS algorithm as model-based RL and analyze it with a guarantee of monotonic improvement at each trail. Then, based on this analysis, we propose a simple procedure of predictor usage, named mixed batch, which contains ground-truth data and prediction data. The proposed procedure can efficiently reduce the impact of predictor errors on search strategies with maintaining performance growth. Our algorithm, Predictor-based Neural Architecture Search with Mixed batch (PNASM), outperforms traditional NAS algorithms and\nprior state-of-the-art predictor-based NAS algorithms on three NAS-Bench-201 tasks.",
    "authors": [
      "TianJin Deng",
      "Jia Wu"
    ],
    "keywords": [
      "Neural Architecture Search",
      "Predictor-based Neural Architecture Search",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      8,
      8
    ],
    "real_confidences": [
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Bi-Level Dynamic Parameter Sharing among Individuals and Teams for Promoting Collaborations in Multi-Agent Reinforcement Learning": {
    "paper_pk": null,
    "title": "Bi-Level Dynamic Parameter Sharing among Individuals and Teams for Promoting Collaborations in Multi-Agent Reinforcement Learning",
    "abstract": "Parameter sharing has greatly contributed to the success of multi-agent reinforcement learning in recent years. However, most existing parameter sharing mechanisms are static, and parameters are indiscriminately shared among individuals, ignoring the dynamic environments and different roles of multiple agents. In addition, although a single-level selective parameter sharing mechanism can promote the diversity of strategies, it is hard to establish complementary and cooperative relationships between agents. To address these issues, we propose a bi-level dynamic parameter sharing mechanism among individuals and teams for promoting effective collaborations (BDPS). Specifically, at the individual level, we define virtual dynamic roles based on the long-term cumulative advantages of agents and share parameters among agents in the same role. At the team level, we combine agents of different virtual roles and share parameters of agents in the same group. Through the joint efforts of these two levels, we achieve a dynamic balance between the individuality and commonality of agents, enabling agents to learn more complex and complementary collaborative relationships. We evaluate BDPS on a challenging set of StarCraft II micromanagement tasks. The experimental results show that our method outperforms the current state-of-the-art baselines, and we demonstrate the reliability of our proposed structure through ablation experiments.",
    "authors": [
      "Yan Liu",
      "Ying Tiffany He",
      "Fei Richard Yu",
      "Zhong Ming"
    ],
    "keywords": [
      "Multi-Agent Reinforcement Learning",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      3,
      5,
      3
    ],
    "real_confidences": [
      4,
      4,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "$Q$-learning with regularization converges with non-linear non-stationary features": {
    "paper_pk": null,
    "title": "$Q$-learning with regularization converges with non-linear non-stationary features",
    "abstract": "The deep $Q$-learning architecture is a neural network composed of non-linear hidden layers that learn features of states and actions and a final linear layer that learns the $Q$-values of the features. The parameters of both components can possibly diverge. Regularization of the updates is known to solve the divergence problem of fully linear architectures, where features are stationary and known a priori. We propose a deep $Q$-learning scheme that uses regularization of the final linear layer of architecture, updating it along a faster time-scale, and stochastic full-gradient descent updates for the non-linear features at a slower time-scale. We prove the proposed scheme converges with probability 1. Finally, we provide a bound on the error introduced by regularization of the final linear layer of the architecture.",
    "authors": [
      "Diogo S. Carvalho",
      "Francisco S. Melo",
      "Pedro A. Santos"
    ],
    "keywords": [
      "Q-learning",
      "Reinforcement Learning",
      "Stochastic Approximation"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      8
    ],
    "real_confidences": [
      3,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Skill Machines: Temporal Logic Composition in Reinforcement Learning": {
    "paper_pk": null,
    "title": "Skill Machines: Temporal Logic Composition in Reinforcement Learning",
    "abstract": "A major challenge in reinforcement learning is specifying tasks in a manner that is both interpretable and verifiable. One common approach is to specify tasks through reward machines---finite state machines that encode the task to be solved. We introduce skill machines, a representation that can be learned directly from these reward machines that encode the solution to such tasks. We propose a framework where an agent first learns a set of base skills in a reward-free setting, and then combines these skills with the learned skill machine to produce composite behaviours specified by any regular language, such as linear temporal logics. This provides the agent with the ability to map from complex logical task specifications to near-optimal behaviours zero-shot. We demonstrate our approach in both a tabular and high-dimensional video game environment, where an agent is faced with several of these complex, long-horizon tasks. Our results indicate that the agent is capable of satisfying extremely complex task specifications, producing near optimal performance with no further learning. Finally, we demonstrate that the performance of skill machines can be improved with regular off-policy reinforcement learning algorithms when optimal behaviours are desired.",
    "authors": [
      "Geraud Nangue Tasse",
      "Devon Jarvis",
      "Steven James",
      "Benjamin Rosman"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Lifelong learning",
      "Multi task learning",
      "Transfer learning",
      "Logical composition",
      "Deep Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "DISCO-DANCE: Learning to Discover Skills with Guidance": {
    "paper_pk": null,
    "title": "DISCO-DANCE: Learning to Discover Skills with Guidance",
    "abstract": "Unsupervised skill discovery (USD) allows agents to learn diverse and discriminable skills without access to pre-defined rewards,\nby maximizing the mutual information (MI) between skills and states reached by each skill.\nThe most common problem of MI-based skill discovery is insufficient exploration, because each skill is heavily penalized when it deviates from its initial settlement. Recent works introduced an auxiliary reward to encourage the exploration of the agent via maximizing the state's epistemic uncertainty or entropy. \nHowever, we have discovered that the performance of these auxiliary rewards decreases as the environment becomes more challenging. Therefore, we introduce a new unsupervised skill discovery algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill which  has the highest potential to reach the unexplored states, (2) guide other skills to follow the guide skill, then (3) the guided skills are diffused to maximize their discriminability in the unexplored states. Empirically, DISCO-DANCE substantially outperforms other USD baselines on challenging environments including two navigation benchmarks and a continuous control benchmark.",
    "authors": [
      "Hyunseung Kim",
      "Byungkun Lee",
      "Sejik Park",
      "Hojoon Lee",
      "Dongyoon Hwang",
      "Kyushik Min",
      "Jaegul Choo"
    ],
    "keywords": [
      "Unsupervised skill discovery",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      3,
      5,
      1
    ],
    "real_confidences": [
      2,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Distributional Meta-Gradient Reinforcement Learning": {
    "paper_pk": null,
    "title": "Distributional Meta-Gradient Reinforcement Learning",
    "abstract": "Meta-gradient reinforcement learning (RL) algorithms have substantially boosted the performance of RL agents by learning an adaptive return. All the existing algorithms adhere to the same reward learning principle, where the adaptive return is simply formulated in the form of expected cumulative rewards, upon which the policy and critic update rules are specified under well-adopted distance metrics. In this paper, we present a novel algorithm that builds on the success of meta-gradient RL algorithms and effectively improves such algorithms by following a simple recipe, i.e., going beyond the expected return to formulate and learn the return in a more expressive form, value distributions. To this end, we first formulate a distributional return that could effectively capture bootstrapping and discounting behaviors over distributions, to form an informative distributional return target in value update. Then we derive an efficient meta update rule to learn the adaptive distributional return with meta-gradients. For empirical evaluation, we first present an illustrative example on a toy two-color grid-world domain, which validates the benefit of learning distributional return over expectation; then we conduct extensive comparisons on a large-scale RL benchmark Atari 2600, where we confirm that our proposed method with distributional return works seamlessly well with the actor-critic framework and leads to state-of-the-art median human normalized score among meta-gradient RL literature.",
    "authors": [
      "Haiyan Yin",
      "Shuicheng YAN",
      "Zhongwen Xu"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Meta Learning"
    ],
    "real_all_scores": [
      6,
      10,
      8,
      8
    ],
    "real_confidences": [
      3,
      4,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model": {
    "paper_pk": null,
    "title": "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model",
    "abstract": "Unsupervised reinforcement learning (URL) poses a promising paradigm to learn useful behaviors in a task-agnostic environment without the guidance of extrinsic rewards to facilitate the fast adaptation of various downstream tasks. Previous works focused on the pre-training in a model-free manner while lacking the study of transition dynamics modeling that leaves a large space for the improvement of sample efficiency in downstream tasks. To this end, we propose an Efficient Unsupervised Reinforcement Learning Framework with Multi-choice Dynamics model (EUCLID), which introduces a novel model-fused paradigm to jointly pre-train the dynamics model and unsupervised exploration policy in the pre-training phase, thus better leveraging the environmental samples and improving the downstream task sampling efficiency. However, constructing a generalizable model which captures the local dynamics under different behaviors remains a challenging problem. We introduce the multi-choice dynamics model that covers different local dynamics under different behaviors concurrently, which uses different heads to learn the state transition under different behaviors during unsupervised pre-training and selects the most appropriate head for prediction in the downstream task. Experimental results in the manipulation and locomotion domains demonstrate that EUCLID achieves state-of-the-art performance with high sample efficiency, basically solving the state-based URLB benchmark and reaching a mean normalized score of 104.0\u00b11.2% in downstream tasks with 100k fine-tuning steps, which is equivalent to DDPG\u2019s performance at 2M interactive steps with 20\u00d7 more data. More visualization videos are released on our homepage.",
    "authors": [
      "Yifu Yuan",
      "Jianye HAO",
      "Fei Ni",
      "Yao Mu",
      "YAN ZHENG",
      "Yujing Hu",
      "Jinyi Liu",
      "Yingfeng Chen",
      "Changjie Fan"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Unsupervised RL",
      "Model-based RL"
    ],
    "real_all_scores": [
      5,
      6,
      3,
      6
    ],
    "real_confidences": [
      5,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Policy Architectures for Compositional Generalization in Control": {
    "paper_pk": null,
    "title": "Policy Architectures for Compositional Generalization in Control",
    "abstract": "Several tasks in control, robotics, and planning can be specified through desired goal configurations for entities in the environment. Learning goal-conditioned policies is a natural paradigm to solve such tasks. However, learning and generalizing on complex tasks can be challenging due to variations in number of entities or compositions of goals. To address this challenge, we introduce the Entity-Factored Markov Decision Process (EFMDP), a formal framework for modeling the entity-based compositional structure in control tasks. Geometrical properties of the EFMDP framework provide theoretical motivation for policy architecture design, particularly Deep Sets and popular relational mechanisms such as graphs and self attention. These structured policy architectures are flexible and can be trained end-to-end with standard reinforcement and imitation learning algorithms. We study and compare the learning and generalization properties of these architectures on a suite of simulated robot manipulation tasks, finding that they achieve significantly higher success rates with less data compared to standard multilayer perceptrons. Structured policies also enable broader and more compositional generalization, producing policies that extrapolate to different numbers of entities than seen in training, and stitch together (i.e. compose) learned skills in novel ways. Video results can be found at https://sites.google.com/view/comp-gen-anon.",
    "authors": [
      "Allan Zhou",
      "Vikash Kumar",
      "Chelsea Finn",
      "Aravind Rajeswaran"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Imitation Learning",
      "Compositionality"
    ],
    "real_all_scores": [
      3,
      3,
      6,
      3
    ],
    "real_confidences": [
      4,
      5,
      2,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "In-Context Policy Iteration": {
    "paper_pk": null,
    "title": "In-Context Policy Iteration",
    "abstract": "This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \u201cfew-shot\u201d quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex Chen et al. (2021b), a language model with no prior knowledge of the domains on which we evaluate it.",
    "authors": [
      "Ethan Brooks",
      "Logan A Walls",
      "Richard Lewis",
      "Satinder Singh"
    ],
    "keywords": [
      "Reinforcement Learning",
      "In-Context Learning",
      "Foundation Models"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Predicting Drug Repurposing Candidates and Their Mechanisms from A Biomedical Knowledge Graph": {
    "paper_pk": null,
    "title": "Predicting Drug Repurposing Candidates and Their Mechanisms from A Biomedical Knowledge Graph",
    "abstract": "Computational drug repurposing is a cost- and time-efficient method to identify new indications of approved or experimental drugs/compounds. It is especially critical for emerging and/or orphan diseases due to its cheaper investment and shorter research cycle compared with traditional wet-lab drug discovery approaches. However, the underlying mechanisms of action between repurposed drugs and their target diseases remain largely unknown, which is still an unsolved issue in existing repurposing methods. As such, computational drug repurposing has not been widely adopted in clinical settings. In this work, based on a massive biomedical knowledge graph, we propose a computational drug repurposing framework that not only predicts the treatment probabilities between drugs and diseases but also predicts the path-based, testable mechanisms of action (MOAs) as their biomedical explanations. Specifically, we utilize the GraphSAGE model in an unsupervised manner to integrate each entity\u2019s neighborhood information and employ a Random Forest model to predict the treatment probabilities between pairs of drugs and diseases. Moreover, we train an adversarial actor-critic reinforcement learning model to predict the potential MOA for explaining drug purposing. To encourage the model to find biologically reasonable paths, we utilize the curated molecular interactions of drugs and a PubMed-publication-based concept distance to extract potential drug MOA paths from the knowledge graph as \u201ddemonstration paths\u201d to guide the model during the process of path-finding. Comprehensive experiments and case studies show that the proposed framework outperforms state-of-the-art baselines in both predictive performance of drug repurposing and explanatory performance of recapitulating human-curated DrugMechDB-based paths.",
    "authors": [
      "Chunyu Ma",
      "Zhihan Zhou",
      "Han Liu",
      "David Koslicki"
    ],
    "keywords": [
      "Drug Repurposing",
      "Reinforcement Learning",
      "Biomedical Knowledge Graph"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Impossibly Good Experts and How to Follow Them": {
    "paper_pk": null,
    "title": "Impossibly Good Experts and How to Follow Them",
    "abstract": "We consider the sequential decision making problem of learning from an expert that has access to more information than the learner.  For many problems this extra information will enable the expert to achieve greater long term reward than any policy without this privileged information access.  We call these experts ``Impossibly Good'' because no learning algorithm will be able to reproduce their behavior.  However, in these settings it is reasonable to attempt to recover the best policy possible given the agent's restricted access to information.  We provide a set of necessary criteria on the expert that will allow a learner to recover the optimal policy in the reduced information space from the expert's advice alone.  We also provide a new approach called Elf Distillation (Explorer Learning from Follower) that can be used in cases where these criteria are not met and environmental rewards must be taken into account.  We show that this algorithm performs better than a variety of strong baselines on a challenging suite of Minigrid and Vizdoom environments.",
    "authors": [
      "Aaron Walsman",
      "Muru Zhang",
      "Sanjiban Choudhury",
      "Dieter Fox",
      "Ali Farhadi"
    ],
    "keywords": [
      "Imitation Learning",
      "Reinforcement Learning",
      "Experts",
      "Distillation"
    ],
    "real_all_scores": [
      5,
      6,
      3
    ],
    "real_confidences": [
      3,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "On Convergence of Average-Reward Off-Policy Control Algorithms in Weakly-Communicating MDPs": {
    "paper_pk": null,
    "title": "On Convergence of Average-Reward Off-Policy Control Algorithms in Weakly-Communicating MDPs",
    "abstract": "We show two average-reward off-policy control algorithms, Differential Q Learning (Wan, Naik, \\& Sutton 2021a) and RVI Q Learning (Abounadi Bertsekas \\& Borkar 2001), converge in weakly-communicating MDPs. Weakly-communicating MDPs are the most general class of MDPs that a learning algorithm with a single stream of experience can guarantee obtaining a policy achieving optimal reward rate. The original convergence proofs of the two algorithms require that all optimal policies induce unichains, which is not necessarily true for weakly-communicating MDPs. To the best of our knowledge, our results are the first showing average-reward off-policy control algorithms converge in weakly-communicating MDPs. As a direct extension, we show that average-reward options algorithms introduced by (Wan, Naik, \\& Sutton 2021b) converge if the Semi-MDP induced by options is weakly-communicating. ",
    "authors": [
      "Yi Wan",
      "Richard S. Sutton"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Average-Reward",
      "Off-Policy",
      "Convergence"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Reinforcement learning for instance segmentation with high-level priors": {
    "paper_pk": null,
    "title": "Reinforcement learning for instance segmentation with high-level priors",
    "abstract": "Instance segmentation is a fundamental computer vision problem which remains challenging despite impressive recent advances due to deep learning-based methods. Given sufficient training data, fully supervised methods can yield excellent performance, but annotation of groundtruth data remains a major bottleneck, especially for biomedical applications where it has to be performed by domain experts. The amount of labels required can be drastically reduced by using rules derived from prior knowledge to guide the segmentation. However, these rules are in general not differentiable and thus cannot be used with existing methods. Here, we revoke this requirement by using stateless actor critic reinforcement learning, which enables non-differentiable rewards. We formulate the instance segmentation problem as graph partitioning and the actor critic predicts the edge weights driven by the rewards, which are based on the conformity of segmented instances to high-level priors on object shape, position or size. The experiments on toy and real data demonstrate that a good set of priors is sufficient to reach excellent performance without any direct object-level supervision.",
    "authors": [
      "Paul Hilt",
      "Edgar Kaziakhmedov",
      "Maedeh Zarvandi",
      "Sourabh Bhide",
      "Maria Leptin",
      "Constantin Pape",
      "Anna Kreshuk"
    ],
    "keywords": [
      "Instance Segmentation",
      "Reinforcement Learning",
      "Biomedical Imaging"
    ],
    "real_all_scores": [
      8,
      6,
      6,
      8
    ],
    "real_confidences": [
      4,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Generalize Learned Heuristics to Solve Large-scale Vehicle Routing Problems in Real-time": {
    "paper_pk": null,
    "title": "Generalize Learned Heuristics to Solve Large-scale Vehicle Routing Problems in Real-time",
    "abstract": "Large-scale Vehicle Routing Problems (VRPs) are widely used in logistics, transportation, supply chain, and robotic systems. Recently, data-driven VRP heuristics are proposed to generate real-time VRP solutions with up to 100 nodes. Despite this progress, current heuristics for large-scale VRPs still face three major challenges: 1) Difficulty in generalizing the heuristics learned on small-scale VRPs to large-scale VRPs without retraining; 2) Challenge in generating real-time solutions for large-scale VRPs; 3) Difficulty in embedding global constraints into learned heuristics. We contribute in the three directions: We propose a Two-stage Divide Method (TAM) to generate sub-route sequence rather than node sequence for generalizing the heuristics learned on small-scale VRPs to solve large-scale VRPs in real-time. A  two-step reinforcement learning method with new reward and padding techniques is proposed to train our TAM.  A global mask function is proposed to keep the global constraints satisfied when dividing a large-scale VRP into several small-scale Traveling Salesman Problems (TSPs). As result, we can solve the small-scale TSPs in parallel quickly. The experiments on synthetic and real-world large-scale VRPs show our method could generalize the learned heuristics trained on datasets of VRP 100 to solve VRPs with over 5000 nodes in real-time while keeping the solution quality better than data-driven heuristics and competitive with traditional heuristics.",
    "authors": [
      "Qingchun Hou",
      "Jingwei Yang",
      "Yiqiang Su",
      "Xiaoqing Wang",
      "Yuming Deng"
    ],
    "keywords": [
      "Learning",
      "Vehicle Routing Problem",
      "Large-scale Vehicle Routing Problem",
      "Generalization",
      "Combinatorial Optimization",
      "Reinforcement Learning",
      "Attention"
    ],
    "real_all_scores": [
      6,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "GoBigger: A Scalable Platform for Cooperative-Competitive Multi-Agent Interactive Simulation": {
    "paper_pk": null,
    "title": "GoBigger: A Scalable Platform for Cooperative-Competitive Multi-Agent Interactive Simulation",
    "abstract": "The emergence of various multi-agent environments has motivated powerful algorithms to explore agents' cooperation or competition. Even though this has greatly promoted the development of multi-agent reinforcement learning  (MARL), it is still not enough to support further exploration on the behavior of swarm intelligence between multiple teams, and cooperation between multiple agents due to their limited scalability. To alleviate this, we introduce GoBigger, a scalable platform for cooperative-competition multi-agent interactive simulation. GoBigger is an enhanced environment for the Agar-like game, enabling the simulation of multiple scales of agent intra-team cooperation and inter-team competition. Compared with existing multi-agent simulation environments, our platform supports multi-team games with more than two teams simultaneously, which dramatically expands the diversity of agent cooperation and competition, and can more effectively simulate the swarm intelligent agent behavior. Besides, in GoBigger, the cooperation between the agents in a team can lead to much higher performance. We offer a diverse set of challenging scenarios, built-in bots, and visualization tools for best practices in benchmarking. We evaluate several state-of-the-art algorithms on GoBigger and demonstrate the potential of the environment. We believe this platform can inspire various emerging research directions in MARL, swarm intelligence, and large-scale agent interactive learning. Both GoBigger and its related benchmark are open-sourced. More information could be found at https://github.com/opendilab/GoBigger.",
    "authors": [
      "Ming Zhang",
      "Shenghan Zhang",
      "Zhenjie Yang",
      "Lekai Chen",
      "Jinliang Zheng",
      "Chao Yang",
      "Chuming Li",
      "Hang Zhou",
      "Yazhe Niu",
      "Yu Liu"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Environment",
      "Cooperation",
      "Competition",
      "Scalable"
    ],
    "real_all_scores": [
      3,
      8,
      3
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "The Impact of Approximation Errors on Warm-Start Reinforcement Learning: A Finite-time Analysis": {
    "paper_pk": null,
    "title": "The Impact of Approximation Errors on Warm-Start Reinforcement Learning: A Finite-time Analysis",
    "abstract": "Warm-Start reinforcement learning (RL), aided by a prior policy obtained from offline training, is emerging as a  promising RL approach for practical applications. Recent empirical studies have demonstrated that the performance of Warm-Start RL can be improved \\textit{quickly} in some cases but become \\textit{stagnant} in other cases, calling for  a fundamental understanding, especially when the function approximation is used. To fill this void, we take a finite time analysis approach to quantify the impact of approximation errors on the learning performance of Warm-Start RL. Specifically, we consider the widely used Actor-Critic (A-C) method with a prior policy.  We first quantify the approximation errors in the Actor update and the Critic update, respectively. Next, we cast the Warm-Start A-C algorithm   as Newton's method with perturbation, and study the impact of the approximation errors on the finite-time learning performance with inaccurate Actor/Critic updates. Under some general technical conditions, we obtain lower bounds on the sub-optimality gap of the Warm-Start A-C algorithm to quantify the impact of the bias and error propagation. We also derive the upper bounds, which provide insights on achieving the desired finite-learning performance in the Warm-Start A-C algorithm.",
    "authors": [
      "Hang Wang",
      "Sen Lin",
      "Junshan Zhang"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Finite-time Analysis",
      "Approximation Error",
      "Warm Start"
    ],
    "real_all_scores": [
      5,
      6,
      3,
      6
    ],
    "real_confidences": [
      3,
      2,
      5,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning to Communicate using Contrastive Learning ": {
    "paper_pk": null,
    "title": "Learning to Communicate using Contrastive Learning ",
    "abstract": "Communication is a powerful tool for coordination in multi-agent RL.  Inducing an effective, common language has been a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. Based on this perspective, we propose to learn to communicate using contrastive learning by maximizing the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures task-relevant information from the environment. Finally, we demonstrate promising results on zero-shot communication, a first for MARL. Overall, we show the power of contrastive learning, and self-supervised learning in general, as a method for learning to communicate.",
    "authors": [
      "Yat Long Lo",
      "Biswa Sengupta",
      "Jakob Nicolaus Foerster",
      "Michael Noukhovitch"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Multi-Agent Reinforcement Learning",
      "Multi-Agent Communication"
    ],
    "real_all_scores": [
      8,
      3,
      6
    ],
    "real_confidences": [
      3,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Cheap Talk Discovery and Utilization in Multi-Agent Reinforcement Learning": {
    "paper_pk": null,
    "title": "Cheap Talk Discovery and Utilization in Multi-Agent Reinforcement Learning",
    "abstract": "By enabling agents to communicate, recent cooperative multi-agent reinforcement learning (MARL) methods have demonstrated better task performance and more coordinated behavior. Most existing approaches facilitate inter-agent communication by allowing agents to send messages to each other through free communication channels, i.e., \\emph{cheap talk channels}. Current methods require these channels to be constantly accessible and known to the agents a priori. In this work, we lift these requirements such that the agents must discover the cheap talk channels and learn how to use them. Hence, the problem has two main parts: \\emph{cheap talk discovery} (CTD) and \\emph{cheap talk utilization} (CTU). We introduce a novel conceptual framework for both parts and develop a new algorithm based on mutual information maximization that outperforms existing algorithms in CTD/CTU settings. We also release a novel benchmark suite to stimulate future research in CTD/CTU.",
    "authors": [
      "Yat Long Lo",
      "Christian Schroeder de Witt",
      "Samuel Sokota",
      "Jakob Nicolaus Foerster",
      "Shimon Whiteson"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Multi-Agent Reinforcement Learning"
    ],
    "real_all_scores": [
      3,
      5,
      6,
      3
    ],
    "real_confidences": [
      2,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Evaluating Long-Term Memory in 3D Mazes": {
    "paper_pk": null,
    "title": "Evaluating Long-Term Memory in 3D Mazes",
    "abstract": "Intelligent agents need to remember salient information to reason in partially-observed environments. For example, agents with a first-person view should remember the positions of relevant objects even if they go out of view. Similarly, to effectively navigate through rooms agents need to remember the floor plan of how rooms are connected. However, most benchmark tasks in reinforcement learning do not test long-term memory in agents, slowing down progress in this important research direction. In this paper, we introduce the Memory Maze, a 3D domain of randomized mazes specifically designed for evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze measures long-term memory separate from confounding agent abilities and requires the agent to localize itself by integrating information over time. With Memory Maze, we propose an online reinforcement learning benchmark, a diverse offline dataset, and an offline probing evaluation. Recording a human player establishes a strong baseline and verifies the need to build up and retain memories, which is reflected in their gradually increasing rewards within each episode. We find that current algorithms benefit from training with truncated backpropagation through time and succeed on small mazes, but fall short of human performance on the large mazes, leaving room for future algorithmic designs to be evaluated on the Memory Maze.",
    "authors": [
      "Jurgis Pa\u0161ukonis",
      "Timothy P Lillicrap",
      "Danijar Hafner"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Memory",
      "Benchmark",
      "Dataset",
      "Representation Learning"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      8
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "C3PO: Learning to Achieve Arbitrary Goals via Massively Entropic Pretraining": {
    "paper_pk": null,
    "title": "C3PO: Learning to Achieve Arbitrary Goals via Massively Entropic Pretraining",
    "abstract": "Given a particular embodiment, we propose a novel method (C3PO) that learns policies able to achieve any arbitrary position and pose.  Such a policy would allow for easier control, and would be re-useable as a key building block for downstream tasks.  The method is two-fold: First, we introduce a novel exploration algorithm that optimizes for uniform coverage, is able to discover a set of achievable states, and investigates its abilities in attaining both high coverage, and hard-to-discover states;  Second,  we leverage this set of achievable states as training data for a universal goal-achievement policy, a goal-based SAC variant. We demonstrate the trained policy's performance in achieving a large number of novel states. Finally, we showcase the influence of massive unsupervised training of a goal-achievement policy with state-of-the-art pose-based control of the Hopper, Walker, Halfcheetah, Humanoid and Ant embodiments.",
    "authors": [
      "Alexis D. Jacq",
      "Manu Orsini",
      "Gabriel Dulac-Arnold",
      "Olivier Pietquin",
      "Matthieu Geist",
      "Olivier Bachem"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Exploration",
      "Goal-conditioned Policy",
      "Continuous Control"
    ],
    "real_all_scores": [
      8,
      6,
      8,
      6
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning": {
    "paper_pk": null,
    "title": "Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning",
    "abstract": "The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Transferable Skills (APES), a hierarchical KL-regularized method, heavily benefiting from both priors and hierarchy. Unlike existing approaches, APES automates the choice of asymmetry by learning it in a data-driven, domain-dependent, way based on our expressivity-transferability theorems. Experiments over complex transfer domains of varying levels of extrapolation and sparsity, such as robot block stacking, demonstrate the criticality of the correct asymmetric choice, with APES drastically outperforming previous methods.",
    "authors": [
      "Sasha Salter",
      "Kristian Hartikainen",
      "Walter Goodwin",
      "Ingmar Posner"
    ],
    "keywords": [
      "Skills",
      "Transfer Learning",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      8,
      8,
      8
    ],
    "real_confidences": [
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "ResAct: Reinforcing Long-term Engagement in Sequential Recommendation with Residual Actor": {
    "paper_pk": null,
    "title": "ResAct: Reinforcing Long-term Engagement in Sequential Recommendation with Residual Actor",
    "abstract": "Long-term engagement is preferred over immediate engagement in sequential recommendation as it directly affects product operational metrics such as daily active users (DAUs) and dwell time. Meanwhile, reinforcement learning (RL) is widely regarded as a promising framework for optimizing long-term engagement in sequential recommendation. However, due to expensive online interactions, it is very difficult for RL algorithms to perform state-action value estimation, exploration and feature extraction when optimizing long-term engagement. In this paper, we propose ResAct which seeks a policy that is close to, but better than, the online-serving policy. In this way, we can collect sufficient data near the learned policy so that state-action values can be properly estimated, and there is no need to perform online exploration. ResAct optimizes the policy by first reconstructing the online behaviors and then improving it via a Residual Actor. To extract long-term information, ResAct utilizes two information-theoretical regularizers to confirm the expressiveness and conciseness of features. We conduct experiments on a benchmark dataset and a large-scale industrial dataset which consists of tens of millions of recommendation requests. Experimental results show that our method significantly outperforms the state-of-the-art baselines in various long-term engagement optimization tasks.",
    "authors": [
      "Wanqi Xue",
      "Qingpeng Cai",
      "Ruohan Zhan",
      "Dong Zheng",
      "Peng Jiang",
      "Kun Gai",
      "Bo An"
    ],
    "keywords": [
      "Sequential Recommendation",
      "Long-term Engagement",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      5,
      8,
      5
    ],
    "real_confidences": [
      3,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation ": {
    "paper_pk": null,
    "title": "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation ",
    "abstract": "Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithm (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks:1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re$^2$ consistently outperforms advanced baselines and achieves the State Of The Art (SOTA). Our code is available on  https://github.com/yeshenpy/ERL-Re2.",
    "authors": [
      "Jianye HAO",
      "Pengyi Li",
      "Hongyao Tang",
      "YAN ZHENG",
      "Xian Fu",
      "Zhaopeng Meng"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Evolutionary Algorithm",
      "Representation"
    ],
    "real_all_scores": [
      6,
      3,
      6
    ],
    "real_confidences": [
      3,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Closing the Gap Between SVRG and TD-SVRG with Gradient Splitting": {
    "paper_pk": null,
    "title": "Closing the Gap Between SVRG and TD-SVRG with Gradient Splitting",
    "abstract": "Temporal difference (TD) learning is a simple algorithm for policy evaluation\nin reinforcement learning. The performance of TD learning is affected by high\nvariance and it can be naturally enhanced with variance reduction techniques, such\nas the Stochastic Variance Reduced Gradient (SVRG) method. Recently, multiple\nworks have sought to fuse TD learning with SVRG to obtain a policy evaluation\nmethod with a linear rate of convergence. However, the resulting convergence rate\nis significantly weaker than what is achieved by SVRG in the setting of convex\noptimization. In this work we utilize a recent interpretation of TD-learning as the\nsplitting of the gradient of an appropriately chosen function, thus simplifying the\nalgorithm and fusing TD with SVRG. We prove a linear convergence bound that\nis identical to the convergence bound available for SVRG in the convex setting.",
    "authors": [
      "Arsenii Mustafin",
      "Ioannis Paschalidis",
      "Alex Olshevsky"
    ],
    "keywords": [
      "Temporal Difference learning",
      "Reinforcement Learning",
      "SVRG",
      "Optimization"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      6
    ],
    "real_confidences": [
      4,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Look Back When Surprised: Stabilizing Reverse Experience Replay for Neural Approximation": {
    "paper_pk": null,
    "title": "Look Back When Surprised: Stabilizing Reverse Experience Replay for Neural Approximation",
    "abstract": "Experience replay-based sampling techniques are essential to several reinforcement learning (RL) algorithms since they aid in convergence by breaking spurious correlations.  The most popular techniques, such as uniform experience replay(UER) and prioritized experience replay (PER), seem to suffer from sub-optimal convergence and significant bias error, respectively. To alleviate this, we introduce a new experience replay method for reinforcement learning, called IntrospectiveExperience Replay (IER). IER picks batches corresponding to data points consecutively before the \u2018surprising\u2019 points. Our proposed approach is based on the theoretically rigorous reverse experience replay (RER), which can be shown to remove bias in the linear approximation setting but can be sub-optimal with neural approximation. We show empirically that IER is stable with neural function approximation and has a superior performance compared to the state-of-the-art techniques like uniform experience replay (UER), prioritized experience replay(PER), and hindsight experience replay (HER) on the majority of tasks.",
    "authors": [
      "Ramnath Kumar",
      "Dheeraj Mysore Nagaraj"
    ],
    "keywords": [
      "Experience Replay",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      3,
      3,
      1,
      3
    ],
    "real_confidences": [
      4,
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching": {
    "paper_pk": null,
    "title": "Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching",
    "abstract": "Graph matching (GM) has been a building block in various areas including computer vision and pattern recognition. Despite recent impressive progress, existing deep GM methods often have obvious difficulty in handling outliers, which are ubiquitous in practice. We propose a deep reinforcement learning based approach RGM, whose sequential node matching scheme naturally fits the strategy for selective inlier matching against outliers. A revocable action framework is devised to improve the agent's flexibility against the complex constrained GM. Moreover, we propose a quadratic approximation technique to regularize the affinity score, in the presence of outliers. As such, the agent can finish inlier matching timely when the affinity score stops growing, for which otherwise an additional parameter i.e. the number of inliers is needed to avoid matching outliers. In this paper, we focus on learning the back-end solver under the most general form of GM: the Lawler's QAP, whose input is the affinity matrix. Especially, our approach can also boost existing GM methods that use such input. Experiments on multiple real-world datasets demonstrate its performance regarding both accuracy and robustness.",
    "authors": [
      "Chang Liu",
      "Zetian Jiang",
      "Runzhong Wang",
      "Lingxiao Huang",
      "Pinyan Lu",
      "Junchi Yan"
    ],
    "keywords": [
      "Graph Matching",
      "Reinforcement Learning",
      "Quadratic Assignment",
      "Affinity Regularization",
      "Combinatorial Optimization."
    ],
    "real_all_scores": [
      6,
      3,
      5,
      3
    ],
    "real_confidences": [
      4,
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization": {
    "paper_pk": null,
    "title": "Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization",
    "abstract": "Training long-horizon robotic policies in complex physical environments is essential for many applications, such as robotic manipulation. However, learning a policy that can generalize to unseen tasks is challenging. In this work, we propose to achieve one-shot task generalization by decoupling plan generation and plan execution. Specifically, our method solves complex long-horizon tasks in three steps: build a paired abstract environment by simplifying geometry and physics, generate abstract trajectories, and solve the original task by an abstract-to-executable trajectory translator. In the abstract environment, complex dynamics such as physical manipulation are removed, making abstract trajectories easier to generate. However, this introduces a large domain gap between abstract trajectories and the actual executed trajectories as abstract trajectories lack low-level details and aren\u2019t aligned frame-to-frame with the executed trajectory. In a manner reminiscent of language translation, our approach leverages a seq-to-seq model to overcome the large domain gap between the abstract and executable trajectories, enabling the low-level policy to follow the abstract trajectory. Experimental results on various unseen long-horizon tasks with different robot embodiments demonstrate the practicability of our methods to achieve one-shot task generalization. Videos and more details can be found in the supplementary materials and project page: https://sites.google.com/view/abstract-to-executable-iclr23/",
    "authors": [
      "Stone Tao",
      "Xiaochen Li",
      "Tongzhou Mu",
      "Zhiao Huang",
      "Yuzhe Qin",
      "Hao Su"
    ],
    "keywords": [
      "Trajectory Translation",
      "One-Shot Generalization",
      "Long-Horizon Task",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      3,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Representation Learning for Low-rank General-sum Markov Games": {
    "paper_pk": null,
    "title": "Representation Learning for Low-rank General-sum Markov Games",
    "abstract": "We study multi-agent general-sum Markov games with nonlinear function approximation. We focus on low-rank Markov games whose transition matrix admits a hidden low-rank structure on top of an unknown non-linear representation. The goal is to design an algorithm that (1) finds an $\\varepsilon$-equilibrium policy sample efficiently without prior knowledge of the environment or the representation, and (2) permits a deep-learning friendly implementation. We leverage representation learning and present a model-based and a model-free approach to construct an effective representation from collected data. For both approaches, the algorithm achieves a sample complexity of poly$(H,d,A,1/\\varepsilon)$, where $H$ is the game horizon, $d$ is the dimension of the feature vector, $A$ is the size of the joint action space and $\\varepsilon$ is the optimality gap. When the number of players is large, the above sample complexity can scale exponentially with the number of players in the worst case. To address this challenge, we consider Markov Games with a factorized transition structure and present an algorithm that escapes such exponential scaling. To our best knowledge, this is the first sample-efficient algorithm for multi-agent general-sum Markov games that incorporates (non-linear) function approximation. We accompany our theoretical result with a neural network-based implementation of our algorithm and evaluate it against the widely used deep RL baseline, DQN with fictitious play.",
    "authors": [
      "Chengzhuo Ni",
      "Yuda Song",
      "Xuezhou Zhang",
      "Zihan Ding",
      "Chi Jin",
      "Mengdi Wang"
    ],
    "keywords": [
      "Reinforcement Learning",
      "Multi Agent",
      "Representation Learning"
    ],
    "real_all_scores": [
      5,
      3,
      5,
      6,
      3
    ],
    "real_confidences": [
      3,
      3,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Example-based Planning via Dual Gradient Fields": {
    "paper_pk": null,
    "title": "Example-based Planning via Dual Gradient Fields",
    "abstract": "Path planning is one of the key abilities of an intelligent agent. However, both the learning-based and sample-based planners remain to require explicitly defining the task by manually designing the reward function or optimisation objectives, which limits the scope of implementation. Formulating the path planning problem from a new perspective, Example-based planning is to find the most efficient path to increase the likelihood of the target distribution by giving a set of target examples. In this work, we introduce Dual Gradient Fields (DualGFs), an offline-learning example-based planning framework built upon score matching. There are two gradient fields in DualGFs: a target gradient field that guides task completion and a support gradient field that ensures moving with environmental constraints. In the learning process, instead of interacting with the environment, the agents are trained with two offline examples, i.e., the target gradients and support gradients are trained by target examples and support examples, respectively. The support examples are randomly sampled from free space, e.g., states without collisions. DualGF is a weighted mixture of the two fields, combining the merits of the two fields together. To update the mixing ratio adaptively, we further propose a fields-balancing mechanism based on Lagrangian-Relaxation. Experimental results across four tasks (navigation, tracking, particle rearrangement, and room rearrangement) demonstrate the scalability and effectiveness of our method.",
    "authors": [
      "Mingdong Wu",
      "fangwei zhong",
      "Yizhou Wang",
      "Hao Dong"
    ],
    "keywords": [
      "Example-based Planning",
      "Score-Matching",
      "Path Planning",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      1,
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      5,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Deep Reinforcement Learning based Insight Selection Policy": {
    "paper_pk": null,
    "title": "Deep Reinforcement Learning based Insight Selection Policy",
    "abstract": "We live in the era of ubiquitous sensing and computing. More and more data is being collected and processed from devices, sensors and systems. This opens up opportunities to discover patterns from these data that could help in gaining better understanding into the source that produces them. This is useful in a wide range of domains, especially in the area of personal health, in which such knowledge could help in allowing users to comprehend their behaviour and indirectly improve their lifestyle. Insight generators are systems that identify such patterns and verbalise them in a readable text format, referred to as insights. The selection of insights is done using a scoring algorithm which aims at optimizing this process based on multiple objectives, e.g., factual correctness, usefulness and interestingness of insights. In this paper, we propose a novel Reinforcement Learning (RL) framework for insight selection where the scoring model is trained by user feedback on interestingness and their lifestyle quality estimates. With the use of highly reusable and simple principles of automatic user simulation based on real data, we demonstrate in this preliminary study that the RL solution may improve the selection of insights towards multiple pre-defined objectives.",
    "authors": [
      "Libio Goncalves Braz",
      "Allmin Pradhap Singh Susaiyah",
      "Milan Petkovic",
      "Aki H\u00e4rm\u00e4"
    ],
    "keywords": [
      "recommender",
      "insight",
      "reinforcement learning",
      "behavior change support system",
      "health coaching",
      "lifestyle simulator",
      "Gaussian mixture modeling"
    ],
    "real_all_scores": [
      6,
      5,
      3
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Self-Supervised Off-Policy Ranking via Crowd Layer": {
    "paper_pk": null,
    "title": "Self-Supervised Off-Policy Ranking via Crowd Layer",
    "abstract": "Off-policy evaluation (OPE) aims to estimate the online performance of target policies given dataset collected by some behavioral policies. OPE is crucial in many applications where online policy evaluation is expensive. However, existing OPE methods are far from reliable. Fortunately, in many real-world scenarios, we care only about the ranking of the evaluating policies, rather than their exact online performance. Existing works on off-policy ranking (OPR) adopt a supervised training paradigm, which assumes that there are plenty of deployed policies and the labels of their performance are available. However, this assumption does not apply to most OPE scenarios because collecting such training data might be highly expensive. In this paper, we propose a novel OPR framework called SOCCER, where the existing OPE methods are modeled as workers in a crowdsourcing system. SOCCER can be trained in a self-supervised way as it does not require any ground-truth labels of policies. Moreover, in order to capture the relative discrepancies between policies, we propose a novel transformer-based architecture to learn effective pairwise policy representations. Experimental results show that SOCCER achieves significantly high accuracy in a variety of OPR tasks. Surprisingly, SOCCER even performs better than baselines trained in a supervised way using additional labeled data, which further demonstrates the superiority of SOCCER in OPR tasks.",
    "authors": [
      "Pengjie Gu",
      "Mengchen Zhao",
      "Jianye HAO",
      "Bo An"
    ],
    "keywords": [
      "off-policy ranking",
      "policy representation learning",
      "reinforcement learning"
    ],
    "real_all_scores": [
      3,
      1,
      3
    ],
    "real_confidences": [
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "When and Why Is Pretraining Object-Centric Representations Good for Reinforcement Learning?": {
    "paper_pk": null,
    "title": "When and Why Is Pretraining Object-Centric Representations Good for Reinforcement Learning?",
    "abstract": "Unsupervised object-centric representation (OCR) learning has recently been drawing a lot of attention as a new paradigm of visual representation. This is because of its potential of being an effective pretraining technique for various downstream tasks in terms of sample efficiency, systematic generalization, and reasoning. Although image-based reinforcement learning (RL) is one of the most important and thus frequently mentioned such downstream tasks, the benefit in RL has surprisingly not been investigated systematically thus far. Instead, most of the evaluations have focused on rather indirect metrics such as segmentation quality and object property prediction accuracy. In this paper, we investigate the effectiveness of OCR pretraining for image-based reinforcement learning via empirical experiments. For systematic evaluation, we introduce a simple object-centric visual RL benchmark and verify a series of hypotheses answering questions such as \"Does OCR pretraining provide better sample efficiency?\", \"Which types of RL tasks benefit most from OCR pretraining?\", and \"Can OCR pretraining help with out-of-distribution generalization?\". The results suggest that OCR pretraining is particularly effective in tasks where the relationship between objects is important, improving both task performance and sample efficiency when compared to single-vector representations. Furthermore, OCR models facilitate generalization to out-of-distribution tasks such as changing the number of objects or the appearance of the objects in the scene.",
    "authors": [
      "Jaesik Yoon",
      "Yi-Fu Wu",
      "Sungjin Ahn"
    ],
    "keywords": [
      "object-centric representation",
      "reinforcement learning"
    ],
    "real_all_scores": [
      3,
      5,
      6,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Guarded Policy Optimization with Imperfect Online Demonstrations": {
    "paper_pk": null,
    "title": "Guarded Policy Optimization with Imperfect Online Demonstrations",
    "abstract": "The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.",
    "authors": [
      "Zhenghai Xue",
      "Zhenghao Peng",
      "Quanyi Li",
      "Zhihan Liu",
      "Bolei Zhou"
    ],
    "keywords": [
      "reinforcement learning",
      "guarded policy optimization",
      "imperfect demonstrations",
      "shared control",
      "metadrive simulator"
    ],
    "real_all_scores": [
      1,
      3,
      3,
      3
    ],
    "real_confidences": [
      5,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation": {
    "paper_pk": null,
    "title": "Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation",
    "abstract": "We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches regarding computational requirements, final reward return, and satisfying the safety constraints. ",
    "authors": [
      "Yannick Hogewind",
      "Thiago D. Sim\u00e3o",
      "Tal Kachman",
      "Nils Jansen"
    ],
    "keywords": [
      "safety",
      "reinforcement learning",
      "safe reinforcement learning",
      "constrained Markov decision process",
      "partially observable Markov decision process",
      "MDP",
      "POMDP"
    ],
    "real_all_scores": [
      6,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Discrete State-Action Abstraction via the Successor Representation": {
    "paper_pk": null,
    "title": "Discrete State-Action Abstraction via the Successor Representation",
    "abstract": "While the difficulty of reinforcement learning problems is typically related to the complexity of their state spaces, Abstraction proposes that solutions often lie in simpler underlying latent spaces. Prior works have focused on learning either a continuous or dense abstraction, or require a human to provide one. Information-dense representations capture features irrelevant for solving tasks, and continuous spaces can struggle to represent discrete objects. In this work we automatically learn a sparse discrete abstraction of the underlying environment. We do so using a simple end-to-end trainable model based on the successor representation and max-entropy regularization. We describe an algorithm to apply our model, named Discrete State-Action Abstraction (DSAA), which computes an action abstraction in the form of temporally extended actions, i.e., Options, to transition between discrete abstract states. Empirically, we demonstrate the effects of different exploration schemes on our resulting abstraction, and show that it is efficient for solving downstream tasks.",
    "authors": [
      "Amnon Attali",
      "Pedro Cisneros-Velarde",
      "Marco Morales",
      "Nancy Amato"
    ],
    "keywords": [
      "reinforcement learning",
      "abstraction",
      "successor representation",
      "options",
      "discrete",
      "sparse reward",
      "representation learning",
      "intrinsic motivation"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      2,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks": {
    "paper_pk": null,
    "title": "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks",
    "abstract": "Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well-understood; in practice, how-ever, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent\u2019s network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan & Maggioni (2007)\u2019s proto-value functions to deep reinforcement learning \u2013 accordingly, we call the resulting object proto-value networks. Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment\u2019s reward function.",
    "authors": [
      "Jesse Farebrother",
      "Joshua Greaves",
      "Rishabh Agarwal",
      "Charline Le Lan",
      "Ross Goroshin",
      "Pablo Samuel Castro",
      "Marc G Bellemare"
    ],
    "keywords": [
      "reinforcement learning",
      "representation learning"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Raisin: Residual Algorithms for Versatile Offline Reinforcement Learning": {
    "paper_pk": null,
    "title": "Raisin: Residual Algorithms for Versatile Offline Reinforcement Learning",
    "abstract": "The residual gradient algorithm (RG), gradient descent of the Mean Squared Bellman Error, brings robust convergence guarantees to bootstrapped value estimation. Meanwhile, the far more common semi-gradient algorithm (SG) suffers from well-known instabilities and divergence. Unfortunately, RG often converges slowly in practice. Baird (1995) proposed residual algorithms (RA), weighted averaging of RG and SG, to combine RG's robust convergence and SG's speed. RA works moderately well in the online setting. We find, however, that RA works disproportionately well in the offline setting. Concretely, we find that merely adding a variable residual component to SAC increases its score on D4RL gym tasks by a median factor of 54. We further show that using the minimum of ten critics lets our algorithm match SAC-$N$'s state-of-the-art returns using 50$\\times$ less compute and no additional hyperparameters. In contrast, TD3+BC with the same minimum-of-ten-critics trick does not match SAC-$N$'s returns on a handful of environments.",
    "authors": [
      "Braham Snyder",
      "Yuke Zhu"
    ],
    "keywords": [
      "reinforcement learning",
      "offline RL",
      "residual algorithms",
      "residual gradient"
    ],
    "real_all_scores": [
      3,
      6,
      5,
      5
    ],
    "real_confidences": [
      3,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Return Augmentation gives Supervised RL Temporal Compositionality": {
    "paper_pk": null,
    "title": "Return Augmentation gives Supervised RL Temporal Compositionality",
    "abstract": "Offline Reinforcement Learning (RL) methods that use supervised learning or sequence modeling (e.g., Decision Transformer) work by training a return-conditioned policy. A fundamental limitation of these approaches, as compared to value-based methods, is that they have trouble generalizing to behaviors that have a higher return than what was seen at training. Value-based offline-RL algorithms like CQL use bootstrapping to combine training data from multiple trajectories to learn strong behaviors from sub-optimal data. We set out to endow RL via Supervised Learning (RvS) methods with this form of temporal compositionality. To do this, we introduce SuperB, a dynamic programming algorithm for data augmentation that augments the returns in the offline dataset by combining rewards from intersecting trajectories. We show theoretically that SuperB can improve sample complexity and enable RvS to find optimal policies in cases where it previously fell behind the performance of value-based methods. Empirically, we find that SuperB improves the performance of RvS in several offline RL environments, surpassing the prior state-of-the-art RvS agents in AntMaze by orders of magnitude and offering performance competitive with value-based algorithms on the D4RL-gym tasks.",
    "authors": [
      "Keiran Paster",
      "Silviu Pitis",
      "Sheila A. McIlraith",
      "Jimmy Ba"
    ],
    "keywords": [
      "reinforcement learning",
      "offline reinforcement learning",
      "decision transformer",
      "behavioral cloning",
      "dynamic programming",
      "data augmentation"
    ],
    "real_all_scores": [
      3,
      3,
      6
    ],
    "real_confidences": [
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Simple Emergent Action Representations from Multi-Task Policy Training": {
    "paper_pk": null,
    "title": "Simple Emergent Action Representations from Multi-Task Policy Training",
    "abstract": "The low-level sensory and motor signals in deep reinforcement learning, which exist in high-dimensional spaces such as image observations or motor torques, are inherently challenging to understand or utilize directly for downstream tasks. While sensory representations have been extensively studied, the representations of motor actions are still an area of active exploration. Our work reveals that a space containing meaningful action representations emerges when a multi-task policy network takes as inputs both states and task embeddings. Moderate constraints are added to improve its representation ability. Therefore, interpolated or composed embeddings can function as a high-level interface within this space, providing instructions to the agent for executing meaningful action sequences. Empirical results demonstrate that the proposed action representations are effective for intra-action interpolation and inter-action composition with limited or no additional learning. Furthermore, our approach exhibits superior task adaptation ability compared to strong baselines in Mujoco locomotion tasks. Our work sheds light on the promising direction of learning action representations for efficient, adaptable, and composable RL, forming the basis of abstract action planning and the understanding of motor signal space. Project page: https://sites.google.com/view/emergent-action-representation/",
    "authors": [
      "Pu Hua",
      "Yubei Chen",
      "Huazhe Xu"
    ],
    "keywords": [
      "action representation",
      "reinforcement learning",
      "representation learning"
    ],
    "real_all_scores": [
      5,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "High-dimensional Continuum Armed and High-dimensional Contextual Bandit: with Applications to Assortment and Pricing": {
    "paper_pk": null,
    "title": "High-dimensional Continuum Armed and High-dimensional Contextual Bandit: with Applications to Assortment and Pricing",
    "abstract": "The bandit problem with high-dimensional continuum arms and high-dimensional contextual covariates is often faced by decision-makers but remains unsolved. Existing bandit algorithms are impracticable due to the complexity of the double-layer high dimensionality. We formulate this problem as a high-dimensional continuum armed contextual bandit with high-dimensional covariates and propose a novel model that captures the effect of the arm and contextual on the reward with a low-rank representation matrix. The representation matrix is endowed with interpretability and predictive power. We further propose an efficient bandit algorithm based on a low-rank matrix estimator with theoretical justifications. The generality of our model allows wide applications including business and healthcare. In particular, we apply our method to assortment and pricing, both of which are important decisions for firms such as online retailers. Our method can solve the assortment-pricing problem simultaneously while most existing methods address them separately. We demonstrate the effectiveness of our method to jointly optimize assortment and pricing for revenue maximization for a giant online retailer.",
    "authors": [
      "Junhui Cai",
      "Ran Chen",
      "Martin J. Wainwright",
      "Linda H. Zhao"
    ],
    "keywords": [
      "bandit",
      "high-dimensional statistics",
      "assortment",
      "pricing",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      3,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models": {
    "paper_pk": null,
    "title": "Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models",
    "abstract": "Humans are excellent at understanding language and vision to accomplish a wide range of tasks. In contrast, creating general instruction-following embodied agents remains a difficult challenge. Prior work that uses pure language-only models lack visual grounding, making it difficult to connect language instructions with visual observations. On the other hand, methods that use pre-trained vision-language models typically come with divided language and visual representations, requiring designing specialized network architecture to fuse them together. We propose a simple yet effective model for robots to solve instruction-following tasks in vision-based environments. Our InstructRL method consists of a multimodal transformer that encodes visual observations and language instructions, and a policy transformer that predicts actions based on encoded representations. The multimodal transformer is pre-trained on millions of image-text pairs and natural language text, thereby producing generic cross-modal representations of observations and instructions. The policy transformer keeps track of the full history of observations and actions, and predicts actions autoregressively. We show that this unified transformer model outperforms all state-of-the-art pre-trained or trained-from-scratch methods in both single-task and multi-task settings. Our model also shows better model scalability and generalization ability than prior work.",
    "authors": [
      "Hao Liu",
      "Lisa Lee",
      "Kimin Lee",
      "Pieter Abbeel"
    ],
    "keywords": [
      "reinforcement learning",
      "pre-training",
      "multimodal representation",
      "representation learning",
      "transformer"
    ],
    "real_all_scores": [
      3,
      6,
      1,
      5,
      5,
      5
    ],
    "real_confidences": [
      3,
      4,
      4,
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Augmentation Curriculum Learning For Generalization in RL": {
    "paper_pk": null,
    "title": "Augmentation Curriculum Learning For Generalization in RL",
    "abstract": "Many Reinforcement Learning tasks rely solely on pixel-based observations of\nthe environment. During deployment, these observations can fall victim to visual\nperturbations and distortions, causing the agent\u2019s policy to significantly degrade\nin performance. This motivates the need for robust agents that can generalize in\nthe face of visual distribution shift. One common technique for doing this is to ap-\nply augmentations during training; however, it comes at the cost of performance.\nWe propose Augmentation Curriculum Learning a novel curriculum learning ap-\nproach that schedules augmentation into training into a weak augmentation phase\nand strong augmentation phase. We also introduce a novel visual augmentation\nstrategy that proves to aid in the benchmarks we evaluate on. Our method achieves\nstate-of-the-art performance on Deep Mind Control Generalization Benchmark.",
    "authors": [
      "Dylan Yung",
      "Andrew Szot",
      "Prithvijit Chattopadhyay",
      "Judy Hoffman",
      "Zsolt Kira"
    ],
    "keywords": [
      "reinforcement learning",
      "generalization",
      "pixel-based RL",
      "embodied learning"
    ],
    "real_all_scores": [
      6,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Multi-Task Option Learning and Discovery for Stochastic Path Planning": {
    "paper_pk": null,
    "title": "Multi-Task Option Learning and Discovery for Stochastic Path Planning",
    "abstract": "This paper addresses the problem of reliably and efficiently solving broad classes of long-horizon stochastic path planning problems. Starting with a vanilla RL formulation with a stochastic dynamics simulator and an occupancy matrix of the environment, our approach computes useful options with policies as well as high-level paths that compose the discovered options. \nOur main contributions are (1) data-driven methods for creating abstract states that serve as endpoints for helpful options, (2) methods for computing option policies using auto-generated option guides in the form of dense pseudo-reward functions, and (3) an overarching algorithm for composing the computed options. We show that this approach yields strong guarantees of executability and solvability: under fairly general conditions, the computed option guides lead to composable option policies and consequently ensure downward refinability. Empirical evaluation on a range of robots, environments, and tasks shows that this approach effectively transfers knowledge across related tasks and that it outperforms existing approaches by a significant margin.",
    "authors": [
      "Naman Shah",
      "Siddharth Srivastava"
    ],
    "keywords": [
      "Option discovery",
      "learning abstractions",
      "planning and learning",
      "reinforcement learning",
      "RL for robotics",
      "hierarchical methods",
      "stochastic path planning"
    ],
    "real_all_scores": [
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning": {
    "paper_pk": null,
    "title": "Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning",
    "abstract": "Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call minimal value-equivalent partial models. After providing the formal definitions of these models, we provide theoretical results demonstrating the scalability advantages of performing planning with minimal value-equivalent partial models and then perform experiments to empirically illustrate our theoretical results. Finally, we provide some useful heuristics on how to learn such models with deep learning architectures and empirically demonstrate that models learned in such a way can allow for performing planning that is robust to distribution shifts and compounding model errors. Overall, both our theoretical and empirical results suggest that minimal value-equivalent partial models can provide significant benefits to performing scalable and robust planning in lifelong reinforcement learning scenarios. ",
    "authors": [
      "Safa Alver",
      "Doina Precup"
    ],
    "keywords": [
      "reinforcement learning",
      "lifelong learning",
      "transfer learning",
      "model-based reinforcement learning"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      2,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Confidence-Conditioned Value Functions for Offline Reinforcement Learning": {
    "paper_pk": null,
    "title": "Confidence-Conditioned Value Functions for Offline Reinforcement Learning",
    "abstract": "Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of OOD actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Bellman backup that simultaneously learns Q-values for any degree of confidence with high probability. By conditioning on confidence, our value functions enable adaptive strategies during online evaluation by controlling for confidence level using the history of observations thus far. This approach can be implemented in practice by conditioning the Q-function from existing conservative algorithms on the confidence. We theoretically show that our learned value functions produce conservative estimates of the true value at any desired confidence. Finally, we empirically show that our algorithm outperforms existing conservative offline RL algorithms on multiple discrete control domains. ",
    "authors": [
      "Joey Hong",
      "Aviral Kumar",
      "Sergey Levine"
    ],
    "keywords": [
      "reinforcement learning",
      "offline reinforcement learning",
      "ensembles",
      "adaptation"
    ],
    "real_all_scores": [
      5,
      3,
      8,
      1
    ],
    "real_confidences": [
      3,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "BC-IRL: Learning Generalizable Reward Functions from Demonstrations": {
    "paper_pk": null,
    "title": "BC-IRL: Learning Generalizable Reward Functions from Demonstrations",
    "abstract": "How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generalization settings.",
    "authors": [
      "Andrew Szot",
      "Amy Zhang",
      "Dhruv Batra",
      "Zsolt Kira",
      "Franziska Meier"
    ],
    "keywords": [
      "inverse reinforcement learning",
      "reward learning",
      "reinforcement learning",
      "imitation learning"
    ],
    "real_all_scores": [
      1,
      1,
      3,
      3
    ],
    "real_confidences": [
      1,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Feasible Adversarial Robust Reinforcement Learning for Underspecified Environments": {
    "paper_pk": null,
    "title": "Feasible Adversarial Robust Reinforcement Learning for Underspecified Environments",
    "abstract": "Robust reinforcement learning (RL) considers the problem of learning policies that perform well in the worst case among a set of possible environment parameter values. In real-world environments, choosing the set of possible values for robust RL can be a difficult task. When that set is specified too narrowly, the agent will be left vulnerable to reasonable parameter values unaccounted for. When specified too broadly, the agent will be too cautious. In this paper, we propose Feasible Adversarial Robust RL (FARR), a novel problem formulation and objective for automatically determining the set of environment parameter values over which to be robust. FARR implicitly defines the set of feasible parameter values as those on which an agent could achieve a benchmark reward given enough training resources. By formulating this problem as a two-player zero-sum game, optimizing the FARR objective jointly produces an adversarial distribution over parameter values with feasible support and a policy robust over this feasible parameter set. We demonstrate that approximate Nash equilibria for this objective can be found using a variation of the PSRO algorithm. Furthermore, we show that an optimal agent trained with FARR is more robust to feasible adversarial parameter selection than with existing minimax, domain-randomization, and regret objectives in a parameterized gridworld and three MuJoCo control environments.",
    "authors": [
      "John Banister Lanier",
      "Stephen Marcus McAleer",
      "Pierre Baldi",
      "Roy Fox"
    ],
    "keywords": [
      "reinforcement learning",
      "robust rl",
      "sim-to-real",
      "game-theory",
      "psro"
    ],
    "real_all_scores": [
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "AsymQ: Asymmetric Q-loss to mitigate overestimation bias in off-policy reinforcement learning": {
    "paper_pk": null,
    "title": "AsymQ: Asymmetric Q-loss to mitigate overestimation bias in off-policy reinforcement learning",
    "abstract": "It is well-known that off-policy deep reinforcement learning algorithms suffer from overestimation bias in value function approximation. Existing methods to reduce overestimation bias often utilize multiple value function estimators. Consequently, these methods have a larger time and memory consumption. In this work, we propose a new class of policy evaluation algorithms dubbed, \\textbf{AsymQ}, that use asymmetric loss functions to train the Q-value network. Departing from the symmetric loss functions such as mean squared error~(MSE) and Huber loss on the Temporal difference~(TD) error, we adopt asymmetric loss functions of the TD-error to impose a higher penalty on overestimation error. We present one such AsymQ loss called \\textbf{Softmax MSE~(SMSE)} that can be implemented with minimal modifications to the standard policy evaluation. Empirically, we show that using SMSE loss helps reduce estimation bias, and subsequently improves policy performance when combined with standard reinforcement learning algorithms. With SMSE, even the Deep Deterministic Policy Gradients~(DDPG) algorithm can achieve performance comparable to that of state-of-the-art methods such as the Twin-Delayed DDPG (TD3) and Soft Actor Critic~(SAC) on challenging environments in the OpenAI Gym MuJoCo benchmark. We additionally demonstrate that the proposed SMSE loss can also boost the performance of Deep Q learning (DQN) in Atari games with discrete action spaces.",
    "authors": [
      "Qinsheng Zhang",
      "Arjun Krishna",
      "Sehoon Ha",
      "Yongxin Chen"
    ],
    "keywords": [
      "reinforcement learning",
      "estimation bias"
    ],
    "real_all_scores": [
      3,
      6,
      6,
      3
    ],
    "real_confidences": [
      3,
      2,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Provably Efficient Reinforcement Learning for Online Adaptive Influence Maximization": {
    "paper_pk": null,
    "title": "Provably Efficient Reinforcement Learning for Online Adaptive Influence Maximization",
    "abstract": "Online influence maximization aims to maximize the influence spread of a content in a social network with an unknown network model by selecting a few seed nodes. Recent studies followed a non-adaptive setting, where the seed nodes are selected before the start of the diffusion process and network parameters are updated when the diffusion stops. We consider an adaptive version of content-dependent online influence maximization problem where the seed nodes are sequentially activated based on real-time feedback. In this paper, we formulate the problem as an infinite-horizon discounted MDP under a linear diffusion process and present a model-based reinforcement learning solution. Our algorithm maintains a network model estimate and selects seed users adaptively, exploring the social network while improving the optimal policy optimistically. We establish $\\widetilde O(\\sqrt{T})$ regret bound for our algorithm. Empirical evaluations on synthetic and real-world networks demonstrate the efficiency of our algorithm. ",
    "authors": [
      "Kaixuan Huang",
      "Yu Wu",
      "Xuezhou Zhang",
      "Shenyinying Tu",
      "Qingyun Wu",
      "Mengdi Wang",
      "Huazheng Wang"
    ],
    "keywords": [
      "influence maximization",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      6,
      3,
      5
    ],
    "real_confidences": [
      3,
      4,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "CrystalBox: Efficient Model-Agnostic Explanations for Deep RL Controllers": {
    "paper_pk": null,
    "title": "CrystalBox: Efficient Model-Agnostic Explanations for Deep RL Controllers",
    "abstract": "Practical adoption of Reinforcement Learning (RL) controllers is hindered by a lack of explainability. Particularly, in input-driven environments such as computer systems where the state dynamics are affected by external processes, explainability can serve as a key towards increased real-world deployment of RL controllers. In this work, we propose a novel framework, CrystalBox, for generating black-box post-hoc explanations for RL controllers in input-driven environments. CrystalBox is built on the principle of separation between policy learning and explanation computation. As the explanations are generated completely outside the training loop, CrystalBox is generalizable to a large family of input-driven RL controllers.To generate explanations, CrystalBox combines the natural decomposability of reward functions in systems environments with the explanatory power of decomposed returns. CrystalBox predicts these decomposed future returns  using on policy Q-function approximations. Our design leverages two complementary approaches for this computation: sampling- and learning-based methods.  We evaluate CrystalBox with RL controllers in real-world settings and demonstrate that it generates high-fidelity explanations.\n",
    "authors": [
      "Sagar Patel",
      "Sangeetha Abdu Jyothi",
      "Nina Narodytska"
    ],
    "keywords": [
      "explainability",
      "reinforcement learning"
    ],
    "real_all_scores": [
      3,
      5,
      6,
      3,
      8
    ],
    "real_confidences": [
      4,
      3,
      2,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Reward Design with Language Models": {
    "paper_pk": null,
    "title": "Reward Design with Language Models",
    "abstract": "Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by using a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent's behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user's objectives and outperforms RL agents trained with reward functions learned via supervised learning. ",
    "authors": [
      "Minae Kwon",
      "Sang Michael Xie",
      "Kalesha Bullard",
      "Dorsa Sadigh"
    ],
    "keywords": [
      "reward design",
      "foundation models",
      "gpt3",
      "reward specification",
      "reinforcement learning",
      "human-ai interaction"
    ],
    "real_all_scores": [
      5,
      5,
      5
    ],
    "real_confidences": [
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Deep Learning of Intrinsically Motivated Options in the Arcade Learning Environment": {
    "paper_pk": null,
    "title": "Deep Learning of Intrinsically Motivated Options in the Arcade Learning Environment",
    "abstract": "In Reinforcement Learning, Intrinsic Motivation motivates directed behaviors through a wide range of reward-generating methods. Depending on the task and environment, these rewards can be useful, might complement each other, but can also break down entirely, as seen with the noisy TV problem for curiosity. We therefore argue that scalability and robustness, among others, are key desirable properties of a method to incorporate intrinsic rewards, which a simple weighted sum of reward lacks. In a tabular setting, Explore Options let the agent call an intrinsically motivated policy in order to learn from its trajectories. We introduce Deep Explore Options, revising Explore Options within the Deep Reinforcement Learning paradigm to tackle complex visual problems. Deep Explore Options can naturally learn from several unrelated intrinsic rewards, ignore harmful intrinsic rewards, learn to balance exploration, but also isolate exploitative and exploratory behaviors for independent usage. \nWe test Deep Explore Options on hard and easy exploration games of the Atari Suite, following a benchmarking study to ensure fairness. Our empirical results show that they achieve similar results than weighted sum baselines, while maintaining their key properties.\n",
    "authors": [
      "Louis Bagot",
      "Kevin Mets",
      "Tom De Schepper",
      "Steven Latre"
    ],
    "keywords": [
      "reinforcement learning",
      "intrinsic motivation",
      "exploration",
      "options",
      "auxiliary task learning"
    ],
    "real_all_scores": [
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Solving Continuous Control via Q-learning": {
    "paper_pk": null,
    "title": "Solving Continuous Control via Q-learning",
    "abstract": "While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a variety of continuous control tasks.",
    "authors": [
      "Tim Seyde",
      "Peter Werner",
      "Wilko Schwarting",
      "Igor Gilitschenski",
      "Martin Riedmiller",
      "Daniela Rus",
      "Markus Wulfmeier"
    ],
    "keywords": [
      "reinforcement learning",
      "continuous control",
      "learning efficiency"
    ],
    "real_all_scores": [
      6,
      3,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Illusory Adversarial Attacks on Sequential Decision-Makers and Countermeasures": {
    "paper_pk": null,
    "title": "Illusory Adversarial Attacks on Sequential Decision-Makers and Countermeasures",
    "abstract": "Autonomous decision-making agents deployed in the real world need to be robust against possible adversarial attacks on sensory inputs. Existing work on adversarial attacks focuses on the notion of perceptual invariance popular in computer vision. We observe that such attacks can often be detected by victim agents, since they result in action-observation sequences that are not consistent with the dynamics of the environment. Furthermore, real-world agents, such as physical robots, commonly operate under human supervisors who are not susceptible to such attacks. We propose to instead focus on attacks that are statistically undetectable. Specifically, we propose illusory attacks, a novel class of adversarial attack that is consistent with the environment dynamics. We introduce a novel algorithm that can learn illusory attacks end-to-end. We empirically verify that our algorithm generates attacks that, in contrast to current methods, are undetectable to both AI\nagents with an environment dynamics model, as well as to humans. Furthermore, we show that existing robustification approaches are relatively ineffective against illusory attacks. Our findings highlight the need to ensure that real-world AI, and human-AI, systems are designed to make it difficult to corrupt sensory observations in ways that are consistent with the environment dynamics.",
    "authors": [
      "Tim Franzmeyer",
      "Stephen Marcus McAleer",
      "Joao F. Henriques",
      "Philip Torr",
      "Jakob Nicolaus Foerster",
      "Adel Bibi",
      "Christian Schroeder de Witt"
    ],
    "keywords": [
      "reinforcement learning",
      "adversarial attacks"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Advantage Constrained Proximal Policy Optimization in Multi-Agent Reinforcement Learning": {
    "paper_pk": null,
    "title": "Advantage Constrained Proximal Policy Optimization in Multi-Agent Reinforcement Learning",
    "abstract": "We explore the value-based method and policy gradient combination in multi-agent reinforcement learning (MARL). In value-based MARL, {\\itshape{Individual-Global-Max}} (IGM) principle plays an important role, which maintains the consistency between joint and local action values. At the same time, IGM is difficult to guarantee in multi-agent policy gradient methods due to stochastic exploration and conflicting gradient directions. In this paper, we propose a novel multi-agent policy gradient algorithm called {\\itshape{Advantage Constrained Proximal Policy Optimization}} (ACPPO). Based on {\\itshape{multi-agent advantage decomposition lemma}}, ACPPO introduces an advantage network for each agent to estimate current local state-action advantage. The coefficient of each agent constrains the joint-action advantage according to the consistency of the estimated joint-action advantage and local advantage. Unlike previous policy gradient-based MARL algorithms, ACPPO does not need an extra sampled baseline to reduce variance. We evaluate the proposed methods for continuous matrix game and Multi-Agent MuJoCo tasks. Results show that ACPPO outperforms the baselines such as MAPPO, MADDPG, and HAPPO.",
    "authors": [
      "Weifan Li"
    ],
    "keywords": [
      "Multi agent",
      "reinforcement learning",
      "neural network",
      "deep learning",
      "trust region."
    ],
    "real_all_scores": [
      8,
      8,
      3,
      6
    ],
    "real_confidences": [
      4,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling": {
    "paper_pk": null,
    "title": "SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling",
    "abstract": "We propose SPRINT, an approach for scalable offline policy pre-training based on natural language instructions. SPRINT pre-trains an agent\u2019s policy to execute a diverse set of semantically meaningful skills that it can leverage to learn new tasks faster. Prior work on offline pre-training required tedious manual definition of pre-training tasks or learned semantically meaningless skills via random goal-reaching. Instead, our approach SPRINT (Scalable Pre-training via Relabeling Language INsTructions) leverages natural language instruction labels on offline agent experience, collected at scale (e.g., via crowd-sourcing), to define a rich set of tasks with minimal human effort. Furthermore, by using natural language to define tasks, SPRINT can use pre-trained large language models to automatically expand the initial task set. By relabeling and aggregating task instructions, even across multiple training trajectories, we can learn a large set of new skills during pre-training. In experiments using a realistic household simulator, we show that agents pre-trained with SPRINT learn new long-horizon household tasks substantially faster than with previous pre-training approaches.",
    "authors": [
      "Jesse Zhang",
      "Karl Pertsch",
      "Jiahui Zhang",
      "Taewook Nam",
      "Sung Ju Hwang",
      "Xiang Ren",
      "Joseph J Lim"
    ],
    "keywords": [
      "reinforcement learning",
      "language-guided RL",
      "offline RL",
      "policy pre-training"
    ],
    "real_all_scores": [
      8,
      5,
      6
    ],
    "real_confidences": [
      4,
      3,
      2
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Evaluation of Active Feature Acquisition Methods under Missing Data": {
    "paper_pk": null,
    "title": "Evaluation of Active Feature Acquisition Methods under Missing Data",
    "abstract": "Machine learning (ML) methods generally assume the full set of features are available at no cost. If the acquisition of a certain feature is costly at run-time, one might want to balance the acquisition cost and the predictive value of the feature for the ML task. The task of training an AI agent to decide which features are necessary to be acquired is called active feature acquisition (AFA). Current AFA methods, however, are challenged when the AFA agent has to be trained/tested with datasets that contain missing data. We formulate, for the first time, the problem of active feature acquisition performance evaluation (AFAPE) under missing data, i.e. the problem of adjusting for the inevitable missingness distribution shift between train/test time and run-time. We first propose a new causal graph, the AFA graph, that characterizes the AFAPE problem as an intervention on the reinforcement learning environment used to train AFA agents. Here, we discuss that for handling missing data in AFAPE, the conventional approaches (off-policy reinforcement learning,  blocked feature acquisitions, imputation and inverse probability weighting (IPW)) often lead to biased results or are data inefficient. We then propose active feature acquisition importance sampling (AFAIS), a novel estimator that is more data efficient than IPW. We demonstrate the detrimental conclusions to which biased estimators can lead as well as the high data efficiency of AFAIS in multiple experiments using simulated and real-world data under induced MCAR, MAR and MNAR missingness.",
    "authors": [
      "Henrik von Kleist",
      "Alireza Zamanian",
      "Ilya Shpitser",
      "Narges Ahmidi"
    ],
    "keywords": [
      "Active feature acquisition",
      "active sensing",
      "missing data",
      "reinforcement learning",
      "distribution-shift",
      "off-environment policy evaluation",
      "AFA graph",
      "AFAPE",
      "AFAIS",
      "MCAR",
      "MAR",
      "MNAR",
      "causal inference"
    ],
    "real_all_scores": [
      5,
      8,
      3,
      6
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning": {
    "paper_pk": null,
    "title": "Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning",
    "abstract": "Reinforcement learning algorithms typically require tons of training data, resulting in long training time, especially on challenging tasks. With the recent advance in GPU-based simulation, such as Isaac Gym, data collection speed has been improved thousands of times on a commodity GPU. Most prior works have been using on-policy methods such as PPO to train policies in Isaac Gym due to its simpleness and effectiveness in scaling up. Off-policy methods are usually more sample-efficient but more challenging to be scaled up, resulting in a much longer wall-clock training time in practice. In this work, we presented a novel parallel $Q$-learning framework that not only gains better sample efficiency but also reduces the training wall-clock time compared to PPO. Different from prior works on distributed off-policy learning, such as Apex, our framework is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. We demonstrate the capability of scaling up $Q$ learning methods to tens of thousands of parallel environments. We also investigate various factors that can affect the policy learning training speed, including the number of parallel environments, exploration schemes, batch size, GPU models, etc.",
    "authors": [
      "Zechu Li",
      "Tao Chen",
      "Zhang-Wei Hong",
      "Anurag Ajay",
      "Pulkit Agrawal"
    ],
    "keywords": [
      "GPU-based simulation",
      "off-policy learning",
      "distributed training",
      "reinforcement learning"
    ],
    "real_all_scores": [
      8,
      8,
      8,
      8
    ],
    "real_confidences": [
      3,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Speeding up Policy Optimization with Vanishing Hypothesis and Variable Mini-Batch Size": {
    "paper_pk": null,
    "title": "Speeding up Policy Optimization with Vanishing Hypothesis and Variable Mini-Batch Size",
    "abstract": "Reinforcement learning-based algorithms have been used extensively in recent years due to their flexible nature, good performance, and the increasing number of said algorithms. However, the largest drawback of these techniques remains unsolved, that is, it usually takes a long time for the agents to learn how to solve a given problem. In this work, we outline a novel method that can be used to drastically reduce the training time of current state-of-the-art algorithms like Proximal Policy Optimization (PPO). We evaluate the performance of this approach in a unique environment where we use reinforcement learning to help with a practical astronomical problem: where to place a fixed number of observatory stations in the Solar System to observe space objects (e.g. asteroids) as permanently as possible. That is, the reward in this scenario corresponds to the total coverage of the trajectories of these objects. We apply noisy evaluation for calculating the reward to speed up the training, which technique has already been efficiently applied in stochastic optimization. Namely, we allow the incorporation of some additional noise in the reward function in the form of a hypothesis term and a varying mini-batch size. However, in order to follow the theoretical guidelines, both of them are forced to vanish during training to let the noise converge to zero. Our experimental results show that using this approach we can reduce the training time remarkably, even by 75%.",
    "authors": [
      "Tam\u00e1s Tardi",
      "Gergo Bogacsovics",
      "Andras Hajdu"
    ],
    "keywords": [
      "reinforcement learning",
      "policy optimization",
      "noisy evaluation",
      "variable mini-batch size",
      "vanishing hypothesis",
      "optimal placement of sensors"
    ],
    "real_all_scores": [
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Reinforcement Learning for Bandits with Continuous Actions and Large Context Spaces": {
    "paper_pk": null,
    "title": "Reinforcement Learning for Bandits with Continuous Actions and Large Context Spaces",
    "abstract": "We consider the challenging scenario of contextual bandits with continuous actions and large input ``context'' spaces, e.g. images. We posit that by modifying reinforcement learning (RL) algorithms for continuous control, we can outperform hand-crafted contextual bandit algorithms for continuous actions on standard benchmark datasets, i.e. vector contexts. We demonstrate that parametric policy networks outperform recently published tree-based policies in both average regret and costs on held-out samples. Furthermore, in contrast to previous work, we successfully demonstrate that RL algorithms can generalise contextual bandit problems with continuous actions to large context spaces. We obtain state-of-the-art performance using RL and significantly outperform previous methods on image contexts. Lastly, we introduce a new contextual bandits domain with multi-dimensional continuous action space and image context. ",
    "authors": [
      "Paul Duckworth",
      "Bruno Lacerda",
      "Katherine Vallis",
      "Nick Hawes"
    ],
    "keywords": [
      "Contextual bandits",
      "Continuous actions",
      "Image context",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Training Equilibria in Reinforcement Learning": {
    "paper_pk": null,
    "title": "Training Equilibria in Reinforcement Learning",
    "abstract": "In partially observable environments, reinforcement learning algorithms such as policy gradient and Q-learning may have multiple equilibria---policies that are stable under further training---and can converge to policies that are strictly suboptimal. \nPrior work blames insufficient exploration, but suboptimal equilibria can arise despite full exploration and other favorable circumstances like a flexible policy parametrization.\nWe show theoretically that the core problem is that in partially observed environments, an agent's past actions induce a distribution on hidden states.\nEquipping the policy with memory helps it model the hidden state and leads to convergence to a higher reward equilibrium, \\emph{even when there exists a memoryless optimal policy}.\nExperiments show that \npolicies with insufficient memory tend to learn to use the environment as auxiliary memory,and parameter noise helps policies escape suboptimal equilibria. ",
    "authors": [
      "Lauro Langosco",
      "David Krueger",
      "Adam Gleave"
    ],
    "keywords": [
      "theory",
      "reinforcement learning",
      "learning dynamics",
      "partial observability",
      "MDP",
      "POMDP",
      "markov decision processes"
    ],
    "real_all_scores": [
      8,
      8,
      5,
      3
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Off Policy Average Reward Actor Critic with Deterministic Policy Search": {
    "paper_pk": null,
    "title": "Off Policy Average Reward Actor Critic with Deterministic Policy Search",
    "abstract": "The average reward criterion is relatively less explored as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this paper, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We show a finite time analysis of the resulting three-timescale stochastic approximation scheme and obtain an $\\epsilon$-optimal stationary policy with a sample complexity of $\\Omega(\\epsilon^{-2.5})$. We compare the average reward performance of our proposed algorithm and observe better empirical performance compared to state-of-the-art on-policy average reward actor-critic algorithms over MuJoCo based environments. ",
    "authors": [
      "Naman Saxena",
      "Subhojyoti Khastagir",
      "Shishir N Y",
      "Shalabh Bhatnagar"
    ],
    "keywords": [
      "reinforcement learning",
      "actor critic algorithm",
      "deterministic policy",
      "off-policy",
      "target network",
      "average reward",
      "finite time analysis",
      "convergence",
      "three time scale stochastic approximation",
      "DeepMind control suite"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      8
    ],
    "real_confidences": [
      3,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Robust Exploration via Clustering-based Online Density Estimation": {
    "paper_pk": null,
    "title": "Robust Exploration via Clustering-based Online Density Estimation",
    "abstract": "Intrinsic motivation is a critical ingredient in reinforcement learning to enable progress when rewards are sparse. However, many existing approaches that measure the novelty of observations are brittle, or rely on restrictive assumptions about the environment which limit generality. We propose to decompose the exploration problem into two orthogonal sub-problems: (i) finding the right representation (metric) for exploration (ii) estimating densities in this representation space.\n\nTo address (ii), we introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method that estimates visitation counts for clusters of states that are similar according to the metric induced by any arbitrary representation learning technique. We adapt classical clustering algorithms to design a new type of memory that allows RECODE to keep track of the history of interactions over thousands of episodes, thus effectively tracking global visitation counts. This is in contrast to existing non-parametric approaches, that can only store the recent history, typically the current episode.\n\nThe generality of RECODE allows us to easily address (i) by leveraging both off-the-shelf and novel representation learning techniques. In particular, we introduce a novel generalization of the action-prediction representation that leverages multi-step predictions and that we find to be better suited to a suite of challenging 3D-exploration tasks in DM-HARD-8. We show experimentally that our approach can work with a variety of RL agents, and obtain state-of-the-art performance on Atari and DM-HARD-8.",
    "authors": [
      "Alaa Saade",
      "Steven Kapturowski",
      "Daniele Calandriello",
      "Charles Blundell",
      "Michal Valko",
      "Pablo Sprechmann",
      "Bilal Piot"
    ],
    "keywords": [
      "exploration",
      "representation learning",
      "density estimation",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Learning About Progress From Experts": {
    "paper_pk": null,
    "title": "Learning About Progress From Experts",
    "abstract": "Many important tasks involve some notion of long-term progress in multiple phases: e.g. to clean a shelf it must be cleared of items, cleaning products applied, and then the items placed back on the shelf. In this work, we explore the use of expert demonstrations in long-horizon tasks to learn a monotonically increasing function that summarizes progress. This function can then be used to aid agent exploration in environments with sparse rewards. As a case study we consider the NetHack environment, which requires long-term progress at a variety of scales and is far from being solved by existing approaches. In this environment, we demonstrate that by learning a model of long-term progress from expert data containing only observations, we can achieve efficient exploration in challenging sparse tasks, well beyond what is possible with current state-of-the-art approaches. We have made the curated gameplay dataset used in this work available at https://github.com/deepmind/nao_top10.",
    "authors": [
      "Jake Bruce",
      "Ankit Anand",
      "Bogdan Mazoure",
      "Rob Fergus"
    ],
    "keywords": [
      "learning from demonstrations",
      "reinforcement learning",
      "exploration",
      "nethack"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      6
    ],
    "real_confidences": [
      5,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "CASA: Bridging the Gap between Policy Improvement and Policy Evaluation with Conflict Averse Policy Iteration": {
    "paper_pk": null,
    "title": "CASA: Bridging the Gap between Policy Improvement and Policy Evaluation with Conflict Averse Policy Iteration",
    "abstract": "We study the problem of model-free reinforcement learning, which is often solved following the principle of Generalized Policy Iteration (GPI). While GPI is typically an interplay between policy evaluation and policy improvement, most conventional model-free methods with function approximation assume the independence of GPI steps, despite of the inherent connections between them. In this paper, we present a method that attempts to eliminate the inconsistency between policy evaluation step and policy improvement step, leading to a conflict averse GPI solution with gradient-based functional approximation. Our method is capital to balancing exploitation and exploration between policy-based and value-based methods and is applicable to existed policy-based and value-based methods. We conduct extensive experiments to study theoretical properties of our method and demonstrate the effectiveness of our method on Atari 200M benchmark.",
    "authors": [
      "Changnan Xiao",
      "Haosen Shi",
      "Jiajun Fan",
      "Shihong Deng",
      "Haiyan Yin"
    ],
    "keywords": [
      "reinforcement learning",
      "policy iteration"
    ],
    "real_all_scores": [
      3,
      8,
      8,
      6
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Memory-Efficient Reinforcement Learning with Priority based on Surprise and On-policyness": {
    "paper_pk": null,
    "title": "Memory-Efficient Reinforcement Learning with Priority based on Surprise and On-policyness",
    "abstract": "In off-policy reinforcement learning, an agent collects transition data (a.k.a. experience tuples) from the environment and stores them in a replay buffer for the incoming parameter updates. Storing those tuples consumes a large amount of memory when the environment observations are given as images. Large memory consumption is especially problematic when reinforcement learning methods are applied in scenarios where the computational resources are limited. In this paper, we introduce a method to prune relatively unimportant experience tuples by a simple metric that estimates the importance of experiences and saves the overall memory consumption by the buffer. To measure the importance of experiences, we use $\\textit{surprise}$ and $\\textit{on-policyness}$. Surprise is quantified by the information gain the model can obtain from the experiences and on-policyness ensures that they are relevant to the current policy. In our experiments, we empirically show that our method can significantly reduce the memory consumption by the replay buffer without decreasing the performance in vision-based environments.",
    "authors": [
      "Ryosuke Unno",
      "Yoshimasa Tsuruoka"
    ],
    "keywords": [
      "replay buffer",
      "reinforcement learning",
      "memory efficiency"
    ],
    "real_all_scores": [
      3,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Graph Backup: Data Efficient Backup Exploiting Markovian Transitions": {
    "paper_pk": null,
    "title": "Graph Backup: Data Efficient Backup Exploiting Markovian Transitions",
    "abstract": "The successes of deep Reinforcement Learning (RL) are limited to settings where we have a large stream of online experiences, but applying RL in the data-efficient setting with limited access to online interactions is still challenging. A key to data-efficient RL is good value estimation, but current methods in this space fail to fully utilise the structure of the trajectory data gathered from the environment. In this paper, we treat the transition data of the MDP as a graph, and define a novel backup operator, Graph Backup, which exploits this graph structure for better value estimation. Compared to multi-step backup methods such as $n$-step $Q$-Learning and TD($\\lambda$), Graph Backup can perform counterfactual credit assignment and gives stable value estimates for a state regardless of which trajectory the state is sampled from. Our method, when combined with popular off-policy value-based methods, provides improved performance over one-step and multi-step methods on a suite of data-efficient RL benchmarks including MiniGrid, Minatar and Atari100K. We further analyse the reasons for this performance boost through a novel visualisation of the transition graphs of Atari games.",
    "authors": [
      "zhengyao jiang",
      "Tianjun Zhang",
      "Robert Kirk",
      "Tim Rockt\u00e4schel",
      "Edward Grefenstette"
    ],
    "keywords": [
      "reinforcement learning",
      "graph structure",
      "neuro-symbolic methods",
      "data efficient reinforcement learning"
    ],
    "real_all_scores": [
      3,
      8,
      3,
      6
    ],
    "real_confidences": [
      4,
      4,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning": {
    "paper_pk": null,
    "title": "A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning",
    "abstract": "As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One-step methods perform regularization by doing just a single step of policy improvement, while critic regularization methods do many steps of policy improvement with a regularized objective. These methods appear distinct. One-step methods, such as advantage-weighted regression and conditional behavioral cloning, truncate policy iteration after just one step. This ``early stopping'' makes one-step RL simple and stable, but can limit its asymptotic performance. Critic regularization typically requires more compute but has appealing lower-bound guarantees. In this paper, we draw a close connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL. While practical implementations violate our assumptions and critic regularization is typically applied with smaller regularization coefficients, our experiments nevertheless show that our analysis makes accurate, testable predictions about practical offline RL methods (CQL and one-step RL) with commonly-used hyperparameters. Our results  that every problem can be solved with a single step of policy improvement, but rather that one-step RL might be competitive with critic regularization on RL problems that demand strong regularization.",
    "authors": [
      "Benjamin Eysenbach",
      "Matthieu Geist",
      "Sergey Levine",
      "Ruslan Salakhutdinov"
    ],
    "keywords": [
      "reinforcement learning",
      "regularization",
      "one-step RL",
      "theory"
    ],
    "real_all_scores": [
      8,
      3,
      6,
      6
    ],
    "real_confidences": [
      3,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints": {
    "paper_pk": null,
    "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints",
    "abstract": "Many real-world settings involve costs for performing actions; transaction costs\nin financial systems and fuel costs being common examples. In these settings,\nperforming actions at each time step quickly accumulates costs leading to vastly\nsuboptimal outcomes. Additionally, repeatedly acting produces wear and tear and\nultimately, damage. Determining when to act is crucial for achieving successful\noutcomes and yet, the challenge of efficiently learning to behave optimally when\nactions incur minimally bounded costs remains unresolved. In this paper, we intro-\nduce a reinforcement learning (RL) framework named Learnable Impulse Control\nReinforcement Algorithm (LICRA), for learning to optimally select both when\nto act and which actions to take when actions incur costs. At the core of LICRA\nis a nested structure that combines RL and a form of policy known as impulse\ncontrol which learns to maximise objectives when actions incur costs. We prove\nthat LICRA, which seamlessly adopts any RL method, converges to policies that\noptimally select when to perform actions and their optimal magnitudes. We then\naugment LICRA to handle problems in which the agent can perform at most k < \u221e\nactions and more generally, faces a budget constraint. We show LICRA learns the\noptimal value function and ensures budget constraints are satisfied almost surely.\nWe demonstrate empirically LICRA\u2019s superior performance against benchmark\nRL methods in OpenAI gym\u2019s Lunar Lander and in Highway environments and a\nvariant of the Merton portfolio problem within finance.",
    "authors": [
      "David Henry Mguni",
      "Aivar Sootla",
      "Juliusz Krzysztof Ziomek",
      "Oliver Slumbers",
      "Zipeng Dai",
      "Kun Shao",
      "Jun Wang"
    ],
    "keywords": [
      "dynamic programming",
      "impulse control",
      "optimal stopping",
      "reinforcement learning"
    ],
    "real_all_scores": [
      5,
      3,
      3
    ],
    "real_confidences": [
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Query The Agent: Improving Sample Efficiency Through Epistemic Uncertainty Estimation": {
    "paper_pk": null,
    "title": "Query The Agent: Improving Sample Efficiency Through Epistemic Uncertainty Estimation",
    "abstract": "Curricula for goal-conditioned reinforcement learning agents typically rely on poor estimates of the agent's epistemic uncertainty or fail to consider the agents' epistemic uncertainty altogether, resulting in poor sample efficiency. We propose a novel algorithm, Query The Agent (QTA), which significantly improves sample efficiency by estimating the agent's epistemic uncertainty throughout the state space and setting goals in highly uncertain areas. Encouraging the agent to collect data in highly uncertain states allows the agent to improve its estimation of the value function rapidly. QTA utilizes a novel technique for estimating epistemic uncertainty, Predictive Uncertainty Networks (PUN), to allow QTA to assess the agent's uncertainty in all previously observed states. We demonstrate that QTA offers decisive sample efficiency improvements over preexisting methods.",
    "authors": [
      "Julian Alverio",
      "Boris Katz",
      "Andrei Barbu"
    ],
    "keywords": [
      "goal-conditioned reinforcement learning",
      "reinforcement learning",
      "goal-conditioned",
      "goal",
      "model-free",
      "sample efficiency",
      "deep reinforcement learning"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      3
    ],
    "real_confidences": [
      3,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments": {
    "paper_pk": null,
    "title": "Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments",
    "abstract": "Conventional Reinforcement Learning (RL) algorithms assume the distribution of the data to be uniform or mostly uniform. However, this is not the case with most real-world applications like autonomous driving or in nature, where animals roam. Some objects are encountered frequently, and most of the remaining experiences occur rarely; the resulting distribution is called \\emph{Zipfian}. Taking inspiration from the theory of \\emph{complementary learning systems}, an architecture for learning from Zipfian distributions is proposed where long tail states are discovered in an unsupervised manner and states along with their recurrent activation are kept longer in episodic memory. The recurrent activations are then reinstated from episodic memory using a similarity search, giving weighted importance. The proposed architecture yields improved performance in a Zipfian task over conventional architectures. Our method outperforms IMPALA by a significant margin of 20.3\\% when maps/objects occur with a uniform distribution and by 50.2\\% on the rarest 20\\% of the distribution.",
    "authors": [
      "Dolton Milagres Fernandes",
      "Pramod Kaushik",
      "Harsh Shukla",
      "Bapi Raju Surampudi"
    ],
    "keywords": [
      "long tail distribution",
      "reinforcement learning",
      "representation learning",
      "contrastive learning",
      "complementary learning system",
      "hippocampus"
    ],
    "real_all_scores": [
      3,
      3,
      3,
      3
    ],
    "real_confidences": [
      5,
      4,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Coordination Scheme Probing for Generalizable Multi-Agent Reinforcement Learning": {
    "paper_pk": null,
    "title": "Coordination Scheme Probing for Generalizable Multi-Agent Reinforcement Learning",
    "abstract": "Coordinating with previously unknown teammates without joint learning is a crucial need for real-world multi-agent applications, such as human-AI interaction. An active research topic on this problem is ad hoc teamwork, which improves agents' coordination ability in zero-shot settings. However, previous works can only solve the problem of a single agent's coordination with different teams, which is not in line with arbitrary group-to-group coordination in complex multi-agent scenarios. Moreover, they commonly suffer from limited adaptation ability within an episode in a zero-shot setting. To address these problems, we introduce the Coordination Scheme Probing (CSP) approach that applies a disentangled scheme probing module to represent and classify the newly arrived teammates beforehand with limited pre-collected episodic data and makes multi-agent control accordingly. To achieve generalization, CSP learns a meta-policy with multiple sub-policies that follow distinguished coordination schemes in an end-to-end fashion and automatically reuses it to coordinate with unseen teammates. Empirically, we show that the proposed method achieves remarkable performance compared to existing ad hoc teamwork and policy generalization methods in various multi-agent cooperative scenarios.",
    "authors": [
      "Hao Ding",
      "Chengxing Jia",
      "Cong Guan",
      "Feng Chen",
      "Lei Yuan",
      "Zongzhang Zhang",
      "Yang Yu"
    ],
    "keywords": [
      "reinforcement learning",
      "multi-agent reinforcement learning",
      "agent modeling"
    ],
    "real_all_scores": [
      8,
      8,
      8
    ],
    "real_confidences": [
      2,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning": {
    "paper_pk": null,
    "title": "MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning",
    "abstract": "Open-ended learning methods that automatically generate a curriculum of increasingly challenging tasks serve as a promising avenue toward generally capable reinforcement learning agents. Existing methods adapt curricula independently over either environment parameters (in single-agent settings) or co-player policies (in multi-agent settings). However, the strengths and weaknesses of co-players can manifest themselves differently depending on environmental features. It is thus crucial to consider the dependency between the environment and co-player when shaping a curriculum in multi-agent domains. In this work, we use this insight and extend Unsupervised Environment Design (UED) to multi-agent environments. We then introduce Multi-Agent Environment Design Strategist for Open-Ended Learning (MAESTRO), the first multi-agent UED approach for two-player zero-sum settings. MAESTRO efficiently produces adversarial, joint curricula over both environments and co-players and attains minimax-regret guarantees at Nash equilibrium. Our experiments show that MAESTRO outperforms a number of strong baselines on competitive two-player games, spanning discrete and continuous control settings.",
    "authors": [
      "Mikayel Samvelyan",
      "Akbir Khan",
      "Michael D Dennis",
      "Minqi Jiang",
      "Jack Parker-Holder",
      "Jakob Nicolaus Foerster",
      "Roberta Raileanu",
      "Tim Rockt\u00e4schel"
    ],
    "keywords": [
      "reinforcement learning",
      "multi-agent learning",
      "unsupervised environment design"
    ],
    "real_all_scores": [
      5,
      5,
      5
    ],
    "real_confidences": [
      3,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Policy-Based Self-Competition for Planning Problems": {
    "paper_pk": null,
    "title": "Policy-Based Self-Competition for Planning Problems",
    "abstract": "AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent\u2019s historical performances and to reshape an episode\u2019s reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ \u2018Play-to-Plan\u2019 (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our approach in two well-known combinatorial optimization problems, the Traveling Salesman Problem and the Job-Shop Scheduling Problem. With only half of the simulation budget for search, GAZ PTP consistently outperforms all selected single-player variants of GAZ.",
    "authors": [
      "Jonathan Pirnay",
      "Quirin G\u00f6ttl",
      "Jakob Burger",
      "Dominik Gerhard Grimm"
    ],
    "keywords": [
      "reinforcement learning",
      "alphazero",
      "self-competition",
      "self-critical",
      "gumbel",
      "mcts"
    ],
    "real_all_scores": [
      5,
      8,
      3
    ],
    "real_confidences": [
      4,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Quality-Similar Diversity via Population Based Reinforcement Learning": {
    "paper_pk": null,
    "title": "Quality-Similar Diversity via Population Based Reinforcement Learning",
    "abstract": "Diversity is a growing research topic in Reinforcement Learning (RL). Previous research on diversity has mainly focused on promoting diversity to encourage exploration and thereby improve quality (the cumulative reward), maximizing diversity subject to quality constraints, or jointly maximizing quality and diversity, known as the quality-diversity problem. In this work, we present the quality-similar diversity problem that features diversity among policies of similar qualities. In contrast to task-agnostic diversity, we focus on task-specific diversity defined by a set of user-specified Behavior Descriptors (BDs). A BD is a scalar function of a trajectory (e.g., the fire action rate for an Atari game), which delivers the type of diversity the user prefers. To derive the gradient of the user-specified diversity with respect to a policy, which is not trivially available, we introduce a set of BD estimators and connect it with the classical policy gradient theorem. Based on the diversity gradient, we develop a population-based RL algorithm to adaptively and efficiently optimize the population diversity at multiple quality levels throughout training. Extensive results on MuJoCo and Atari demonstrate that our algorithm significantly outperforms previous methods in terms of generating user-specified diverse policies across different quality levels.",
    "authors": [
      "Shuang Wu",
      "Jian Yao",
      "Haobo Fu",
      "Ye Tian",
      "Chao Qian",
      "Yaodong Yang",
      "QIANG FU",
      "Yang Wei"
    ],
    "keywords": [
      "quality diversity",
      "reinforcement learning",
      "user-defined",
      "population"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning": {
    "paper_pk": null,
    "title": "CLUTR: Curriculum Learning via Unsupervised Task Representation Learning",
    "abstract": "Reinforcement Learning (RL) algorithms are often known for sample inefficiency and difficult generalization. Recently, Unsupervised Environment Design (UED) emerged as a new paradigm for zero-shot generalization by simultaneously learning a task distribution and agent policies on the sampled tasks. This is a non-stationary process where the task distribution evolves along with agent policies; creating an instability over time. While past works demonstrated the potential of such approaches, sampling effectively from the task space remains an open challenge, bottlenecking these approaches. To this end, we introduce CLUTR: a novel curriculum learning algorithm that decouples task representation and curriculum learning into a two-stage optimization. It first trains a recurrent variational autoencoder on randomly generated tasks to learn a latent task manifold. Next, a teacher agent creates a curriculum by optimizing a minimax REGRET-based objective on a set of latent tasks sampled from this manifold. By keeping the task manifold fixed, we show that CLUTR successfully overcomes the non-stationarity problem and improves stability. Our experimental results show CLUTR outperforms PAIRED, a principled and popular UED method, in terms of generalization and sample efficiency in the challenging CarRacing and navigation environments: showing an 18x improvement on the F1 CarRacing benchmark. CLUTR also performs comparably to the non-UED state-of-the-art for CarRacing, outperforming it in nine of the 20 tracks. CLUTR also achieves a 33% higher solved rate than PAIRED on a set of 18 out-of-distribution navigation tasks.",
    "authors": [
      "Abdus Salam Azad",
      "Izzeddin Gur",
      "Aleksandra Faust",
      "Pieter Abbeel",
      "Ion Stoica"
    ],
    "keywords": [
      "reinforcement learning",
      "curriculum learning"
    ],
    "real_all_scores": [
      6,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Continuous Goal Sampling: A Simple Technique to Accelerate Automatic Curriculum Learning": {
    "paper_pk": null,
    "title": "Continuous Goal Sampling: A Simple Technique to Accelerate Automatic Curriculum Learning",
    "abstract": "Goal-conditioned reinforcement learning (RL) tackles the problem of training an RL agent to reach multiple goals in an environment, often with sparse rewards only administered upon reaching the goal. \nIn this regard, automatic curriculum learning can improve an agent's learning by sampling goals in a structured order catered to the agent's current ability. \nThis work presents two contributions to improve learning in goal-conditioned RL environments.\nFirst, we present a simple, algorithm-agnostic technique to accelerate learning by continuous goal sampling, in which an agent's goals are sampled and changed multiple times within a single episode. \nSuch continuous goal sampling enables faster exploration of the goal space and allows curriculum methods to have a more significant impact on an agent's learning.\nSecond, we propose VDIFF, an automatic curriculum learning method that uses an agent's value function to create a self-paced curriculum by sampling goals on which the agent is demonstrating high learning progress.\nThrough results on 17 multi-goal robotic environments and navigation tasks, we show that continuous goal sampling and VDIFF work synergistically and result in performance gains over current state-of-the-art methods.",
    "authors": [
      "Mehul Damani",
      "Lerrel Pinto"
    ],
    "keywords": [
      "reinforcement learning",
      "curriculum learning",
      "goal-conditioned reinforcement learning"
    ],
    "real_all_scores": [
      3,
      3,
      3
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Extreme Q-Learning: MaxEnt RL without Entropy": {
    "paper_pk": null,
    "title": "Extreme Q-Learning: MaxEnt RL without Entropy",
    "abstract": "Modern Deep Reinforcement Learning (RL) algorithms require estimates of the maximal Q-value, which are difficult to compute in continuous domains with an infinite number of possible actions. In this work, we introduce a new update rule for online and offline RL which directly models the maximal value using Extreme Value Theory (EVT), drawing inspiration from economics. By doing so, we avoid computing Q-values using out-of-distribution actions which is often a substantial source of error. Our key insight is to introduce an objective that directly estimates the optimal soft-value functions (LogSumExp) in the maximum entropy RL setting without needing to sample from a policy. Using EVT, we derive our \\emph{Extreme Q-Learning} framework and consequently online and, for the first time, offline MaxEnt Q-learning algorithms, that do not explicitly require access to a policy or its entropy. Our method obtains consistently strong performance in the D4RL benchmark, outperforming prior works by \\emph{10+ points} on the challenging Franka Kitchen tasks while offering moderate improvements over SAC and TD3 on online DM Control tasks. Visualizations and code can be found on our website.",
    "authors": [
      "Divyansh Garg",
      "Joey Hejna",
      "Matthieu Geist",
      "Stefano Ermon"
    ],
    "keywords": [
      "reinforcement learning",
      "offline reinforcement learning",
      "statistical learning",
      "extreme value analysis",
      "maximum entropy rl",
      "gumbel"
    ],
    "real_all_scores": [
      5,
      8,
      5,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "DITTO: Offline Imitation Learning with World Models": {
    "paper_pk": null,
    "title": "DITTO: Offline Imitation Learning with World Models",
    "abstract": "We propose DITTO, a fully offline approach to imitation learning which addresses the problem of covariate shift without access to an oracle or any additional online interactions. By unrolling agent policies in the latent space of a learned world model and penalizing drift from expert demonstrations, we can use online reinforcement learning algorithms to learn policies which solve the imitation objective, without access to the underlying environment or reward function. Decoupling policy and world model learning lets us leverage datasets of any quality to learn latent representations which provide a natural reward signal for imitation learning, avoiding the need for complex adversarial or sparse imitation-inducing rewards. Compared to competitive baselines, our method achieves state-of-the-art performance in a variety of challenging environments from pixel observations alone.",
    "authors": [
      "Branton DeMoss",
      "Paul Duckworth",
      "Nick Hawes",
      "Ingmar Posner"
    ],
    "keywords": [
      "world models",
      "imitation learning",
      "reinforcement learning"
    ],
    "real_all_scores": [
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication": {
    "paper_pk": null,
    "title": "Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication",
    "abstract": "Communication is important in many multi-agent reinforcement learning (MARL) problems for agents to share information and make good decisions. However, when deploying trained communicative agents in a real-world application where noise and potential attackers exist, the safety of communication-based policies becomes a severe issue that is underexplored. Specifically, if communication messages are manipulated by malicious attackers, agents relying on untrustworthy communication may take unsafe actions that lead to catastrophic consequences. Therefore, it is crucial to ensure that agents will not be misled by corrupted communication, while still benefiting from benign communication. In this work, we consider an environment with $N$ agents, where the attacker may arbitrarily change the communication from any $C<\\frac{N-1}{2}$ agents to a victim agent. For this strong threat model, we propose a certifiable defense by constructing a message-ensemble policy that aggregates multiple randomly ablated message sets. Theoretical analysis shows that this message-ensemble policy can utilize benign communication while being certifiably robust to adversarial communication, regardless of the attacking algorithm. Experiments in multiple environments verify that our defense significantly improves the robustness of trained policies against various types of attacks.",
    "authors": [
      "Yanchao Sun",
      "Ruijie Zheng",
      "Parisa Hassanzadeh",
      "Yongyuan Liang",
      "Soheil Feizi",
      "Sumitra Ganesh",
      "Furong Huang"
    ],
    "keywords": [
      "certifiable robustness",
      "reinforcement learning",
      "multi-agent system",
      "adversarial communication",
      "adversarial attack"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      5
    ],
    "real_confidences": [
      2,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Planning Goals for Exploration": {
    "paper_pk": null,
    "title": "Planning Goals for Exploration",
    "abstract": "Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose \"Planning Exploratory Goals\" (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to \"plan goal commands\". In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://sites.google.com/view/exploratory-goals",
    "authors": [
      "Edward S. Hu",
      "Richard Chang",
      "Oleh Rybkin",
      "Dinesh Jayaraman"
    ],
    "keywords": [
      "model-based reinforcement learning",
      "exploration",
      "goal-conditioned reinforcement learning",
      "planning",
      "intrinsic motivation",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      6,
      3
    ],
    "real_confidences": [
      2,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Guiding Safe Exploration with Weakest Preconditions": {
    "paper_pk": null,
    "title": "Guiding Safe Exploration with Weakest Preconditions",
    "abstract": "In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions.",
    "authors": [
      "Greg Anderson",
      "Swarat Chaudhuri",
      "Isil Dillig"
    ],
    "keywords": [
      "reinforcement learning",
      "safe learning",
      "safe exploration"
    ],
    "real_all_scores": [
      5,
      3,
      3
    ],
    "real_confidences": [
      3,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Co-Evolution As More Than a Scalable Alternative for Multi-Agent Reinforcement Learning": {
    "paper_pk": null,
    "title": "Co-Evolution As More Than a Scalable Alternative for Multi-Agent Reinforcement Learning",
    "abstract": "In recent years, gradient based multi-agent reinforcement learning is growing in success. One contributing factor is the use of shared parameters for learning policy networks. While this approach scales well with the number of agents during execution it lacks this ambiguity for training as the number of produced samples grows linearly with the number of agents. For a very large number of agents, this could lead to an inefficient use of the circumstantial amount of produced samples. Moreover in single-agent reinforcement learning policy search with evolutionary algorithms showed viable success when sampling can be parallelized on a larger scale. The here proposed method does not only consider sampling in concurrent environments but further investigates sampling diverse parameters from the population in co-evolution in joint environments during training. This co-evolutionary policy search has shown to be capable of training a large number of agents. Beyond that, it has been shown to produce competitive results in smaller environments in comparison to gradient descent based methods. This surprising result make evolutionary algorithms a promising candidate for further research in the context of multi-agent reinforcement learning.",
    "authors": [
      "Patrick Grzywok"
    ],
    "keywords": [
      "reinforcement learning",
      "multi-agent reinforcement learning",
      "policy search",
      "co-evolution",
      "evolutionary algorithm"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Minimum Description Length Control": {
    "paper_pk": null,
    "title": "Minimum Description Length Control",
    "abstract": "We propose a novel framework for multitask reinforcement learning based on the minimum description length (MDL) principle. In this approach, which we term MDL-control (MDL-C), the agent learns the common structure among the tasks with which it is faced and then distills it into a simpler representation which facilitates faster convergence and generalization to new tasks. In doing so, MDL-C naturally balances adaptation to each task with epistemic uncertainty about the task distribution. We motivate MDL-C via formal connections between the MDL principle and Bayesian inference, derive theoretical performance guarantees, and demonstrate MDL-C's empirical effectiveness on both discrete and high-dimensional continuous control tasks.",
    "authors": [
      "Ted Moskovitz",
      "Ta-Chu Kao",
      "Maneesh Sahani",
      "Matthew Botvinick"
    ],
    "keywords": [
      "multitask reinforcement learning",
      "RL",
      "reinforcement learning",
      "MDL"
    ],
    "real_all_scores": [
      6,
      8,
      10,
      8,
      8
    ],
    "real_confidences": [
      4,
      5,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "A Reinforcement Learning Approach to Estimating Long-term Treatment Effects": {
    "paper_pk": null,
    "title": "A Reinforcement Learning Approach to Estimating Long-term Treatment Effects",
    "abstract": "Randomized experiments (a.k.a. A/B tests) are a powerful tool for estimating treatment effects, to inform decisions making in business, healthcare and other applications. In many problems, the treatment has a lasting effect that evolves over time. A limitation with randomized experiments is that they do not easily extend to measure long-term effects, since running long experiments is time-consuming and expensive. In this paper, we take a reinforcement learning (RL) approach that estimates the average reward in a Markov process. Motivated by real-world scenarios where the observed state transition is nonstationary, we develop a new algorithm for a class of nonstationary problems, and demonstrate promising results in two synthetic datasets and one online store dataset.",
    "authors": [
      "Ziyang Tang",
      "Yiheng Duan",
      "Stephanie Zhang",
      "Lihong Li"
    ],
    "keywords": [
      "reinforcement learning",
      "off-policy evaluation",
      "A/B testing"
    ],
    "real_all_scores": [
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier": {
    "paper_pk": null,
    "title": "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier",
    "abstract": "Increasing the replay ratio, the number of updates of an agent's parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.",
    "authors": [
      "Pierluca D'Oro",
      "Max Schwarzer",
      "Evgenii Nikishin",
      "Pierre-Luc Bacon",
      "Marc G Bellemare",
      "Aaron Courville"
    ],
    "keywords": [
      "reinforcement learning",
      "sample efficiency",
      "resets"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      6
    ],
    "real_confidences": [
      5,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Powderworld: A Platform for Understanding Generalization via Rich Task Distributions": {
    "paper_pk": null,
    "title": "Powderworld: A Platform for Understanding Generalization via Rich Task Distributions",
    "abstract": "One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment' for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating task distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment's complexity improves generalization for world models, yet causes reinforcement learning agents to struggle. Powderworld aims to support the study of generalization by providing a source of diverse tasks arising from the same core rules.",
    "authors": [
      "Kevin Frans",
      "Phillip Isola"
    ],
    "keywords": [
      "reinforcement learning",
      "environment",
      "generalization",
      "out-of-distribution",
      "multi-task"
    ],
    "real_all_scores": [
      3,
      3,
      3
    ],
    "real_confidences": [
      3,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Integrating Episodic and Global Novelty Bonuses for Efficient Exploration": {
    "paper_pk": null,
    "title": "Integrating Episodic and Global Novelty Bonuses for Efficient Exploration",
    "abstract": "Exploration in environments which differ across episodes has received increasing attention in recent years. Current methods use some combination of global novelty bonuses, computed using the agent's entire training experience, and episodic novelty bonuses, computed using only experience from the current episode. However, the use of these two types of bonuses has been ad-hoc and poorly understood. In this work, we first shed light on the behavior these two kinds of bonuses on hard exploration tasks through easily interpretable examples. We find that the two types of bonuses succeed in different settings, with episodic bonuses being most effective when there is little shared structure between environments and global bonuses being effective when more structure is shared. We also find that combining the two bonuses leads to more robust behavior across both of these settings. Motivated by these findings, we then investigate different algorithmic choices for defining and combining function approximation-based global and episodic bonuses. This results in a new algorithm which sets a new state of the art across 18 tasks from the MiniHack suite used in prior work. Our code is public at \\url{web-link}. ",
    "authors": [
      "Mikael Henaff",
      "Minqi Jiang",
      "Roberta Raileanu"
    ],
    "keywords": [
      "reinforcement learning",
      "exploration",
      "generalization"
    ],
    "real_all_scores": [
      8,
      5,
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games": {
    "paper_pk": null,
    "title": "A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games",
    "abstract": "This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.",
    "authors": [
      "Samuel Sokota",
      "Ryan D'Orazio",
      "J Zico Kolter",
      "Nicolas Loizou",
      "Marc Lanctot",
      "Ioannis Mitliagkas",
      "Noam Brown",
      "Christian Kroer"
    ],
    "keywords": [
      "reinforcement learning",
      "quantal response equilibria",
      "two-player zero-sum games",
      "mirror descent",
      "variational inequalities",
      "Nash equilibria",
      "algorithmic game theory",
      "proximal gradient"
    ],
    "real_all_scores": [
      3,
      3,
      5
    ],
    "real_confidences": [
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Offline Policy Comparison with Confidence: Benchmarks and Baselines": {
    "paper_pk": null,
    "title": "Offline Policy Comparison with Confidence: Benchmarks and Baselines",
    "abstract": "Decision makers often wish to use offline historical data to compare sequential-action policies at various world states. Importantly, computational tools should produce confidence values for such offline policy comparison (OPC) to account for statistical variance and limited data coverage. Nevertheless, there is little work that directly evaluates the quality of confidence values for OPC. In this work, we address this issue by creating benchmarks for OPC with Confidence (OPCC), derived by adding sets of policy comparison queries to datasets from offline reinforcement learning. In addition, we present an empirical evaluation of the \"risk versus coverage\" trade-off for a class of model-based baselines. In particular, the baselines learn ensembles of dynamics models, which are used in various ways to produce simulations for answering queries with confidence values. While our results suggest advantages for certain baseline variations, there appears to be significant room for improvement in future work.",
    "authors": [
      "Anurag Koul",
      "Mariano Phielipp",
      "Alan Fern"
    ],
    "keywords": [
      "offline reinforcement learning",
      "reinforcement learning",
      "benchmark",
      "uncertainty",
      "model based reinforcement learning"
    ],
    "real_all_scores": [
      3,
      3,
      3
    ],
    "real_confidences": [
      3,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning": {
    "paper_pk": null,
    "title": "Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning",
    "abstract": "In off-policy deep reinforcement learning with continuous action spaces, exploration is often implemented by injecting action noise into the action selection process. Popular algorithms based on stochastic policies, such as SAC or MPO, inject white noise by sampling actions from uncorrelated Gaussian distributions. In many tasks, however, white noise does not provide sufficient exploration, and temporally correlated noise is used instead. A common choice is Ornstein-Uhlenbeck (OU) noise, which is closely related to Brownian motion (red noise). Both red noise and white noise belong to the broad family of colored noise. In this work, we perform a comprehensive experimental evaluation on MPO and SAC to explore the effectiveness of other colors of noise as action noise. We find that pink noise, which is halfway between white and red noise, significantly outperforms white noise, OU noise, and other alternatives on a wide range of environments. Thus, we recommend it as the default choice for action noise in continuous control.\n",
    "authors": [
      "Onno Eberhard",
      "Jakob Hollenstein",
      "Cristina Pinneri",
      "Georg Martius"
    ],
    "keywords": [
      "reinforcement learning",
      "exploration",
      "action noise",
      "continuous control"
    ],
    "real_all_scores": [
      6,
      5,
      5
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Contrastive Unsupervised Learning of World Model with Invariant Causal Features": {
    "paper_pk": null,
    "title": "Contrastive Unsupervised Learning of World Model with Invariant Causal Features",
    "abstract": "In this paper we present a world model, which learns the causal features using invariance principle. We use contrastive unsupervised learning to learn the invariant causal features, which enforces invariance across augmentations of irrelevant parts or styles of the observation. Since the world model based reinforcement learning methods optimize representation learning and policy of the agent independently, contrastive loss collapses due to lack of supervisory signal to the representation learning module. We propose depth reconstruction as an auxiliary task to explicitly enforce the invariance and data augmentation as style intervention on the RGB space to mitigate this issue. Our design help us to leverage state-of-the-art unsupervised representation learning method to learn the world model with invariant causal features, which outperforms current state-of-the-art model-based as well as model-free reinforcement learning methods on out-of-distribution point navigation tasks on Gibson and iGibson dataset at 100k and 500k interaction step benchmarks. Further experiments on DeepMind control suite even without depth reconstruction, our proposed model performs on par with the state-of-the-art counterpart models.",
    "authors": [
      "Rudra P. K. Poudel",
      "Harit Pandya",
      "Roberto Cipolla"
    ],
    "keywords": [
      "world models",
      "causality",
      "contrastive learning",
      "model-based reinforcement learning",
      "reinforcement learning",
      "out-of-distribution generalisation",
      "sim-to-real transfer",
      "robot navigation"
    ],
    "real_all_scores": [
      5,
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      2,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Neural Agents Struggle to Take Turns in Bidirectional Emergent Communication": {
    "paper_pk": null,
    "title": "Neural Agents Struggle to Take Turns in Bidirectional Emergent Communication",
    "abstract": "The spontaneous exchange of turns is a central aspect of human communication. Although turn-taking conventions come to us naturally, artificial dialogue agents struggle to coordinate, and must rely on hard-coded rules to engage in interactive conversations with human interlocutors. In this paper, we investigate the conditions under which artificial agents may naturally develop turn-taking conventions in a simple language game. We describe a cooperative task where success is contingent on the exchange of information along a shared communication channel where talking over each other hinders communication. Despite these environmental constraints, neural-network based agents trained to solve this task with reinforcement learning do not systematically adopt turn-taking conventions. However, we find that agents that do agree on turn-taking protocols end up performing better. \nMoreover, agents that are forced to perform turn-taking can learn to solve the task more quickly. \nThis suggests that turn-taking may help to generate conversations that are easier for speakers to interpret.",
    "authors": [
      "Valentin Taillandier",
      "Dieuwke Hupkes",
      "Beno\u00eet Sagot",
      "Emmanuel Dupoux",
      "Paul Michel"
    ],
    "keywords": [
      "language emergence",
      "turn-taking",
      "conversation",
      "communication",
      "neural agents",
      "cooperative game",
      "reinforcement learning"
    ],
    "real_all_scores": [
      3,
      5,
      3
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Jump-Start Reinforcement Learning": {
    "paper_pk": null,
    "title": "Jump-Start Reinforcement Learning",
    "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent\u2019s behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks that present exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tasks. We show via experiments that it is able to significantly outperform existing imitation and reinforcement learning algorithms, particularly in the small-data regime. In addition, we provide an upper bound on the sample complexity of JSRL and show that with the help of a guide-policy, one can improve the sample complexity for non-optimism exploration methods from exponential in horizon to polynomial.",
    "authors": [
      "Ikechukwu Uchendu",
      "Ted Xiao",
      "Yao Lu",
      "Banghua Zhu",
      "Mengyuan Yan",
      "Jos\u00e9phine Simon",
      "Matthew Bennice",
      "Chuyuan Fu",
      "Cong Ma",
      "Jiantao Jiao",
      "Sergey Levine",
      "Karol Hausman"
    ],
    "keywords": [
      "reinforcement learning",
      "offline reinforcement learning",
      "fine-tuning"
    ],
    "real_all_scores": [
      5,
      5,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Population-Based Reinforcement Learning for Combinatorial Optimization Problems": {
    "paper_pk": null,
    "title": "Population-Based Reinforcement Learning for Combinatorial Optimization Problems",
    "abstract": "Applying reinforcement learning to combinatorial optimization problems is attractive as it obviates the need for expert knowledge or pre-solved instances. However, it is unrealistic to expect an agent to solve these (often NP-)hard problems in a single shot at inference due to their inherent complexity, thus leading approaches are often augmented with additional search strategies, from stochastic sampling and beam-search to explicit fine-tuning.\nIn this paper, we argue for the benefits of learning a population of complementary agents, which can be simultaneously rolled out at inference. To this end, we introduce Poppy, a simple theoretically grounded training procedure for populations. Instead of relying on a predefined or hand-crafted notion of diversity, Poppy induces an unsupervised specialization targeted solely at maximizing the performance of the whole population. We show that Poppy leads to a set of complementary heuristics, and obtain state-of-the-art results on three popular NP-hard problems: the traveling salesman (TSP), the capacitated vehicle routing (CVRP), and 0-1 knapsack (KP). On TSP specifically, Poppy divides by 5 the optimality gap while reducing the inference time by more than 10 compared to previous state-of-the-art reinforcement learning approaches.",
    "authors": [
      "Nathan Grinsztajn",
      "Daniel Furelos-Blanco",
      "Thomas D Barrett"
    ],
    "keywords": [
      "reinforcement learning",
      "combinatorial optimization",
      "population"
    ],
    "real_all_scores": [
      3,
      6,
      3,
      3
    ],
    "real_confidences": [
      5,
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Contrastive Value Learning: Implicit Models for Simple Offline RL": {
    "paper_pk": null,
    "title": "Contrastive Value Learning: Implicit Models for Simple Offline RL",
    "abstract": "Model-based reinforcement learning (RL) methods are appealing in the offline setting because they allow an agent to reason about the consequences of actions without interacting with the environment. Prior methods learn a 1-step dynamics model, which predicts the next state given the current state and action. These models do not immediately tell the agent which actions to take, but must be integrated into a larger RL framework. Can we model the environment dynamics in a different way, such that the learned model does directly indicate the value of each action? In this paper, we propose Contrastive Value Learning (CVL), which learns an implicit, multi-step model of the environment dynamics. This model can be learned without access to reward functions, but nonetheless can be used to directly estimate the value of each action, without requiring any TD learning. Because this model represents the multi-step transitions implicitly, it avoids having to predict high-dimensional observations and thus scales to high-dimensional tasks. Our experiments demonstrate that CVL outperforms prior offline RL methods on complex continuous control benchmarks.",
    "authors": [
      "Bogdan Mazoure",
      "Benjamin Eysenbach",
      "Ofir Nachum",
      "Jonathan Tompson"
    ],
    "keywords": [
      "reinforcement learning",
      "contrastive learning",
      "implicit density models",
      "reward-free learning",
      "offline reinforcement learning",
      "metaworld"
    ],
    "real_all_scores": [
      3,
      5,
      1
    ],
    "real_confidences": [
      3,
      2,
      1
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Multi-Agent Multi-Game Entity Transformer": {
    "paper_pk": null,
    "title": "Multi-Agent Multi-Game Entity Transformer",
    "abstract": "Building large-scale generalist pre-trained models for many tasks is becoming an emerging and potential direction in reinforcement learning (RL). Research such as Gato and Multi-Game Decision Transformer have displayed outstanding performance and generalization capabilities on many games and domains. However, there exists a research blank about developing highly capable and generalist models in multi-agent RL (MARL), which can substantially accelerate progress towards general AI. To fill this gap, we propose Multi-Agent multi-Game ENtity TrAnsformer (MAGENTA) from the entity perspective as an orthogonal research to previous time-sequential modeling. Specifically, to deal with different state/observation spaces in different games, we analogize games as languages, thus training different \"tokenizers\" for various games. The feature inputs are split according to different entities and tokenized in the same continuous space. Then, two types of transformer-based model are proposed as permutation-invariant architectures to deal with various numbers of entities and capture the attention over different entities. MAGENTA is trained on Honor of Kings, Starcraft II micromanagement, and Neural MMO with a single set of transformer weights. Extensive experiments show that MAGENTA can play games across various categories with arbitrary numbers of agents and increase the efficiency of fine-tuning in new games and scenarios by 50\\%-100\\%. See our project page at \\url{https://sites.google.com/view/rl-magenta}.",
    "authors": [
      "Rundong Wang",
      "Weixuan Wang",
      "Xianhan Zeng",
      "Liang Wang",
      "Zhenjie Lian",
      "Yiming Gao",
      "Feiyu Liu",
      "Siqin Li",
      "Xianliang Wang",
      "QIANG FU",
      "Yang Wei",
      "Lanxiao Huang",
      "Longtao Zheng",
      "Zinovi Rabinovich",
      "Bo An"
    ],
    "keywords": [
      "reinforcement learning",
      "multi-agent reinforcement learing",
      "transformer",
      "pretrained model"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Addressing High-dimensional Continuous Action Space via Decomposed Discrete Policy-Critic": {
    "paper_pk": null,
    "title": "Addressing High-dimensional Continuous Action Space via Decomposed Discrete Policy-Critic",
    "abstract": "Reinforcement learning (RL) methods for discrete action spaces like DQNs are being widely used in tasks such as Atari games. However, they encounter difficulties when addressing continuous control tasks, since discretizing continuous action space incurs the curse-of-dimensionality. To tackle continuous control tasks via discretized actions, we propose a decomposed discrete policy-critic (D2PC) architecture, which was inspired by multi-agent RL (MARL) and associates with each action dimension a discrete policy, while leveraging a single critic network to provide a shared evaluation. Building on D2PC, we advocate soft stochastic D2PC (SD2PC)  and deterministic D2PC (D3PC) methods with a discrete stochastic or deterministic policy, which show comparable or  superior training performances relative to even continuous actor-critic methods. Additionally, we design a mechanism that allows D3PC to interact with continuous actor-critic methods, contributing to the Q-policy-critic (QPC) algorithm, which inherits the training efficiency of discrete RL and the near-optimal final performance of continuous RL algorithms. Substantial experimental results on several continuous benchmark tasks validate our claims.",
    "authors": [
      "Yechen Zhang",
      "Jian Sun",
      "Gang Wang",
      "Jie Chen"
    ],
    "keywords": [
      "reinforcement learning",
      "continuous control",
      "actor-critic",
      "decomposed policy",
      "discretized action"
    ],
    "real_all_scores": [
      3,
      6,
      6,
      8
    ],
    "real_confidences": [
      5,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems": {
    "paper_pk": null,
    "title": "DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems",
    "abstract": "Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. \nReinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance.  \nWe conjecture that ineffective exploration in large overactuated action spaces is a key problem.\nThis is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. \nWe identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. \nBy integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.",
    "authors": [
      "Pierre Schumacher",
      "Daniel Haeufle",
      "Dieter B\u00fcchler",
      "Syn Schmitt",
      "Georg Martius"
    ],
    "keywords": [
      "reinforcement learning",
      "musculoskeletal",
      "correlated exploration"
    ],
    "real_all_scores": [
      3,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning Combinatorial Node Labeling Algorithms": {
    "paper_pk": null,
    "title": "Learning Combinatorial Node Labeling Algorithms",
    "abstract": "We present the combinatorial node labeling framework, which generalizes many prior approaches to solving hard graph optimization problems by supporting problems where solutions consist of arbitrarily many node labels, such as graph coloring. We then introduce a neural network architecture to implement this framework. Our architecture builds on a graph attention network with several inductive biases to improve solution quality and is trained using policy gradient reinforcement learning. We demonstrate our approach on both graph coloring and minimum vertex cover. Our learned heuristics match or outperform classical hand-crafted greedy heuristics and machine learning approaches while taking only seconds on large graphs. We conduct a detailed analysis of the learned heuristics and architecture choices and show that they successfully adapt to different graph structures.",
    "authors": [
      "Lukas Gianinazzi",
      "Maximilian Fries",
      "Nikoli Dryden",
      "Tal Ben-Nun",
      "Maciej Besta",
      "Torsten Hoefler"
    ],
    "keywords": [
      "graph learning",
      "reinforcement learning",
      "combinatorial optimization"
    ],
    "real_all_scores": [
      3,
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      2,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Personalized Federated Hypernetworks for Privacy Preservation in Multi-Task Reinforcement Learning": {
    "paper_pk": null,
    "title": "Personalized Federated Hypernetworks for Privacy Preservation in Multi-Task Reinforcement Learning",
    "abstract": "Multi-Agent Reinforcement Learning currently focuses on implementations where all data and training can be centralized to one machine. But what if local agents are split across multiple tasks, and need to keep data private between each? We develop the first application of Personalized Federated Hypernetworks (PFH) to Reinforcement Learning (RL). We then present a novel application of PFH to few-shot transfer, and demonstrate significant initial increases in learning. PFH has never been demonstrated beyond supervised learning benchmarks, so we apply PFH to an important domain: RL price-setting for energy demand response. We consider a general case across where agents are split across multiple microgrids, wherein energy consumption data must be kept private within each microgrid. Together, our work explores how the fields of personalized federated learning and RL can come together to make learning efficient across multiple tasks while keeping data secure.",
    "authors": [
      "Doseok Jang",
      "Larry Yan",
      "Lucas Spangher",
      "Selvaprabu Nadarajah",
      "Costas Spanos"
    ],
    "keywords": [
      "microgrid clusters",
      "energy demand response",
      "transactive energy control",
      "neural networks",
      "multi-agent reinforcement learning",
      "reinforcement learning",
      "multi-task learning",
      "transfer learning",
      "hypernetworks",
      "federated learning",
      "personalized federated learning",
      "microgrids"
    ],
    "real_all_scores": [
      5,
      5,
      3
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Intrinsic Motivation via Surprise Memory": {
    "paper_pk": null,
    "title": "Intrinsic Motivation via Surprise Memory",
    "abstract": "We present a new computing model for intrinsic rewards in reinforcement learning that addresses the limitations of existing surprise-driven explorations. The reward is the novelty of the surprise rather than the surprise norm. We estimate the surprise novelty as retrieval errors of a memory network wherein the memory stores and reconstructs surprises. Our surprise memory (SM) augments the capability of surprise-based intrinsic motivators, maintaining the agent's interest in exciting exploration while reducing unwanted attraction to unpredictable or noisy observations. Our experiments demonstrate that the SM combined with various surprise predictors exhibits efficient exploring behaviors and significantly boosts the final performance in sparse reward environments, including Noisy-TV, navigation and challenging Atari games. ",
    "authors": [
      "Hung Le",
      "Kien Do",
      "Dung Nguyen",
      "Svetha Venkatesh"
    ],
    "keywords": [
      "reinforcement learning",
      "intrinsic motivation",
      "exploration",
      "memory"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting": {
    "paper_pk": null,
    "title": "Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting",
    "abstract": "Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any offline RL algorithm. We further analyze that the opportunity for performance improvement over the behavior policy correlates with the positive-sided variance of the returns of the trajectories in the dataset. We empirically show that while CQL, IQL, and TD3+BC achieve only a part of this potential policy improvement, these same algorithms combined with our reweighted sampling strategy fully exploit the dataset. Furthermore, we empirically demonstrate that, despite its theoretical limitation, the approach may still be efficient in stochastic environments. ",
    "authors": [
      "Zhang-Wei Hong",
      "Pulkit Agrawal",
      "Remi Tachet des Combes",
      "Romain Laroche"
    ],
    "keywords": [
      "offline reinforcement learning",
      "reinforcement learning",
      "sampling",
      "experience replay"
    ],
    "real_all_scores": [
      6,
      1,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Near-optimal Policy Identification in Active Reinforcement Learning": {
    "paper_pk": null,
    "title": "Near-optimal Policy Identification in Active Reinforcement Learning",
    "abstract": "Many real-world reinforcement learning tasks require control of complex dynamical systems that involve both costly data acquisition processes and large state spaces. In cases where the expensive transition dynamics can be readily evaluated at specified states (e.g., via a simulator), agents can operate in what is often referred to as planning with a \\emph{generative model}. We propose the AE-LSVI algorithm for best policy identification, a novel variant of the kernelized least-squares value iteration (LSVI) algorithm that combines optimism with pessimism for active exploration (AE). AE-LSVI provably identifies a near-optimal policy \\emph{uniformly} over an entire state space and achieves polynomial sample complexity guarantees that are independent of the number of states. When specialized to the recently introduced offline contextual Bayesian optimization setting, our algorithm achieves improved sample complexity bounds. Experimentally, we demonstrate that AE-LSVI outperforms other RL algorithms in a variety of environments when robustness to the initial state is required. ",
    "authors": [
      "Xiang Li",
      "Viraj Mehta",
      "Johannes Kirschner",
      "Ian Char",
      "Willie Neiswanger",
      "Jeff Schneider",
      "Andreas Krause",
      "Ilija Bogunovic"
    ],
    "keywords": [
      "reinforcement learning",
      "contextual bayesian optimization",
      "kernelized least-squares value iteration"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees": {
    "paper_pk": null,
    "title": "Learning for Edge-Weighted Online Bipartite Matching with Robustness Guarantees",
    "abstract": "Many real-world problems, such as online ad display, can be formulated as online bipartite matching. The crucial challenge lies in the nature of sequentially-revealed online item information, based on which we make irreversible matching decisions at each step. While numerous expert online algorithms have been proposed with bounded worst-case competitive ratios, they may not offer satisfactory performance in average cases. On the other hand, reinforcement learning (RL) has been applied to improve the average performance, but they lack robustness and can perform arbitrarily badly. In this paper, we propose a novel RL-based approach to edge-weighted online bipartite matching with robustness guarantees (LOMAR), achieving both good average-case and good worst-case performance. The key novelty of LOMAR is a new online switching operation which, based on a judiciously-designed condition to hedge against future uncertainties, decides whether to follow the expert's decision or the RL decision for each online item arrival. We prove that for any $\\rho \\in [0,1]$, LOMAR is $\\rho$-competitive against any given expert online algorithm. To improve the average performance, we  train the RL policy by explicitly considering the online switching operation. Finally, we run empirical experiments to demonstrate the advantages of LOMAR compared to existing baselines.",
    "authors": [
      "Pengfei Li",
      "Jianyi Yang",
      "Shaolei Ren"
    ],
    "keywords": [
      "Robustness",
      "online bipartite matching",
      "reinforcement learning"
    ],
    "real_all_scores": [
      8,
      6,
      5,
      8
    ],
    "real_confidences": [
      4,
      3,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Solving and Learning non-Markovian Stochastic Control problems in continuous-time with Neural RDEs": {
    "paper_pk": null,
    "title": "Solving and Learning non-Markovian Stochastic Control problems in continuous-time with Neural RDEs",
    "abstract": "We propose a novel framework for solving continuous-time, non-Markovian stochastic control problems with the use of neural rough differential equations (Neural RDEs). By parameterising the control process as the solution of a Neural RDE driven by the state process, we show that the control-state joint dynamics are governed by an uncontrolled RDE with structured vector fields, allowing for efficient trajectories simulation, Monte-Carlo estimation of the value function and backpropagation. To deal with input paths of infinite 1-variation, we refine the existing universal approximation result to a probabilistic density result for Neural RDEs driven by random rough paths. Experiments on various non-Markovian problems indicate how the proposed framework is time-resolution-invariant and capable of learning optimal solutions with higher accuracy than traditional RNN-based approaches. Finally, we discuss possible extensions of this framework to the setting of non-Markovian continuous-time reinforcement learning and provide promising empirical evidence in this direction.",
    "authors": [
      "Melker H\u00f6glund",
      "Emilio Ferrucci",
      "Camilo Hern\u00e1ndez",
      "Aitor Muguruza Gonzalez",
      "Cristopher Salvi",
      "Leandro S\u00e1nchez-Betancourt",
      "Yufei Zhang"
    ],
    "keywords": [
      "stochastic control",
      "neural RDEs",
      "rough paths",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      4,
      2,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization": {
    "paper_pk": null,
    "title": "Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization",
    "abstract": "Hindsight goal relabeling has become a foundational technique for multi-goal reinforcement learning (RL). The idea is quite simple: any arbitrary trajectory can be seen as an expert demonstration for reaching the trajectory's end state. Intuitively, this procedure trains a goal-conditioned policy to imitate a sub-optimal expert. However, this connection between imitation and hindsight relabeling is not well understood. Modern imitation learning algorithms are described in the language of divergence minimization, and yet it remains an open problem how to recast hindsight goal relabeling into that framework. In this work, we develop a unified objective for goal-reaching that explains such a connection, from which we can derive goal-conditioned supervised learning (GCSL) and the reward function in hindsight experience replay (HER) from first principles. Experimentally, we find that despite recent advances in goal-conditioned behaviour cloning (BC), multi-goal Q-learning can still outperform BC-like methods; moreover, a vanilla combination of both actually hurts model performance. Under our framework, we study when BC is expected to help, and empirically validate our findings. Our work further bridges goal-reaching and generative modeling, illustrating the nuances and new pathways of extending the success of generative models to RL.",
    "authors": [
      "Lunjun Zhang",
      "Bradly C. Stadie"
    ],
    "keywords": [
      "reinforcement learning",
      "multi-goal reinforcement learning",
      "imitation learning"
    ],
    "real_all_scores": [
      5,
      6,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Transformers are Sample-Efficient World Models": {
    "paper_pk": null,
    "title": "Transformers are Sample-Efficient World Models",
    "abstract": "Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.",
    "authors": [
      "Vincent Micheli",
      "Eloi Alonso",
      "Fran\u00e7ois Fleuret"
    ],
    "keywords": [
      "deep learning",
      "reinforcement learning",
      "model-based reinforcement learning",
      "world models",
      "learning in imagination",
      "transformers",
      "discrete autoencoders",
      "generative modeling",
      "sequence modeling"
    ],
    "real_all_scores": [
      6,
      8,
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      4,
      3,
      4,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Auto-Encoding Adversarial Imitation Learning": {
    "paper_pk": null,
    "title": "Auto-Encoding Adversarial Imitation Learning",
    "abstract": "Reinforcement learning (RL) provides a powerful framework for decision-making, but its application in practice often requires a carefully designed reward function. Adversarial Imitation Learning (AIL) sheds light on automatic policy acquisition without access to the reward signal from the environment. In this work, we propose Auto-Encoding Adversarial Imitation Learning (AEAIL), a robust and scalable AIL framework. To induce expert policies from demonstrations, AEAIL utilizes the reconstruction error of an auto-encoder as a reward signal, which provides more information for optimizing policies than the prior discriminator-based ones. Subsequently, we use the derived objective functions to train the auto-encoder and the agent policy. Experiments show that our AEAIL performs superior compared to state-of-the-art methods in the MuJoCo environments. More importantly, AEAIL shows much better robustness when the expert demonstrations are noisy. Specifically, our method achieves $11\\%$ and $50.7\\%$ relative improvement overall compared to the best baseline GAIL and PWIL on clean and noisy expert data, respectively. Video results, open-source code and dataset are available in supplementary materials. ",
    "authors": [
      "Kaifeng Zhang",
      "Rui Zhao",
      "Ziming Zhang",
      "Yang Gao"
    ],
    "keywords": [
      "imitation learning",
      "reinforcement learning",
      "auto-encoders"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      8
    ],
    "real_confidences": [
      3,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Universal embodied intelligence: learning from crowd, recognizing the world, and reinforced with experience": {
    "paper_pk": null,
    "title": "Universal embodied intelligence: learning from crowd, recognizing the world, and reinforced with experience",
    "abstract": "The interactive artificial intelligence in the motion control field is an interesting topic, especially when universal knowledge adaptive to multiple task and universal environments is wanted. Although there are increasing efforts on Reinforcement learning (RL) studies with the assistance of transformers, it might subject to the limitation of the offline training pipeline, in which the exploration and generalization ability is prohibited. Motivated by the cognitive and behavioral psychology, such agent should have the ability to learn from others, recognize the world, and practice itself based its own experience. In this study, we propose the framework of Online Decision MetaMorphFormer (ODM) which attempts to achieve the above learning modes, with a unified model architecture to both highlight its own body perception and produce action and observation predictions. ODM can be applied on any arbitrary agent with a multi-joint body, located in different environments, trained with different type of tasks. Large-scale pretrained dataset are used to warmup ODM while the targeted environment continues to reinforce the universal policy. Substantial interactive experiments as well as few-shot and zero-shot tests in unseen environments and never-experienced tasks verify ODM's performance, and generalization ability. Our study shed some lights on research of general artificial intelligence on the embodied and cognitive field studies. ",
    "authors": [
      "Luo Ji",
      "Longfei Ma",
      "Chang Zhou",
      "Fei Wu",
      "Hongxia Yang"
    ],
    "keywords": [
      "reinforcement learning",
      "transformer",
      "morphology",
      "pretrain",
      "finetune",
      "generalization"
    ],
    "real_all_scores": [
      3,
      6,
      5,
      6
    ],
    "real_confidences": [
      4,
      5,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Backstepping Temporal Difference Learning": {
    "paper_pk": null,
    "title": "Backstepping Temporal Difference Learning",
    "abstract": " Off-policy learning ability is an important feature of reinforcement learning (RL) for practical applications. However, even one of the most elementary RL algorithms, temporal-difference (TD) learning, is known to suffer form divergence issue when the off-policy scheme is used together with linear function approximation. To overcome the divergent behavior, several off-policy TD learning algorithms have been developed until now. In this work, we provide a unified view of such algorithms from a purely control-theoretic perspective. Our method relies on the backstepping technique, which is widely used in nonlinear control theory.",
    "authors": [
      "Han-Dong Lim",
      "Donghwan Lee"
    ],
    "keywords": [
      "reinforcement learning",
      "temporal difference learning",
      "policy evaluation"
    ],
    "real_all_scores": [
      5,
      3,
      8,
      5
    ],
    "real_confidences": [
      3,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Light-weight probing of unsupervised representations for Reinforcement Learning": {
    "paper_pk": null,
    "title": "Light-weight probing of unsupervised representations for Reinforcement Learning",
    "abstract": "Unsupervised visual representation learning offers the opportunity to leverage large corpora of unlabeled trajectories to form useful visual representations, which can benefit the training of reinforcement learning (RL) algorithms. However, evaluating the fitness of such representations requires training RL algorithms which is both computationally intensive and has high variance outcomes. To alleviate this issue, we design an evaluation protocol for unsupervised RL representations with lower variance and up to 600x lower computational cost. Inspired by the vision community, we propose two linear probing tasks: predicting the reward observed in a given state, and predicting the action of an expert in a given state. These two tasks are generally applicable to many RL domains, and we show through rigorous experimentation that they correlate strongly with the actual downstream control performance on the Atari100k Benchmark. This provides a better method for exploring the space of pretraining algorithms without the need of running RL evaluations for every setting. Leveraging this framework, we further improve existing self-supervised learning (SSL) recipes for RL, highlighting the importance of the forward model, the size of the visual backbone, and the precise formulation of the unsupervised objective.",
    "authors": [
      "Wancong Zhang",
      "Anthony GX-Chen",
      "Vlad Sobal",
      "Yann LeCun",
      "Nicolas Carion"
    ],
    "keywords": [
      "machine learning",
      "unsupervised learning",
      "reinforcement learning",
      "computer vision"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "POPGym: Benchmarking Partially Observable Reinforcement Learning": {
    "paper_pk": null,
    "title": "POPGym: Benchmarking Partially Observable Reinforcement Learning",
    "abstract": "Real world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines -- the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-and-play compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https://github.com/proroklab/popgym.",
    "authors": [
      "Steven Morad",
      "Ryan Kortvelesy",
      "Matteo Bettini",
      "Stephan Liwicki",
      "Amanda Prorok"
    ],
    "keywords": [
      "partially observable",
      "POMDP",
      "reinforcement learning",
      "memory"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      8
    ],
    "real_confidences": [
      4,
      2,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Highway Reinforcement Learning": {
    "paper_pk": null,
    "title": "Highway Reinforcement Learning",
    "abstract": "Traditional Dynamic Programming (DP) approaches suffer from slow backward credit-assignment (CA): only a one-step search is performed at each update. A popular solution for multi-step CA is to use multi-step Bellman operators. Unfortunately, in the control settings, existing methods typically suffer from the large variance of multi-step off-policy corrections or are biased, preventing convergence. To overcome these problems, we introduce a novel multi-step Bellman optimality equation with adaptive lookahead steps. We first derive a new multi-step Value Iteration (VI) method that converges to the optimal Value Function (VF) with an exponential contraction rate but linear computational complexity. Given some trial, our so-called Highway RL performs rapid CA, by picking a policy and a possible lookahead (up to the trial end) that maximize the near-term reward during lookahead plus a DP-based estimate of the cumulative reward for the remaining part of the trial. Highway RL does not require off-policy corrections. Under mild assumptions, it achieves better convergence rates than the traditional one-step Bellman Optimality Operator. We then derive Highway Q-Learning, a convergent multi-step off-policy variant of Q-learning. We show that our Highway algorithms significantly outperform DP approaches on toy tasks. Finally, we propose a deep function approximation variant called Highway DQN. We evaluate it on visual MinAtar Games, outperforming similar multi-step methods.",
    "authors": [
      "Yuhui Wang",
      "Haozhe Liu",
      "Miroslav Strupl",
      "Francesco Faccio",
      "Qingyuan Wu",
      "Xiaoyang Tan",
      "J\u00fcrgen Schmidhuber"
    ],
    "keywords": [
      "reinforcement learning",
      "off-policy learning",
      "credit assignment",
      "Bellman Equation"
    ],
    "real_all_scores": [
      8,
      8,
      8,
      5
    ],
    "real_confidences": [
      5,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Accelerating Inverse Reinforcement Learning with Expert Bootstrapping": {
    "paper_pk": null,
    "title": "Accelerating Inverse Reinforcement Learning with Expert Bootstrapping",
    "abstract": "Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL) search over candidate reward functions and solve a reinforcement learning problem in the inner loop. This creates a rather strange inversion where a harder problem, reinforcement learning, is in the inner loop of a presumably easier problem, imitation learning. In this work, we show that better utilization of expert demonstrations can reduce the need for hard exploration in the inner RL loop, hence accelerating learning. Specifically, we propose two simple recipes: (1) placing expert transitions into the replay buffer of the inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner about high reward states instead of forcing the learner to discover them through extensive exploration, and (2) using expert actions in Q value bootstrapping in order to improve the target Q value estimates and more accurately describe high value expert states. Our methods show significant gains over a MaxEntIRL baseline on the benchmark MuJoCo suite of tasks, speeding up recovery to 70\\% of deterministic expert performance by 2.13x on HalfCheetah-v2, 2.6x on Ant-v2, 18x on Hopper-v2, and 3.36x on Walker2d-v2.",
    "authors": [
      "David Wu",
      "Sanjiban Choudhury"
    ],
    "keywords": [
      "inverse reinforcement learning",
      "imitation learning",
      "reinforcement learning"
    ],
    "real_all_scores": [
      5,
      8,
      6,
      3,
      6
    ],
    "real_confidences": [
      4,
      3,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Visual Imitation Learning with Patch Rewards": {
    "paper_pk": null,
    "title": "Visual Imitation Learning with Patch Rewards",
    "abstract": "Visual imitation learning enables reinforcement learning agents to learn to behave from expert visual demonstrations such as videos or image sequences, without explicit, well-defined rewards. \nPrevious reseaches either adopt supervised learning techniques or induce simple and coarse scalar rewards from pixels, neglecting the dense information contained in the image demonstrations.\nIn this work, we propose to measure the expertise of various local regions of image samples, or called patches, and recover multi-dimensional patch rewards accordingly. \nPatch reward is a more precise rewarding characterization that serves as fine-grained expertise measurement and visual explainability tool.\nSpecifically, we present Adversarial Imitation Learning with Patch Rewards (PatchAIL), which employs a patch-based discriminator to measure the expertise of different local parts from given images and provide patch rewards.\nThe patch-based knowledge is also used to regularize the aggregated reward and stabilize the training.\nWe evaluate our method on the standard pixel-based benchmark DeepMind Control Suite. \nThe experiment results have demonstrated that PatchAIL outperforms baseline methods and provides valuable interpretations for visual demonstrations.  ",
    "authors": [
      "Minghuan Liu",
      "Tairan He",
      "Weinan Zhang",
      "Shuicheng YAN",
      "Zhongwen Xu"
    ],
    "keywords": [
      "imitation learning",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      5,
      5,
      6
    ],
    "real_confidences": [
      4,
      2,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Planning Immediate Landmarks of Targets for Model-Free Skill Transfer across Agents": {
    "paper_pk": null,
    "title": "Planning Immediate Landmarks of Targets for Model-Free Skill Transfer across Agents",
    "abstract": "In reinforcement learning applications, agents usually need to deal with various input/output features when specified with different state and action spaces by their developers or physical restrictions, indicating re-training from scratch and considerable sample inefficiency, especially when agents follow similar solution steps to achieve tasks.\nIn this paper, we aim to transfer pre-trained skills to alleviate the above challenge. Specifically, we propose PILoT, i.e., Planning Immediate Landmarks of Targets. PILoT utilizes the universal decoupled policy optimization to learn a goal-conditioned state planner; then, we distill a goal-planner to plan immediate landmarks in a model-free style that can be shared among different agents. In our experiments, we show the power of PILoT on various transferring challenges, including few-shot transferring across action spaces and dynamics, from low-dimensional vector states to image inputs, from simple robot to complicated morphology; and we also illustrate PILoT provides a zero-shot transfer solution from a simple 2D navigation task to the harder Ant-Maze task.",
    "authors": [
      "Minghuan Liu",
      "Zhengbang Zhu",
      "Menghui Zhu",
      "Yuzheng Zhuang",
      "Weinan Zhang",
      "Jianye HAO"
    ],
    "keywords": [
      "reinforcement learning",
      "transfer learning"
    ],
    "real_all_scores": [
      8,
      6,
      8,
      8
    ],
    "real_confidences": [
      3,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Go-Explore with a guide: Speeding up search in sparse reward settings with goal-directed intrinsic rewards": {
    "paper_pk": null,
    "title": "Go-Explore with a guide: Speeding up search in sparse reward settings with goal-directed intrinsic rewards",
    "abstract": "Reinforcement Learning (RL) agents have traditionally been very sample-intensive to train, especially in environments with sparse rewards. Seeking inspiration from neuroscience experiments of rats learning the structure of a maze without needing extrinsic rewards, we seek to incorporate additional intrinsic rewards to guide behavior. We propose a potential-based goal-directed intrinsic reward (GDIR), which provides a reward signal regardless of whether the task is achieved, and ensures that learning can always take place. While GDIR may be similar to approaches such as reward shaping in incorporating goal-based rewards, we highlight that GDIR is innate to the agent and hence applicable across a wide range of environments without needing to rely on a properly shaped environment reward. We also note that GDIR is different from curiosity-based intrinsic motivation, which can diminish over time and lead to inefficient exploration. Go-Explore is a well-known state-of-the-art algorithm for sparse reward domains, and we demonstrate that by incorporating GDIR in the ``Go\" and ``Explore\" phases, we can improve Go-Explore's performance and enable it to learn faster across multiple environments, for both discrete (2D grid maze environments, Towers of Hanoi, Game of Nim) and continuous (Cart Pole and Mountain Car) state spaces. Furthermore, to consolidate learnt trajectories better, our method also incorporates a novel approach of hippocampal replay to update the values of GDIR and reset state visit and selection counts of states along the successful trajectory. As a benchmark, we also show that our proposed approaches learn significantly faster than traditional extrinsic-reward-based RL algorithms such as Proximal Policy Optimization, TD-learning, and Q-learning.",
    "authors": [
      "Chong Min John Tan",
      "Mehul Motani"
    ],
    "keywords": [
      "reinforcement learning",
      "intrinsic motivation",
      "goal-directed rewards",
      "hippocampal replay",
      "hard-exploration",
      "sparse rewards"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      3,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Multi-skill Mobile Manipulation for Object Rearrangement": {
    "paper_pk": null,
    "title": "Multi-skill Mobile Manipulation for Object Rearrangement",
    "abstract": "We study a modular approach to tackle long-horizon mobile manipulation tasks for object rearrangement, which decomposes a full task into a sequence of subtasks. To tackle the entire task, prior work chains multiple stationary manipulation skills with a point-goal navigation skill, which are learned individually on subtasks. Although more effective than monolithic end-to-end RL policies, this framework suffers from compounding errors in skill chaining, e.g., navigating to a bad location where a stationary manipulation skill can not reach its target to manipulate. To this end, we propose that the manipulation skills should include mobility to have flexibility in interacting with the target object from multiple locations and at the same time the navigation skill could have multiple end points which lead to successful manipulation. We operationalize these ideas by implementing mobile manipulation skills rather than stationary ones and training a navigation skill trained with region goal instead of point goal. We evaluate our multi-skill mobile manipulation method M3 on 3 challenging long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), and show superior performance as compared to the baselines.",
    "authors": [
      "Jiayuan Gu",
      "Devendra Singh Chaplot",
      "Hao Su",
      "Jitendra Malik"
    ],
    "keywords": [
      "mobile manipulation",
      "reinforcement learning"
    ],
    "real_all_scores": [
      6,
      6,
      3,
      5
    ],
    "real_confidences": [
      2,
      3,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Contextual Symbolic Policy For Meta-Reinforcement Learning": {
    "paper_pk": null,
    "title": "Contextual Symbolic Policy For Meta-Reinforcement Learning",
    "abstract": "Context-based Meta-Reinforcement Learning (Meta-RL), which conditions the RL agent on the context variables, is a powerful method for learning a generalizable agent. \nCurrent context-based Meta-RL methods often construct their contextual policy with a neural network (NN) and directly take the context variables as a part of the input. However, the NN-based policy contains tremendous parameters which possibly result in overfitting, the difficulty of deployment and poor interpretability. \nTo improve the generation ability, efficiency and interpretability, we propose a novel Contextual Symbolic Policy (CSP) framework, which generates contextual policy with a symbolic form based on the context variables for unseen tasks in meta-RL. Our key insight is that the symbolic expression is capable of capturing complex relationships by composing various operators and has a compact form that helps strip out irrelevant information. Thus, the CSP learns to produce symbolic policy for meta-RL tasks and extract the essential common knowledge to achieve higher generalization ability. Besides, the symbolic policies with a compact form are efficient to be deployed and easier to understand.\nIn the implementation, we construct CSP as a gradient-based framework to learn the symbolic policy from scratch in an end-to-end and differentiable way. The symbolic policy is represented by a symbolic network composed of various symbolic operators. We also employ a path selector to decide the proper symbolic form of the policy and a parameter generator to produce the coefficients of the symbolic policy. Empirically, we evaluate the proposed CSP method on several Meta-RL tasks and demonstrate that the contextual symbolic policy achieves higher performance and efficiency and shows the potential to be interpretable.",
    "authors": [
      "Jiaming Guo",
      "Rui Zhang",
      "Shaohui Peng",
      "Qi Yi",
      "Xing Hu",
      "Ruizhi Chen",
      "Kehan Long",
      "Zidong Du",
      "Xishan Zhang",
      "Ling Li",
      "Qi Guo",
      "Yunji Chen"
    ],
    "keywords": [
      "meta learning",
      "reinforcement learning",
      "context variables",
      "symbolic policy"
    ],
    "real_all_scores": [
      6,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Adversarial Counterfactual Environment Model Learning": {
    "paper_pk": null,
    "title": "Adversarial Counterfactual Environment Model Learning",
    "abstract": "A good model for action-effect prediction, i.e., the environment model, is essential for sample-efficient policy learning, in which the agent can take numerous free trials to find good policies. Currently, the model is commonly learned by fitting historical transition data through empirical risk minimization (ERM). However, we discover that simple data fitting can lead to a model that will be totally wrong in guiding policy learning due to the selection bias in offline dataset collection. In this work, we introduce weighted empirical risk minimization (WERM) to handle this problem in model learning.  A typical WERM method utilizes inverse propensity scores to re-weight the training data to approximate the target distribution. However, during the policy training, the data distributions of the candidate policies can be various and unknown. Thus, we propose an adversarial weighted empirical risk minimization (AWRM) objective that learns the model with respect to the worst case of the target distributions. We implement AWRM in a sequential decision structure, resulting in the GALILEO model learning algorithm. We also discover that GALILEO is closely related to adversarial model learning, explaining the empirical effectiveness of the latter. We apply GALILEO in synthetic tasks and verify that GALILEO makes accurate predictions on counterfactual data. We finally applied GALILEO in real-world offline policy learning tasks and found that GALILEO significantly improves policy performance in real-world testing.",
    "authors": [
      "Xiong-Hui Chen",
      "Yang Yu",
      "Zhengmao Zhu",
      "ZhiHua Yu",
      "Chen Zhenjun",
      "Chenghe Wang",
      "Yinan Wu",
      "Hongqiu Wu",
      "Rong-Jun Qin",
      "Ruijin Ding",
      "Huang Fangsheng"
    ],
    "keywords": [
      "offline environment model learning",
      "reinforcement learning",
      "causal inference"
    ],
    "real_all_scores": [
      5,
      6,
      5,
      5
    ],
    "real_confidences": [
      3,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills": {
    "paper_pk": null,
    "title": "ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills",
    "abstract": "Generalizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.",
    "authors": [
      "Jiayuan Gu",
      "Fanbo Xiang",
      "Xuanlin Li",
      "Zhan Ling",
      "Xiqiang Liu",
      "Tongzhou Mu",
      "Yihe Tang",
      "Stone Tao",
      "Xinyue Wei",
      "Yunchao Yao",
      "Xiaodi Yuan",
      "Pengwei Xie",
      "Zhiao Huang",
      "Rui Chen",
      "Hao Su"
    ],
    "keywords": [
      "object manipulation",
      "benchmark",
      "computer vision",
      "robotics",
      "reinforcement learning"
    ],
    "real_all_scores": [
      8,
      8,
      8,
      6,
      6
    ],
    "real_confidences": [
      3,
      3,
      2,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Generating Diverse Cooperative Agents by Learning Incompatible Policies": {
    "paper_pk": null,
    "title": "Generating Diverse Cooperative Agents by Learning Incompatible Policies",
    "abstract": "Training a robust cooperative agent requires diverse partner agents. However, obtaining those agents is difficult. Previous works aim to learn diverse behaviors by changing the state-action distribution of agents. But, without information about the task's goal, the diversified agents are not guided to find other important, albeit sub-optimal, solutions: the agents might learn only variations of the same solution. In this work, we propose to learn diverse behaviors via policy compatibility. Conceptually, policy compatibility measures whether policies of interest can coordinate effectively. We theoretically show that incompatible policies are not similar. Thus, policy compatibility\u2014which has been used exclusively as a measure of robustness\u2014can be used as a proxy for learning diverse behaviors. Then, we incorporate the proposed objective into a population-based training scheme to allow concurrent training of multiple agents. Additionally, we use state-action information to induce local variations of each policy. Empirically, the proposed method consistently discovers more solutions than baseline methods across various multi-goal cooperative environments. Finally, in multi-recipe Overcooked, we show that our method produces populations of behaviorally diverse agents, which enables generalist agents trained with such a population to be more robust.\n\nSee our project page at https://bit.ly/marl-lipo\n",
    "authors": [
      "Rujikorn Charakorn",
      "Poramate Manoonpong",
      "Nat Dilokthanakul"
    ],
    "keywords": [
      "multi-agent systems",
      "cooperation",
      "collaboration",
      "reinforcement learning",
      "diversity",
      "robustness"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Parameterized projected Bellman operator": {
    "paper_pk": null,
    "title": "Parameterized projected Bellman operator",
    "abstract": "The Bellman operator is a cornerstone of reinforcement learning, widely used in a plethora of works, from value-based methods to modern actor-critic approaches. In problems with unknown models, the Bellman operator requires transition samples that strongly determine its behavior, as uninformative samples can result in negligible updates or long detours before reaching the fixed point. In this work, we introduce the novel idea of obtaining an approximation of the Bellman operator, which we call projected Bellman operator (PBO). Our PBO is a parametric operator on the parameter space of a given value function. Given the parameters of a value function, PBO outputs the parameters of a new value function and converges to a fixed point in the limit, as a standard Bellman operator. Notably, our PBO can approximate repeated applications of the true Bellman operator at once, as opposed to the sequential nature of the standard Bellman operator. We prove the important consequences of this finding for different classes of problems by analyzing PBO in terms of stability, convergence, and approximation error. Eventually, we propose an approximate value-iteration algorithm to show how PBO can overcome the limitations of classical methods, opening up multiple research directions as a novel paradigm in reinforcement learning.",
    "authors": [
      "Th\u00e9o Vincent",
      "Alberto Maria Metelli",
      "Jan Peters",
      "Marcello Restelli",
      "Carlo D'Eramo"
    ],
    "keywords": [
      "reinforcement learning",
      "bellman operator",
      "operator learning",
      "approximate value iteration"
    ],
    "real_all_scores": [
      5,
      5,
      8,
      8
    ],
    "real_confidences": [
      3,
      4,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Learning GFlowNets from partial episodes for improved convergence and stability": {
    "paper_pk": null,
    "title": "Learning GFlowNets from partial episodes for improved convergence and stability",
    "abstract": "Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD($\\lambda$) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB($\\lambda$), a GFlowNet training objective that can learn from partial action subsequences of varying lengths. We show that SubTB($\\lambda$) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before. We also perform a comparative analysis of stochastic gradient dynamics, shedding light on the bias-variance tradeoff in GFlowNet training and the advantages of subtrajectory balance.",
    "authors": [
      "Kanika Madan",
      "Jarrid Rector-Brooks",
      "Maksym Korablyov",
      "Emmanuel Bengio",
      "Moksh Jain",
      "Andrei Cristian Nica",
      "Tom Bosc",
      "Yoshua Bengio",
      "Nikolay Malkin"
    ],
    "keywords": [
      "GFlowNets",
      "probabilistic modeling",
      "reinforcement learning"
    ],
    "real_all_scores": [
      5,
      3,
      6
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Reward Learning with Trees: Methods and Evaluation": {
    "paper_pk": null,
    "title": "Reward Learning with Trees: Methods and Evaluation",
    "abstract": "Recent efforts to learn reward functions from human feedback have tended to use deep neural networks, whose lack of transparency hampers our ability to explain agent behaviour or verify alignment. We explore the merits of learning intrinsically interpretable tree models instead. We develop a recently proposed method for learning reward trees from preference labels, and show it to be broadly competitive with neural networks on challenging high-dimensional tasks, with good robustness to limited or corrupted data. Having found that reward tree learning can be done effectively in complex settings, we then consider why it should be used, demonstrating that the interpretable reward structure gives significant scope for traceability, verification and explanation.",
    "authors": [
      "Tom Bewley",
      "Jonathan Lawry",
      "Arthur Richards",
      "Rachel Craddock",
      "Ian Henderson"
    ],
    "keywords": [
      "reinforcement learning",
      "reward learning",
      "alignment",
      "human-agent interaction",
      "explainable AI",
      "XAI",
      "interpretability",
      "decision trees"
    ],
    "real_all_scores": [
      6,
      6,
      3
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Efficient Exploration via Fragmentation and Recall": {
    "paper_pk": null,
    "title": "Efficient Exploration via Fragmentation and Recall",
    "abstract": "Efficient exploration and model-building are critical for learning in large state- spaces. However, agents typically face problems like getting stuck locally during exploration and catastrophic forgetting in their construction of models when the environments are heterogeneous. Here, we propose and apply the concept of Fragmentation-and-Recall to solve spatial (FarMap) and reinforcement learning problems (FarCuriosity). Agents construct local maps or local models, respectively, which are used to predict the current observation. High surprisal points lead to a fragmentation event. At fracture points, we store the current map or model fragment in a long-term memory (LTM) and initialize a new fragment. On the other hand, Fragments are recalled (and thus reused) from LTM if the observations of their fracture points match the agent\u2019s current observation during exploration. The set of fracture points defines a set of intrinsic potential subgoals. Agents choose their next subgoal from the set of near and far potential subgoals in the current fragment or LTM, respectively. Thus, local maps and model fragments guide exploration locally and avoid catastrophic forgetting in learning heterogeneous environments, while LTM promotes exploration more globally. We evaluate FarMap and FarCuriosity on complex procedurally-generated spatial environments and on reinforcement learning benchmarks and demonstrate that the proposed methods are more efficient at exploration and memory use, and in harvesting extrinsic rewards, respectively.",
    "authors": [
      "Jaedong Hwang",
      "Zhang-Wei Hong",
      "Eric R Chen",
      "Akhilan Boopathy",
      "Pulkit Agrawal",
      "Ila R Fiete"
    ],
    "keywords": [
      "fragmentation",
      "recall",
      "exploration",
      "cognitive science",
      "neuroscience",
      "curiosity",
      "reinforcement learning",
      "spatial navigation"
    ],
    "real_all_scores": [
      8,
      6,
      8,
      6,
      6
    ],
    "real_confidences": [
      2,
      4,
      3,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Learning Representations for Reinforcement Learning with Hierarchical Forward Models": {
    "paper_pk": null,
    "title": "Learning Representations for Reinforcement Learning with Hierarchical Forward Models",
    "abstract": "Learning control from pixels is difficult for reinforcement learning (RL) agents because representation learning and policy learning are intertwined. Previous approaches remedy this issue with auxiliary representation learning tasks, but they either do not consider the temporal aspect of the problem or only consider single-step transitions, which may miss relevant information if important environmental changes take many steps to manifest. We propose Hierarchical $k$-Step Latent (HKSL), an auxiliary task that learns representations via a hierarchy of forward models that operate at varying magnitudes of step skipping while also learning to communicate between levels in the hierarchy. We evaluate HKSL in a suite of 30 robotic control tasks with and without distractors and a task of our creation. We find that HKSL either converges to higher episodic returns or optimal performance more quickly than several current baselines. Furthermore, we find that HKSL's representations capture task-relevant details accurately across timescales (even in the presence of distractors) and that communication channels between hierarchy levels organize information based on both sides of the communication process, both of which improve sample efficiency.",
    "authors": [
      "Trevor McInroe",
      "Lukas Sch\u00e4fer",
      "Stefano V Albrecht"
    ],
    "keywords": [
      "Reinforcement learning",
      "Representation learning",
      "Continuous control"
    ],
    "real_all_scores": [
      8,
      6,
      6,
      6
    ],
    "real_confidences": [
      2,
      3,
      2,
      2
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Explicitly Maintaining Diverse Playing Styles in Self-Play": {
    "paper_pk": null,
    "title": "Explicitly Maintaining Diverse Playing Styles in Self-Play",
    "abstract": "Self-play has proven to be an effective training schema to obtain a high-level agent in complex games through iteratively playing against an opponent from its historical versions. However, its training process may prevent it from generating a well-generalised policy since the trained agent rarely encounters diversely-behaving opponents along its own historical path. In this paper, we aim to improve the generalisation of the policy by maintaining a population of agents with diverse playing styles and high skill levels throughout the training process. Specifically, we propose a bi-objective optimisation model to simultaneously optimise the agents' skill level and playing style. A feature of this model is that we do not regard the skill level and playing style as two objectives to maximise directly since they are not equally important (i.e., agents with diverse playing styles but low skill levels are meaningless). Instead, we create a meta bi-objective model to enable high-level agents with diverse playing styles more likely to be incomparable (i.e. Pareto non-dominated), thereby playing against each other through the training process. We then present an evolutionary algorithm working with the proposed model. Experiments in a classic table tennis game Pong and a commercial role-playing game Justice Online show that our algorithm can learn a well generalised policy and at the same time is able to provide a set of high-level policies with various playing styles.",
    "authors": [
      "Yuan Liu",
      "Ruimin Shen",
      "Miqing Li",
      "Yingfeng Chen",
      "Juan Zou",
      "Changjie Fan"
    ],
    "keywords": [
      "Reinforcement learning",
      "evolutionary algorithm",
      "self-play",
      "diverse playing styles",
      "high skill levels"
    ],
    "real_all_scores": [
      5,
      6,
      8,
      8,
      5
    ],
    "real_confidences": [
      4,
      3,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "A Mixture-of-Expert Approach to RL-based Dialogue Management": {
    "paper_pk": null,
    "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management",
    "abstract": "Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge. We use reinforcement learning (RL) to develop a dialogue agent that avoids being short-sighted (outputting generic utterances) and maximizes overall user satisfaction. Most existing RL approaches to DM train the agent at the word-level, and thus, have to deal with a combinatorially complex action space even for a medium-size vocabulary. As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM. To address this issue, we develop a RL-based DM using a novel mixture of expert language model (MoE-LM) that consists of (i) a LM capable of learning diverse semantics for conversation histories, (ii) a number of specialized LMs (or experts) capable of generating utterances corresponding to a particular attribute or personality, and (iii) a RL-based DM that performs dialogue planning with the utterances generated by the experts. Our MoE approach provides greater flexibility to generate sensible utterances with different intents and allows RL to focus on conversational-level DM. We compare it with SOTA baselines on open-domain dialogues and demonstrate its effectiveness both in terms of the diversity and sensibility of the generated utterances and the overall DM performance. ",
    "authors": [
      "Yinlam Chow",
      "Azamat Tulepbergenov",
      "Ofir Nachum",
      "Dhawal Gupta",
      "Moonkyung Ryu",
      "Mohammad Ghavamzadeh",
      "Craig Boutilier"
    ],
    "keywords": [
      "Reinforcement learning"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      3,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret": {
    "paper_pk": null,
    "title": "ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret",
    "abstract": "Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promis- ing line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art\u2014DREAM and neural fictitious self play (NFSP)\u2014on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.",
    "authors": [
      "Stephen Marcus McAleer",
      "Gabriele Farina",
      "Marc Lanctot",
      "Tuomas Sandholm"
    ],
    "keywords": [
      "game theory",
      "two-player zero-sum",
      "CFR",
      "Reinforcement learning"
    ],
    "real_all_scores": [
      3,
      1,
      5
    ],
    "real_confidences": [
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Towards Understanding How Machines Can Learn Causal Overhypotheses ": {
    "paper_pk": null,
    "title": "Towards Understanding How Machines Can Learn Causal Overhypotheses ",
    "abstract": "Recent work in machine learning and cognitive science has suggested that understanding causal information is essential to the development of intelligence. One of the key challenges for current machine learning algorithms is modeling and understanding causal overhypotheses: transferable abstract hypotheses about sets of causal relationships. In contrast, even young children spontaneously learn causal overhypotheses, and use these to guide their exploration or to generalize to new situations. This has been demonstrated in a variety of cognitive science experiments using the \u201cblicket detector\u201d environment. We present a causal learning benchmark adapting the \u201cblicket\" environment for machine learning agents and evaluate a range of state-of-the-art methods in this environment. We find that although most agents have no problem learning causal structures seen during training, they are unable to learn causal overhypotheses from these experiences, and thus cannot generalize to new settings. ",
    "authors": [
      "Eliza Kosoy",
      "David Chan",
      "Adrian Liu",
      "Jasmine Collins",
      "Bryanna Kaufmann",
      "Sandy Huang",
      "Jessica B Hamrick",
      "John Canny",
      "Nan Rosemary Ke",
      "Alison Gopnik"
    ],
    "keywords": [
      "causal reasoning",
      "intervention",
      "causal overhypotheses",
      "Reinforcement learning",
      "gpt-3"
    ],
    "real_all_scores": [
      8,
      3,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Performance Bounds for Model and Policy Transfer in Hidden-parameter MDPs": {
    "paper_pk": null,
    "title": "Performance Bounds for Model and Policy Transfer in Hidden-parameter MDPs",
    "abstract": "In the Hidden-Parameter MDP (HiP-MDP) framework, a family of reinforcement learning tasks is generated by varying hidden parameters specifying the dynamics and reward function for each individual task. HiP-MDP is a natural model for families of tasks in which meta- and lifelong-reinforcement learning approaches can succeed. Given a learned context encoder that infers the hidden parameters from previous experience, most existing algorithms fall into two categories: $\\textit{model transfer}$ and $\\textit{policy transfer}$, depending on which function the hidden parameters are used to parameterize. We characterize the robustness of model and policy transfer algorithms with respect to hidden parameter estimation error. We first show that the value function of HiP-MDPs is Lipschitz continuous under certain conditions. We then derive regret bounds for both settings through the lens of Lipschitz continuity. Finally, we empirically corroborate our theoretical analysis by experimentally varying the hyper-parameters governing the Lipschitz constants of two continuous control problems; the resulting performance is consistent with our predictions.",
    "authors": [
      "Haotian Fu",
      "Jiayu Yao",
      "Omer Gottesman",
      "Finale Doshi-Velez",
      "George Konidaris"
    ],
    "keywords": [
      "Reinforcement learning",
      "Meta learning",
      "Transfer learning",
      "Theory"
    ],
    "real_all_scores": [
      8,
      8,
      10
    ],
    "real_confidences": [
      2,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Cyclophobic Reinforcement Learning": {
    "paper_pk": null,
    "title": "Cyclophobic Reinforcement Learning",
    "abstract": "In environments with sparse rewards finding a good inductive bias for exploration is crucial to the agent\u2019s success. However, there are two competing goals: novelty search and systematic exploration. While existing approaches such as curiousity- driven exploration find novelty, they sometimes do not systematically explore the whole state space, akin to depth-first-search vs breadth-first-search. In this paper, we propose a new intrinsic reward that is cyclophobic, i.e. it does not reward novelty, but punishes redundancy by avoiding cycles. Augmenting the cyclophobic intrinsic reward with a sequence of hierarchical representations based on the agent\u2019s cropped observations we are able to achieve excellent results in the MiniGrid and MiniHack environments. Both are particularly hard, as they require complex interactions with different objects in order to be solved. Detailed comparisons with previous approaches and thorough ablation studies show that our newly proposed cyclophobic reinforcement learning is vastly more efficient than other state of the art methods.",
    "authors": [
      "Stefan Sylvius Wagner",
      "Peter Arndt",
      "Jan Robine",
      "Stefan Harmeling"
    ],
    "keywords": [
      "Reinforcement learning",
      "intrinsic rewards",
      "exploration",
      "transfer learning",
      "objects"
    ],
    "real_all_scores": [
      8,
      8,
      6
    ],
    "real_confidences": [
      2,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees": {
    "paper_pk": null,
    "title": "Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees",
    "abstract": "Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the distilled policy, for which the formal guarantees apply. Our approach yields bisimulation guarantees while learning the distilled policy, allowing concrete optimization of the abstraction and representation model quality. Our experiments show that, besides distilling policies up to 10 times faster, the latent model quality is indeed better in general. Moreover, we present experiments from a simple time-to-failure verification algorithm on the latent space. The fact that our approach enables such simple verification techniques highlights its applicability.",
    "authors": [
      "Florent Delgrange",
      "Ann Nowe",
      "Guillermo Perez"
    ],
    "keywords": [
      "Reinforcement learning",
      "Formal Verification",
      "Representation Learning"
    ],
    "real_all_scores": [
      5,
      1,
      3,
      3
    ],
    "real_confidences": [
      3,
      4,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "SDAC: Efficient Safe Reinforcement Learning with Low-Biased Distributional Actor-Critic": {
    "paper_pk": null,
    "title": "SDAC: Efficient Safe Reinforcement Learning with Low-Biased Distributional Actor-Critic",
    "abstract": "To apply reinforcement learning (RL) to real-world practical applications, agents are required to adhere to the safety guidelines of their respective domains.\nSafe RL can effectively handle the guidelines by maximizing returns while maintaining safety satisfaction.\nIn this paper, we develop a safe distributional RL method based on the trust region method which has the capability of satisfying safety constraints consistently.\nHowever, importance sampling required for the trust region method can hinder performance due to its significant variance, and policies may not meet the safety guidelines due to the estimation bias of distributional critics.\nHence, we enhance safety performance through the following approaches.\nFirst, we propose novel surrogates for the trust region method expressed with Q-functions using the reparameterization trick.\nSecond, we utilize distributional critics trained with a target distribution where bias-variance can be traded off.\nIn addition, if an initial policy violates safety constraints, there can be no policy satisfying safety constraints within the trust region.\nThus, we propose a gradient integration method which is guaranteed to find a policy satisfying multiple constraints from an unsafe initial policy.\nFrom extensive experiments, the proposed method shows minimal constraint violations while achieving high returns compared to existing safe RL methods.\nFurthermore, we demonstrate the benefit of safe RL for problems in which the reward function cannot be easily specified.",
    "authors": [
      "Dohyeong Kim",
      "Kyungjae Lee",
      "Songhwai Oh"
    ],
    "keywords": [
      "Reinforcement learning",
      "Safety",
      "Distributional Critic"
    ],
    "real_all_scores": [
      5,
      3,
      5,
      5
    ],
    "real_confidences": [
      3,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Hyperbolic Deep Reinforcement Learning": {
    "paper_pk": null,
    "title": "Hyperbolic Deep Reinforcement Learning",
    "abstract": "In deep reinforcement learning (RL), useful information about the state is inherently tied to its possible future successors. Consequently, encoding features that capture the hierarchical relationships between states into the model's latent representations is often conducive to recovering effective policies. In this work, we study a new class of deep RL algorithms that promote encoding such relationships by using hyperbolic space to model latent representations. However, we find that a naive application of existing methodology from the hyperbolic deep learning literature leads to fatal instabilities due to the non-stationarity and variance characterizing common gradient estimators in RL. Hence, we design a new general method that directly addresses such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope this work will inspire future RL research to consider hyperbolic representations as a standard tool.",
    "authors": [
      "Edoardo Cetin",
      "Benjamin Paul Chamberlain",
      "Michael M. Bronstein",
      "Jonathan J Hunt"
    ],
    "keywords": [
      "Reinforcement learning",
      "Hyperbolic space",
      "Representation learning",
      "Machine learning"
    ],
    "real_all_scores": [
      5,
      3,
      5,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "A sampling framework for value-based reinforcement learning": {
    "paper_pk": null,
    "title": "A sampling framework for value-based reinforcement learning",
    "abstract": "Value-based algorithms have achieved great successes in solving Reinforcement Learning problems via minimizing the mean squared Bellman error (MSBE). Temporal-difference (TD) algorithms such as Q-learning and SARSA often use stochastic gradient descent based optimization approaches to estimate the value function parameters, but fail to quantify their uncertainties. In our work, under the Kalman filtering paradigm, we establish a novel and scalable sampling framework based on stochastic gradient Markov chain Monte Carlo, which allows us to efficiently generate samples from the posterior distribution of deep neural network parameters. For TD-learning with both linear and nonlinear function approximation, we prove that the proposed algorithm converges to a stationary distribution, which allows us to measure uncertainties of the value function and its parameters.",
    "authors": [
      "Frank Shih",
      "Faming Liang"
    ],
    "keywords": [
      "Reinforcement learning",
      "Bayesian sampling",
      "Stochastic gradient MCMC",
      "Value function approximation"
    ],
    "real_all_scores": [
      8,
      6,
      8
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Risk-aware Bayesian RL for Cautious Exploration": {
    "paper_pk": null,
    "title": "Risk-aware Bayesian RL for Cautious Exploration",
    "abstract": "This paper addresses the problem of maintaining safety during training in Reinforcement Learning (RL), such that the safety constraint violations are bounded at any point during learning. Whilst enforcing safety during training might limit the agent's exploration, we propose a new architecture that handles the trade-off between efficient progress in exploration and safety maintenance. \nAs the agent's exploration progresses, we update Dirichlet-Categorical models of the transition probabilities of the Markov decision process that describes the agent's behavior within the environment by means of Bayesian inference. We then propose a way to approximate moments of the agent's belief about the risk associated with the agent's behavior originating from local action selection. We demonstrate that this approach can be easily coupled with RL, we provide rigorous theoretical guarantees, and we present experimental results to showcase the performance of the overall architecture.",
    "authors": [
      "Rohan Mitta",
      "Hosein Hasanbeig",
      "Daniel Kroening",
      "Alessandro Abate"
    ],
    "keywords": [
      "Reinforcement learning",
      "Bayesian inference",
      "Safe learning",
      "Risk",
      "Safety Specification"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Planning With Uncertainty: Deep Exploration in Model-Based Reinforcement Learning": {
    "paper_pk": null,
    "title": "Planning With Uncertainty: Deep Exploration in Model-Based Reinforcement Learning",
    "abstract": "Deep model-based reinforcement learning has shown super-human performance in many challenging domains. Low sample efficiency and limited exploration remain however as leading obstacles in the field. In this paper, we demonstrate deep exploration in model-based RL by incorporating epistemic uncertainty into planning trees, circumventing the standard approach of propagating uncertainty through value learning. We evaluate this approach with the state of the art model-based RL algorithm MuZero, and extend its training process to stabilize learning from explicitly-exploratory decisions. Our results demonstrate that planning with uncertainty is able to achieve effective deep exploration with standard uncertainty estimation mechanisms, and with it significant gains in sample efficiency.",
    "authors": [
      "Yaniv Oren",
      "Matthijs T. J. Spaan",
      "Wendelin Boehmer"
    ],
    "keywords": [
      "Reinforcement learning",
      "exploration",
      "uncertainty",
      "planning"
    ],
    "real_all_scores": [
      6,
      8,
      10,
      6,
      6
    ],
    "real_confidences": [
      3,
      3,
      5,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Unified neural representation model for physical and conceptual spaces": {
    "paper_pk": null,
    "title": "Unified neural representation model for physical and conceptual spaces",
    "abstract": "The spatial processing system of the brain uses grid-like neural representations (grid cells) for supporting vector-based navigation. Experiments also suggest that neural representations for concepts (concept cells) exist in the human brain, and conceptual inference relies on navigation in conceptual spaces. We propose a unified model called ``disentangled successor information (DSI)'' that explains neural representations for both physical and conceptual spaces. DSI generates grid-like representations in a 2-dimensional space that highly resemble those observed in the brain. Moreover, the same model creates concept-specific representations from linguistic inputs, corresponding to concept cells. Mathematically, DSI vectors approximate value functions for navigation and word vectors obtained by word embedding methods, thus enabling both spatial navigation and conceptual inference based on vector-based calculation. Our results suggest that a single principle can explain computation of physical and conceptual spaces in the human brain.",
    "authors": [
      "Tatsuya Haga",
      "Yohei Oseki",
      "Tomoki Fukai"
    ],
    "keywords": [
      "Neuroscience",
      "Grid cell",
      "Concept cell",
      "Spatial navigation",
      "Reinforcement learning",
      "Word embedding"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Posterior Sampling Model-based Policy Optimization under Approximate Inference": {
    "paper_pk": null,
    "title": "Posterior Sampling Model-based Policy Optimization under Approximate Inference",
    "abstract": "Model-based reinforcement learning algorithms (MBRL) hold tremendous promise for improving the sample efficiency in online RL. However, many existing popular MBRL algorithms cannot deal with exploration and exploitation properly. Posterior sampling reinforcement learning (PSRL) serves as a promising approach for automatically trading off the exploration and exploitation, but the theoretical guarantees only hold under exact inference. In this paper, we show that adopting the same methodology as in exact PSRL can be fairly suboptimal under approximate inference. Motivated by the analysis, we propose an improved factorization for the posterior distribution of polices by removing the conditional independence between the policy and data given the model. By adopting such a posterior factorization, we further propose a general algorithmic framework for PSRL under approximate inference and a practical instantiation of it. Empirically, our algorithm can surpass the baseline methods by a significant margin on both dense rewards and sparse rewards tasks from DM control suite, OpenAI Gym and Metaworld benchmarks.",
    "authors": [
      "Chaoqi Wang",
      "Yuxin Chen",
      "Kevin Patrick Murphy"
    ],
    "keywords": [
      "Reinforcement learning",
      "Posterior",
      "Model-based reinforcement learning"
    ],
    "real_all_scores": [
      6,
      5,
      6
    ],
    "real_confidences": [
      4,
      2,
      1
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Entropy-Regularized Model-Based Offline Reinforcement Learning": {
    "paper_pk": null,
    "title": "Entropy-Regularized Model-Based Offline Reinforcement Learning",
    "abstract": "Model-based approaches to offline Reinforcement Learning (RL) aim to remedy the problem of sample complexity in offline learning via first estimating a pessimistic Markov Decision Process (MDP) from offline data, followed by freely exploring in the learned model for policy optimization. Recent advances in model-based RL techniques mainly rely on an ensemble of models to quantify the uncertainty of the empirical MDP which is leveraged to penalize out-of-distribution state-action pairs during the policy learning. However, the performance of ensembles for uncertainty quantification highly depends on  how they are implemented in practice, which can be a limiting factor. In this paper, we propose a systematic way to measure the epistemic uncertainty and present \\abbrv, an Entropy-regularized Model-based Offline RL approach, to provide a smooth error estimation when leaving the support of data toward uncertain areas. Subsequently, we optimize a single neural architecture that maximizes the likelihood of offline data distribution while regularizing the transitions that are outside of the data support.  Empirical results demonstrate that our framework achieves competitive performance compared to state-of-the-art offline RL methods on D4RL benchmark datasets.",
    "authors": [
      "Soroush Ghandi",
      "Maryam Tavakol"
    ],
    "keywords": [
      "Reinforcement learning",
      "model-based",
      "offline RL",
      "entropy regularization"
    ],
    "real_all_scores": [
      5,
      3,
      8
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Become a Proficient Player with Limited Data through Watching Pure Videos": {
    "paper_pk": null,
    "title": "Become a Proficient Player with Limited Data through Watching Pure Videos",
    "abstract": "Recently, RL has shown its strong ability for visually complex tasks. However, it suffers from the low sample efficiency and poor generalization ability, which prevent RL from being useful in real-world scenarios. Inspired by the huge success of unsupervised pre-training methods on language and vision domains, we propose to improve the sample efficiency via a novel pre-training method for model-based RL. \nInstead of using pre-recorded agent trajectories that come with their own actions, we consider the setting where the pre-training data are action-free videos, which are more common and available in the real world. We introduce a two-phase training pipeline as follows: for the pre-training phase, we implicitly extract the hidden action embedding from videos and pre-train the visual representation and the environment dynamics network through a novel \\Changes{forward-inverse} cycle consistency \\Changes{(FICC)} objective based on vector quantization; for down-stream tasks, we finetune with small amount of task data based on the learned models. Our framework can significantly improve the sample efficiency on Atari Games with data of only one hour of game playing. We achieve 118.4\\% mean human performance and 36.0\\% median performance with only 50k environment steps, which is 85.6\\% and 65.1\\% better than the scratch EfficientZero model. We believe such pre-training approach can provide an option for solving real-world RL problems. The code is available at \\url{https://github.com/YeWR/FICC.git}.",
    "authors": [
      "Weirui Ye",
      "Yunsheng Zhang",
      "Pieter Abbeel",
      "Yang Gao"
    ],
    "keywords": [
      "Pre-training",
      "Fine-tune",
      "MCTS",
      "Reinforcement learning",
      "Vector Quantization"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Partial Advantage Estimator for Proximal Policy Optimization": {
    "paper_pk": null,
    "title": "Partial Advantage Estimator for Proximal Policy Optimization",
    "abstract": "Estimation of value in policy gradient methods is a fundamental problem. Generalized Advantage Estimation (GAE) is an exponentially-weighted estimator of an advantage function similar to TD($\\lambda$). It substantially reduces the variance of policy gradient estimates at the expense of bias. In practical applications, a truncated GAE is used due to the incompleteness of the trajectory, which results in a large bias during estimation. To address this challenge, instead of using the all truncated GAE, we propose to take a part of the calculated GAE for updates, which significantly reduces the bias due to the incomplete trajectory.  We perform experiments in MuJoCo and $\\mu$RTS to investigate the effect of different partial coefficient and sampling lengths. We show that our partial GAE approach yields better empirical results in both environments.",
    "authors": [
      "Xiulei Song",
      "Yizhao Jin",
      "Gregory Slabaugh",
      "Simon Lucas"
    ],
    "keywords": [
      "Reinforcement learning",
      "value estimator"
    ],
    "real_all_scores": [
      3,
      1,
      6
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Deep reinforced active learning for multi-class image classification": {
    "paper_pk": null,
    "title": "Deep reinforced active learning for multi-class image classification",
    "abstract": "High accuracy medical image classification can be limited by the costs of acquiring more data as well as the time and expertise needed to label existing images. In this paper, we apply active learning to medical image classification, a method which aims to maximise model performance on a minimal subset from a larger pool of data. We present a new active learning framework, based on deep reinforcement learning, to learn an active learning query strategy to label images based on predictions from a convolutional neural network. Our framework modifies the deep-Q network formulation, allowing us to pick data based additionally on geometric arguments in the latent space of the classifier, allowing for high accuracy multi-class classification in a batch-based active learning setting, enabling the agent to label datapoints that are both diverse and about which it is most uncertain. We apply our framework to two medical imaging datasets and compare with standard query strategies as well as the most recent reinforcement learning based active learning approach for image classification.",
    "authors": [
      "Emma Slade",
      "Kim Branson"
    ],
    "keywords": [
      "Reinforcement learning",
      "active learning",
      "image classification"
    ],
    "real_all_scores": [
      6,
      5,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward": {
    "paper_pk": null,
    "title": "Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward",
    "abstract": "We propose Structured Exploration with Achievements (SEA), a multi-stage reinforcement learning algorithm designed for achievement-based environments, a particular type of environment with an internal achievement set. SEA first uses offline data to learn a representation of the known achievements with a determinant loss function, then recovers the dependency graph of the learned achievements with a heuristic algorithm, and finally interacts with the environment online to learn policies that master known achievements and explore new ones with a controller built with the recovered dependency graph. We empirically demonstrate that SEA can recover the achievement structure accurately and improve exploration in hard domains such as Crafter that are procedurally generated with high-dimensional observations like images.",
    "authors": [
      "Zihan Zhou",
      "Animesh Garg"
    ],
    "keywords": [
      "deep reinforcement learning",
      "structured exploration"
    ],
    "real_all_scores": [
      3,
      3,
      6,
      1
    ],
    "real_confidences": [
      5,
      3,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "PALM: Preference-based Adversarial Manipulation against Deep Reinforcement Learning": {
    "paper_pk": null,
    "title": "PALM: Preference-based Adversarial Manipulation against Deep Reinforcement Learning",
    "abstract": "To improve the robustness of DRL agents, it is important to study their vulnerability under  adversarial attacks that would lead to extreme behaviors desired by adversaries. Preference-based RL (PbRL) aims for learning desired behaviors with human preferences. In this paper, we propose PALM, a preference-based adversarial manipulation method against DRL agents  which adopts human preferences to perform targeted attacks with the assistance of an intention policy and a weighting function. The intention policy is trained based on the PbRL framework to guide the adversarial policy  to mitigate restrictions of the victim policy during exploration, and the weighting function learns weight assignment to improve the performance of the adversarial policy. Theoretical analysis demonstrates that PALM converges to critical points under some mild conditions. Empirical results on a few manipulation tasks of Meta-world show that PALM exceeds the performance of state-of-the-art adversarial attack methods under the targeted setting. Additionally, we show the vulnerability of the offline RL agents by fooling them into behaving as human desires on several Mujoco tasks. Our code and videos are available in https://sites.google.com/view/palm-adversarial-attack.",
    "authors": [
      "Fengshuo Bai",
      "Runze Liu",
      "Yaodong Yang",
      "Yali Du"
    ],
    "keywords": [
      "adversarial attack",
      "deep reinforcement learning",
      "preference-based reinforcement learning",
      "bi-level optimization"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      5
    ],
    "real_confidences": [
      5,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Human-AI Coordination via Human-Regularized Search and Learning": {
    "paper_pk": null,
    "title": "Human-AI Coordination via Human-Regularized Search and Learning",
    "abstract": "We consider the problem of making AI agents that collaborate well with humans in partially observable fully cooperative environments given datasets of human behavior. Inspired by piKL, a human-data-regularized search method that improves upon a behavioral cloning policy without diverging far away from it, we develop a three-step algorithm that achieve strong performance in coordinating with real humans in the Hanabi benchmark. We first use a regularized search algorithm and behavioral cloning to produce a better human model that captures diverse skill levels. Then, we integrate the policy regularization idea into reinforcement learning to train a human-like best response to the human model. Finally, we apply regularized search on top of the best response policy at test time to handle out-of-distribution challenges when playing with humans. We evaluate our method in two large scale experiments with humans. First, we show that our method outperforms experts when playing with a group of diverse human players in ad-hoc teams. Second, we show that our method beats a vanilla best response to behavioral cloning baseline by having experts play repeatedly with the two agents.",
    "authors": [
      "Hengyuan Hu",
      "David J Wu",
      "Adam Lerer",
      "Jakob Nicolaus Foerster",
      "Noam Brown"
    ],
    "keywords": [
      "human-ai collaboration",
      "multi-agent",
      "search",
      "deep reinforcement learning"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning to Cooperate and Communicate Over Imperfect Channels": {
    "paper_pk": null,
    "title": "Learning to Cooperate and Communicate Over Imperfect Channels",
    "abstract": "Information exchange in multi-agent systems improves the cooperation among agents, especially in partially observable settings. This can be seen as part of the problem in which the agents learn how to communicate and to solve a shared task simultaneously. In the real world, communication is often carried out over imperfect channels and this requires the agents to deal with uncertainty due to potential information loss. In this paper, we consider a cooperative multi-agent system where the agents act and exchange information in a decentralized manner using a limited and unreliable channel. To cope with such channel constraints, we propose a novel communication approach based on independent Q-learning. Our method allows agents to dynamically adapt how much information to share by sending messages of different size, depending on their local observations and the channel properties. In addition to this message size selection, agents learn to encode and decode messages to improve their policies. We show that our approach outperforms approaches without adaptive capabilities and discuss its limitations in different environments.",
    "authors": [
      "Jannis Weil",
      "Gizem Ekinci",
      "Heinz Koeppl",
      "Tobias Meuser"
    ],
    "keywords": [
      "multi-agent systems",
      "deep reinforcement learning",
      "emergent communication",
      "imperfect communication channels"
    ],
    "real_all_scores": [
      3,
      3,
      6,
      3
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Toward Effective Deep Reinforcement Learning for 3D Robotic Manipulation: End-to-End Learning from Multimodal Raw Sensory Data": {
    "paper_pk": null,
    "title": "Toward Effective Deep Reinforcement Learning for 3D Robotic Manipulation: End-to-End Learning from Multimodal Raw Sensory Data",
    "abstract": "Sample-efficient reinforcement learning (RL) methods capable of learning directly from raw sensory data without the use of human-crafted representations would open up real-world applications in robotics and control. Recent advances in visual RL have shown that learning a latent representation together with existing RL algorithms closes the gap between state-based and image-based training. However, image-based training is still significantly sample-inefficient with respect to learning in 3D continuous control problems (for example, robotic manipulation) compared to state-based training. In this study, we propose an effective model-free off-policy RL method for 3D robotic manipulation that can be trained in an end-to-end manner from multimodal raw sensory data obtained from a vision camera and a robot's joint encoders, without the need for human-crafted representations. Notably, our method is capable of learning a latent multimodal representation and a policy in an efficient, joint, and end-to-end manner from multimodal raw sensory data. Our method, which we dub MERL: Multimodal End-to-end Reinforcement Learning, results in a simple but effective approach capable of significantly outperforming both current state-of-the-art visual RL and state-based RL methods with respect to sample efficiency, learning performance, and training stability in relation to 3D robotic manipulation tasks from DeepMind Control.",
    "authors": [
      "Samyeul Noh",
      "Hyun Myung"
    ],
    "keywords": [
      "deep reinforcement learning",
      "robotic manipulation",
      "end-to-end learning",
      "multimodal representation learning"
    ],
    "real_all_scores": [
      5,
      3,
      6,
      6
    ],
    "real_confidences": [
      2,
      5,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Building a Subspace of Policies for Scalable Continual Learning": {
    "paper_pk": null,
    "title": "Building a Subspace of Policies for Scalable Continual Learning",
    "abstract": "The ability to continuously acquire new knowledge and skills is crucial for autonomous agents. Existing methods are typically based on either fixed-size models that struggle to learn a large number of diverse behaviors, or growing-size models that scale poorly with the number of tasks. In this work, we aim to strike a better balance between scalability and performance by designing a method whose size grows adaptively depending on the task sequence. We introduce Continual Subspace of Policies (CSP), a new approach that incrementally builds a subspace of policies for training a reinforcement learning agent on a sequence of tasks. The subspace's high expressivity allows CSP to perform well for many different tasks while growing more slowly than the number of tasks. Our method does not suffer from forgetting and also displays positive transfer to new tasks. CSP outperforms a number of popular baselines on a wide range of scenarios from two challenging domains, Brax (locomotion) and Continual World (robotic manipulation). Interactive visualizations of the subspace can be found at https://share.streamlit.io/continual-subspace/policies/main.",
    "authors": [
      "Jean-Baptiste Gaya",
      "Thang Doan",
      "Lucas Caccia",
      "Laure Soulier",
      "Ludovic Denoyer",
      "Roberta Raileanu"
    ],
    "keywords": [
      "continual learning",
      "deep reinforcement learning"
    ],
    "real_all_scores": [
      5,
      5,
      3
    ],
    "real_confidences": [
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Preference Transformer: Modeling Human Preferences using Transformers for RL": {
    "paper_pk": null,
    "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
    "abstract": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
    "authors": [
      "Changyeon Kim",
      "Jongjin Park",
      "Jinwoo Shin",
      "Honglak Lee",
      "Pieter Abbeel",
      "Kimin Lee"
    ],
    "keywords": [
      "preference-based reinforcement learning",
      "human-in-the-loop reinforcement learning",
      "deep reinforcement learning"
    ],
    "real_all_scores": [
      3,
      5,
      6,
      3
    ],
    "real_confidences": [
      3,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Iteratively Learning Novel Strategies with Diversity Measured in State Distances": {
    "paper_pk": null,
    "title": "Iteratively Learning Novel Strategies with Diversity Measured in State Distances",
    "abstract": "In complex reinforcement learning (RL) problems, policies with similar rewards may have substantially different behaviors. Yet, to not only optimize rewards but also discover as many diverse strategies as possible remains a challenging problem. A natural approach to this task is constrained population-based training (PBT), which simultaneously learns a collection of policies subject to diversity constraints. However, due to the unaffordable computation cost of PBT, we adopt an alternative approach, iterative learning (IL), which repeatedly learns a single novel policy that is sufficiently different from previous ones. We first analyze these two frameworks and prove that, for any policy pool derived by PBT, we can always use IL to obtain another policy pool of the same rewards and competitive diversity scores. In addition, we also present a novel state-based diversity measure with two tractable realizations. Such a metric can impose a stronger and much smoother diversity constraint than existing action-based metrics. Combining IL and the state-based diversity measure, we develop a powerful diversity-driven RL algorithm, State-based Intrinsic-reward Policy Optimization (SIPO), with provable convergence properties. We empirically examine our algorithm in complex multi-agent environments including StarCraft Multi-Agent Challenge and Google Research Football. SIPO is able to consistently derive strategically diverse and human-interpretable policies that cannot be discovered by existing baselines.",
    "authors": [
      "Wei Fu",
      "Weihua Du",
      "Jingwei Li",
      "Sunli Chen",
      "Jingzhao Zhang",
      "Yi Wu"
    ],
    "keywords": [
      "diverse behavior",
      "multi-agent reinforcement learning",
      "deep reinforcement learning"
    ],
    "real_all_scores": [
      5,
      5,
      6
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "On the Geometry of Reinforcement Learning in Continuous State and Action Spaces": {
    "paper_pk": null,
    "title": "On the Geometry of Reinforcement Learning in Continuous State and Action Spaces",
    "abstract": "Advances in reinforcement learning have led to its successful application in complex tasks  with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose  building  a theoretical understanding of continuous state and action spaces by employing a geometric lens. Central to our work is the idea that the transition dynamics induce a low dimensional manifold of reachable states  embedded in the high-dimensional nominal state space. We prove that, under certain conditions, the dimensionality of this manifold is at most the dimensionality of the action space plus one. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments.We further demonstrate the applicability of our result by learning a policy in this low dimensional representation.   To do so we introduce an algorithm that learns a mapping to a low dimensional representation, as a narrow hidden layer of a deep neural network, in tandem with the policy using DDPG. Our experiments show that a policy learnt this way perform on par or better for four MuJoCo control suite tasks.",
    "authors": [
      "Saket Tiwari",
      "Omer Gottesman",
      "George Konidaris"
    ],
    "keywords": [
      "geometry",
      "deep reinforcement learning",
      "manifold"
    ],
    "real_all_scores": [
      6,
      6,
      8
    ],
    "real_confidences": [
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Rule-based policy regularization for reinforcement learning-based building control": {
    "paper_pk": null,
    "title": "Rule-based policy regularization for reinforcement learning-based building control",
    "abstract": "Rule-based control (RBC) is widely adopted in buildings due to its stability and robustness. It resembles a behavior cloning methodology refined by human expertise. However, it is unlikely for RBC to exceed a reinforcement learning (RL) agent\u2019s performance since it is challenging to ingest a large number of parameters during decision-making. In this paper, we explore how to incorporate rule-based control into reinforcement learning to learn a more robust policy in both online and offline settings with a unified approach. We start with state-of-the-art online and offline RL methods, TD3 and TD3+BC, then improve on them using a dynamically weighted actor loss function to selectively choose which policy should RL models learn from at each time step of training. With experiments across multiple tasks and various weather conditions in both deterministic and stochastic scenarios, we empirically demonstrate that our dynamically weighted rule-based incorporated control regularization (RUBICON) method outperforms representative baseline methods in offline settings by 40.7% in a reward settings consisting of the combination of thermal comfort and energy consumption and by 49.7% in online settings in building-RL environments.",
    "authors": [
      "Hsin Yu Liu",
      "Bharathan Balaji",
      "Rajesh Gupta",
      "Dezhi Hong"
    ],
    "keywords": [
      "deep reinforcement learning",
      "batch reinforcement learning",
      "buildings",
      "HVAC"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model": {
    "paper_pk": null,
    "title": "Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model",
    "abstract": "Cutting planes (cuts) are important for solving mixed-integer linear programs (MILPs), which formulate a wide range of important real-world applications. Cut selection---which aims to select a proper subset of the candidate cuts to improve the efficiency of solving MILPs---heavily depends on (P1) which cuts should be preferred, and (P2) how many cuts should be selected. Although many modern MILP solvers tackle (P1)-(P2) by manually designed heuristics, machine learning offers a promising approach to learn more effective heuristics from MILPs collected from specific applications. However, many existing learning-based methods focus on learning which cuts should be preferred, neglecting the importance of learning the number of cuts that should be selected. Moreover, we observe from extensive empirical results that (P3) what order of selected cuts should be preferred has a significant impact on the efficiency of solving MILPs as well. To address this challenge, we propose a novel hierarchical sequence model (HEM) to learn cut selection policies via reinforcement learning. Specifically, HEM consists of a two-level model: (1) a higher-level model to learn the number of cuts that should be selected, (2) and a lower-level model---that formulates the cut selection task as a sequence to sequence learning problem---to learn policies selecting an ordered subset with the size determined by the higher-level model. To the best of our knowledge, HEM is the first method that can tackle (P1)-(P3) in cut selection simultaneously from a data-driven perspective. Experiments show that HEM significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs, including MIPLIB 2017. Moreover, experiments demonstrate that HEM well generalizes to MILPs that are significantly larger than those seen during training.",
    "authors": [
      "Zhihai Wang",
      "Xijun Li",
      "Jie Wang",
      "Yufei Kuang",
      "Mingxuan Yuan",
      "Jia Zeng",
      "Yongdong Zhang",
      "Feng Wu"
    ],
    "keywords": [
      "mixed-integer linear programming",
      "cut selection",
      "deep reinforcement learning",
      "sequence to sequence learning"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      3
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective": {
    "paper_pk": null,
    "title": "Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective",
    "abstract": "MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective human-agent collaboration. Experimental results in Honor of Kings demonstrate that MCC agents can collaborate reasonably well with human teammates and even generalize to collaborate with different levels and numbers of human teammates. Videos are available at https://sites.google.com/view/mcc-demo.",
    "authors": [
      "Yiming Gao",
      "Feiyu Liu",
      "Liang Wang",
      "Zhenjie Lian",
      "Weixuan Wang",
      "Siqin Li",
      "Xianliang Wang",
      "Xianhan Zeng",
      "Rundong Wang",
      "jiawei wang",
      "QIANG FU",
      "Yang Wei",
      "Lanxiao Huang",
      "Wei Liu"
    ],
    "keywords": [
      "game playing",
      "multi-agent",
      "human-ai communication",
      "human-ai collaboration",
      "deep reinforcement learning"
    ],
    "real_all_scores": [
      6,
      5,
      10,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "On the Robustness of Safe Reinforcement Learning under Observational Perturbations": {
    "paper_pk": null,
    "title": "On the Robustness of Safe Reinforcement Learning under Observational Perturbations",
    "abstract": "Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting.  We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward.  One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \\url{https://github.com/liuzuxin/safe-rl-robustness}",
    "authors": [
      "Zuxin Liu",
      "Zijian Guo",
      "Zhepeng Cen",
      "Huan Zhang",
      "Jie Tan",
      "Bo Li",
      "Ding Zhao"
    ],
    "keywords": [
      "Safe reinforcement learning",
      "deep reinforcement learning",
      "state robust reinforcement learning"
    ],
    "real_all_scores": [
      8,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "LS-IQ: Implicit Reward Regularization for Inverse Reinforcement Learning": {
    "paper_pk": null,
    "title": "LS-IQ: Implicit Reward Regularization for Inverse Reinforcement Learning",
    "abstract": "Recent methods for imitation learning directly learn a $Q$-function using an implicit reward formulation rather than an explicit reward function. However, these methods generally require implicit reward regularization to improve stability and often mistreat absorbing states. Previous works show that a squared norm regularization on the implicit reward function is effective, but do not provide a theoretical analysis of the resulting properties of the algorithms. In this work, we show that using this regularizer under a mixture distribution of the policy and the expert provides a particularly illuminating perspective: the original objective can be understood as squared Bellman error minimization, and the corresponding optimization problem minimizes a bounded $\\chi^2$-Divergence between the expert and the mixture distribution. This perspective allows us to address instabilities and properly treat absorbing states. We show that our method, Least Squares Inverse Q-Learning (LS-IQ), outperforms state-of-the-art algorithms, particularly in environments with absorbing states. Finally, we propose to use an inverse dynamics model to learn from observations only. Using this approach, we retain performance in settings where no expert actions are available.",
    "authors": [
      "Firas Al-Hafez",
      "Davide Tateo",
      "Oleg Arenz",
      "Guoping Zhao",
      "Jan Peters"
    ],
    "keywords": [
      "Inverse Reinforcement Learning",
      "Imitation Learning",
      "Reward Regularization",
      "Deep Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      5,
      6
    ],
    "real_confidences": [
      1,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Robust Policy Optimization in Deep Reinforcement Learning": {
    "paper_pk": null,
    "title": "Robust Policy Optimization in Deep Reinforcement Learning",
    "abstract": "Entropy can play an essential role in policy optimization by selecting the stochastic policy, which eventually helps better explore the environment in reinforcement learning (RL). A proper balance between exploration and exploitation is challenging and might depend on the particular RL task. However, the stochasticity often reduces as the training progresses; thus, the policy becomes less exploratory. Therefore, in many cases, the policy can converge to sub-optimal due to a lack of representative data during training. Moreover, this issue can even be severe in high-dimensional environments. This paper investigates whether keeping a certain entropy threshold throughout training can help better policy learning. In particular, we propose an algorithm Robust Policy Optimization (RPO), which leverages a perturbed Gaussian distribution to encourage high-entropy actions. We evaluated our methods on various continuous control tasks from DeepMind Control, OpenAI Gym, Pybullet, and IsaacGym. We observed that in many settings, RPO increases the policy entropy early in training and then maintains a certain level of entropy throughout the training period. Eventually, our agent RPO shows consistently improved performance compared to PPO and other techniques such as data augmentation and entropy regularization. Furthermore, in several settings, our method stays robust in performance, while other baseline mechanisms fail to improve and even worsen the performance.",
    "authors": [
      "Md Masudur Rahman",
      "Yexiang Xue"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Policy Optimization"
    ],
    "real_all_scores": [
      6,
      8,
      6
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes": {
    "paper_pk": null,
    "title": "Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes",
    "abstract": "Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an \"interpretable-by-design\" deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.\n",
    "authors": [
      "Eoin M. Kenny",
      "Mycal Tucker",
      "Julie Shah"
    ],
    "keywords": [
      "Interpretable Machine Learning",
      "Deep Reinforcement Learning",
      "Prototypes",
      "User Study"
    ],
    "real_all_scores": [
      6,
      6,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "ACQL: An Adaptive Conservative Q-Learning Framework for Offline Reinforcement Learning": {
    "paper_pk": null,
    "title": "ACQL: An Adaptive Conservative Q-Learning Framework for Offline Reinforcement Learning",
    "abstract": "Offline Reinforcement Learning (RL), which relies only on static datasets without additional interactions with the environment, provides an appealing alternative to learning a safe and promising control policy. Most existing offline RL methods did not consider relative data quality and only crudely constrained the distribution gap between the learned policy and the behavior policy in general. Moreover, these algorithms cannot adaptively control the conservative level in more fine-grained ways, like for each state-action pair, leading to a performance drop, especially over highly diversified datasets. In this paper, we propose an Adaptive Conservative Q-Learning (ACQL) framework that enables more flexible control over the conservative level of Q-function for offline RL. Specifically, we present two adaptive weight functions to shape the Q-values for collected and out-of-distribution data. Then we discuss different conditions under which the conservative level of the learned Q-function changes and define the monotonicity with respect to data quality and similarity. Motivated by the theoretical analysis, we propose a novel algorithm with the ACQL framework, using neural networks as the adaptive weight functions. To learn proper adaptive weight functions, we design surrogate losses incorporating the conditions for adjusting conservative levels and a contrastive loss to maintain the monotonicity of adaptive weight functions. We evaluate ACQL on the commonly-used D4RL benchmark and conduct extensive ablation studies to illustrate the effectiveness and state-of-the-art performance compared to existing offline DRL baselines.",
    "authors": [
      "Kun Wu",
      "Yinuo Zhao",
      "Zhiyuan Xu",
      "Zhengping Che",
      "Chengxiang Yin",
      "Chi Harold Liu",
      "Qinru Qiu",
      "Feifei Feng",
      "Jian Tang"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Offline Deep Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning": {
    "paper_pk": null,
    "title": "Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning",
    "abstract": "The Vision Transformer architecture has shown to be competitive in the computer vision (CV) space where it has dethroned convolution-based networks in several benchmarks. Nevertheless, Convolutional Neural Networks (CNN) remain the preferential architecture for the representation module in Reinforcement Learning. In this work, we study pretraining a Vision Transformer using several state-of-the-art self-supervised methods and assess data-efficiency gains from this training framework. We propose a new self-supervised learning method called TOV-VICReg that extends VICReg to better capture temporal relations between observations by adding a temporal order verification task. Furthermore, we evaluate the resultant encoders with Atari games in a sample-efficiency regime. Our results show that the vision transformer, when pretrained with TOV-VICReg, outperforms the other self-supervised methods but still struggles to overcome a CNN. Nevertheless, we were able to outperform a CNN in two of the ten games where we perform a 100k steps evaluation. Ultimately, we believe that such approaches in Deep Reinforcement Learning (DRL) might be the key to achieving new levels of performance as seen in natural language processing and computer vision.",
    "authors": [
      "Manuel Goul\u00e3o",
      "Arlindo L. Oliveira"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Transformers",
      "Self-Supervised Learning",
      "Pre-training"
    ],
    "real_all_scores": [
      6,
      3,
      3,
      5
    ],
    "real_confidences": [
      2,
      3,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch": {
    "paper_pk": null,
    "title": "RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch",
    "abstract": "Training deep reinforcement learning (DRL) models usually requires high computation costs. Therefore, compressing DRL models possesses immense potential for training acceleration and model deployment. However, existing methods that generate small models mainly adopt the knowledge distillation-based approach by iteratively training a dense network. As a result, the training process still demands massive computing resources. Indeed, sparse training from scratch in DRL has not been well explored and is particularly challenging due to non-stationarity in bootstrap training. In this work, we propose a novel sparse DRL training framework, \u201cthe Rigged Reinforcement Learning Lottery\u201d (RLx2), which builds upon gradient-based topology evolution and is capable of training a sparse DRL model based entirely on a sparse network. Specifically, RLx2 introduces a novel multi-step TD target mechanism with a dynamic-capacity replay buffer to achieve robust value learning and efficient topology exploration in sparse models. It also reaches state-of-the-art sparse training performance in several tasks, showing $7.5\\times$-$20\\times$ model compression with less than $3\\%$ performance degradation and up to $20\\times$ and $50\\times$ FLOPs reduction for training and inference, respectively.",
    "authors": [
      "Yiqin Tan",
      "Pihe Hu",
      "Ling Pan",
      "Jiatai Huang",
      "Longbo Huang"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Lottery Ticket Hypothesis",
      "Model Compression",
      "Value Learning"
    ],
    "real_all_scores": [
      3,
      6,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Improving Deep Policy Gradients with Value Function Search": {
    "paper_pk": null,
    "title": "Improving Deep Policy Gradients with Value Function Search",
    "abstract": "Deep Policy Gradient (PG) algorithms employ value networks to drive the learning of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. This paper focuses on improving value approximation and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the supervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains.",
    "authors": [
      "Enrico Marchesini",
      "Christopher Amato"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Deep Policy Gradients"
    ],
    "real_all_scores": [
      8,
      3,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Memory Gym: Partially Observable Challenges to Memory-Based Agents": {
    "paper_pk": null,
    "title": "Memory Gym: Partially Observable Challenges to Memory-Based Agents",
    "abstract": "Memory Gym is a novel benchmark for challenging Deep Reinforcement Learning agents to memorize events across long sequences, be robust to noise, and generalize. It consists of the partially observable 2D and discrete control environments Mortar Mayhem, Mystery Path, and Searing Spotlights. These environments are believed to be unsolvable by memory-less agents because they feature strong dependencies on memory and frequent agent-memory interactions. Empirical results based on Proximal Policy Optimization (PPO) and Gated Recurrent Unit (GRU) underline the strong memory dependency of the contributed environments. The hardness of these environments can be smoothly scaled, while different levels of difficulty (some of them unsolved yet) emerge for Mortar Mayhem and Mystery Path. Surprisingly, Searing Spotlights poses a tremendous challenge to GRU-PPO, which remains an open puzzle. Even though the\nrandomly moving spotlights reveal parts of the environment\u2019s ground truth, environmental ablations hint that these pose a severe perturbation to agents that leverage recurrent model architectures as their memory. \nSource Code: https://github.com/MarcoMeter/drl-memory-gym/",
    "authors": [
      "Marco Pleines",
      "Matthias Pallasch",
      "Frank Zimmer",
      "Mike Preuss"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Memory",
      "Benchmark",
      "Proximal Policy Optimization",
      "Gated Recurrent Unit",
      "HELM"
    ],
    "real_all_scores": [
      8,
      8,
      6
    ],
    "real_confidences": [
      2,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning": {
    "paper_pk": null,
    "title": "Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning",
    "abstract": "Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like Procgen, and provides a pathway towards more efficient progress.",
    "authors": [
      "Lakshmi Narasimhan Govindarajan",
      "Rex G Liu",
      "Drew Linsley",
      "Alekh Karkada Ashok",
      "Max Reuter",
      "Michael Frank",
      "Thomas Serre"
    ],
    "keywords": [
      "Cognitive Science",
      "Deep Reinforcement Learning",
      "Perceptual Grouping",
      "Neuroscience"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Robust Reinforcement Learning with Distributional Risk-averse formulation": {
    "paper_pk": null,
    "title": "Robust Reinforcement Learning with Distributional Risk-averse formulation",
    "abstract": "The purpose of robust reinforcement learning is to make predictions more robust to changes in the dynamics or rewards of the system. This problem is particularly important when dynamics and rewards of the environment are estimated from the data. However, without constraints, this problem is intractable. In this paper, we approximate the Robust Reinforcement Learning constrained with a $f$-divergence using an approximate Risk-Averse formulation. We show that the classical Reinforcement Learning formulation can be robustified using a standard deviation penalization of the objective. Two algorithms based on Distributional Reinforcement Learning, one for discrete and one for continuous action spaces, are proposed and tested in a classical Gym environment to demonstrate the robustness of the algorithms.",
    "authors": [
      "Pierre Clavier",
      "Stephanie Allassonniere",
      "Erwan Le Pennec"
    ],
    "keywords": [
      "Robust Reinforcement Learning",
      "Risk-Averse Reinforcement Learning",
      "Deep Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      3,
      3
    ],
    "real_confidences": [
      5,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Neural Discrete Reinforcement Learning": {
    "paper_pk": null,
    "title": "Neural Discrete Reinforcement Learning",
    "abstract": "Designing effective action spaces for complex environments is a fundamental and challenging problem in reinforcement learning (RL). \nSome recent works have revealed that naive RL algorithms utilizing well-designed handcrafted discrete action spaces can achieve promising results even when dealing with high-dimensional continuous or hybrid decision-making problems. However, elaborately designing such action spaces requires comprehensive domain knowledge.\nIn this paper, we systemically analyze the advantages of discretization for different action spaces and then propose a unified framework, Neural Discrete Reinforcement Learning (NDRL), to automatically learn how to effectively discretize almost arbitrary action spaces.\nSpecifically, we propose the Action Discretization Variational AutoEncoder (AD-VAE), an action representation learning method that can learn compact latent action spaces while maintain the essential properties of original environments, such as boundary actions and the relationship between different action dimensions. Moreover, we uncover a key issue that parallel optimization of the AD-VAE and online RL agents is often unstable. To address it, we further design several techniques to adapt RL agents to learned action representations, including latent action remapping and ensemble Q-learning. Quantitative experiments and visualization results demonstrate the efficiency and stability of our proposed framework for complex action spaces in various environments. ",
    "authors": [
      "Yazhe Niu",
      "Yuan Pu",
      "Chuming Li",
      "Zhenjie Yang",
      "Hongsheng Li",
      "Yu Liu"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Representation Learning",
      "Action Space"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Constrained Hierarchical Deep Reinforcement Learning with Differentiable Formal Specifications": {
    "paper_pk": null,
    "title": "Constrained Hierarchical Deep Reinforcement Learning with Differentiable Formal Specifications",
    "abstract": "Formal logic specifications are a useful tool to describe desired agent behavior and have been explored as a means to shape rewards in Deep Reinforcement Learning (DRL) systems over a variety of problems and domains. Prior work, however, has failed to consider the possibility of making these specifications differentiable, which would yield a more informative signal of the objective via the specification gradient. This paper examines precisely such an approach by exploring a Lagrangian method to constrain policy updates using a differentiable style of temporal logic specifications that associates logic formulae with real-valued quantitative semantics. This constrained learning mechanism is then used in a hierarchical setting where a high-level specification-guided neural network path planner works with a low-level control policy to navigate through planned waypoints. The effectiveness of our approach is demonstrated over four robot dynamics with five different types of Linear Temporal Logic (LTL) specifications. Our demo videos are collected at https://sites.google.com/view/schrl.",
    "authors": [
      "Zikang Xiong",
      "Joe Eappen",
      "Ahmed H Qureshi",
      "Suresh Jagannathan"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "Differentiable Formal Specification Language",
      "Robot Navigation",
      "Robot Planning and Control"
    ],
    "real_all_scores": [
      6,
      5,
      8,
      8,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      3,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Offline Reinforcement Learning with Closed-Form Policy Improvement Operators": {
    "paper_pk": null,
    "title": "Offline Reinforcement Learning with Closed-Form Policy Improvement Operators",
    "abstract": "Behavior constrained policy optimization has been demonstrated to be a successful paradigm for tackling Offline Reinforcement Learning. By exploiting historical transitions, a policy is trained to maximize a learned value function while constrained by the behavior policy to avoid a significant distributional shift. In this paper, we propose our closed-form policy improvement operators. We make a novel observation that the behavior constraint naturally motivates the use of first-order Taylor approximation, leading to a linear approximation of the policy objective. Additionally, as practical datasets are usually collected by heterogeneous policies, we model the behavior policies as a Gaussian Mixture and overcome the induced optimization difficulties by leveraging the LogSumExp's lower bound and Jensen's Inequality, giving rise to a closed-form policy improvement operator. We instantiate an offline RL algorithm with our novel policy improvement operator and empirically demonstrate its effectiveness over state-of-the-art algorithms on the standard D4RL benchmark.",
    "authors": [
      "Jiachen Li",
      "Edwin Zhang",
      "Ming Yin",
      "Qinxun Bai",
      "Yu-Xiang Wang",
      "William Yang Wang"
    ],
    "keywords": [
      "Offline Reinforcement Learning algorithms",
      "Deep Reinforcement Learning"
    ],
    "real_all_scores": [
      6,
      6,
      5
    ],
    "real_confidences": [
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Explainability of deep reinforcement learning algorithms in robotic domains by using Layer-wise Relevance Propagation": {
    "paper_pk": null,
    "title": "Explainability of deep reinforcement learning algorithms in robotic domains by using Layer-wise Relevance Propagation",
    "abstract": "A key component to the recent success of reinforcement learning is the introduction of neural networks for representation learning. Doing so allows for solving challenging problems in several domains, one of which is robotics. However, a major criticism of deep reinforcement learning (DRL) algorithms is their lack of explainability and interpretability. This problem is even exacerbated in robotics as they oftentimes cohabitate space with humans, making it imperative to be able to reason about their behaviour.\nIn this paper, we propose to analyze the learned representation in a robotic setting by utilizing graph neural networks. Using the graphical neural networks and Layer-wise Relevance Propagation (LRP), we represent the observations as an entity-relationship to allow us to interpret the learned policy. We evaluate our approach in two environments in MuJoCo. These two environments were delicately designed to effectively measure the value of knowledge gained by our approach to analyzing learned representations. This approach allows us to analyze not only how different parts of the observation space contribute to the decision-making process but also differentiate between policies and their differences in performance. This difference in performance also allows for reasoning about the agent's recovery from faults. These insights are key contributions to explainable deep reinforcement learning in robotic settings.",
    "authors": [
      "Mehran Taghian Jazi",
      "Shotaro Miwa",
      "Yoshihiro Mitsuka",
      "Johannes G\u00fcnther",
      "Osmar Zaiane"
    ],
    "keywords": [
      "Explainability",
      "Deep Reinforcement Learning",
      "Graph Network",
      "Layer-wise Relevance Propagation",
      "Robotic"
    ],
    "real_all_scores": [
      3,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection": {
    "paper_pk": null,
    "title": "Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection",
    "abstract": "The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity.  In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency.",
    "authors": [
      "Jiajun Fan",
      "Yuzheng Zhuang",
      "Yuecheng Liu",
      "Jianye HAO",
      "Bin Wang",
      "Jiangcheng Zhu",
      "Hao Wang",
      "Shu-Tao Xia"
    ],
    "keywords": [
      "Deep Reinforcement Learning",
      "The Arcade Learning Environment",
      "Human World Records",
      "Behavioral Control"
    ],
    "real_all_scores": [
      3,
      1,
      6,
      3
    ],
    "real_confidences": [
      5,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  }
}
