{
  "Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing": {
    "paper_pk": null,
    "title": "Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing",
    "abstract": "Most graph neural networks follow the message passing mechanism. However, it faces the over-smoothing problem when multiple times of message passing is applied to a graph, causing indistinguishable node representations and prevents the model to effectively learn dependencies between farther-away nodes. On the other hand, features of neighboring nodes with different labels are likely to be falsely mixed, resulting in the heterophily problem. In this work, we propose to order the messages passing into the node representation, with specific blocks of neurons targeted for message passing within specific hops. This is achieved by aligning the hierarchy of the rooted-tree of a central node with the ordered neurons in its node representation. Experimental results on an extensive set of datasets show that our model can simultaneously achieve the state-of-the-art in both homophily and heterophily settings, without any targeted design. Moreover, its performance maintains pretty well while the model becomes really deep, effectively preventing the over-smoothing problem. Finally, visualizing the gating vectors shows that our model learns to behave differently between homophily and heterophily settings, providing an explainable graph neural model.",
    "authors": [
      "Yunchong Song",
      "Chenghu Zhou",
      "Xinbing Wang",
      "Zhouhan Lin"
    ],
    "keywords": [
      "GNN",
      "heterophily",
      "over-smoothing"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      8
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization": {
    "paper_pk": null,
    "title": "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization",
    "abstract": "Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train a GNN on large graphs with over millions of nodes & billions of edges, which are prevalent in many graph-based applications such as social networks, recommender systems, and knowledge graphs. Traditional sampling-based methods accelerate GNN training by dropping edges and nodes, which impairs the graph integrity and model performance. Differently, distributed GNN algorithms accelerate GNN training by utilizing multiple computing devices and can be classified into two types: \"partition-based\" methods enjoy low communication cost but suffer from information loss due to dropped edges, while \"propagation-based\" methods avoid information loss but suffer from prohibitive communication overhead caused by neighbor explosion. To jointly address these problems, this paper proposes DIGEST (DIstributed Graph reprEsentation SynchronizaTion), a novel distributed GNN training framework that synergizes the complementary strength of both categories of existing methods. We propose to allow each device utilize the stale representations of its neighbors in other subgraphs during subgraph parallel training. This way, out method preserves global graph information from neighbors to avoid information loss and reduce the communication cost. Therefore, DIGEST is both computation-efficient and communication-efficient as it does not need to frequently (re-)compute and transfer the massive representation data across the devices, due to neighbor explosion. DIGEST provides synchronous and asynchronous training manners for homogeneous and heterogeneous training environment, respectively. We proved that the approximation error induced by the staleness of the representations can be upper-bounded. More importantly, our convergence analysis demonstrates that DIGEST enjoys the state-of-the-art convergence rate. Extensive experimental evaluation on large, real-world graph datasets shows that DIGEST achieves up to 21.82\u00d7 speedup without compromising the performance compared to state-of-the-art distributed GNN training frameworks",
    "authors": [
      "Zheng Chai",
      "Guangji Bai",
      "Liang Zhao",
      "Yue Cheng"
    ],
    "keywords": [
      "GNN",
      "Distributed training"
    ],
    "real_all_scores": [
      6,
      8,
      6,
      6
    ],
    "real_confidences": [
      5,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Learnable Graph Convolutional Attention Networks": {
    "paper_pk": null,
    "title": "Learnable Graph Convolutional Attention Networks",
    "abstract": "Existing Graph Neural Networks (GNNs) compute the message exchange between nodes by either aggregating uniformly (convolving) the features of all the neighbor- ing nodes, or by applying a non-uniform score (attending) to the features. Recent works have shown the strengths and weaknesses of the resulting GNN architectures, respectively, GCNs and GATs. In this work, we aim at exploiting the strengths of both approaches to their full extent. To this end, we first introduce the graph convolutional attention layer (CAT), which relies on convolutions to compute the attention scores. Unfortunately, as in the case of GCNs and GATs, we show that there exists no clear winner between the three\u2014neither theoretically nor in practice\u2014as their performance directly depends on the nature of the data (i.e., of the graph and features). This result brings us to the main contribution of our work, the learnable graph convolutional attention network (L-CAT): a GNN architecture that automatically interpolates between GCN, GAT and CAT in each layer, by adding only two scalar parameters. Our results demonstrate that L-CAT is able to efficiently combine different GNN layers along the network, outperforming competing methods in a wide range of datasets, and resulting in a more robust model that reduces the need of cross-validating.",
    "authors": [
      "Adri\u00e1n Javaloy",
      "Pablo Sanchez Martin",
      "Amit Levi",
      "Isabel Valera"
    ],
    "keywords": [
      "GNN",
      "GCN",
      "GAT"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "PREDICTION OF TOURISM FLOW WITH SPARSE DATA INCORPORATING TOURIST GEOLOCATIONS": {
    "paper_pk": null,
    "title": "PREDICTION OF TOURISM FLOW WITH SPARSE DATA INCORPORATING TOURIST GEOLOCATIONS",
    "abstract": "Modern tourism in the 21st century is facing numerous challenges. One of these\nchallenges is the rapidly growing number of tourists in space-limited regions such\nas historical city centers, museums, or geographical bottlenecks like narrow val-\nleys. In this context, a proper and accurate prediction of tourism volume and\ntourism flow within a certain area is important and critical for visitor management\ntasks such as sustainable treatment of the environment and prevention of over-\ncrowding. Static flow control methods like conventional low-level controllers or\nlimiting access to overcrowded venues could not solve the problem yet. In this\npaper, we empirically evaluate the performance of state-of-the-art deep-learning\nmethods such as RNNs, GNNs, and Transformers as well as the classic statistical\nARIMA method. Granular limited data supplied by a tourism region is extended\nby exogenous data such as geolocation trajectories of individual tourists, weather\nand holidays. In the field of visitor flow prediction with sparse data, we are thereby\ncapable of increasing the accuracy of our predictions, incorporating modern input\nfeature handling as well as mapping geolocation data on top of discrete POI data.",
    "authors": [
      "Julian Lemmel",
      "Zahra Babaiee",
      "Marvin Kleinlehner",
      "Ivan Majic",
      "Philipp Neubauer",
      "Johannes Scholz",
      "Radu Grosu",
      "Sophie Neubauer"
    ],
    "keywords": [
      "GNN",
      "RNN",
      "Transformer",
      "Tourism",
      "Tourism flow prediction"
    ],
    "real_all_scores": [
      6,
      5,
      3,
      8
    ],
    "real_confidences": [
      2,
      3,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "ReG-NAS: Graph Neural Network Architecture Search using Regression Proxy Task": {
    "paper_pk": null,
    "title": "ReG-NAS: Graph Neural Network Architecture Search using Regression Proxy Task",
    "abstract": "Neural Architecture Search (NAS) has become a focus that has been extensively researched in recent years. Innovative achievements are yielded from the area like convolutional neural networks (CNN), recurrent neural networks (RNN) and so on. However, research on NAS for graph neural networks (GNN) is still in a preliminary stage. Because of the special structure of graph data, some conclusions drew from CNN cannot be directly applied to GNN. At the same time, for NAS, the models' ranking stability is of great importance for it reflects the reliability of the NAS performance. Unfortunately, little research attention has been paid to it, making it a pitfall in the development of NAS research. In this paper, we proposed a novel NAS pipeline, ReG-NAS, which balances stability, reliability and time cost to search the best GNN architecture. Besides, for the first time, we systematically analyzed factors that will affect models' ranking stability in a given search space, which can be used as a guideline for subsequent studies. Our codes are available at https://anonymous.4open.science/r/ReG-NAS-4D21",
    "authors": [
      "Boyi Wei",
      "Cong Hao"
    ],
    "keywords": [
      "Neural Architecture Search",
      "GNN",
      "Machine Learning"
    ],
    "real_all_scores": [
      8,
      5,
      3
    ],
    "real_confidences": [
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Bi-Stride Multi-Scale Graph Neural Network for Mesh-Based Physical Simulation": {
    "paper_pk": null,
    "title": "Bi-Stride Multi-Scale Graph Neural Network for Mesh-Based Physical Simulation",
    "abstract": "Learning physical systems on unstructured meshes by flat Graph neural networks (GNNs) faces the challenge of modeling the long-range interactions due to the scaling complexity w.r.t. the number of nodes, limiting the generalization under mesh refinement. On regular grids, the convolutional neural networks (CNNs) with a U-net structure can resolve this challenge by efficient stride, pooling, and upsampling operations. Nonetheless, these tools are much less developed for graph neural networks (GNNs), especially when GNNs are employed for learning large-scale mesh-based physics. The challenges arise from the highly irregular meshes and the lack of effective ways to construct the multi-level structure without losing connectivity. Inspired by the bipartite graph determination algorithm, we introduce Bi-Stride Multi-Scale Graph Neural Network (BSMS-GNN) by proposing \\textit{bi-stride} as a simple pooling strategy for building the multi-level GNN. \\textit{Bi-stride} pools nodes by striding every other BFS frontier; it 1) works robustly on any challenging mesh in the wild, 2) avoids using a mesh generator at coarser levels, 3) avoids the spatial proximity for building coarser levels, and 4) uses non-parametrized aggregating/returning instead of MLPs during pooling and unpooling. Experiments show that our framework significantly outperforms the state-of-the-art method's computational efficiency in representative physics-based simulation cases.",
    "authors": [
      "Yadi Cao",
      "Menglei Chai",
      "Minchen Li",
      "Chenfanfu Jiang"
    ],
    "keywords": [
      "GNN",
      "physics-based simulation"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Vectorial Graph Convolutional Networks": {
    "paper_pk": null,
    "title": "Vectorial Graph Convolutional Networks",
    "abstract": "   Graph Convolutional Networks (GCN) have drawn considerable attention recently due to their outstanding performance in processing graph-structured data. However, GCNs still limited to the undirected graph because they theoretically require a symmetric matrix as the basis for the Laplacian transform. This causes the isotropic problem of the operator and reduced sensitivity in response to different information. In order to solve the problem, we generalize the spectral convolution operator to directed graphs by field extension, which improves the edge representations from scalars to vectors. Therefore, it brings in the concept of direction. That is to say, and even homogeneous information can become distinguishable by its differences in directions.In this paper, we propose the Vectorial Graph Convolutional Network(VecGCN) and the experimental evidence showing the advantages of a variety of directed graph node classification and link prediction tasks. ",
    "authors": [
      "ZhongYu Li",
      "Geng Zhao",
      "Hao Ning"
    ],
    "keywords": [
      "GNN",
      "GCN"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      3
    ],
    "real_confidences": [
      4,
      5,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Graph Contrastive Learning Under Heterophily: Utilizing Graph Filters to Generate Graph Views": {
    "paper_pk": null,
    "title": "Graph Contrastive Learning Under Heterophily: Utilizing Graph Filters to Generate Graph Views",
    "abstract": "Graph Neural Networks have achieved tremendous success in (semi-)supervised tasks for which task-specific node labels are available. However, obtaining labels is expensive in many domains, specially as the graphs grow larger in size. Hence, there has been a growing interest in the application of self-supervised techniques, in particular contrastive learning (CL), to graph data. In general, CL methods work by maximizing the agreement between encoded augmentations of the same example, and minimizing agreement between encoded augmentations of different examples. However, we show that existing graph CL methods perform very poorly on graphs with heterophily, in which connected nodes tend to belong to different classes. First, we show that this is attributed to the ineffectiveness of existing graph augmentation methods. Then, we leverage graph filters to directly generate augmented graph views for graph CL under heterophily. In particular, instead of explicitly augmenting the graph topology and encoding the augmentations, we use a high-pass filter in the encoder to generate node representations only based on high-frequency graph signals. Then, we contrast the high-pass filtered representations with their low-pass counterparts produced by the same encoder, to generate representations. Our experimental results confirm that our proposed method, HLCL, outperforms state-of-the-art CL methods on benchmark graphs with heterophily, by up to 10%.",
    "authors": [
      "Wenhan Yang",
      "Baharan Mirzasoleiman"
    ],
    "keywords": [
      "GNN",
      "Contrastive learning",
      "Heterophily",
      "Graph Representation Learning"
    ],
    "real_all_scores": [
      3,
      5,
      5,
      3
    ],
    "real_confidences": [
      5,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "FoSR: First-order spectral rewiring for addressing oversquashing in GNNs": {
    "paper_pk": null,
    "title": "FoSR: First-order spectral rewiring for addressing oversquashing in GNNs",
    "abstract": "Graph neural networks (GNNs) are able to leverage the structure of graph data by passing messages along the edges of the graph. While this allows GNNs to learn features depending on the graph structure, for certain graph topologies it leads to inefficient information propagation and a problem known as oversquashing. This has recently been linked with the curvature and spectral gap of the graph. On the other hand, adding edges to the message-passing graph can lead to increasingly similar node representations and a problem known as oversmoothing. We propose a computationally efficient algorithm that prevents oversquashing by systematically adding edges to the graph based on spectral expansion. We combine this with a relational architecture, which lets the GNN preserve the original graph structure and provably prevents oversmoothing. We find experimentally that our algorithm outperforms existing graph rewiring methods in several graph classification tasks.",
    "authors": [
      "Kedar Karhadkar",
      "Pradeep Kr. Banerjee",
      "Guido Montufar"
    ],
    "keywords": [
      "oversquashing",
      "oversmoothing",
      "graph rewiring",
      "graph neural networks",
      "GNN",
      "relational GNN",
      "spectral expansion"
    ],
    "real_all_scores": [
      3,
      3,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Agent-based Graph Neural Networks": {
    "paper_pk": null,
    "title": "Agent-based Graph Neural Networks",
    "abstract": "We present a novel graph neural network we call AgentNet, which is designed specifically for graph-level tasks. AgentNet is inspired by sublinear algorithms, featuring a computational complexity that is independent of the graph size. The architecture of AgentNet differs fundamentally from the architectures of traditional graph neural networks. In AgentNet, some trained \\textit{neural agents} intelligently walk the graph, and then collectively decide on the output. We provide an extensive theoretical analysis of AgentNet: We show that the agents can learn to systematically explore their neighborhood and that AgentNet can distinguish some structures that are even indistinguishable by 2-WL. Moreover, AgentNet is able to separate any two graphs which are sufficiently different in terms of subgraphs. We confirm these theoretical results with synthetic experiments on hard-to-distinguish graphs and real-world graph classification tasks. In both cases, we compare favorably not only to standard GNNs but also to computationally more expensive GNN extensions.",
    "authors": [
      "Karolis Martinkus",
      "P\u00e1l Andr\u00e1s Papp",
      "Benedikt Schesch",
      "Roger Wattenhofer"
    ],
    "keywords": [
      "Graph Neural Networks",
      "GNN",
      "Graph Classification",
      "Expressive Graph Neural Networks",
      "Sublinear algorithms"
    ],
    "real_all_scores": [
      6,
      8,
      6,
      8
    ],
    "real_confidences": [
      3,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks": {
    "paper_pk": null,
    "title": "DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks",
    "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure \u2013 GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its\u2019 reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.",
    "authors": [
      "Wenqian Li",
      "Yinchuan Li",
      "Zhigang Li",
      "Jianye HAO",
      "Yan Pang"
    ],
    "keywords": [
      "GNN",
      "Interpretability"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Graph Spline Networks for Efficient Continuous Simulation of Dynamical Systems": {
    "paper_pk": null,
    "title": "Graph Spline Networks for Efficient Continuous Simulation of Dynamical Systems",
    "abstract": "While complex simulations of physical systems have been widely studied in engineering and scientific computing, lowering their often prohibitive computational requirements has only recently been tackled by deep learning approaches. In this paper, we present GraphSplineNets, a novel deep learning approach to speed up simulation of physical systems with spatio-temporal continuous outputs by exploiting the synergy between graph neural networks (GNN) and orthogonal spline collocation (OSC). Two differentiable time-oriented OSC and spatial-oriented OSC are applied to bridge the gap between discrete GNN outputs and generate continuous solutions at any location in space and time without explicit prior knowledge of underlying differential equations. Moreover, we introduce an adaptive collocation strategy in space to enable the model to sample from the most important regions. Our model improves on widely used graph neural networks for physics simulation on both efficiency and solution accuracy. We demonstrate SplineGraphNets in predicting complex dynamical systems such as the heat equation, damped wave propagation and the Navier-Stokes equations for incompressible flow, where they improve accuracy of more than 25% while providing at least 60% speedup. ",
    "authors": [
      "Chuanbo Hua",
      "Federico Berto",
      "Michael Poli",
      "Stefano Massaroli",
      "Jinkyoo Park"
    ],
    "keywords": [
      "Graph",
      "Spline Collocation Method",
      "Graph Neural Networks",
      "Simulation",
      "Partial Differential Equations",
      "PDEs",
      "Physics",
      "Scientific Computing"
    ],
    "real_all_scores": [
      6,
      5,
      8,
      3
    ],
    "real_confidences": [
      5,
      3,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks": {
    "paper_pk": null,
    "title": "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks",
    "abstract": "Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation.",
    "authors": [
      "Xiaoqi Wang",
      "Han Wei Shen"
    ],
    "keywords": [
      "AI Interpretability",
      "Graph Neural Networks",
      "Model-Level Explanation of Neural Networks"
    ],
    "real_all_scores": [
      8,
      6,
      8,
      8
    ],
    "real_confidences": [
      4,
      3,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Topology Matters in Fair Graph Learning: a Theoretical Pilot Study": {
    "paper_pk": null,
    "title": "Topology Matters in Fair Graph Learning: a Theoretical Pilot Study",
    "abstract": "Recent advances in fair graph learning observe that graph neural networks (GNNs) further amplify prediction bias compared with multilayer perception (MLP), while the reason behind this is unknown. In this paper, we conduct a theoretical analysis of the bias amplification mechanism in GNNs. This is a challenging task since GNNs are difficult to be interpreted, and real-world networks are complex. To bridge the gap, we theoretically and experimentally demonstrate that aggregation operation in representative GNNs accumulates bias in node representation due to topology bias induced by graph topology. We provide a sufficient condition identifying the statistical information of graph data, so that graph aggregation enhances prediction bias in GNNs. \n Motivated by this data-centric finding, we propose a fair graph refinement algorithm, named \\textit{FairGR}, to rewire graph topology to reduce sensitive homophily coefficient while preserving useful graph topology. Experiments on node classification tasks demonstrate that \\textit{FairGR} can mitigate the prediction bias with comparable performance on three real-world datasets. Additionally, \\textit{FairGR} is compatible with many state-of-the-art methods, such as adding regularization, adversarial debiasing, and Fair mixup via refining graph topology. Therefore, \\textit{FairGR} is a plug-in fairness method and can be adapted to improve existing fair graph learning strategies. ",
    "authors": [
      "Zhimeng Jiang",
      "Xiaotian Han",
      "Chao Fan",
      "Zirui Liu",
      "Xiao Huang",
      "Na Zou",
      "Ali Mostafavi",
      "Xia Hu"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Fairness",
      "Topology"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      5
    ],
    "real_confidences": [
      3,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "CktGNN:  Circuit Graph Neural Network for Electronic Design Automation": {
    "paper_pk": null,
    "title": "CktGNN:  Circuit Graph Neural Network for Electronic Design Automation",
    "abstract": "The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have only been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves efficiency by reducing the number of subgraphs to perform message passing.\n\nNonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains $10$K distinct operational amplifiers with carefully-extracted circuit specifications from physical implementations. OCB also equips with communicative circuit generation and evaluation capabilities such that it can be used to generalize the applicability of CktGNN to design various analog circuits by efficiently producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and manual design from human experts. Our work paves the way toward a learning-based open-sourced design automation flow for analog circuits.",
    "authors": [
      "Zehao Dong",
      "Weidong Cao",
      "Muhan Zhang",
      "Dacheng Tao",
      "Yixin Chen",
      "Xuan Zhang"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Electronic Design Automation",
      "Benchmark Graph Dataset"
    ],
    "real_all_scores": [
      8,
      6,
      6,
      3
    ],
    "real_confidences": [
      4,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Faster Hyperparameter Search for GNNs via Calibrated Dataset Condensation": {
    "paper_pk": null,
    "title": "Faster Hyperparameter Search for GNNs via Calibrated Dataset Condensation",
    "abstract": "Dataset condensation aims to reduce the computational cost of training multiple models on a large dataset by condensing the training dataset into a small synthetic set. State-of-the-art approaches rely on matching the model gradients for the real and synthetic data and have recently been applied to condense large-scale graphs for node classification tasks. Although dataset condensation may be efficient when training multiple models for hyperparameter optimization, there is no theoretical guarantee on the generalizability of the condensed data: data condensation often generalizes poorly across hyperparameters/architectures in practice, while we find and prove this overfitting is much more severe on graphs. In this paper, we consider a different condensation objective specifically geared towards hyperparameter search. We aim to generate the synthetic dataset so that the validation-performance rankings of the models, with different hyperparameters, on the condensed and original datasets are comparable. We propose a novel hyperparameter-calibrated dataset condensation algorithm, which obtains the synthetic validation data by matching the hyperparameter gradients computed via implicit differentiation and efficient inverse Hessian approximation. HCDC employs a supernet with differentiable hyperparameters, making it suitable for modeling GNNs with widely different convolution filters. Experiments demonstrate that the proposed framework effectively maintains the validation-performance rankings of GNNs and speeds up hyperparameter/architecture search on graphs.",
    "authors": [
      "Mucong Ding",
      "Xiaoyu Liu",
      "Tahseen Rabbani",
      "Teresa Ranadive",
      "Tai-Ching Tuan",
      "Furong Huang"
    ],
    "keywords": [
      "Graph Condensation",
      "Dataset Condensation",
      "Hyperparameter Optimization",
      "Graph Neural Networks",
      "Graph Compression"
    ],
    "real_all_scores": [
      5,
      3,
      5,
      3
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "3D-IntPhys: Learning 3D Visual Intuitive Physics for Fluids, Rigid Bodies, and Granular Materials": {
    "paper_pk": null,
    "title": "3D-IntPhys: Learning 3D Visual Intuitive Physics for Fluids, Rigid Bodies, and Granular Materials",
    "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models purely from unlabeled images. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, in which we impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We evaluate the models on three challenging scenarios involving fluid, granular materials, and rigid objects, where standard detection and tracking methods are not applicable. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that, once trained, our model can achieve strong generalization in complex scenarios under extrapolate settings.",
    "authors": [
      "Haotian Xue",
      "Antonio Torralba",
      "Daniel LK Yamins",
      "Joshua B. Tenenbaum",
      "Yunzhu Li",
      "Hsiao-Yu Tung"
    ],
    "keywords": [
      "Visual Intuitive Physics",
      "Neural Implicit Representations",
      "Graph Neural Networks",
      "Learning-Based Dynamics Modeling",
      "Particle-Based Dynamics"
    ],
    "real_all_scores": [
      5,
      6,
      5
    ],
    "real_confidences": [
      4,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Graph Neural Networks as Multi-View Learning": {
    "paper_pk": null,
    "title": "Graph Neural Networks as Multi-View Learning",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated powerful representation capability in semi-supervised node classification. In this task, there are often three types of information  -- graph structure, node features, and node labels. Existing GNNs usually leverage both node features and graph structure by feature transformation and aggregation, following end-to-end training via node labels. In this paper, we change our perspective by considering these three types of information as three views of nodes. This perspective motivates us to design a new GNN framework as multi-view learning which enables alternating optimization training instead of end-to-end training,  resulting in significantly improved computation and memory efficiency. Extensive experiments with different settings demonstrate the effectiveness and efficiency of the proposed method. \n",
    "authors": [
      "Haoyu Han",
      "Xiaorui Liu",
      "Haitao Mao",
      "MohamadAli Torkamani",
      "Feng Shi",
      "Victor Lee",
      "Jiliang Tang"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Alternating Optimization",
      "Semi-Supervised Learning",
      "Multi-View Learning"
    ],
    "real_all_scores": [
      8,
      3,
      5,
      8
    ],
    "real_confidences": [
      3,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Interpretable Geometric Deep Learning via Learnable Randomness Injection": {
    "paper_pk": null,
    "title": "Interpretable Geometric Deep Learning via Learnable Randomness Injection",
    "abstract": "Point cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at https://github.com/Graph-COM/LRI.",
    "authors": [
      "Siqi Miao",
      "Yunan Luo",
      "Mia Liu",
      "Pan Li"
    ],
    "keywords": [
      "Geometric Deep Learning",
      "Interpretation",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      6,
      8,
      6,
      8
    ],
    "real_confidences": [
      5,
      4,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Fair Graph Message Passing with Transparency": {
    "paper_pk": null,
    "title": "Fair Graph Message Passing with Transparency",
    "abstract": "Recent advanced works achieve fair representations and predictions through regularization, adversarial debiasing, and contrastive learning in graph neural networks (GNNs). These methods \\textit{implicitly} encode the sensitive attribute information in the well-trained model weight via \\textit{backward propagation}. In practice, we not only pursue a fair machine learning model but also lend such fairness perception to the public. For current fairness methods,\nhow the sensitive attribute information usage makes the model achieve fair prediction still remains a black box. In this work, we first propose the concept \\textit{transparency} to describe \\textit{whether} the model embraces the ability of lending fairness perception to the public \\textit{or not}. Motivated by the fact that current fairness models lack of transparency, we aim to pursue a fair machine learning model with transparency via \\textit{explicitly} rendering sensitive attribute usage for fair prediction in \\textit{forward propagation} . Specifically, we develop an effective and transparent \\textsf{F}air \\textsf{M}essage \\textsf{P}assing (FMP) scheme adopting sensitive attribute information in forward propagation. In this way, FMP explicitly uncovers how sensitive attributes influence final prediction. Additionally, FMP scheme can aggregate useful information from neighbors and mitigate bias in a unified framework to simultaneously achieve graph smoothness and fairness objectives. An acceleration approach is also adopted to improve the efficiency of FMP. Experiments on node classification tasks demonstrate that the proposed FMP outperforms the state-of-the-art baselines in terms of fairness and accuracy on three real-world datasets. The code is available in {\\color{blue}\\url{https://anonymous.4open.science/r/FMP-AD84}}.",
    "authors": [
      "Zhimeng Jiang",
      "Xiaotian Han",
      "Chao Fan",
      "Zirui Liu",
      "Na Zou",
      "Ali Mostafavi",
      "Xia Hu"
    ],
    "keywords": [
      "Fairness",
      "Transparency",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Serving Graph Compression for Graph Neural Networks": {
    "paper_pk": null,
    "title": "Serving Graph Compression for Graph Neural Networks",
    "abstract": "Serving a GNN model online is challenging --- in many applications when testing nodes are connected to training nodes, one has to propagate information from training nodes to testing nodes to achieve the best performance, and storing the whole training set (including training graph and node features) during inference stage is prohibitive for large-scale problems. In this paper, we study graph compression to reduce the storage requirement for GNN in serving. Given a GNN model to be served, we propose to construct a compressed graph with  a smaller number of nodes. In serving time, one just needs to replace the original training set graph by this compressed graph, without the need of changing the actual GNN model and the forward pass. We carefully analyze the error in the forward pass and derive simple ways to construct the compressed graph to minimize the approximation error. Experimental results on semi-supervised node classification demonstrate that the proposed method can significantly reduce the serving space requirement for GNN inference.",
    "authors": [
      "Si Si",
      "Felix Yu",
      "Ankit Singh Rawat",
      "Cho-Jui Hsieh",
      "Sanjiv Kumar"
    ],
    "keywords": [
      "Model compression",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      8,
      8,
      5,
      6,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Double Wins: Boosting Accuracy and Efficiency of Graph Neural Networks by Reliable Knowledge Distillation": {
    "paper_pk": null,
    "title": "Double Wins: Boosting Accuracy and Efficiency of Graph Neural Networks by Reliable Knowledge Distillation",
    "abstract": "The recent breakthrough achieved by graph neural networks (GNNs) with few labeled data accelerates the pace of deploying GNNs on real-world applications. While several efforts have been made to scale GNNs training for large-scale graphs, GNNs still suffer from the scalability challenge of model inference, due to the graph dependency issue incurred by the message passing mechanism, therefore hindering its deployment in resource-constrained applications. A recent study~\\citep{zhang2021graph} revealed that GNNs can be compressed to inference-friendly multi-layer perceptrons (MLPs), by training MLPs using the soft labels of labeled and unlabeled nodes from the teacher. However, blindly leveraging the soft labels of all unlabeled nodes may be suboptimal, since the teacher model would inevitably make wrong predictions. This intriguing observation motivates us to ask: \\textit{Is it possible to train a stronger MLP student by making better use of the unlabeled data?} \n\nThis paper studies cross-model knowledge distillation - from GNN teacher to MLP student in a semi-supervised setting, showing their strong promise in achieving a ``sweet point'' in co-optimizing model accuracy and efficiency. Our proposed solution, dubbed \\textit{Reliable Knowledge Distillation for MLP optimization} (\\textbf{RKD-MLP}), is the first noise-aware knowledge distillation framework for GNNs distillation. Its core idea is to use a meta-policy to filter out those unreliable soft labels. To train the meta-policy, we design a reward-driven objective based on a meta-set and adopt policy gradient to optimize the expected reward. Then we apply the meta-policy to the unlabeled nodes and select the most reliable soft labels for distillation. Extensive experiments across various GNN backbones, on 7 small graphs and 2 large-scale datasets from the challenging Open Graph Benchmark, demonstrate the superiority of our proposal. Moreover, our RKD-MLP model shows good robustness w.r.t. graph topology and node feature noises. The code is available at \\url{https://anonymous.4open.science/r/RKD-MLP-F2A6/}.",
    "authors": [
      "Qiaoyu Tan",
      "Daochen Zha",
      "Soo-Hyun Choi",
      "Li Li",
      "Rui Chen",
      "Xia Hu"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Reliable Knowledge Distillation",
      "Model Inference Acceleration"
    ],
    "real_all_scores": [
      8,
      5,
      5,
      8
    ],
    "real_confidences": [
      5,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "In-distribution and Out-of-distribution Generalization for Graph Neural Networks": {
    "paper_pk": null,
    "title": "In-distribution and Out-of-distribution Generalization for Graph Neural Networks",
    "abstract": "Graph neural networks (GNNs) are models that allow learning with structured data of varying size. Despite their popularity, theoretical understanding of the generalization of GNNs is an under-explored topic. In this work, we expand the theoretical understanding of both in-distribution and out-of-distribution generalization of GNNs. Firstly, we improve upon the state-of-the-art PAC-Bayes (in-distribution) generalization bound primarily by reducing an exponential dependency on the node degree to a linear dependency. Secondly, utilizing tools from spectral graph theory, we prove some rigorous guarantees about the out-of-distribution (OOD) size generalization of GNNs, where graphs in the training set have different numbers of nodes and edges from those in the test set. To empirically verify our theoretical findings, we conduct experiments on both synthetic and real-world graph datasets. Our computed generalization gaps for the in-distribution case significantly improve the state-of-the-art PAC-Bayes results. For the OOD case, experiments on community classification tasks in large social networks show that GNNs achieve strong size generalization performance in cases guaranteed by our theory.",
    "authors": [
      "Emmanuel Sales",
      "Renjie Liao",
      "Nick Harvey"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Generalization Bounds",
      "Out-of-distribution generalization",
      "Learning theory"
    ],
    "real_all_scores": [
      6,
      3,
      6,
      3
    ],
    "real_confidences": [
      4,
      3,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "On the Expressive Power of Geometric Graph Neural Networks": {
    "paper_pk": null,
    "title": "On the Expressive Power of Geometric Graph Neural Networks",
    "abstract": "The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the lens of the Weisfeiler-Leman (WL) graph isomorphism test. Yet, many graphs arising in real-world applications come embedded in Euclidean space with an additional notion of geometric isomorphism, which is not covered by the WL framework. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutation, rotation, reflection, and translation. We use GWL to characterise the expressive power of GNNs that are invariant or equivariant to physical symmetries by studying the classes of geometric graphs that can or cannot be distinguished by these architectures. This allows us to formalise the advantages equivariant GNN layers have over their invariant counterparts in the Geometric Deep Learning blueprint. Finally, we connect our discrimination-based perspective with the universal approximation properties of geometric GNNs and prove they are two sides of the same coin.",
    "authors": [
      "Chaitanya K. Joshi",
      "Cristian Bodnar",
      "Simon V Mathis",
      "Taco Cohen",
      "Pietro Lio"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Geometric Deep Learning",
      "Equivariance",
      "Expressive Power",
      "Graph Isomorphism"
    ],
    "real_all_scores": [
      5,
      3,
      3
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Temporal Dynamics Aware Adversarial Attacks On Discrete-Time Graph Models": {
    "paper_pk": null,
    "title": "Temporal Dynamics Aware Adversarial Attacks On Discrete-Time Graph Models",
    "abstract": "Real-world graphs such as social networks, communication networks, and rating networks are constantly evolving over time. Many architectures have been developed to learn effective node representations using both graph structure and its dynamics. While the robustness of static graph models is well-studied, the vulnerability of the dynamic graph models to adversarial attacks is underexplored. In this work, we design a novel adversarial attack on discrete-time dynamic graph models where we desire to perturb the input graph sequence in a manner that preserves the temporal dynamics of the graph. To this end, we motivate a novel Temporal Dynamics-Aware Perturbation (TDAP) constraint, which ensures that perturbations introduced at each time step are restricted to only a small fraction of the number of changes in the graph since the previous time step. We present a theoretically-grounded Projected Gradient Descent approach for dynamic graphs to find the effective perturbations under the TDAP constraint. Experiments on two tasks \u2014 dynamic link prediction and node classification, show that our approach is up to 4x more effective than the baseline methods for attacking these models. We also consider the practical online setting where graph snapshots become available in real-time and extend our attack approach to use Online Gradient Descent for performing attacks under the TDAP constraint. In this more challenging setting, we demonstrate that our method achieves upto 5x superior performance when compared to representative baselines.",
    "authors": [
      "Kartik Sharma",
      "Rakshit Trivedi",
      "Rohit Sridhar",
      "Srijan Kumar"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Dynamic Graphs",
      "Adversarial Attacks",
      "Evolution-preserving"
    ],
    "real_all_scores": [
      5,
      6,
      3
    ],
    "real_confidences": [
      3,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Representation Power of Graph Convolutions : Neural Tangent Kernel Analysis": {
    "paper_pk": null,
    "title": "Representation Power of Graph Convolutions : Neural Tangent Kernel Analysis",
    "abstract": "The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution\u2019. Therefore, understanding its influence on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix $A$, defined as $D^{-1/2}AD^{\u22121/2}$, being the most widely adopted one, where $D$ is the degree matrix. However, some empirical studies show that row normalization $D^{\u22121}A$ outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolution operators, that could explain this behavior. In this work, we analyze the influence of the graph convolutions theoretically using Graph Neural Tangent Kernel in a semi-supervised node classification setting. Under a Degree Corrected Stochastic Block Model, we prove that: (i) row normalization preserves the underlying class structure better than other graph convolutions; (ii) performance degrades with network depth due to over-smoothing, but the loss in class information is the slowest in row normalization; (iii) skip connections retain the class information even at infinite depth, thereby eliminating over-smoothing. We finally validate our theoretical findings on real datasets.",
    "authors": [
      "Mahalakshmi Sabanayagam",
      "Pascal Esser",
      "Debarghya Ghoshdastidar"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Neural Tangent Kernels",
      "Node classification",
      "Stochastic Block Model"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      6
    ],
    "real_confidences": [
      3,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Latent Graph Inference using Product Manifolds": {
    "paper_pk": null,
    "title": "Latent Graph Inference using Product Manifolds",
    "abstract": "Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer similarity measures that are leveraged by the latent graph learning model to obtain optimized latent graphs. Moreover, the curvature of the product manifold is learned during training alongside the rest of the network parameters and based on the downstream task, rather than it being a static embedding space. Our novel approach is tested on a wide range of datasets, and outperforms the original dDGM model.",
    "authors": [
      "Haitz S\u00e1ez de Oc\u00e1riz Borde",
      "Anees Kazi",
      "Federico Barbero",
      "Pietro Lio"
    ],
    "keywords": [
      "Latent Graph Inference",
      "Product Manifolds",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      3,
      5,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Interactive Sequential Generative Models": {
    "paper_pk": null,
    "title": "Interactive Sequential Generative Models",
    "abstract": "Understanding spatiotemporal relationships among several agents is of considerable relevance for many domains. Team sports represent a particularly interesting real-world proving ground since modeling interacting athletes requires capturing highly dynamic and complex agent-agent dependencies in addition to temporal components. However, existing generative methods in this field either entangle all latent factors into a single variable and are thus constrained in practical applicability, or they focus on uncovering interaction structures, which restricts their generative ability. To address this gap, we propose a framework for multiagent trajectories that augments sequential generative models with latent social structures. First, we derive a novel objective via approximate inference using a disentangled latent space that accurately describes the data generating process in such systems. Based on the proposed training criterion, we then present a model architecture that unifies insights from neural interaction inference and graph-structured variational recurrent neural networks for generating collective movements while allocating latent information. We validate our model on data from professional soccer and basketball. Our framework not only improves upon existing state-of-the-art approaches in forecasting trajectories, but also infers semantically meaningful representations that can be used in downstream tasks.",
    "authors": [
      "Dennis Fassmeyer",
      "Pascal Fassmeyer",
      "Ulf Brefeld"
    ],
    "keywords": [
      "Generative Models and Autoencoders",
      "Graph Neural Networks",
      "Recurrent Networks",
      "Sequential Models",
      "Multi-Agent"
    ],
    "real_all_scores": [
      3,
      3,
      3
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Graph Neural Networks as Gradient Flows: understanding graph convolutions via energy": {
    "paper_pk": null,
    "title": "Graph Neural Networks as Gradient Flows: understanding graph convolutions via energy",
    "abstract": "Gradient flows are differential equations that minimize an energy functional and constitute the main descriptors of physical systems. We apply this formalism to Graph Neural Networks (GNNs) to develop new frameworks for learning on graphs as well as provide a better theoretical understanding of existing ones. We derive GNNs as a gradient flow equation of a parametric energy that provides a physics-inspired interpretation of GNNs as learning particle dynamics in the feature space. In particular, we show that in graph convolutional models (GCN), the positive/negative eigenvalues of the channel mixing matrix correspond to attractive/repulsive forces between adjacent features. We rigorously prove how the channel-mixing can learn to steer the dynamics towards low or high frequencies, which allows to deal with heterophilic graphs. We show that the same class of energies is decreasing along a larger family of GNNs; albeit not gradient flows, they retain their inductive bias. We experimentally evaluate an instance of the gradient flow framework that is principled, more efficient than GCN, and achieves competitive performance on graph datasets of varying homophily often outperforming recent baselines specifically designed to target heterophily.",
    "authors": [
      "Francesco Di Giovanni",
      "James Rowbottom",
      "Benjamin Paul Chamberlain",
      "Thomas Markovich",
      "Michael M. Bronstein"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Gradient flows",
      "Energy functionals",
      "Spectral theory"
    ],
    "real_all_scores": [
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Sampling-free Inference for Ab-Initio Potential Energy Surface Networks": {
    "paper_pk": null,
    "title": "Sampling-free Inference for Ab-Initio Potential Energy Surface Networks",
    "abstract": "Recently, it has been shown that neural networks not only approximate the ground-state wave functions of a single molecular system well but can also generalize to multiple geometries. While such generalization significantly speeds up training, each energy evaluation still requires Monte Carlo integration which limits the evaluation to a few geometries. In this work, we address the inference shortcomings by proposing the Potential learning from ab-initio Networks (PlaNet) framework, in which we simultaneously train a surrogate model in addition to the neural wave function. At inference time, the surrogate avoids expensive Monte-Carlo integration by directly estimating the energy, accelerating the process from hours to milliseconds. In this way, we can accurately model high-resolution multi-dimensional energy surfaces for larger systems that previously were unobtainable via neural wave functions. Finally, we explore an additional inductive bias by introducing physically-motivated restricted neural wave function models. We implement such a function with several additional improvements in the new PESNet++ model. In our experimental evaluation, PlaNet accelerates inference by 7 orders of magnitude for larger molecules like ethanol while preserving accuracy. Compared to previous energy surface networks, PESNet++ reduces energy errors by up to 74%.",
    "authors": [
      "Nicholas Gao",
      "Stephan G\u00fcnnemann"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Computational Physics",
      "Self-Generative Learning",
      "Machine Learning for Science",
      "Online Learning",
      "Self-Supervised Learning",
      "Molecules"
    ],
    "real_all_scores": [
      6,
      8,
      6,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "$\\omega$GNNs: Deep Graph Neural Networks Enhanced by Multiple Propagation Operators": {
    "paper_pk": null,
    "title": "$\\omega$GNNs: Deep Graph Neural Networks Enhanced by Multiple Propagation Operators",
    "abstract": "Graph Neural Networks (GNNs) are limited in their propagation operators. These operators often contain non-negative elements only and are shared across channels and layers, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs.\nIn this paper, we bridge this gap by incorporating trainable channel-wise weighting factors $\\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called $\\omega$GNN, and we study two variants: $\\omega$GCN and $\\omega$GAT.\nFor $\\omega$GCN, we theoretically analyse its behaviour and the impact of $\\omega$ on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth.\nAdditionally, we experiment with 15 real-world datasets on node- and graph-classification tasks, where our $\\omega$GCN and $\\omega$GAT perform better or on par with state-of-the-art methods. ",
    "authors": [
      "Moshe Eliasof",
      "Lars Ruthotto",
      "Eran Treister"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Deep Learning",
      "Graph Propagation Operators",
      "Over-Smoothing"
    ],
    "real_all_scores": [
      1,
      6,
      5,
      1
    ],
    "real_confidences": [
      5,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "ProtoGNN: Prototype-Assisted Message Passing Framework for Non-Homophilous Graphs": {
    "paper_pk": null,
    "title": "ProtoGNN: Prototype-Assisted Message Passing Framework for Non-Homophilous Graphs",
    "abstract": "Many well-known Graph Neural Network (GNN) models assume the underlying graphs are homophilous, where nodes share similar features and labels with their neighbours. They rely on message passing that iteratively aggregates neighbour's features and often suffer performance degradation on non-homophilous graphs where useful information is hardly available in the local neighbourhood. In addition, earlier studies show that in some cases GNNs are even outperformed by Multi-Layer Perceptron, indicating insufficient exploitation of node feature information. Motivated by the two limitations, we propose ProtoGNN, a novel message passing framework that augments existing GNNs by effectively combining node features with structural information. ProtoGNN learns multiple class prototypes for each class from raw node features with the slot-attention mechanism. These prototype representations are then transferred onto the structural node features with explicit message passing to all non-training nodes irrespective of distance. This form of message passing, from training nodes to class prototypes to non-training nodes, also serves as a shortcut that bypasses local graph neighbourhoods and captures global information. ProtoGNN is a generic framework which can be applied onto any of the existing GNN backbones to improve node representations when node features are strong and local graph information is scarce. We demonstrate through extensive experiments that ProtoGNN brings performance improvement to various GNN backbones and achieves state-of-the-art on several non-homophilous datasets.",
    "authors": [
      "Yanfei Dong",
      "Mohammed Haroon Dupty",
      "Lambert Deng",
      "Yong Liang Goh",
      "Wee Sun Lee"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Graph representation learning",
      "Non-homophilous Graph",
      "Heterophily",
      "Non-homophily",
      "Node Classification"
    ],
    "real_all_scores": [
      5,
      6,
      8,
      10
    ],
    "real_confidences": [
      5,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Better with Less: Data-Active Pre-training of Graph Neural Networks": {
    "paper_pk": null,
    "title": "Better with Less: Data-Active Pre-training of Graph Neural Networks",
    "abstract": "Recently, pre-training on graph neural networks (GNNs) has become an active research area and is used to learn transferable knowledge for downstream tasks with unlabeled data. The success of graph pre-training models is often attributed to the massive amount of input data. In this paper, however, we identify the curse of big data phenomenon in graph pre-training: more training samples and graph datasets do not necessarily lead to better performance. Motivated by this observation, we propose a better-with-less framework for graph pre-training: few, but carefully chosen data are fed into a GNN model to enhance pre-training. This novel pre-training pipeline is called the data-active graph pre-training (APT) framework, and is composed of a graph selector and a pre-training model. The graph selector chooses the most representative and instructive data points based on the inherent properties of graphs as well as the predictive uncertainty. The proposed uncertainty, as feedback from the pre-training model, measures the confidence level of the model to the data. When fed with the chosen data, on the other hand, the pre-training model grasps an initial understanding of the new, unseen data, and at the same time attempts to remember the knowledge learnt from the previous data. Therefore, the integration and interaction between these two components form a unified framework, in which graph pre-training is performed in a progressive way. Experiment results show that the proposed APT framework is able to obtain an efficient pre-training model with fewer training data and better downstream performance.",
    "authors": [
      "Jiarong Xu",
      "Renhong Huang",
      "XIN JIANG",
      "Yuxuan Cao",
      "Carl Yang",
      "Chunping Wang",
      "Yang Yang"
    ],
    "keywords": [
      "Pre-training",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      10
    ],
    "real_confidences": [
      5,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Global Explainability of GNNs via Logic Combination of Learned Concepts": {
    "paper_pk": null,
    "title": "Global Explainability of GNNs via Logic Combination of Learned Concepts",
    "abstract": "While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned.\nIn this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. \nContrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.",
    "authors": [
      "Steve Azzolin",
      "Antonio Longa",
      "Pietro Barbiero",
      "Pietro Lio",
      "Andrea Passerini"
    ],
    "keywords": [
      "Explainability",
      "Graph Neural Networks",
      "Concept Learning"
    ],
    "real_all_scores": [
      6,
      5,
      5
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Adversarial Causal Augmentation for Graph Covariate Shift": {
    "paper_pk": null,
    "title": "Adversarial Causal Augmentation for Graph Covariate Shift",
    "abstract": "Out-of-distribution (OOD) generalization on graphs is drawing widespread attention. However, existing efforts mainly focus on the OOD issue of correlation shift. While another type, covariate shift, remains largely unexplored but is the focus of this work. From a data generation view, causal features are stable substructures in data, which play key roles in OOD generalization. While their complementary parts, environments, are unstable features that often lead to various distribution shifts. Correlation shift establishes spurious statistical correlations between environments and labels. In contrast, covariate shift means that there exist unseen environmental features in test data. Existing strategies of graph invariant learning and data augmentation suffer from limited environments or unstable causal features, which greatly limits their generalization ability on covariate shift. In view of that, we propose a novel graph augmentation strategy: Adversarial Causal Augmentation (AdvCA), to alleviate the covariate shift. Specifically, it adversarially augments the data to explore diverse distributions of the environments. Meanwhile, it keeps the causal features invariant across diverse environments. It maintains the environmental diversity while ensuring the invariance of the causal features, thereby effectively alleviating the covariate shift. Extensive experimental results with in-depth analyses demonstrate that AdvCA can outperform 14 baselines on synthetic and real-world datasets with various covariate shifts.",
    "authors": [
      "Yongduo Sui",
      "Xiang Wang",
      "Jiancan Wu",
      "An Zhang",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "keywords": [
      "Graph Data Augmentation",
      "Graph Neural Networks",
      "Covariate Shift",
      "OOD Generalization"
    ],
    "real_all_scores": [
      3,
      5,
      3
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "UNREAL: Unlabeled Nodes Retrieval and Labeling for Heavily-imbalanced Node Classification": {
    "paper_pk": null,
    "title": "UNREAL: Unlabeled Nodes Retrieval and Labeling for Heavily-imbalanced Node Classification",
    "abstract": "Extremely skewed label distributions are common in real-world node classification tasks. If not dealt with appropriately, it significantly hurts the performance of GNNs on minority classes. Due to the practical importance, there have been a series of recent researches devoted to this challenge. Existing over-sampling techniques smooth the label distribution by generating ''fake'' minority nodes and synthesize their features and local topology, which largely ignore the rich information of unlabeled nodes on graphs. Recent methods based on loss function modification re-weight different samples or change classification margins, which achieve good performance. However, representative methods need label information to estimate the distance of each node to its class center, which is unavailable on unlabeled nodes.  In this paper, we propose UNREAL, which is an iterative over-sampling method. The first key difference is that we only add unlabeled nodes instead of synthetic nodes, which eliminates the challenge of feature and neighborhood generation. To select which unlabeled nodes to add, we propose geometric ranking, which ranks unlabeled nodes based on unsupervised learning results in the node embedding space. Finally, we identify the issue of geometric imbalance in the embedding space and provide a simple metric to filter out geometrically imbalanced nodes. Extensive experiments on real-world benchmark datasets are conducted, and the empirical results show that our method significantly outperforms current state-of-the-art methods consistent on different datasets with different imbalance ratios.",
    "authors": [
      "Divin Yan",
      "Shengzhong Zhang",
      "Bisheng Li",
      "min zhou",
      "Zengfeng Huang"
    ],
    "keywords": [
      "Node Classification",
      "Heavily-imbalanced Representation Learning",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      8,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "On Regularization for Explaining Graph Neural Networks: An Information Theory Perspective": {
    "paper_pk": null,
    "title": "On Regularization for Explaining Graph Neural Networks: An Information Theory Perspective",
    "abstract": "This work studies the explainability of graph neural networks (GNNs), which is important for the credibility of GNNs in practical usage. Existing work mostly follows the two-phase paradigm to interpret a prediction: feature attribution and selection. However, another important component --- regularization, which is crucial to facilitate the above paradigm --- has been seldom studied. In this work, we explore the role of regularization in GNNs explainability from the perspective of information theory. Our main findings are: 1) regularization is essentially pursuing the balance between two phases, 2) its optimal coefficient is proportional to the sparsity of explanations, 3) existing methods imply an implicit regularization effect of stochastic mechanism, and 4) its contradictory effects on two phases are responsible for the out-of-distribution (OOD) issue in post-hoc explainability. Based on these findings, we propose two common optimization methods, which can bolster the performance of the current explanation methods via sparsity-adaptive and OOD-resistant regularization schemes. Extensive empirical studies validate our findings and proposed methods. Code is available at  https://anonymous.4open.science/r/Rethink_Reg-07F0.\n",
    "authors": [
      "Junfeng Fang",
      "Wei Liu",
      "An Zhang",
      "Xiang Wang",
      "Xiangnan He",
      "Kun Wang",
      "Tat-Seng Chua"
    ],
    "keywords": [
      "Explainability",
      "Graph Neural Networks",
      "Regularization"
    ],
    "real_all_scores": [
      5,
      6,
      5,
      3
    ],
    "real_confidences": [
      4,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs": {
    "paper_pk": null,
    "title": "ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs",
    "abstract": "Solving combinatorial optimization (CO) on graphs has been attracting increasing interests from the machine learning community whereby data-driven approaches were recently devised to go beyond traditional manually-designated algorithms. In this paper, we study the robustness of a combinatorial solver as a blackbox regardless it is classic or learning-based though the latter can often be more interesting to the ML community. Specifically, we develop a practically feasible robustness metric for general CO solvers. A no-worse optimal cost guarantee is developed as such the optimal solutions are not required to achieve for solvers, and we tackle the non-differentiable challenge in input instance disturbance by resorting to black-box adversarial attack methods. Extensive experiments are conducted on 14 unique combinations of solvers and CO problems, and we demonstrate that the performance of state-of-the-art solvers like Gurobi can degenerate by over 20% under the given time limit bound on the hard instances discovered by our robustness metric, raising concerns about the robustness of combinatorial optimization solvers.",
    "authors": [
      "Han Lu",
      "Zenan Li",
      "Runzhong Wang",
      "Qibing Ren",
      "Xijun Li",
      "Mingxuan Yuan",
      "Jia Zeng",
      "Xiaokang Yang",
      "Junchi Yan"
    ],
    "keywords": [
      "Combinatorial Optimization",
      "Robustness",
      "Graph Neural Networks",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      5,
      6,
      6
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "TuneUp: A Training Strategy for Improving Generalization of Graph Neural Networks": {
    "paper_pk": null,
    "title": "TuneUp: A Training Strategy for Improving Generalization of Graph Neural Networks",
    "abstract": "Despite many advances in Graph Neural Networks (GNNs), their training strategies simply focus on minimizing a loss over nodes in a graph. However, such simplistic training strategies may be sub-optimal as they neglect that certain nodes are much harder to make accurate predictions on than others. Here we present TuneUp, a curriculum learning strategy for better training GNNs. Crucially, TuneUp trains a GNN in two stages. The first stage aims to produce a strong base GNN. Such base GNNs tend to perform well on head nodes (nodes with large degrees) but less so on tail nodes (nodes with small degrees). So, the second stage of TuneUp specifically focuses on improving prediction on tail nodes. Concretely, TuneUp synthesizes many additional supervised tail node data by dropping edges from head nodes and reusing the supervision on the original head nodes. TuneUp then minimizes the loss over the synthetic tail nodes to finetune the base GNN. TuneUp is a general training strategy that can be used with any GNN architecture and any loss, making TuneUp applicable to a wide range of prediction tasks. Extensive evaluation of TuneUp on two GNN architectures, three types of prediction tasks, and both inductive and transductive settings shows that TuneUp significantly improves the performance of the base GNN on tail nodes, while often even improving the performance on head nodes, which together leads up to 58.5% relative improvement in GNN predictive performance. Moreover, TuneUp significantly outperforms its variants without the two-stage curriculum learning, existing graph data augmentation techniques, as well as other specialized methods for tail nodes.",
    "authors": [
      "Weihua Hu",
      "Kaidi Cao",
      "Kexin Huang",
      "Edward W Huang",
      "Karthik Subbian",
      "Jure Leskovec"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Curriculum learning",
      "Tail nodes"
    ],
    "real_all_scores": [
      1,
      6,
      5,
      1
    ],
    "real_confidences": [
      4,
      2,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "AutoTransfer: AutoML with Knowledge Transfer - An Application to Graph Neural Networks": {
    "paper_pk": null,
    "title": "AutoTransfer: AutoML with Knowledge Transfer - An Application to Graph Neural Networks",
    "abstract": "AutoML has demonstrated remarkable success in finding an effective neural architecture for a given machine learning task defined by a specific dataset and an evaluation metric. However, most present AutoML techniques consider each task independently from scratch, which requires exploring many architectures, leading to high computational cost. Here we propose AutoTransfer, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest. Our key innovation includes a task-model bank that captures the model performance over a diverse set of GNN architectures and tasks, and a computationally efficient task embedding that can accurately measure the similarity among different tasks. Based on the task-model bank and the task embeddings, we estimate the design priors of desirable models of the novel task, by aggregating a similarity-weighted sum of the top-K design distributions on tasks that are similar to the task of interest. The computed design priors can be used with any AutoML search algorithm. We evaluate AutoTransfer on six datasets in the graph machine learning domain. Experiments demonstrate that (i) our proposed task embedding can be computed efficiently, and that tasks with similar embeddings have similar best-performing architectures; (ii) AutoTransfer significantly improves search efficiency with the transferred design priors, reducing the number of explored architectures by an order of magnitude. Finally, we release GNN-Bank-101, a large-scale dataset of detailed GNN training information of 120,000 task-model combinations to facilitate and inspire future research.",
    "authors": [
      "Kaidi Cao",
      "Jiaxuan You",
      "Jiaju Liu",
      "Jure Leskovec"
    ],
    "keywords": [
      "Graph Neural Networks",
      "AutoML",
      "Knowledge Transfer"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      1
    ],
    "real_confidences": [
      3,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "REDUCING OVERSMOOTHING IN GRAPH NEURAL NETWORKS BY CHANGING THE ACTIVATION FUNCTION": {
    "paper_pk": null,
    "title": "REDUCING OVERSMOOTHING IN GRAPH NEURAL NETWORKS BY CHANGING THE ACTIVATION FUNCTION",
    "abstract": "The performance of Graph Neural Networks (GNNs) deteriorates as the depth of the network increases. That performance drop is mainly attributed to oversmoothing, which leads to similar node representations through repeated graph convolutions. We show that in deep GNNs the activation function plays a crucial role in oversmoothing. We explain theoretically why this is the case and propose a simple modification to the slope of ReLU to reduce oversmoothing. The proposed approach enables deep architectures without the need to change the network architecture or to add residual connections. We verify the theoretical results experimentally and further show that deep networks, which do not suffer from oversmoothing are beneficial in the presence of the \u201ccold start\u201d problem, i.e. when there is no feature information about unlabeled nodes.",
    "authors": [
      "Dimitrios Kelesis",
      "Dimitrios Vogiatzis",
      "Georgios Katsimpras",
      "Dimitris Fotakis",
      "Georgios Paliouras"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Oversmoothing"
    ],
    "real_all_scores": [
      5,
      6,
      6,
      8
    ],
    "real_confidences": [
      3,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs": {
    "paper_pk": null,
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training.\n\nThis finding provides a new perspective for understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems including expressivity, generalization, over-smoothing and heterophily. As an initial step to analyze PMLP, we show its essential difference to MLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, through extrapolation analysis (i.e., generalization under distribution shifts), we find that though most GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extreme out-of-distribution data, they have greater potential to generalize to testing data near the training data support as natural advantages of the GNN architecture used for inference.",
    "authors": [
      "Chenxiao Yang",
      "Qitian Wu",
      "Jiahua Wang",
      "Junchi Yan"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Generalization"
    ],
    "real_all_scores": [
      8,
      6,
      8
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Pre-training via Denoising for Molecular Property Prediction": {
    "paper_pk": null,
    "title": "Pre-training via Denoising for Molecular Property Prediction",
    "abstract": "Many important problems involving molecular property prediction from 3D structures have limited data, posing a generalization challenge for neural networks. In this paper, we describe a pre-training technique based on denoising that achieves a new state-of-the-art in molecular property prediction by utilizing large datasets of 3D molecular structures at equilibrium to learn meaningful representations for downstream tasks. Relying on the well-known link between denoising autoencoders and score-matching, we show that the denoising objective corresponds to learning a molecular force field -- arising from approximating the Boltzmann distribution with a mixture of Gaussians -- directly from equilibrium structures. Our experiments demonstrate that using this pre-training objective significantly improves performance on multiple benchmarks, achieving a new state-of-the-art on the majority of targets in the widely used QM9 dataset. Our analysis then provides practical insights into the effects of different factors -- dataset sizes, model size and architecture, and the choice of upstream and downstream datasets -- on pre-training.",
    "authors": [
      "Sheheryar Zaidi",
      "Michael Schaarschmidt",
      "James Martens",
      "Hyunjik Kim",
      "Yee Whye Teh",
      "Alvaro Sanchez-Gonzalez",
      "Peter Battaglia",
      "Razvan Pascanu",
      "Jonathan Godwin"
    ],
    "keywords": [
      "Molecular Property Prediction",
      "Pre-training",
      "Graph Neural Networks",
      "Denoising",
      "Molecules"
    ],
    "real_all_scores": [
      6,
      5,
      6,
      6
    ],
    "real_confidences": [
      3,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Structural Privacy in Graphs": {
    "paper_pk": null,
    "title": "Structural Privacy in Graphs",
    "abstract": "Graph Neural Networks (GNNs) gained popularity to address the tasks over the graph-structured data that best represent many real-world systems. The privacy of the participants of these systems is at risk if the GNNs are not carefully designed. Existing works in privacy-preserving GNNs primarily ensure the privacy of features and labels of a node. In order to ensure complete privacy related to graph data, its structure also needs to be privatized. We provide a method SPGraph to privatize the graph structure by adding noise to the neighborhood data of the node. Our method addresses two challenges in introducing structural privacy in graphs. Applying randomization on the set of actual neighbors to introduce noise leads to a reduction in the degree of a node, which is undesirable. To overcome this first challenge, we introduce $\\lambda$-selector that samples nodes to be added to the set of neighbors. The second challenge is to denoise the neighborhood so that the noise added in the neighborhood does not significantly impact the accuracy. In this view, we use $p$-hop neighborhood to compensate for the loss of actual neighbors in the randomization. We continue to use the node and label privacy as implemented in the previous methods for privacy in GNNs. We conduct extensive experiments over real-world datasets to show the impact of perturbation in the graph structure. ",
    "authors": [
      "Rucha Bhalchandra Joshi",
      "Subhankar Mishra"
    ],
    "keywords": [
      "Privacy",
      "Graph Neural Networks",
      "Differential Privacy",
      "Graph Structure"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "$\\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks": {
    "paper_pk": null,
    "title": "$\\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks",
    "abstract": "As graph data size increases, the vast latency and memory consumption during inference pose a significant challenge to the real-world deployment of Graph Neural Networks (GNNs). While quantization is a powerful approach to reducing GNNs complexity, most previous works on GNNs quantization fail to exploit the unique characteristics of GNNs, suffering from severe accuracy degradation. Through an in-depth analysis of the topology of GNNs, we observe that the topology of the graph leads to significant differences between nodes, and most of the nodes in a graph appear to have a small aggregation value. Motivated by this, in this paper, we propose the Aggregation-Aware mixed-precision Quantization ($\\rm A^2Q$) for GNNs, where an appropriate bitwidth is automatically learned and assigned to each node in the graph. To mitigate the vanishing gradient problem caused by sparse connections between nodes, we propose a Local Gradient method to serve the quantization error of the node features as the supervision during training. We also develop a Nearest Neighbor Strategy to deal with the generalization on unseen graphs. Extensive experiments on eight public node-level and graph-level datasets demonstrate the generality and robustness of our proposed method. Compared to the FP32 models, our method can achieve up to $18.8\\times$ (i.e., 1.70bits) compression ratio with negligible accuracy degradation. Moreover, compared to the state-of-the-art quantization method, our method can achieve up to $11.4\\%$ and $9.5\\%$ accuracy improvements on the node-level and graph-level tasks, respectively, and up to $2\\times$ speedup on a dedicated hardware accelerator.",
    "authors": [
      "Zeyu Zhu",
      "Fanrong Li",
      "Zitao Mo",
      "Qinghao Hu",
      "Gang Li",
      "Zejian Liu",
      "Xiaoyao Liang",
      "Jian Cheng"
    ],
    "keywords": [
      "Graph Neural Networks",
      "MPNN framework",
      "Mixed-precision",
      "Quantization"
    ],
    "real_all_scores": [
      6,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Graph Neural Networks for Link Prediction with Subgraph Sketching": {
    "paper_pk": null,
    "title": "Graph Neural Networks for Link Prediction with Subgraph Sketching",
    "abstract": "Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs).  It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.",
    "authors": [
      "Benjamin Paul Chamberlain",
      "Sergey Shirobokov",
      "Emanuele Rossi",
      "Fabrizio Frasca",
      "Thomas Markovich",
      "Nils Yannick Hammerla",
      "Michael M. Bronstein",
      "Max Hansmire"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Link Prediction",
      "Data Sketching"
    ],
    "real_all_scores": [
      5,
      3,
      1
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Deformable Graph Transformer": {
    "paper_pk": null,
    "title": "Deformable Graph Transformer",
    "abstract": "Transformer-based models have recently shown success in representation learning on graph-structured data beyond natural language processing and computer vision. However, the success is limited to small-scale graphs due to the drawbacks of full dot-product attention on graphs such as the quadratic complexity with respect to the number of nodes and message aggregation from enormous irrelevant nodes. To address these issues, we propose Deformable Graph Transformer (DGT) that performs sparse attention via dynamically sampled relevant nodes for efficiently handling large-scale graphs with a linear complexity in the number of nodes. Specifically, our framework first constructs multiple node sequences with various criteria to consider both structural and semantic proximity. Then, combining with our learnable Katz Positional Encodings, the sparse attention is applied to the node sequences for learning node representations with a significantly reduced computational cost. Extensive experiments demonstrate that our DGT achieves state-of-the-art performance on 7 graph benchmark datasets with 2.5 \u223c 449 times less computational cost compared to transformer-based graph models with full attention.",
    "authors": [
      "Jinyoung Park",
      "Seongjun Yun",
      "Hyeonjin Park",
      "Jaewoo Kang",
      "Jisu Jeong",
      "Kyung-Min Kim",
      "Jung-Woo Ha",
      "Hyunwoo J. Kim"
    ],
    "keywords": [
      "Graph Transformer",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      8,
      5,
      5,
      5
    ],
    "real_confidences": [
      3,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Characterizing the Influence of Graph Elements": {
    "paper_pk": null,
    "title": "Characterizing the Influence of Graph Elements",
    "abstract": "Influence function, a method from the robust statistics, measures the changes of model parameters or some functions about model parameters with respect to the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need of expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph, and formulated an influence function to approximate the changes of model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of a SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to effectively guide the adversarial attacks on GCNs.",
    "authors": [
      "Zizhang Chen",
      "Peizhao Li",
      "Hongfu Liu",
      "Pengyu Hong"
    ],
    "keywords": [
      "Interpretable Machine Learning",
      "Influence functions",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      6,
      5,
      5,
      5
    ],
    "real_confidences": [
      3,
      3,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "GCINT: Dynamic Quantization Algorithm for Training Graph Convolution Neural Networks Using Only Integers": {
    "paper_pk": null,
    "title": "GCINT: Dynamic Quantization Algorithm for Training Graph Convolution Neural Networks Using Only Integers",
    "abstract": "Quantization approaches can minimize storage costs while decreasing the computational complexity of a model, although there is minimal study in the GNN field on quantization networks. We studied the four primary reasons why existing quantization approaches cannot be employed extensively with GNNs: (1)Quantifying the distinctions between data sources; (2)Quantifying the distinctions between data streams; (3)Quantifying the distinctions between concentrations; (4)QAT\u2019s Limitations. Based on this, we propose GCINT, which is an efficient quantization framework prepared for GNN training. The entire forward, backward, optimizer, and loss functions are calculated using integer data. We achieved a training acceleration ratio of nearly 10\u00d7 compared to FP32 Cuda Core in RTX 2080TI INT8 Tensor Core. Our quantization is independent of the dataset and weight distribution, and more than 2,000 randomized trials have been undertaken on the 8 popular GNN benchmark datasets, with all achieving errors within 1% of the FP32.",
    "authors": [
      "Qizhe Wu",
      "Letian Zhao",
      "Huawen Liang",
      "Xiaotian Wang",
      "LinFeng Tao",
      "Teng Tian",
      "Tingxin Wang",
      "Zerong He",
      "Wei Wu",
      "Xi Jin"
    ],
    "keywords": [
      "Quantization",
      "Graph Neural Networks",
      "Acceleration Training",
      "Integers Networks"
    ],
    "real_all_scores": [
      8,
      8,
      8,
      8
    ],
    "real_confidences": [
      2,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Relational Attention: Generalizing Transformers for Graph-Structured Tasks": {
    "paper_pk": null,
    "title": "Relational Attention: Generalizing Transformers for Graph-Structured Tasks",
    "abstract": "Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. As set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention's inherent ability to leverage the greater expressivity of graphs over sets.",
    "authors": [
      "Cameron Diao",
      "Ricky Loynd"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Transformers",
      "Graph Representation Learning",
      "Neural Algorithmic Reasoning"
    ],
    "real_all_scores": [
      6,
      6,
      5,
      8,
      3,
      6
    ],
    "real_confidences": [
      4,
      2,
      4,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "DEEPER-GXX: DEEPENING ARBITRARY GNNS": {
    "paper_pk": null,
    "title": "DEEPER-GXX: DEEPENING ARBITRARY GNNS",
    "abstract": "Recently, motivated by real applications, a major research direction in graph neural networks (GNNs) is to explore deeper structures.\nFor instance, the graph connectivity is not always consistent with the label distribution (e.g., the closest neighbors of some nodes are not from the same category). In this case, GNNs need to stack more layers, in order to find the same categorical neighbors in a longer path for capturing the class-discriminative information. However, two major problems hinder the deeper GNNs to obtain satisfactory performance, i.e., vanishing gradient and over-smoothing. On one hand, stacking layers makes the neural network hard to train as the gradients of the first few layers vanish. Moreover, when simply addressing vanishing gradient in GNNs, we discover the shading neighbors effect (i.e., stacking layers inappropriately distorts the non-IID information of graphs and degrade the performance of GNNs). On the other hand, deeper GNNs aggregate much more information from common neighbors such that individual node representations share more overlapping features, which makes the final output representations not discriminative (i.e., overly smoothed). In this paper, for the first time, we address both problems to enable deeper GNNs, and propose Deeper-GXX, which consists of the Weight-Decaying Graph Residual Connection module (WDG-ResNet) and Topology-Guided Graph Contrastive Loss (TGCL). Extensive experiments on real-world data sets demonstrate that Deeper-GXX outperforms state-of-the-art deeper baselines.",
    "authors": [
      "Lecheng Zheng",
      "Dongqi Fu",
      "Ross Maciejewski",
      "Jingrui He"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Deep Learning"
    ],
    "real_all_scores": [
      3,
      5,
      6
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Anisotropic Message Passing: Graph Neural Networks with Directional and Long-Range Interactions": {
    "paper_pk": null,
    "title": "Anisotropic Message Passing: Graph Neural Networks with Directional and Long-Range Interactions",
    "abstract": "Graph neural networks have shown great potential for the description of a variety of chemical systems.\nHowever, standard message passing does not explicitly account for long-range and directional interactions, for instance due to electrostatics.\nIn this work, an anisotropic state based on Cartesian multipoles is proposed as an addition to the existing hidden features.\nWith the anisotropic state, message passing can be modified to explicitly account for directional interactions.\nCompared to existing models, this modification results in relatively little additional computational cost. \nMost importantly, the proposed formalism offers as a distinct advantage the seamless integration of (1) anisotropic long-range interactions, (2) interactions with surrounding fields and particles that are not part of the graph, and (3) the fast multipole method.\nAs an exemplary use case, the application to quantum mechanics/molecular mechanics (QM/MM) systems is demonstrated.",
    "authors": [
      "Moritz Th\u00fcrlemann",
      "Sereina Riniker"
    ],
    "keywords": [
      "Message Passing",
      "Graph Neural Networks",
      "Directional",
      "Long-Range",
      "Equivariant",
      "Quantum Chemistry",
      "QM/MM"
    ],
    "real_all_scores": [
      5,
      3,
      3,
      6
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Neural Field Discovery Disentangles Equivariance in Interacting Dynamical Systems": {
    "paper_pk": null,
    "title": "Neural Field Discovery Disentangles Equivariance in Interacting Dynamical Systems",
    "abstract": "Systems of interacting objects often evolve under the influence of underlying field effects that govern their dynamics, \\emph{e.g.} electromagnetic fields in physics, or map topologies and traffic rules in traffic scenes. While the interactions between objects depend on local information, the underlying fields depend on global states. Pedestrians and vehicles in traffic scenes, for example, follow different traffic rules and social norms depending on their absolute geolocation. The entanglement of global and local effects makes recently popularized equivariant networks inapplicable, since they fail to capture global information. To address this, in this work, we propose to \\emph{disentangle} local object interactions --which are equivariant to global roto-translations and depend on relative positions and orientations-- from external global field effects --which depend on absolute positions and orientations. We theorize the presence of latent fields, which we aim to discover \\emph{without} directly observing them, but infer them instead from the dynamics alone. We propose neural fields to learn the latent fields, and model the interactions with equivariant graph networks operating in local coordinate frames. We combine the two components in a graph network that transforms field effects in local frames and operates solely there. Our experiments show that we can accurately discover the underlying fields in charged particles settings, traffic scenes, and gravitational n-body problems, and effectively use them to learn the system and forecast future trajectories.",
    "authors": [
      "Miltiadis Kofinas",
      "Erik J Bekkers",
      "Naveen Shankar Nagaraja",
      "Efstratios Gavves"
    ],
    "keywords": [
      "Interacting Dynamical systems",
      "Graph Neural Networks",
      "Neural Fields",
      "Equivariance"
    ],
    "real_all_scores": [
      8,
      5,
      3,
      5
    ],
    "real_confidences": [
      2,
      4,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Personalized Subgraph Federated Learning": {
    "paper_pk": null,
    "title": "Personalized Subgraph Federated Learning",
    "abstract": "In real-world scenarios, subgraphs of a larger global graph may be distributed across multiple devices or institutions, and only locally accessible due to privacy restrictions, although there may be links between them. Recently proposed subgraph Federated Learning (FL) methods deal with those missing links across private local subgraphs while distributively training Graph Neural Networks (GNNs) on them. However, they have overlooked the inevitable heterogeneity among subgraphs, caused by subgraphs comprising different communities of a global graph, therefore, consequently collapsing the incompatible knowledge from local GNN models trained on heterogeneous graph distributions. To overcome such a limitation, we introduce a new subgraph FL problem, personalized subgraph FL, which focuses on the joint improvement of the interrelated local GNN models rather than learning a single global GNN model, and propose a novel framework, FEDerated Personalized sUBgraph learning (FED-PUB), to tackle it. A crucial challenge in personalized subgraph FL is that the server does not know which subgraph each client has. FED-PUB thus utilizes functional embeddings of the local GNNs using random graphs as inputs to compute similarities between them, and use them to perform weighted averaging for server-side aggregation. Further, it learns a personalized sparse mask at each client to select and update only the subgraph-relevant subset of the aggregated parameters. We validate FED-PUB for its subgraph FL performance on six datasets, considering both non-overlapping and overlapping subgraphs, on which ours largely outperforms relevant baselines.",
    "authors": [
      "Jinheon Baek",
      "Wonyong Jeong",
      "Jiongdao Jin",
      "Jaehong Yoon",
      "Sung Ju Hwang"
    ],
    "keywords": [
      "Graph Representation Learning",
      "Graph Neural Networks",
      "Federated Learning",
      "Subgraph Federated Learning"
    ],
    "real_all_scores": [
      5,
      6,
      6
    ],
    "real_confidences": [
      2,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Towards predicting dynamic stability of power grids with Graph Neural Networks": {
    "paper_pk": null,
    "title": "Towards predicting dynamic stability of power grids with Graph Neural Networks",
    "abstract": "To mitigate climate change, the share of renewable energies in power production needs to be increased. Renewables introduce new challenges to power grids regarding the dynamic stability due to decentralization, reduced inertia and volatility in production. However, dynamic stability simulations are intractable and exceedingly expensive for large grids. Graph Neural Networks (GNNs) are a promising method to reduce the computational effort of analyzing dynamic stability of power grids. We provide new datasets of dynamic stability of synthetic power grids and find that GNNs are surprisingly effective at predicting highly non-linear targets from topological information only. We show that large GNNs outperform GNNs from previous work as well as as handcrafted graph features and semi-analytic approximations. Further, we demonstrate GNNs can accurately identify \\emph{trouble maker}-nodes in the power grids. Lastly, we show that GNNs trained on small grids can perform accurately on a large synthetic Texan power grid model, which illustrates the potential of our approach.",
    "authors": [
      "Christian Nauck",
      "Michael Lindner",
      "Konstantin Sch\u00fcrholt",
      "Frank Hellmann"
    ],
    "keywords": [
      "Power grids",
      "dynamic stability",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      3,
      1,
      1
    ],
    "real_confidences": [
      4,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Revisiting Embeddings for Graph Neural Networks": {
    "paper_pk": null,
    "title": "Revisiting Embeddings for Graph Neural Networks",
    "abstract": "Current graph representation learning techniques use Graph Neural Networks\n(GNNs) to extract features from dataset embeddings. In this work, we examine\nthe quality of these embeddings and assess how changing them can affect the ac-\ncuracy of GNNs. We explore different embedding extraction techniques for both\nimages and texts; and find that the choice of embedding biases the performance\nof different GNN architectures and thus the choice of embedding influences the\nselection of GNNs regardless of the underlying dataset. In addition, we only see\nan improvement in accuracy from some GNN models compared to the accuracy of\nmodels trained from scratch or fine-tuned on the underlying data without utilising\nthe graph connections. As an alternative, we propose Graph-connected Network\n(GraNet) layers to better leverage existing unconnected models within a GNN.\nExisting language and vision models are thus improved by allowing neighbour-\nhood aggregation. This gives a chance for the model to use pre-trained weights, if\npossible, and we demonstrate that this approach improves the accuracy compared\nto traditional GNNs: on Flickr v2, GraNet beats GAT2 and GraphSAGE by 7.7%\nand 1.7% respectively.",
    "authors": [
      "Skye Purchase",
      "Yiren Zhao",
      "Robert D. Mullins"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Embeddings",
      "Graph Attention",
      "Large Pretrained Models",
      "Transfer Learning"
    ],
    "real_all_scores": [
      3,
      3,
      5,
      5
    ],
    "real_confidences": [
      4,
      5,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "MotifExplainer: a Motif-based Graph Neural Network Explainer": {
    "paper_pk": null,
    "title": "MotifExplainer: a Motif-based Graph Neural Network Explainer",
    "abstract": "We consider the explanation problem of Graph Neural Networks (GNNs). Most existing GNN explanation methods identify the most important edges or nodes but fail to consider substructures, which are more important for graph data. One method considering subgraphs tries to search all possible subgraphs and identifies the most significant ones. However, the subgraphs identified may not be recurrent or statistically important for interpretation. This work proposes a novel method, named MotifExplainer, to explain GNNs by identifying important motifs, which are recurrent and statistically significant patterns in graphs. Our proposed motif-based methods can provide better human-understandable explanations than methods based on nodes, edges, and regular subgraphs. Given an instance graph and a pre-trained GNN model, our method first extracts motifs in the graph using domain-specific motif extraction rules. Then, a motif embedding is encoded by feeding motifs into the pre-trained GNN. Finally, we employ an attention-based method to identify the most influential motifs as explanations for the prediction results. The empirical studies on both synthetic and real-world datasets demonstrate the effectiveness of our method.",
    "authors": [
      "Zhaoning Yu",
      "Hongyang Gao"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Explainer",
      "Motif"
    ],
    "real_all_scores": [
      3,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning Symbolic Models for Graph-structured Physical Mechanism": {
    "paper_pk": null,
    "title": "Learning Symbolic Models for Graph-structured Physical Mechanism",
    "abstract": "Graph-structured physical mechanisms are ubiquitous in real-world scenarios, thus revealing underneath formulas is of great importance for scientific discovery. However, classical symbolic regression methods fail on this task since they can only handle input-output pairs that are not graph-structured. In this paper, we propose a new approach that generalizes symbolic regression to graph-structured physical mechanisms. The essence of our method is to model the formula skeleton with a message-passing flow, which helps transform the discovery of the skeleton into the search for the message-passing flow. Such a transformation guarantees that we are able to search a message-passing flow, which is efficient and Pareto-optimal in terms of both accuracy and simplicity. Subsequently, the underneath formulas can be identified by interpreting component functions of the searched message-passing flow, reusing classical symbolic regression methods. We conduct extensive experiments on datasets from different physical domains, including mechanics, electricity, and thermology, and on real-world datasets of pedestrian dynamics without ground-truth formulas. The experimental results not only verify the rationale of our design but also demonstrate that the proposed method can automatically learn precise and interpretable formulas for graph-structured physical mechanisms. ",
    "authors": [
      "Hongzhi Shi",
      "Jingtao Ding",
      "Yufan Cao",
      "quanming yao",
      "Li Liu",
      "Yong Li"
    ],
    "keywords": [
      "Symbolic Regression",
      "Graph Neural Networks",
      "Physical Mechanism",
      "Message-Passing Flow"
    ],
    "real_all_scores": [
      3,
      3,
      3,
      3
    ],
    "real_confidences": [
      3,
      3,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Link Prediction without Graph Neural Networks": {
    "paper_pk": null,
    "title": "Link Prediction without Graph Neural Networks",
    "abstract": "Link prediction, which consists of predicting edges based on graph features, is a fundamental task in many graph applications. As for several related problems, Graph Neural Networks (GNNs), which are based on an attribute-centric message-passing paradigm, have become the predominant framework for link prediction. GNNs have consistently outperformed traditional topology-based heuristics, but what contributes to their performance? Are there simpler approaches that achieve comparable or better results? To answer these questions, we first identify important limitations in how GNN-based link prediction methods handle the intrinsic class imbalance of the problem---due to the graph sparsity---in their training and evaluation. Moreover, we propose Gelato, a novel topology-centric framework that applies a topological heuristic to a graph enhanced by attribute information via graph learning. Our model is trained end-to-end with an N-pair loss on an unbiased training set to address class imbalance. Experiments show that Gelato is 145% more accurate, trains 11 times faster, infers 6,000 times faster, and has less than half of the trainable parameters compared to state-of-the-art GNNs for link prediction.",
    "authors": [
      "Zexi Huang",
      "Mert Kosan",
      "Arlei Lopes da Silva",
      "Ambuj Singh"
    ],
    "keywords": [
      "Link Prediction",
      "Graph Neural Networks",
      "Graph Learning",
      "Topological Heuristics"
    ],
    "real_all_scores": [
      3,
      5,
      6
    ],
    "real_confidences": [
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Graph Neural Bandits": {
    "paper_pk": null,
    "title": "Graph Neural Bandits",
    "abstract": "Contextual bandits aim to choose the optimal arm with the highest reward out of a set of candidates based on their contextual information, and various bandit algorithms have been applied to personalized recommendation due to their ability of solving the exploitation-exploration dilemma. Motivated by online recommendation scenarios, in this paper, we propose a framework named Graph Neural Bandits (GNB) to leverage the collaborative nature among users empowered by graph neural networks (GNNs). Instead of estimating rigid user clusters, we model the \"fine-grained'' collaborative effects through estimated user graphs in terms of exploitation and exploration individually. Then, to refine the recommendation strategy, we utilize separate GNN-based models on estimated user graphs for exploitation and adaptive exploration. Theoretical analysis and experimental results on multiple real data sets in comparison with state-of-the-art baselines are provided to demonstrate the effectiveness of our proposed framework.",
    "authors": [
      "Yunzhe Qi",
      "Yikun Ban",
      "Jingrui He"
    ],
    "keywords": [
      "Contextual Bandits",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "GraphPNAS: Learning Distribution of Good Neural Architectures via Deep Graph Generative Models": {
    "paper_pk": null,
    "title": "GraphPNAS: Learning Distribution of Good Neural Architectures via Deep Graph Generative Models",
    "abstract": "Neural architectures can be naturally viewed as computational graphs. Motivated by this perspective, we, in this paper, study neural architecture search (NAS) through the lens of learning random graph models. In contrast to existing NAS methods which largely focus on searching for a single best architecture, i.e, point estimation, we propose GraphPNAS a deep graph generative model that learns a distribution of well-performing architectures. Relying on graph neural networks (GNNs), our GraphPNAS can better capture topologies of good neural architectures and relations between operators therein. Moreover, our graph generator leads to a learnable probabilistic search method that is more flexible and efficient than the commonly used RNN generator and random search methods. Finally, we learn our generator via an efficient reinforcement learning formulation for NAS. To assess the effectiveness of our GraphPNAS, we conduct extensive experiments on three search spaces, including the challenging RandWire on TinyImageNet, ENAS on CIFAR10, and NAS-Bench-101. The complexity of RandWire is significantly larger than other search spaces in the literature. We show that our proposed graph generator consistently outperforms RNN-based one and achieves better or comparable performances than state-of-the-art NAS methods. ",
    "authors": [
      "Muchen Li",
      "Jeffrey Yunfan Liu",
      "Leonid Sigal",
      "Renjie Liao"
    ],
    "keywords": [
      "Neural Architecture Search",
      "Deep Generative Models of Graphs",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      5,
      3,
      6
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Rethinking the Explanation of Graph Neural Network via Non-parametric Subgraph Matching": {
    "paper_pk": null,
    "title": "Rethinking the Explanation of Graph Neural Network via Non-parametric Subgraph Matching",
    "abstract": "The great success in graph neural networks (GNNs) provokes the question about explainability: ``Which fraction of the input graph is the most determinant to the prediction?'' However, current approaches usually resort to a black-box to decipher another black-box (i.e., GNN), making it difficult to understand how the explanation is made. Based on the observation that graphs typically share some joint motif patterns, we propose a novel subgraph matching framework named MatchExplainer to explore explanatory subgraphs. \nIt couples the target graph with other counterpart instances and identifies the most crucial joint substructure by minimizing the node corresponding-based distance between them. After that, an external graph ranking is followed to select the most informative substructure from all subgraph candidates. Thus, MatchExplainer is entirely non-parametric. \nMoreover, present graph sampling or node dropping methods usually suffer from the false positive sampling problem. To ameliorate that issue, we take advantage of MatchExplainer to fix the most informative portion of the graph and merely operate graph augmentations on the rest less informative part, which is dubbed as MatchDrop. \nWe conduct extensive experiments on both synthetic and real-world datasets, showing the effectiveness of our MatchExplainer by outperforming all parametric baselines with large margins. Additional results also demonstrate that our MatchDrop is a general paradigm to be equipped with GNNs for enhanced performance.",
    "authors": [
      "Fang Wu",
      "Lirong Wu",
      "Siyuan Li",
      "Dragomir Radev",
      "Wenbing Huang"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Graph Matching",
      "Explanation"
    ],
    "real_all_scores": [
      5,
      6,
      5,
      6
    ],
    "real_confidences": [
      3,
      2,
      3,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Rethinking the Expressive Power of GNNs via Graph Biconnectivity": {
    "paper_pk": null,
    "title": "Rethinking the Expressive Power of GNNs via Graph Biconnectivity",
    "abstract": "Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs with respect to the Weisfeiler-Lehman (WL) test, for most of them, there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.",
    "authors": [
      "Bohang Zhang",
      "Shengjie Luo",
      "Liwei Wang",
      "Di He"
    ],
    "keywords": [
      "Graph Neural Networks",
      "Expressive Power",
      "Weisfeiler-Lehman test",
      "Graph Transformer",
      "Biconnectivity"
    ],
    "real_all_scores": [
      3,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Limitless Stability for Graph Convolutional Networks ": {
    "paper_pk": null,
    "title": "Limitless Stability for Graph Convolutional Networks ",
    "abstract": "This work establishes rigorous, novel and widely applicable stability guarantees and transferability bounds for general graph convolutional networks  -- without reference to any underlying limit object or statistical distribution. Crucially, utilized graph-shift operators are not necessarily assumed to be normal, allowing for the treatment of networks on both directed- and undirected graphs within the developed framework. In the undirected setting, stability to node-level perturbations is related to an 'adequate spectral covering' property of the filters in each layer. Stability to edge-level perturbations is discussed and related to properties of the utilized filters such as their Lipschitz constants. Results on stability to  vertex-set non-preserving perturbations are obtained by utilizing recently developed mathematical-physics based tools. As an exemplifying application of the developed theory, it is showcased that\ngeneral graph convolutional networks utilizing the un-normalized graph Laplacian as graph-shift-operator  can be rendered stable to collapsing strong edges in the underlying graph if filters are mandated to be constant at infinity. These theoretical results are supported by corresponding numerical investigations showcasing the response of filters and networks to such perturbations. ",
    "authors": [
      "Christian Koke"
    ],
    "keywords": [
      "Graph Convolutional Networks",
      "Graph Neural Networks",
      "Stability",
      "Transferability",
      "Spectral Graph Theory",
      "Rigorous Proofs"
    ],
    "real_all_scores": [
      5,
      8,
      3,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Efficient Automatic Machine Learning via Design Graphs": {
    "paper_pk": null,
    "title": "Efficient Automatic Machine Learning via Design Graphs",
    "abstract": "Despite the success of automated machine learning (AutoML), which aims to find the best design, including the architecture of deep networks and hyper-parameters, conventional AutoML methods are computationally expensive and hardly provide insights into the relations of different model design choices. To tackle the challenges, we propose FALCON, an efficient sample-based method to search for the optimal model design. Our key insight is to model the design space of possible model designs as a design graph, where the nodes represent design choices, and the edges denote design similarities. FALCON features 1) a task-agnostic module, which performs message passing on the design graph via a Graph Neural Network (GNN), and 2) a task-specific module, which conducts label propagation of the known model performance information on the design graph. Both modules are combined to predict the design performances in the design space, navigating the search direction. We conduct extensive experiments on 27 node and graph classification tasks from various application domains, and an image classification task on the CIFAR-10 dataset. We empirically show that FALCON can efficiently obtain the well-performing designs for each task using only 30 explored nodes. Specifically, FALCON has a comparable time cost with the one-shot approaches while achieving an average improvement of 3.3% compared with the best baselines.",
    "authors": [
      "Ying-Xin Wu",
      "Jiaxuan You",
      "Jure Leskovec",
      "Zhitao Ying"
    ],
    "keywords": [
      "Automated Machine Learning",
      "Sample efficiency",
      "Design graph",
      "Graph Neural Networks"
    ],
    "real_all_scores": [
      5,
      6,
      8
    ],
    "real_confidences": [
      5,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Spatiotemporal Modeling of Multivariate Signals with Graph Neural Networks and Structured State Space Models": {
    "paper_pk": null,
    "title": "Spatiotemporal Modeling of Multivariate Signals with Graph Neural Networks and Structured State Space Models",
    "abstract": "Multivariate signals are prevalent in various domains, such as healthcare, transportation systems, and space sciences. Modeling spatiotemporal dependencies in multivariate signals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between sensors. To address these challenges, we propose representing multivariate signals as graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that captures both spatial and temporal dependencies in multivariate signals. Specifically, (1) we leverage Structured State Spaces model (S4), a state-of-the-art sequence model, to capture long-term temporal dependencies and (2) we propose a graph structure learning layer in GraphS4mer to automatically learn the underlying graph structures in the data.  We evaluate our proposed model on three distinct tasks and show that GraphS4mer consistently improves over existing models, including (1) seizure detection from electroencephalography signals, outperforming a previous GNN with self-supervised pretraining by 3.1 points in AUROC; (2) sleep staging from polysomnography signals, a 4.1 points improvement in macro-F1 score compared to existing sleep staging models; and (3) traffic forecasting, reducing MAE by 8.8% compared to existing GNNs and by 1.4% compared to transformer-based models.",
    "authors": [
      "Siyi Tang",
      "Jared Dunnmon",
      "Liangqiong Qu",
      "Khaled Kamal Saab",
      "Christopher Lee-Messer",
      "Daniel Rubin"
    ],
    "keywords": [
      "Multivariate Signals",
      "Graph Neural Network",
      "Graph Structure Learning",
      "Structured State Spaces",
      "Time Series"
    ],
    "real_all_scores": [
      3,
      1,
      3
    ],
    "real_confidences": [
      5,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "$\\mathcal{O}$-GNN: incorporating ring priors into molecular modeling": {
    "paper_pk": null,
    "title": "$\\mathcal{O}$-GNN: incorporating ring priors into molecular modeling",
    "abstract": "Cyclic compounds that contain at least one ring play an important role in drug design. Despite the recent success of molecular modeling with graph neural networks (GNNs), few models explicitly take rings in compounds into consideration, consequently limiting the expressiveness of the models. In this work, we design a new variant of GNN, ring-enhanced GNN ($\\mathcal{O}$-GNN), that explicitly models rings in addition to atoms and bonds in compounds. In $\\mathcal{O}$-GNN,  each ring is represented by a latent vector, which contributes to and is iteratively updated by atom and bond representations. Theoretical analysis shows that $\\mathcal{O}$-GNN is able to distinguish two isomorphic subgraphs lying on different rings using only one layer while conventional graph convolutional neural networks require multiple layers to distinguish, demonstrating that $\\mathcal{O}$-GNN is more expressive. Through experiments, $\\mathcal{O}$-GNN shows good performance on $\\bf{11}$ public datasets. In particular, it achieves state-of-the-art validation result on the PCQM4Mv1 benchmark (outperforming the previous KDDCup champion solution) and the drug-drug interaction prediction task on DrugBank. Furthermore, $\\mathcal{O}$-GNN outperforms strong baselines (without modeling rings) on the molecular property prediction and retrosynthesis prediction tasks.",
    "authors": [
      "Jinhua Zhu",
      "Kehan Wu",
      "Bohan Wang",
      "Yingce Xia",
      "Shufang Xie",
      "Qi Meng",
      "Lijun Wu",
      "Tao Qin",
      "Wengang Zhou",
      "Houqiang Li",
      "Tie-Yan Liu"
    ],
    "keywords": [
      "Graph Neural Network",
      "Ring",
      "Molecular Modeling"
    ],
    "real_all_scores": [
      8,
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Merging Models Pre-Trained on Different Features with Consensus Graph": {
    "paper_pk": null,
    "title": "Merging Models Pre-Trained on Different Features with Consensus Graph",
    "abstract": "Learning global models effectively on private and decentralized datasets has become an increasingly important challenge of machine learning when applied in practice. Federated Learning (FL) has recently emerged as a solution paradigm to address this challenge. In particular, the FL clients agree to a common model parameterization in advance, which can then be updated collaboratively via synchronous aggregation of their local model updates. However, such strong requirement of modeling homogeneity and synchronicity across clients makes FL inapplicable to many practical learning scenarios that cannot afford such requirements. For example, in distributed sensing, a network of heterogeneous sensors sample from different data modalities of the same phenomenon. Each sensor thus requires its own specialized model. Local learning therefore needs to happen in isolation but inference still requires merging the local models for better performance. \n\nTo enable this, we investigate a feature fusion approach that extracts local feature representations from local models and incorporates them into a global representation to train a more holistic predictive model. We study two key aspects of this feature incorporation. First, we develop an alignment algorithm that draws accurate correspondence between feature components which are arbitrarily arranged across clients. Next, we propose learning a consensus graph that captures the high-order interactions between these feature components, which reveals how data with heterogeneous features can be stitched together coherently to train a better model. The proposed framework is demonstrated on four real-life data sets including monitoring and predicting power grids and traffic networks.",
    "authors": [
      "Tengfei Ma",
      "Trong Nghia Hoang",
      "Jie Chen"
    ],
    "keywords": [
      "Graph Neural Network",
      "Probabilistic Methods"
    ],
    "real_all_scores": [
      6,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Graph-informed Neural Point Process With Monotonic Nets": {
    "paper_pk": null,
    "title": "Graph-informed Neural Point Process With Monotonic Nets",
    "abstract": "Multi-class event data is ubiquitous in real-world applications. The recent neural temporal point processes have used monotonic nets to model the cumulative conditional intensity to avoid an intractable integration in the likelihood. While successful, they are restricted to single-type events and easily sink into poor learning results. To address these limitations and exploit valuable structural information within event participants, we develop a Graph-Informed Neural Point Process (GINPP) that can freely handle multiple event types, greatly improve learning efficiency with the monotonic net, and effectively integrate the graph information to facilitate training. First, we find the bottleneck of the previous model arises from the standard soft-plus transformation over the output of the monotonic net, which greatly enlarges the prediction variations of the monotonic net and increases the training challenge. We propose a shift-scale version that can significantly reduce the variation and promote learning efficiency. Second, we use a conditional mark distribution to model multiple event types, without the need for explicitly estimating the intensity for each type. The latter can be much more challenging. Third, we use random walks to collect the neighborhood of each event participant and use an attention mechanism to update the hidden state of each participant according to the observed events of both the participant itself and its neighborhood. In this way, we can effectively leverage the graph knowledge, and scale up to large graphs. We have shown the advantage of our approach in both ablation studies and real-world applications.",
    "authors": [
      "Zhimeng Pan",
      "Zheng Wang",
      "Shandian Zhe"
    ],
    "keywords": [
      "Point Process",
      "Sequential Model",
      "Graph Neural Network"
    ],
    "real_all_scores": [
      3,
      5,
      6
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning on Large-scale Text-attributed Graphs via Variational Inference": {
    "paper_pk": null,
    "title": "Learning on Large-scale Text-attributed Graphs via Variational Inference",
    "abstract": "This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and  GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.",
    "authors": [
      "Jianan Zhao",
      "Meng Qu",
      "Chaozhuo Li",
      "Hao Yan",
      "Qian Liu",
      "Rui Li",
      "Xing Xie",
      "Jian Tang"
    ],
    "keywords": [
      "Language Model",
      "Graph Neural Network",
      "Node Classification"
    ],
    "real_all_scores": [
      6,
      8,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Node Importance Specific Meta Learning in Graph Neural Networks": {
    "paper_pk": null,
    "title": "Node Importance Specific Meta Learning in Graph Neural Networks",
    "abstract": "While current node classification methods for graphs have enabled significant progress in many applications, they rely on abundant labeled nodes for training. In many real-world datasets, nodes for some classes are always scarce, thus current algorithms are ill-equipped to handle these few-shot node classes. Some meta learning approaches for graphs have demonstrated advantages in tackling such few-shot problems, but they disregard the impact of node importance on a task. Being exclusive to graph data, the dependencies between nodes convey vital information for determining the importance of nodes in contrast to node features only, which poses unique challenges here. In this paper, we investigate the effect of node importance in node classification meta learning tasks. We first theoretically analyze the influence of distinguishing node importance on the lower bound of the model accuracy. Then, based on the theoretical conclusion, we propose a novel Node Importance Meta Learning architecture (NIML) that learns and applies the importance score of each node for meta learning. Specifically, after constructing an attention vector based on the interaction between a node and its neighbors, we train an importance predictor in a supervised manner to capture the distance between node embedding and the expectation of same-class embedding. Extensive experiments on public datasets demonstrate the state-of-the-art performance of NIML on few-shot node classification problems.",
    "authors": [
      "Hao Liu",
      "Yixin Chen",
      "Dacheng Tao",
      "Muhan Zhang"
    ],
    "keywords": [
      "Meta Learning",
      "Graph Neural Network",
      "Node Importance"
    ],
    "real_all_scores": [
      6,
      3,
      3
    ],
    "real_confidences": [
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Neural DAG Scheduling via One-Shot Priority Sampling": {
    "paper_pk": null,
    "title": "Neural DAG Scheduling via One-Shot Priority Sampling",
    "abstract": "We consider the problem of scheduling operations/nodes, the dependency among which is characterized by a Directed Acyclic Graph (DAG). Due to its NP-hard nature, heuristic algorithms were traditionally used to acquire reasonably good solutions, and more recent works have proposed Machine Learning (ML) heuristics that can generalize to unseen graphs and outperform the non-ML heuristics. However, it is computationally costly to generate solutions using existing ML schedulers since they adopt the episodic reinforcement learning framework that necessitates multi-round neural network processing. We propose a novel ML scheduler that uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules. Since the one-shot encoder can efficiently sample the priorities in parallel, our algorithm runs significantly faster than existing ML baselines and has comparable run time with the fast traditional heuristics. We empirically show that our algorithm generates better schedules than both non-neural and neural baselines across various real-world and synthetic scheduling tasks.",
    "authors": [
      "Wonseok Jeon",
      "Mukul Gagrani",
      "Burak Bartan",
      "Weiliang Will Zeng",
      "Harris Teague",
      "Piero Zappi",
      "Christopher Lott"
    ],
    "keywords": [
      "Combinatorial Optimization",
      "Directed Acyclic Graph",
      "Scheduling",
      "Graph Neural Network",
      "Reinforcement Learning"
    ],
    "real_all_scores": [
      8,
      6,
      1,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Active Sampling for Node Attribute Completion on Graphs": {
    "paper_pk": null,
    "title": "Active Sampling for Node Attribute Completion on Graphs",
    "abstract": "Node attribute is one kind of crucial information on graphs, but real-world graphs usually face attribute-missing problem where attributes of partial nodes are missing and attributes of the other nodes are available. It is meaningful to restore the missing attributes so as to benefit downstream graph learning tasks. Popular GNN is not designed for this node attribute completion issue and is not capable of solving it. Recent proposed Structure-attribute Transformer (SAT) framework decouples the input of graph structures and node attributes by a distribution matching technique, and can work on it properly. However, SAT leverages nodes with observed attributes in an equally-treated way and neglects the different contributions of different nodes in learning. In this paper, we propose a novel active sampling algorithm (ATS) to more efficiently utilize the nodes with observed attributes and better restore the missing node attributes. Specifically, ATS contains two metrics that measure the representativeness and uncertainty of each node's information by considering the graph structures, representation similarity and learning bias. Then, these two metrics are linearly combined by a Beta distribution controlled weighting scheme to finally determine which nodes are selected into the train set in the next optimization step. This ATS algorithm can be combined with SAT framework together, and is learned in an iterative manner. Through extensive experiments on 4 public benchmark datasets and two downstream tasks, we show the superiority of ATS in node attribute completion.",
    "authors": [
      "Benyuan Liu",
      "Xu Chen",
      "Yanfeng Wang",
      "Ya Zhang",
      "Zhi Cao",
      "Ivor Tsang"
    ],
    "keywords": [
      "Graph Neural Network",
      "Node Attribute Completion",
      "Active Sampling"
    ],
    "real_all_scores": [
      6,
      8,
      5,
      6
    ],
    "real_confidences": [
      2,
      2,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data": {
    "paper_pk": null,
    "title": "GLASU: A Communication-Efficient Algorithm for Federated Learning with Vertically Distributed Graph Data",
    "abstract": "Vertical federated learning (VFL) is a distributed learning paradigm, where computing clients collectively train a model based on the partial features of the same set of samples they possess. Current research on VFL focuses on the case when samples are independent, but it rarely addresses an emerging scenario when samples are interrelated through a graph. For graph-structured data, graph neural networks (GNNs) are rather competitive machine learning models, but a naive implementation in the VFL setting causes a significant communication overhead; moreover, the analysis is faced with a challenge caused by the biased stochastic gradients. In this paper, we propose a model splitting method that splits a backbone GNN across the clients and the server and a communication-efficient algorithm, GLASU, to train such a model. GLASU adopts lazy aggregation and stale updates to skip aggregation when evaluating the model and skip feature exchanges during training, greatly reducing communication. We offer a theoretical analysis and conduct extensive numerical experiments on real-world datasets, showing that the proposed algorithm effectively trains a GNN model, whose performance matches that of the backbone GNN when trained in a centralized manner.",
    "authors": [
      "Xinwei Zhang",
      "Mingyi Hong",
      "Jie Chen"
    ],
    "keywords": [
      "Federated Learning",
      "Graph Neural Network",
      "Feature Distributed Federated Learning"
    ],
    "real_all_scores": [
      3,
      3,
      3,
      5
    ],
    "real_confidences": [
      3,
      2,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Generated Graph Detection": {
    "paper_pk": null,
    "title": "Generated Graph Detection",
    "abstract": "Graph generative models become increasingly effective for data distribution approximation and data augmentation. Although still in sandboxes, they have aroused public concerns about their malicious misuses or misinformation broadcasts, just as what Deepfake visual and auditory media has been delivering to society. It is never too early to regulate the prevalence of generated graphs. As a preventive response, we pioneer to formulate the generated graph detection problem to distinguish generated graphs from real ones. We propose the first framework to systematically investigate a set of sophisticated models and their performance in four classification scenarios. Each scenario switches between seen and unseen datasets/generators during testing to get closer to real world settings and progressively challenge the classifiers. Extensive experiments evidence that all the models are qualified for generated graph detection, with specific models having advantages in specific scenarios. Resulting from the validated generality and oblivion of the classifiers to unseen datasets/generators, we draw a safe conclusion that our solution can sustain for a decent while to curb generated graph misuses.",
    "authors": [
      "Yihan Ma",
      "Zhikun Zhang",
      "Ning Yu",
      "Xinlei He",
      "Michael Backes",
      "Yun Shen",
      "Yang Zhang"
    ],
    "keywords": [
      "Generated Graph",
      "Graph Neural Network",
      "Contrastive Learning",
      "Metric Learning"
    ],
    "real_all_scores": [
      3,
      3,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "NeuralPCG: Learning Preconditioner for Solving Partial Differential Equations with Graph Neural Network": {
    "paper_pk": null,
    "title": "NeuralPCG: Learning Preconditioner for Solving Partial Differential Equations with Graph Neural Network",
    "abstract": "Fast and accurate partial differential equation (PDE) solvers empower scientific and engineering research. Classic numerical solvers provide unparalleled accuracy but often require extensive computation time. Machine learning solvers are significantly faster but lack convergence and accuracy guarantees. We present Neural-Network-Preconditioned Conjugate Gradient, or NeuralPCG, a novel linear second-order PDE solver that combines the benefits of classic iterative solvers and machine learning approaches. Our key observation is that both neural-network PDE solvers and classic preconditioners excel at obtaining fast but inexact solutions. NeuralPCG proposes to use neural network models to \\emph{precondition} PDE systems in classic iterative solvers. Compared with neural-network PDE solvers, NeuralPCG achieves converging and accurate solutions (e.g.,1e-12 precision) by construction. Compared with classic solvers, NeuralPCG is faster via data-driven preconditioners. We demonstrate the efficacy and generalizability of NeuralPCG by conducting extensive experiments on various 2D and 3D linear second-order PDEs.",
    "authors": [
      "Yichen Li",
      "Tao Du",
      "Peter Yichen Chen",
      "Wojciech Matusik"
    ],
    "keywords": [
      "Physics Simulation",
      "Graph Neural Network",
      "Applied Mathematics"
    ],
    "real_all_scores": [
      3,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      5,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization": {
    "paper_pk": null,
    "title": "Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization",
    "abstract": "Self-supervised learning (SSL) for graph neural networks (GNNs) has attracted increasing attention from the graph machine learning community in recent years, owing to its capability to learn performant node embeddings without costly label information. One weakness of conventional SSL frameworks for GNNs is that they learn through a single philosophy, such as mutual information maximization or generative reconstruction. When applied to various downstream tasks, these frameworks rarely perform equally well for every task, because one philosophy may not span the extensive knowledge required for all tasks. To enhance the task generalization across tasks, as an important first step forward in exploring fundamental graph models, we introduce PARETOGNN, a multi-task SSL framework for node representation learning over graphs. Specifically, PARETOGNN is self-supervised by manifold pretext tasks observing multiple philosophies. To reconcile different philosophies, we explore a multiple-gradient descent algorithm, such that PARETOGNN actively learns from every pretext task while minimizing potential conflicts. We conduct comprehensive experiments over four downstream tasks (i.e., node classification, node clustering, link prediction, and partition prediction), and our proposal achieves the best overall performance across tasks on 11 widely adopted benchmark datasets. Besides, we observe that learning from multiple philosophies enhances not only the task generalization but also the single task performances, demonstrating that PARETOGNN achieves better task generalization via the disjoint yet complementary knowledge learned from different philosophies. Our code is publicly available at https://github.com/jumxglhf/ParetoGNN.",
    "authors": [
      "Mingxuan Ju",
      "Tong Zhao",
      "Qianlong Wen",
      "Wenhao Yu",
      "Neil Shah",
      "Yanfang Ye",
      "Chuxu Zhang"
    ],
    "keywords": [
      "Graph Neural Network",
      "Self-supervised Learning"
    ],
    "real_all_scores": [
      6,
      6,
      5,
      6
    ],
    "real_confidences": [
      3,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "GReTo: Remedying dynamic graph topology-task discordance via target homophily": {
    "paper_pk": null,
    "title": "GReTo: Remedying dynamic graph topology-task discordance via target homophily",
    "abstract": "Dynamic graphs are ubiquitous across disciplines where observations usually change over time. Regressions on dynamic graphs often contribute to diverse critical tasks, such as climate early-warning and traffic controlling. Existing homophily Graph Neural Networks (GNNs) adopt physical connections or feature similarity as adjacent matrix to perform node-level aggregations. However, on dynamic graphs with diverse node-wise relations, exploiting a pre-defined fixed topology for message passing inevitably leads to the aggregations of target-deviated neighbors. We designate such phenomenon as the topology-task discordance, which naturally challenges the homophily assumption. In this work, we revisit node-wise relationships and explore novel homophily measurements on dynamic graphs with both signs and distances, capturing multiple node-level spatial relations and temporal evolutions. We discover that advancing homophily aggregations to signed target-oriented message passing can effectively resolve the discordance and promote aggregation capacity. Therefore, a GReTo is proposed, which performs signed message passing in immediate neighborhood, and exploits both local environments and target awareness to realize high-order message propagation. Empirically, our solution achieves significant improvements against best baselines, notably improving 24.79% on KnowAir and 3.60% on Metr-LA. ",
    "authors": [
      "Zhengyang Zhou",
      "Qihe Huang",
      "Gengyu Lin",
      "Kuo Yang",
      "LEI BAI",
      "Yang Wang"
    ],
    "keywords": [
      "Dynamic graph",
      "graph homophily theory",
      "Graph Neural Network",
      "topology-task discordance"
    ],
    "real_all_scores": [
      5,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "HierBatching: Locality-Aware Out-of-Core Training of Graph Neural Networks": {
    "paper_pk": null,
    "title": "HierBatching: Locality-Aware Out-of-Core Training of Graph Neural Networks",
    "abstract": "As graph neural networks (GNNs) become increasingly more popular for analyzing data organized as massive graphs, how these models can be efficiently trained under economic computing resources becomes a critical subject that influences the widespread adoption of GNNs in practice. We consider the use of a single commodity machine restrained by limited memory but otherwise is attached with ample external storage. In such an under-explored scenario, not only the feature data often exceeds the memory capacity, but also the graph structure may not fit in memory as well. Then, with data stored on disk, gathering features and constructing neighborhood subgraphs in a usual mini-batch training incur inefficient random access and expensive data movement.\n\nTo overcome this bottleneck, we propose a locality-aware training scheme, coined HierBatching, to significantly increase sequential disk access, while maintaining the random nature of stochastic training and its quality. HierBatching exploits the memory hierarchy of a modern GPU machine and constructs batches in an analogously hierarchical manner. Therein, graph nodes are organized in many partitions, each of which is laid out contiguously in disk for maximal spatial locality; while the main memory stores random partitions and is treated as the cache of the disk. Its content is reused multiple times for improving temporal locality. We conduct comprehensive experiments, including locality ablation, to demonstrate that HierBatching is economic, fast, and accurate.",
    "authors": [
      "Tianhao Huang",
      "Xuhao Chen",
      "Muhua Xu",
      "Arvind Arvind",
      "Jie Chen"
    ],
    "keywords": [
      "Graph Neural Network",
      "Out-of-Core Training",
      "Spatial Locality",
      "Temporal Locality",
      "Hierarchical Batching"
    ],
    "real_all_scores": [
      3,
      5,
      3,
      6
    ],
    "real_confidences": [
      3,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization": {
    "paper_pk": null,
    "title": "MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization",
    "abstract": "Training graph neural networks (GNNs) on large graphs is complex and extremely time consuming. This is attributed to overheads caused by sparse matrix multiplication, which are sidestepped when training multi-layer perceptrons (MLPs) with only node features. MLPs, by ignoring graph context, are simple and faster for graph data, however they usually sacrifice prediction accuracy, limiting their applications for graph data. We observe that for most message passing-based GNNs, we can trivially derive an analog MLP (we call this a PeerMLP) with an equivalent weight space, by setting the trainable parameters with the same shapes, making us curious about how do GNNs using weights from a fully trained PeerMLP perform? Surprisingly, we find that GNNs initialized with such weights significantly outperform their PeerMLPs, motivating us to use PeerMLP training as a precursor, initialization step to GNN training. To this end, we propose an embarrassingly simple, yet hugely effective initialization method for GNN training acceleration, called \\mlpinit. Our extensive experiments on multiple large-scale graph datasets with diverse GNN architectures validate that MLPInit can accelerate the training of GNNs (up to 33\u00d7 speedup on OGB-Products) and often improve prediction performance (e.g., up to $7.97\\%$ improvement for GraphSAGE across $7$ datasets for node classification, and up to $17.81\\%$ improvement across $4$ datasets for link prediction on metric Hits@10). The code is available at https://github.com/snap-research/MLPInit-for-GNNs.",
    "authors": [
      "Xiaotian Han",
      "Tong Zhao",
      "Yozen Liu",
      "Xia Hu",
      "Neil Shah"
    ],
    "keywords": [
      "Graph Neural Network",
      "Large-scale Graph",
      "Accleration"
    ],
    "real_all_scores": [
      5,
      5,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Unveiling the sampling density in non-uniform geometric graphs": {
    "paper_pk": null,
    "title": "Unveiling the sampling density in non-uniform geometric graphs",
    "abstract": "A powerful framework for studying graphs is to consider them as geometric graphs: nodes are randomly sampled from an underlying metric space, and any pair of nodes is connected if their distance is less than a specified neighborhood radius. Currently, the literature mostly focuses on uniform sampling and constant neighborhood radius. However, real-world graphs are likely to be better represented by a model in which the sampling density and the neighborhood radius can both vary over the latent space. For instance, in a social network communities can be modeled as densely sampled areas, and hubs as nodes with larger neighborhood radius. In this work, we first perform a rigorous mathematical analysis of this (more general) class of models, including derivations of the resulting graph shift operators. The key insight is that graph shift operators should be corrected in order to avoid potential distortions introduced by the non-uniform sampling. Then, we develop methods to estimate the unknown sampling density in a self-supervised fashion.\u00a0 Finally, we present exemplary applications in which the learnt density is used to 1) correct the graph shift operator and improve performance on a variety of tasks, 2) improve pooling, and 3) extract knowledge from networks. Our experimental findings support our theory and provide strong evidence for our model.",
    "authors": [
      "Raffaele Paolino",
      "Aleksandar Bojchevski",
      "Stephan G\u00fcnnemann",
      "Gitta Kutyniok",
      "Ron Levie"
    ],
    "keywords": [
      "graph neural network",
      "graph representation learning",
      "spectral method",
      "non-uniform sampling",
      "geometric graph",
      "graphon"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "LEARNING CONTEXT-AWARE ADAPTIVE SOLVERS TO ACCELERATE QUADRATIC PROGRAMMING": {
    "paper_pk": null,
    "title": "LEARNING CONTEXT-AWARE ADAPTIVE SOLVERS TO ACCELERATE QUADRATIC PROGRAMMING",
    "abstract": "Quadratic programming (QP) is an important sub-field of mathematical optimization. The alternating direction method of multipliers (ADMM) is a successful method to solve QP. Even though ADMM shows promising results in solving various types of QP, its convergence speed is known to be highly dependent on the step-size parameter $\\rho$. Due to the absence of a general rule for setting $\\rho$, it is often tuned manually or heuristically. In this paper, we propose CA-ADMM (Context-aware Adaptive ADMM)) which learns to adaptively adjust $\\rho$ to accelerate ADMM. CA-ADMM extracts the spatio-temporal context, which captures the dependency of the primal and dual variables of QP and their temporal evolution during the ADMM iterations. CA-ADMM chooses $\\rho$ based on the extracted context. Through extensive numerical experiments, we validated that CA-ADMM effectively generalizes to unseen QP problems with different sizes and classes (i.e., having different QP parameter structures). Furthermore, we verified that CA-ADMM could dynamically adjust $\\rho$ considering the stage of the optimization process to accelerate the convergence speed further.",
    "authors": [
      "Haewon Jung",
      "Junyoung Park",
      "Jinkyoo Park"
    ],
    "keywords": [
      "quadratic optimization",
      "convex optimization",
      "reinforcement learning for optimization",
      "graph neural network",
      "contextual learning"
    ],
    "real_all_scores": [
      6,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Causally-guided Regularization of Graph Attention improves Generalizability": {
    "paper_pk": null,
    "title": "Causally-guided Regularization of Graph Attention improves Generalizability",
    "abstract": "Graph attention networks estimate the relational importance of node neighbors to aggregate relevant information over local neighborhoods for a prediction task. However, the inferred attentions are vulnerable to spurious correlations and connectivity in the training data, hampering the generalizability of the model. We introduce CAR, a general-purpose regularization framework for graph attention networks. Embodying a causal inference approach, CAR aligns the attention mechanism with the causal effects of active interventions on graph connectivity in a scalable manner. CAR is compatible with a variety of graph attention architectures, and we show that it systematically improves generalizability on various node classification tasks. Our ablation studies indicate that CAR hones in on the aspects of graph structure most pertinent to the prediction (e.g., homophily), and does so more effectively than alternative approaches. Finally, we also show that CAR enhances interpretability of attention weights by accentuating node-neighbor relations that point to causal hypotheses. For social media network-sized graphs, a CAR-guided graph rewiring approach could allow us to combine the scalability of graph convolutional methods with the higher performance of graph attention.",
    "authors": [
      "Alexander P Wu",
      "Thomas Markovich",
      "Bonnie Berger",
      "Nils Yannick Hammerla",
      "Rohit Singh"
    ],
    "keywords": [
      "graph neural network",
      "attention",
      "generalization",
      "regularization",
      "causal effect",
      "causal interventions",
      "interpretability"
    ],
    "real_all_scores": [
      5,
      6,
      5,
      3
    ],
    "real_confidences": [
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction": {
    "paper_pk": null,
    "title": "Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction",
    "abstract": "We study the problem of crystal material property prediction. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we propose to model the complete set of potentials among all atoms, instead of only between nearby atoms as in prior methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop efficient algorithms to compute the approximations. Finally, we propose to incorporate our computations of complete interatomic potentials into message passing neural networks for representation learning. We perform experiments on the JARVIS and Materials Project benchmarks for evaluation. Results show that the use of complete interatomic potentials leads to consistent performance improvements with reasonable computational costs.",
    "authors": [
      "Yuchao Lin",
      "Keqiang Yan",
      "Youzhi Luo",
      "Yi Liu",
      "Xiaoning Qian",
      "Shuiwang Ji"
    ],
    "keywords": [
      "graph neural network",
      "material property prediction",
      "crystal property prediction",
      "crystal structure modeling",
      "interatomic potential"
    ],
    "real_all_scores": [
      6,
      6,
      3
    ],
    "real_confidences": [
      4,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "A $2$-parameter Persistence Layer for Learning": {
    "paper_pk": null,
    "title": "A $2$-parameter Persistence Layer for Learning",
    "abstract": "$1$-parameter persistent homology, a cornerstone in Topological Data Analysis (TDA), studies the evolution of topological features such as cycle basis hidden in data. It has found its application in strengthening the representation power of deep learning models like Graph Neural Networks (GNN). To enrich the representations of topological features,  here we propose to study $2$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vectorization on $2$-parameter persistence modules called Generalized Rank Invariant Landscape {\\textsc{Gril}}. We show that this vector representation is stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vectorization and its gradients. We also test our methods on synthetic graph datasets and compare the results with some popular graph neural networks.",
    "authors": [
      "Cheng Xin",
      "Soham Mukherjee",
      "Shreyas N. Samaga",
      "Tamal K. Dey"
    ],
    "keywords": [
      "topological data analysis",
      "graph representation",
      "persistent homology",
      "2-parameter persistence",
      "graph neural network"
    ],
    "real_all_scores": [
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Logical Message Passing Networks with One-hop Inference on Atomic Formulas": {
    "paper_pk": null,
    "title": "Logical Message Passing Networks with One-hop Inference on Atomic Formulas",
    "abstract": "Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct one-hop inferences on atomic formulas, the results of which are regarded as the messages passed in LMPNN. The reasoning process over the overall logical formulas is turned into the forward pass of LMPNN that incrementally aggregates local information to finally predict the answers' embeddings. The complex logical inference across different types of queries will then be learned from training examples based on the LMPNN architecture. Theoretically, our query-graph represenation is more general than the prevailing operator-tree formulation, so our approach applies to a broader range of complex KG queries. Empirically, our approach yields the new state-of-the-art neural CQA model. Our research bridges the gap between complex KG query answering tasks and the long-standing achievements of knowledge graph representation learning. Our implementation can be found at https://github.com/HKUST-KnowComp/LMPNN.",
    "authors": [
      "Zihao Wang",
      "Yangqiu Song",
      "Ginny Wong",
      "Simon See"
    ],
    "keywords": [
      "knowledge graph",
      "complex query answering",
      "graph neural network",
      "representation learning"
    ],
    "real_all_scores": [
      6,
      3,
      5,
      6
    ],
    "real_confidences": [
      4,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Towards graph-level anomaly detection via deep evolutionary mapping": {
    "paper_pk": null,
    "title": "Towards graph-level anomaly detection via deep evolutionary mapping",
    "abstract": "Graph-level anomaly detection aims at depicting anomalous individual graphs in a graph set. Due to its significance in various real-world application fields, such as identifying rare molecules in chemistry and detecting potential frauds in online social networks, graph-level anomaly detection has received great attention. In distinction from node- and edge-level anomaly detection that is devoted to identifying anomalies on a single graph, graph-level anomaly detection faces more significant challenges because both the intra- and inter-graph structural and attribute patterns need to be taken into account to distinguish anomalies that exhibit deviating structures, rare attributes or the both. Although deep graph representation learning shows effectiveness in fusing high-level representations and capturing characters of individual graphs, most of the existing works are defective in graph-level anomaly detection because of their limited capability in exploring information across graphs, the imbalanced data distribution of anomalies, and low interpretability of the black-box graph neural networks (GNNs). To bridge these gaps, we propose a novel deep evolutionary graph mapping framework named GmapAD, which can adaptively map each graph into a new feature space based on its similarity to a set of representative nodes chosen from the graph set. By automatically adjusting the candidate nodes using a specially designed evolutionary algorithm, anomalies and normal graphs are mapped to separate areas in the new feature space where a clear boundary between them can be learned. The selected candidate nodes can therefore be regarded as a benchmark for explaining anomalies because anomalies are more dissimilar/similar to the benchmark than normal graphs. Through our extensive experiments on nine real-world datasets, we demonstrate that exploring both intra- and inter-graph structural and attribute information are critical to spot anomalous graphs, and our framework outperforms the state of the art on all datasets used in the experiments.",
    "authors": [
      "Xiaoxiao Ma",
      "Jian Yang",
      "Jia Wu",
      "Quan Z. Sheng"
    ],
    "keywords": [
      "Graph anomaly detection",
      "anomaly detection",
      "graph representation",
      "deep learning",
      "graph neural network"
    ],
    "real_all_scores": [
      3,
      3,
      3,
      3
    ],
    "real_confidences": [
      5,
      4,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Two Birds, One Stone: An Equivalent Transformation for Hyper-relational Knowledge Graph Modeling": {
    "paper_pk": null,
    "title": "Two Birds, One Stone: An Equivalent Transformation for Hyper-relational Knowledge Graph Modeling",
    "abstract": "By representing knowledge in a primary triple associated with additional attribute value qualifiers, hyper-relational knowledge graph (HKG) that generalizes triple based knowledge graph (KG) has been attracting research attention recently. Compared with KG, HKG is enriched with the semantic difference between the primary triple and additional qualifiers as well as the structural connection between entities in hyper-relational graph structure. However, to model HKG, existing studies mainly focus on either semantic information or structural information therein, fail to capture both simultaneously. To tackle this issue, in this paper, we propose an equivalent transformation for HKG modeling, referred to as TransEQ. Specifically, the equivalent transformation transforms a HKG to a KG, which considers both semantic and structural characteristics. Then a generalized encoder-decoder framework is developed to bridge the modeling research between KG and HKG. In the encoder part, KG-based graph neural networks are leveraged for structural modeling; while in the decoder part, various HKG-based scoring functions are exploited for semantic modeling. Especially, we design the sharing embedding mechanism in the encoder-decoder framework with semantic relatedness captured. We further theoretically prove that TransEQ preserves complete information in the equivalent transformation, and also achieves full expressivity. Finally, extensive experiments on three benchmarks demonstrate the superior performance of TransEQ in terms of both effectiveness and efficiency. On the largest benchmark WikiPeople, TransEQ significantly improves the state-of-the-art models by 15% on MRR.",
    "authors": [
      "Yu Liu",
      "Shu Yang",
      "Jingtao Ding",
      "quanming yao",
      "Yong Li"
    ],
    "keywords": [
      "Hyper-relational knowledge graph",
      "hyperedge expansion",
      "graph neural network"
    ],
    "real_all_scores": [
      8,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "HAS IT REALLY IMPROVED? KNOWLEDGE GRAPH BASED SEPARATION AND FUSION FOR RECOMMENDATION": {
    "paper_pk": null,
    "title": "HAS IT REALLY IMPROVED? KNOWLEDGE GRAPH BASED SEPARATION AND FUSION FOR RECOMMENDATION",
    "abstract": "In this paper we study the knowledge graph (KG) based recommendation systems. We first design the metric to study the relationship between different SOTA models and find that the current recommendation systems based on knowledge graph have poor ability to retain collaborative filtering signals, and higher-order connectivity would introduce noises. In addition, we explore the collaborative filtering recommendation method using GNN and design the experiment to show that the information learned between GNN models stacked with different layers is different, which provides the explanation for the unstable performance of GNN stacking different layers from a new perspective. According to the above findings, we first design the model-agnostic Cross-Layer Fusion Mechanism without any parameters to improve the performance of GNN. Experimental results on three datasets for collaborative filtering show that Cross-Layer Fusion Mechanism is effective for improving GNN performance. Then we design three independent signal extractors to mine the data at three different perspectives and train them separately. Finally, we use the signal fusion mechanism to fuse different signals. Experimental results on three datasets that introduce KG show that our KGSF achieves significant improvements over current SOTA KG based recommendation methods and the results are interpretable.",
    "authors": [
      "Ying Tang",
      "Jintian Zhang"
    ],
    "keywords": [
      "recommendation",
      "knowledge-graph",
      "graph neural network"
    ],
    "real_all_scores": [
      3,
      5,
      6,
      5,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Graph Neural Network-Inspired Kernels for Gaussian Processes in Semi-Supervised Learning": {
    "paper_pk": null,
    "title": "Graph Neural Network-Inspired Kernels for Gaussian Processes in Semi-Supervised Learning",
    "abstract": "Gaussian processes (GPs) are an attractive class of machine learning models because of their simplicity and flexibility as building blocks of more complex Bayesian models. Meanwhile, graph neural networks (GNNs) emerged recently as a promising class of models for graph-structured data in semi-supervised learning and beyond. Their competitive performance is often attributed to a proper capturing of the graph inductive bias. In this work, we introduce this inductive bias into GPs to improve their predictive performance for graph-structured data. We show that a prominent example of GNNs, the graph convolutional network, is equivalent to some GP when its layers are infinitely wide; and we analyze the kernel universality and the limiting behavior in depth. We further present a programmable procedure to compose covariance kernels inspired by this equivalence and derive example kernels corresponding to several interesting members of the GNN family. We also propose a computationally efficient approximation of the covariance matrix for scalable posterior inference with large-scale data. We demonstrate that these graph-based kernels lead to competitive classification and regression performance, as well as advantages in computation time, compared with the respective GNNs.",
    "authors": [
      "Zehao Niu",
      "Mihai Anitescu",
      "Jie Chen"
    ],
    "keywords": [
      "graph neural network",
      "Gaussian process",
      "semi-supervised learning"
    ],
    "real_all_scores": [
      6,
      10,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Attribute Alignment and Enhancement for Generalized Zero-Shot Learning": {
    "paper_pk": null,
    "title": "Attribute Alignment and Enhancement for Generalized Zero-Shot Learning",
    "abstract": "Generalized zero-shot learning (GZSL) aims to recognize both seen and unseen classes, which challenges the generalization ability of a model. In this paper, we propose a novel approach to fully utilize attributes information, referred to as attribute alignment and enhancement (A3E) network. It contains two modules. First, attribute localization (AL) module utilizes the supervision of class attribute vectors to guide visual localization for attributes through the implicit localization capability within the feature extractor, and the visual features corresponding to the attributes (attribute-visual features) are obtained. Second, enhanced attribute scoring (EAS) module employs the supervision of the attribute word vectors (attribute semantics) to project input attribute visual features to attribute semantic space using Graph Attention Network (GAT). Based on the constructed attribute relation graph (ARG), EAS module generates enhanced representation of attributes. Experiments on standard datasets demonstrate that the enhanced attribute representation greatly improves the classification performance, which helps A3E to achieve state-of-the-art performances in both ZSL and GZSL tasks.",
    "authors": [
      "Nannan Lu",
      "MingKai Qiu"
    ],
    "keywords": [
      "zero-shot learning",
      "image classification",
      "attribute alignment",
      "graph neural network",
      "attention network"
    ],
    "real_all_scores": [
      6,
      3,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Causal discovery from conditionally stationary time series": {
    "paper_pk": null,
    "title": "Causal discovery from conditionally stationary time series",
    "abstract": "Causal discovery, i.e., inferring underlying causal relationships from observational data, has been shown to be highly challenging for AI systems. In time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. We develop a causal discovery approach to handle a wide class of non-stationary time-series that are conditionally stationary, where the non-stationary behaviour is modeled as stationarity conditioned on a set of (possibly hidden) state variables. Named state-dependent causal inference (SDCI), our approach is able to recover the underlying causal dependencies, provably with fully-observed states and empirically with hidden states. The latter is confirmed by experiments on synthetic linear system and nonlinear particle interaction data, where SDCI achieves superior performance over baseline causal discovery methods. Improved results over non-causal RNNs on modeling NBA player movements demonstrate the potential of our method and motivate the use causality-driven methods for forecasting.",
    "authors": [
      "Carles Balsells Rodas",
      "Ruibo Tu",
      "Hedvig Kjellstrom",
      "Yingzhen Li"
    ],
    "keywords": [
      "causal discovery",
      "temporal data",
      "graph neural network",
      "time series",
      "non-stationary",
      "probabilistic modelling"
    ],
    "real_all_scores": [
      3,
      5,
      3
    ],
    "real_confidences": [
      5,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "A New Paradigm for Federated Structure Non-IID Subgraph Learning": {
    "paper_pk": null,
    "title": "A New Paradigm for Federated Structure Non-IID Subgraph Learning",
    "abstract": "Federated graph learning (FGL), a distributed training framework for graph neural networks (GNNs) has attracted much attention for breaking the centralized machine learning assumptions. Despite its effectiveness, the differences in data collection perspectives and quality lead to the challenges of heterogeneity, especially the domain-specific graph is partitioned into subgraphs in different institutions. However, existing FGL methods implement graph data augmentation or personalization with community split which follows the cluster homogeneity assumptions. Hence we investigate the above issues and suggest that subgraph heterogeneity is essentially the structure variations. From the observations on FGL, we first define the structure non-independent identical distribution (Non-IID) problem, which presents covariant shift challenges among client-wise subgraphs. Meanwhile, we propose a new paradigm for general federated data settings called Adaptive Federated Graph Learning (AdaFGL). The motivation behind it is to implement adaptive propagation mechanisms based on federated global knowledge and non-params label propagation. We conduct extensive experiments with community split and structure Non-IID settings, our approach achieves state-of-the-art performance on five benchmark datasets.",
    "authors": [
      "Xunkai Li",
      "Wentao Zhang",
      "Rong-Hua Li",
      "Yulin Zhao",
      "Yinlin Zhu",
      "Guoren Wang"
    ],
    "keywords": [
      "graph neural network",
      "federated learning",
      "structure non-iid subgraphs"
    ],
    "real_all_scores": [
      1,
      5,
      3
    ],
    "real_confidences": [
      5,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "A spatiotemporal graph neural network with multi granularity for air quality prediction": {
    "paper_pk": null,
    "title": "A spatiotemporal graph neural network with multi granularity for air quality prediction",
    "abstract": "Air quality prediction is a complex system engineering. How to fully consider the impact of meteorological, spatial and temporal factors on air quality is the core problem. To address this central conundrum, in an elaborate encoder-decoder architecture, we propose a new air quality prediction method based on multi-granularity spatiotemporal graph network. At the encoder, firstly, we use multi granularity graph and the well-known HYSPLIT model to build spatial relationship and dynamic edge relationship between nodes, respectively, while meteorological, temporal and topographic characteristics are used to build node features and LSTM (Long Short Term Memory) is used to learn the time-series relationship of pollutant concentration. At the decoder, secondly, we use the attention mechanism LSTM for decoding and forecasting of pollutant concentration. The proposed model is capable of tracking different influences on prediction resulting from the changes of air quality. On a project-based dataset, we validate the effectiveness of the proposed model and examine its abilities of capturing both fine-grained and long-term influences in pollutant process. We also compare the proposed model with the state-of-the-art air quality forecasting methods on the dataset of Yangtze River Delta city group, the experimental results show  the appealing performance of our model over competitive baselines.",
    "authors": [
      "Haibin Liao",
      "Yuan Li",
      "Mou Wu",
      "Hongsheng Chen"
    ],
    "keywords": [
      "Air quality prediction",
      "graph neural network",
      "long short term memory"
    ],
    "real_all_scores": [
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information": {
    "paper_pk": null,
    "title": "Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information",
    "abstract": "Predicting the responses of a cell under perturbations may bring important benefits to drug discovery and personalized therapeutics. In this work, we propose a novel graph variational Bayesian causal inference framework to predict a cell's gene expressions under counterfactual perturbations (perturbations that this cell did not factually receive), leveraging information representing biological knowledge in the form of gene regulatory networks (GRNs) to aid individualized cellular response predictions. Aiming at a data-adaptive GRN, we also developed an adjacency matrix updating technique for graph convolutional networks and used it to refine GRNs during pre-training, which generated more insights on gene relations and enhanced model performance. Additionally, we propose a robust estimator within our framework for the asymptotically efficient estimation of marginal perturbation effect, which is yet to be carried out in previous works. With extensive experiments, we exhibited the advantage of our approach over state-of-the-art deep learning models for individual response prediction.",
    "authors": [
      "Yulun Wu",
      "Rob Barton",
      "Zichen Wang",
      "Vassilis N. Ioannidis",
      "Carlo De Donno",
      "Layne C Price",
      "Luis F. Voloch",
      "George Karypis"
    ],
    "keywords": [
      "graph neural network",
      "causal inference",
      "variational bayes",
      "asymptotic statistics",
      "single-cell perturbation"
    ],
    "real_all_scores": [
      8,
      6,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "DeSCo: Towards Scalable Deep Subgraph Counting": {
    "paper_pk": null,
    "title": "DeSCo: Towards Scalable Deep Subgraph Counting",
    "abstract": "Subgraph counting is the problem of determining the number of a given query graph in a large targe graph. Despite being a #P problem, subgraph counting is a crucial graph analysis method in domains ranging from biology and social science to risk management and software analysis. However, existing exact counting methods take combinatorially long runtime as target and query sizes increase. Existing approximate heuristic methods and neural approaches fall short in accuracy due to high label dynamic range, limited model expressive power, and inability to predict the distribution of subgraph counts in the target graph. Here we propose DeSCo, a neural deep subgraph counting framework, which aims to accurately predict the count and distribution of query graphs on any given target graph. DeSCo uses canonical partition to divide the large target graph into small neighborhood graphs and predict the canonical count objective on each neighborhood. The proposed partition method avoids missing or double-counting any patterns of the target graph. A novel subgraph-based heterogeneous graph neural network is then used to improve the expressive power. Finally, gossip correction improves counting accuracy via prediction propagation with learnable weights. Compared with state-of-the-art approximate heuristic and neural methods. DeSCo achieves 437x improvement in the mean squared error of count prediction and benefits from the polynomial runtime complexity.\n",
    "authors": [
      "Tianyu Fu",
      "Yu Wang",
      "Zhitao Ying"
    ],
    "keywords": [
      "subgraph counting",
      "graph neural network",
      "graph mining"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      4,
      3,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Learnable Topological Features For Phylogenetic Inference via Graph Neural Networks": {
    "paper_pk": null,
    "title": "Learnable Topological Features For Phylogenetic Inference via Graph Neural Networks",
    "abstract": "Structural information of phylogenetic tree topologies plays an important role in phylogenetic inference. However, finding appropriate topological structures for specific phylogenetic inference tasks often requires significant design effort and domain expertise. In this paper, we propose a novel structural representation method for phylogenetic inference based on learnable topological features. By combining the raw node features that minimize the Dirichlet energy with modern graph representation learning techniques, our learnable topological features can provide efficient structural information of phylogenetic trees that automatically adapts to different downstream tasks without requiring domain expertise. We demonstrate the effectiveness and efficiency of our method on a simulated data tree probability estimation task and a benchmark of challenging real data variational Bayesian phylogenetic inference problems.",
    "authors": [
      "Cheng Zhang"
    ],
    "keywords": [
      "phylogenetic inference",
      "learnable topological features",
      "graph neural network",
      "density estimation",
      "variational inference"
    ],
    "real_all_scores": [
      3,
      6,
      5
    ],
    "real_confidences": [
      3,
      4,
      2
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules": {
    "paper_pk": null,
    "title": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules",
    "abstract": "Recent years have witnessed the prosperity of pre-training graph neural networks (GNNs) for molecules. Typically, atom types as node attributes are randomly masked, and GNNs are then trained to predict masked types as in AttrMask \\citep{hu2020strategies}, following the Masked Language Modeling (MLM) task of BERT~\\citep{devlin2019bert}. However, unlike MLM with a large vocabulary, the AttrMask pre-training does not learn informative molecular representations due to small and unbalanced atom `vocabulary'. To amend this problem, we propose a variant of VQ-VAE~\\citep{van2017neural} as a context-aware tokenizer to encode atom attributes into chemically meaningful discrete codes. This can enlarge the atom vocabulary size and mitigate the quantitative divergence between dominant (e.g., carbons) and rare atoms (e.g., phosphorus). With the enlarged atom `vocabulary', we propose a novel node-level pre-training task, dubbed Masked Atoms Modeling (\\textbf{MAM}), to mask some discrete codes randomly and then pre-train GNNs to predict them. MAM also mitigates another issue of AttrMask, namely the negative transfer. It can be easily combined with various pre-training tasks to improve their performance. Furthermore, we propose triplet masked contrastive learning (\\textbf{TMCL}) for graph-level pre-training to model the heterogeneous semantic similarity between molecules for effective molecule retrieval. MAM and TMCL constitute a novel pre-training framework, \\textbf{Mole-BERT}, which can match or outperform state-of-the-art methods in a fully data-driven manner. We release the code at \\textcolor{magenta}{\\url{https://github.com/junxia97/Mole-BERT}}.",
    "authors": [
      "Jun Xia",
      "Chengshuai Zhao",
      "Bozhen Hu",
      "Zhangyang Gao",
      "Cheng Tan",
      "Yue Liu",
      "Siyuan Li",
      "Stan Z. Li"
    ],
    "keywords": [
      "graph neural networks"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Unsupervised Learning for Combinatorial Optimization Needs Meta Learning": {
    "paper_pk": null,
    "title": "Unsupervised Learning for Combinatorial Optimization Needs Meta Learning",
    "abstract": "A general framework of unsupervised learning for combinatorial optimization (CO) is to train a neural network whose output gives a problem solution by directly optimizing the CO objective. Albeit with some advantages over traditional solvers, current frameworks optimize an averaged performance over the distribution of historical problem instances, which misaligns with the actual goal of CO that looks for a good solution to every future encountered instance. With this observation, we propose a new objective of unsupervised learning for CO where the goal of learning is to search for good initialization for future problem instances rather than give direct solutions. We propose a meta-learning-based training pipeline for this new objective. Our method achieves good performance. We observe that even the initial solution given by our model before fine-tuning can significantly outperform the baselines under various evaluation settings including evaluation across multiple datasets, and the case with big shifts in the problem scale. The reason we conjecture is that meta-learning-based training lets the model be loosely tied to each local optimum for a training instance while being more adaptive to the changes of optimization landscapes across instances.",
    "authors": [
      "Haoyu Peter Wang",
      "Pan Li"
    ],
    "keywords": [
      "combinatorial optimization",
      "unsupervised learning",
      "meta learning",
      "graph neural networks"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Empowering Graph Representation Learning with Test-Time Graph Transformation": {
    "paper_pk": null,
    "title": "Empowering Graph Representation Learning with Test-Time Graph Transformation",
    "abstract": "As powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTrans which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTrans on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTrans performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings. ",
    "authors": [
      "Wei Jin",
      "Tong Zhao",
      "Jiayuan Ding",
      "Yozen Liu",
      "Jiliang Tang",
      "Neil Shah"
    ],
    "keywords": [
      "graph neural networks",
      "out-of-distribution generalization",
      "adversarial robustness"
    ],
    "real_all_scores": [
      6,
      5,
      6,
      6
    ],
    "real_confidences": [
      3,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Graph Neural Networks Are More Powerful Than we Think": {
    "paper_pk": null,
    "title": "Graph Neural Networks Are More Powerful Than we Think",
    "abstract": "Graph Neural Networks (GNNs) are powerful convolutional architectures that have shown remarkable performance in various node-level and graph-level tasks. Despite their success, the common belief is that the expressive power of standard GNNs is limited and that they are at most as discriminative as the Weisfeiler-Lehman (WL) algorithm. In this paper we argue the opposite and show that the WL algorithm is the upper bound only when the input to the GNN is the vector of all ones. In this direction, we derive an alternative analysis that employs linear algebraic tools and characterize the representational power of GNNs with respect to the eigenvalue decomposition of the graph operators. We show that GNNs can distinguish between any graphs that differ in at least one eigenvalue and design simple GNN architectures that are provably more expressive than the WL algorithm. Thorough experimental analysis on graph isomorphism and graph classification datasets corroborates our theoretical results and demonstrates the effectiveness of the proposed architectures.",
    "authors": [
      "Charilaos Kanatsoulis",
      "Alejandro Ribeiro"
    ],
    "keywords": [
      "graph neural networks",
      "expressive power",
      "representation",
      "graph isomorphism",
      "classification",
      "spectral decomposition"
    ],
    "real_all_scores": [
      5,
      3,
      3
    ],
    "real_confidences": [
      4,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "An Exploration of Conditioning Methods in Graph Neural Networks": {
    "paper_pk": null,
    "title": "An Exploration of Conditioning Methods in Graph Neural Networks",
    "abstract": "The flexibility and effectiveness of message passing based graph neural networks (GNNs) induced considerable advances in deep learning on graph-structured data. In such approaches, GNNs recursively update node representations based on their neighbors and they gain expressivity through the use of node and edge attribute vectors. E.g., In computational tasks such as physics and chemistry usage of edge attributes such as relative position or distance proved to be essential. In this work, we address not what kind of attributes to use, but how to condition on this information to improve model performance. We consider three types of conditioning; weak, strong, and pure, which respectively relate to concatenation-based conditioning, gating, and transformations that are causally dependent on the attributes. This categorization provides a unifying viewpoint on different classes of GNNs, from separable convolutions to various forms of message passing networks. We provide an empirical study on the effect of conditioning methods in several tasks in computational chemistry.",
    "authors": [
      "Yeskendir Koishekenov",
      "Erik J Bekkers"
    ],
    "keywords": [
      "graph neural networks",
      "geometric deep learning",
      "deep learning"
    ],
    "real_all_scores": [
      3,
      3,
      5,
      6
    ],
    "real_confidences": [
      3,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Revisiting Robustness in Graph Machine Learning": {
    "paper_pk": null,
    "title": "Revisiting Robustness in Graph Machine Learning",
    "abstract": "Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results suggest: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the inference process of GNNs significantly reduces over-robustness, while having a positive effect on test accuracy and adversarial robustness. Theoretically, leveraging our new semantics-aware notion of robustness, we prove that there is no robustness-accuracy tradeoff for inductively classifying a newly added node. ",
    "authors": [
      "Lukas Gosch",
      "Daniel Sturm",
      "Simon Geisler",
      "Stephan G\u00fcnnemann"
    ],
    "keywords": [
      "graph neural networks",
      "adversarial robustness",
      "label propagation",
      "node-classification",
      "stochastic block models",
      "Bayes classifier",
      "non-i.i.d. data",
      "graph learning",
      "graphs",
      "robustness"
    ],
    "real_all_scores": [
      3,
      6,
      6,
      3
    ],
    "real_confidences": [
      4,
      2,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Multi-Hypothesis 3D human pose estimation metrics favor miscalibrated distributions": {
    "paper_pk": null,
    "title": "Multi-Hypothesis 3D human pose estimation metrics favor miscalibrated distributions",
    "abstract": "Due to depth ambiguities and occlusions, lifting 2D poses to 3D is a highly ill-posed problem. Well-calibrated distributions of possible poses can make these ambiguities explicit and preserve the resulting uncertainty for downstream tasks. This study shows that previous attempts, which account for these ambiguities via multiple hypotheses generation, produce miscalibrated distributions. We identify that miscalibration can be attributed to the use of sample-based metrics such as $\\operatorname{minMPJPE}$. In a series of simulations, we show that minimizing $\\operatorname{minMPJPE}$, as commonly done, should converge to the correct mean prediction. However, it fails to correctly capture the uncertainty, thus resulting in a miscalibrated distribution. To mitigate this problem, we propose an accurate and well-calibrated model called Conditional Graph Normalizing Flow (cGNFs). Our model is structured such that a single cGNF can estimate both conditional and marginal densities within the same model - effectively solving a zero-shot density estimation problem. We evaluate cGNF on the Human 3.6M dataset and show that cGNF provides a well-calibrated distribution estimate while being close to state-of-the-art in terms of overall $\\operatorname{minMPJPE}$. Furthermore, cGNF outperforms previous methods on occluded joints while it remains well-calibrated.",
    "authors": [
      "Pawe\u0142 A. Pierzchlewicz",
      "R. James Cotton",
      "Mohammad Bashiri",
      "Fabian H. Sinz"
    ],
    "keywords": [
      "Pose estimation",
      "calibration",
      "metrics",
      "graph neural networks"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      6
    ],
    "real_confidences": [
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Lightweight Equivariant Graph Representation Learning for Protein Engineering": {
    "paper_pk": null,
    "title": "Lightweight Equivariant Graph Representation Learning for Protein Engineering",
    "abstract": "This work tackles the issue of directed evolution in computational protein design that makes accurate predictions of the function of a protein mutant. We design a lightweight pre-training graph neural network model for multi-task protein representation learning from its 3D structure. Rather than reconstructing and optimizing the protein structure, the trained model recovers the amino acid types and key properties of the central residues from a given noisy three-dimensional local environment. On the prediction task for the higher-order mutants, where many amino acid sites of the protein are mutated, the proposed training strategy achieves remarkably higher performance by 20% improvement at the cost of requiring less than 1% of computational resources that are required by popular transformer-based state-of-the-art deep learning models for protein design.",
    "authors": [
      "Bingxin Zhou",
      "Outongyi Lv",
      "Kai Yi",
      "Xinye Xiong",
      "Pan Tan",
      "Liang Hong",
      "Yu Guang Wang"
    ],
    "keywords": [
      "graph neural networks"
    ],
    "real_all_scores": [
      6,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      2,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Fundamental Limits in Formal Verification of Message-Passing Neural Networks": {
    "paper_pk": null,
    "title": "Fundamental Limits in Formal Verification of Message-Passing Neural Networks",
    "abstract": "Output reachability and adversarial robustness are among the most relevant safety properties of neural networks. \nWe show that in the context of Message Passing Neural Networks (MPNN), a common Graph Neural Network (GNN) model, \nformal verification is impossible. In particular, we show that output reachability of graph-classifier MPNN, \nworking over graphs of unbounded size, non-trivial degree and sufficiently expressive node labels, cannot be verified formally: there\nis no algorithm that answers correctly (with yes or no), given an MPNN, whether there exists some valid input to \nthe MPNN such that the corresponding output satisfies a given specification. However, we also show that \noutput reachability and adversarial robustness of node-classifier MPNN can be verified formally when a limit on\nthe degree of input graphs is given a priori. We discuss the implications of these results, for the purpose of\nobtaining a complete picture of the principle possibility to formally verify GNN, depending on \nthe expressiveness of the involved GNN models and input-output specifications.",
    "authors": [
      "Marco S\u00e4lzer",
      "Martin Lange"
    ],
    "keywords": [
      "formal verification",
      "graph neural networks"
    ],
    "real_all_scores": [
      8,
      6,
      8
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "DeepGRAND: Deep Graph Neural Diffusion": {
    "paper_pk": null,
    "title": "DeepGRAND: Deep Graph Neural Diffusion",
    "abstract": "We propose the Deep Graph Neural Diffusion (DeepGRAND), a class of continuous-depth graph neural networks based on the diffusion process on graphs. DeepGRAND leverages a data-dependent scaling term and a perturbation to the graph diffusivity to make the real part of all eigenvalues of the diffusivity matrix become negative, which ensures two favorable theoretical properties: (i) the node representation does not exponentially converge to a constant vector as the model depth increases, thus alleviating the over-smoothing issue; (ii) the stability of the model is guaranteed by controlling the norm of the node representation. Compared to the baseline GRAND, DeepGRAND mitigates the accuracy drop-off with increasing depth and improves the overall accuracy of the model. We empirically corroborate the advantage of DeepGRAND over many existing graph neural networks on various graph deep learning benchmark tasks.",
    "authors": [
      "Khang Nguyen",
      "Nong Minh Hieu",
      "Tan Minh Nguyen",
      "Nguyen Duy Khuong",
      "Vinh Duc NGUYEN"
    ],
    "keywords": [
      "graph neural diffusion",
      "graph neural networks",
      "oversmoothing"
    ],
    "real_all_scores": [
      6,
      5,
      3
    ],
    "real_confidences": [
      3,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Extending graph transformers with quantum computed aggregation": {
    "paper_pk": null,
    "title": "Extending graph transformers with quantum computed aggregation",
    "abstract": "Recently, efforts have been made in the community to design new Graph Neural Networks (GNN), as limitations of Message Passing Neural Networks became more apparent. This led to the appearance of Graph Transformers using global graph features such as Laplacian Eigenmaps. In our paper, we introduce a GNN architecture where the aggregation weights are computed using the long-range correlations of a quantum system. These correlations are generated by translating the graph topology into the interactions of a set of qubits in a quantum computer. The recent development of quantum processing units enables the computation of a new family of global graph features that would be otherwise out of reach for classical hardware. We give some theoretical insights about the potential benefits of this approach, and benchmark our algorithm on standard datasets. Although not being adapted to all datasets, our model performs similarly to standard GNN architectures, and paves a promising future for quantum enhanced GNNs.",
    "authors": [
      "Slimane Thabet",
      "Romain Fouilland",
      "Loic Henriet"
    ],
    "keywords": [
      "graph neural networks",
      "graph representation learning",
      "quantum computing",
      "graph transformers"
    ],
    "real_all_scores": [
      5,
      6,
      6,
      3
    ],
    "real_confidences": [
      3,
      4,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring": {
    "paper_pk": null,
    "title": "Learning to Boost Resilience of Complex Networks via Neural Edge Rewiring",
    "abstract": "The resilience of complex networks, a critical structural characteristic in network science, measures the network's ability to withstand noise corruption and structural changes. Improving resilience typically resorts to minimal modifications of the network structure via degree-preserving edge rewiring-based methods. Despite their effectiveness, existing methods are learning-free, sharing the limitation of transduction: a learned edge rewiring strategy from one graph cannot be generalized to another. Such a limitation cannot be trivially addressed by existing graph neural networks (GNNs)-based approaches since there is no rich initial node features for GNNs to learn meaningful representations. However, neural edge rewiring relies on GNNs for obtaining meaningful representations from pure graph topologies to select edges. We found existing GNNs degenerate remarkably with only pure topologies on the resilience task, leading to the undesired infinite action backtracking. In this work, inspired by persistent homology, we specifically design a variant of GNN called FireGNN for learning inductive edge rewiring strategies. Based on meaningful representations from FireGNN, we develop the first end-to-end inductive method, ResiNet, to discover $\\textbf{resi}$lient $\\textbf{net}$work topologies while balancing network utility. ResiNet reformulates network resilience optimization as a Markov decision process equipped with edge rewiring action space and learns to select correct edges successively.  Extensive experiments demonstrate that ResiNet achieves a near-optimal resilience gain on various graphs while balancing the utility and outperforms existing approaches by a large margin.",
    "authors": [
      "Shanchao Yang",
      "MA KAILI",
      "Tianshu Yu",
      "Baoxiang Wang",
      "Hongyuan Zha"
    ],
    "keywords": [
      "complex networks",
      "network resilience",
      "network robustness",
      "graph neural networks"
    ],
    "real_all_scores": [
      6,
      5,
      5
    ],
    "real_confidences": [
      5,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Explaining Temporal Graph Models through an Explorer-Navigator Framework": {
    "paper_pk": null,
    "title": "Explaining Temporal Graph Models through an Explorer-Navigator Framework",
    "abstract": "While GNN explanation has recently received significant attention, existing works are consistently designed for static graphs. Due to the prevalence of temporal graphs, many temporal graph models have been proposed, but explaining their predictions remains to be explored. To bridge the gap, in this paper, we propose T-GNNExplainer for temporal graph model explanation. Specifically, we regard a temporal graph constituted by a sequence of temporal events. Given a target event, our task is to find a subset of previously occurred events that lead to the model's prediction for it. To handle this combinatorial optimization problem, T-GNNExplainer includes an explorer to find the event subsets with Monte Carlo Tree Search (MCTS)  and a navigator that learns the correlations between events and helps reduce the search space. In particular, the navigator is trained in advance and then integrated with the explorer to speed up searching and achieve better results. To the best of our knowledge, T-GNNExplainer is the first explainer tailored for temporal graph models. We conduct extensive experiments to evaluate the performance of T-GNNExplainer. Experimental results on both real-world and synthetic datasets demonstrate that T-GNNExplainer can achieve superior performance with up to about 50% improvement in Area under Fidelity-Sparsity Curve. ",
    "authors": [
      "Wenwen Xia",
      "Mincai Lai",
      "Caihua Shan",
      "Yao Zhang",
      "Xinnan Dai",
      "Xiang Li",
      "Dongsheng Li"
    ],
    "keywords": [
      "graph neural networks",
      "gnn explainers",
      "temporal graphs"
    ],
    "real_all_scores": [
      3,
      6,
      6
    ],
    "real_confidences": [
      5,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Link Prediction with Non-Contrastive Learning": {
    "paper_pk": null,
    "title": "Link Prediction with Non-Contrastive Learning",
    "abstract": "Graph neural networks (GNNs) are prominent in the graph machine learning domain, owing to their strong performance across various tasks. A recent focal area is the space of graph self-supervised learning (SSL), which aims to derive useful node representations without labeled data. Notably, many state-of-the-art graph SSL methods are contrastive methods, which use a combination of positive and negative samples to learn node representations. Owing to challenges in negative sampling (slowness and model sensitivity), recent literature introduced non-contrastive methods, which instead only use positive samples. Though such methods have shown promising performance in node-level tasks, their suitability for link prediction tasks, which are concerned with predicting link existence between pairs of nodes (and have broad applicability to recommendation systems contexts) is yet unexplored. In this work, we extensively evaluate the performance of existing non-contrastive methods for link prediction in both transductive and inductive settings. While most existing non-contrastive methods perform poorly overall, we find that, surprisingly, BGRL generally performs well in transductive settings. However, it performs poorly in the more realistic inductive settings where the model has to generalize to links to/from unseen nodes. We find that non-contrastive models tend to overfit to the training graph and use this analysis to propose T-BGRL, a novel non-contrastive framework that incorporates cheap corruptions to improve the generalization ability of the model. This simple modification strongly improves inductive performance in 5/6 of our datasets, with up to a 120% improvement in Hits@50 - all with comparable speed to other non-contrastive baselines, and up to $14\\times$ faster than the best-performing contrastive baseline. Our work imparts interesting findings about non-contrastive learning for link prediction and paves the way for future researchers to further expand upon this area.",
    "authors": [
      "William Shiao",
      "Zhichun Guo",
      "Tong Zhao",
      "Evangelos E. Papalexakis",
      "Yozen Liu",
      "Neil Shah"
    ],
    "keywords": [
      "graph learning",
      "graph neural networks",
      "non-contrastive learning",
      "link prediction"
    ],
    "real_all_scores": [
      8,
      8,
      6,
      10
    ],
    "real_confidences": [
      3,
      4,
      2,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-25%"
  },
  "Effects of Graph Convolutions in Multi-layer Networks": {
    "paper_pk": null,
    "title": "Effects of Graph Convolutions in Multi-layer Networks",
    "abstract": "Graph Convolutional Networks (GCNs) are one of the most popular architectures that are used to solve classification problems accompanied by graphical information. We present a rigorous theoretical understanding of the effects of graph convolutions in multi-layer networks. We study these effects through the node classification problem of a non-linearly separable Gaussian mixture model coupled with a stochastic block model. First, we show that a single graph convolution expands the regime of the distance between the means where multi-layer networks can classify the data by a factor of at least $1/\\sqrt[4]{\\rm deg}$, where ${\\rm deg}$ denotes the expected degree of a node. Second, we show that with a slightly stronger graph density, two graph convolutions improve this factor to at least $1/\\sqrt[4]{n}$, where $n$ is the number of nodes in the graph. Finally, we provide both theoretical and empirical insights into the performance of graph convolutions placed in different combinations among the layers of a neural network, concluding that the performance is mutually similar for all combinations of the placement. We present extensive experiments on both synthetic and real-world data that illustrate our results.",
    "authors": [
      "Aseem Baranwal",
      "Kimon Fountoulakis",
      "Aukosh Jagannath"
    ],
    "keywords": [
      "graph neural networks",
      "node classification",
      "classification threshold",
      "contextual stochastic block model"
    ],
    "real_all_scores": [
      8,
      5,
      6,
      6
    ],
    "real_confidences": [
      4,
      2,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Neural Constraint Inference: Inferring Energy Constraints in Interacting Systems": {
    "paper_pk": null,
    "title": "Neural Constraint Inference: Inferring Energy Constraints in Interacting Systems",
    "abstract": "Systems consisting of interacting agents are prevalent in the world, ranging from dynamical systems in physics to complex biological networks. To build systems which can interact robustly in the real world, it is thus important to be able to infer the precise interactions governing such systems.  Existing approaches typically discover such interactions by explicitly modeling the feedforward dynamics of the trajectories. In this work, we propose Neural Constraint Inference (NCI) model as an alternative approach to discover such interactions: it discovers a set of relational constraints, represented as energy functions, which when optimized reconstruct the original trajectory. We illustrate how NCI can faithfully predict future trajectory dynamics, achieving  more consistent long-rollouts than existing approaches. We show that the constraints discovered by NCI are disentangled and may be intermixed with constraints from other trajectories. Finally, we illustrate how those constraints enable the incorporation of external test-time constraints.",
    "authors": [
      "Armand Comas",
      "Yilun Du",
      "Sandesh Ghimire",
      "Christian Fernandez Lopez",
      "Mario Sznaier",
      "Joshua B. Tenenbaum",
      "Octavia Camps"
    ],
    "keywords": [
      "relational inference",
      "energy-based models",
      "energy constraints",
      "trajectory prediction",
      "graph neural networks"
    ],
    "real_all_scores": [
      8,
      6,
      6,
      5
    ],
    "real_confidences": [
      2,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions": {
    "paper_pk": null,
    "title": "Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions",
    "abstract": "The execution behavior of a program often depends on external resources, such as program inputs or file contents, and so the program cannot be run in isolation. Nevertheless, software developers benefit from fast iteration loops where automated tools identify errors as early as possible, even before programs can be compiled and run. This presents an interesting machine learning challenge: can we predict runtime errors in a \"static\" setting, where program execution is not possible? Here, we introduce a competitive programming dataset and task for predicting runtime errors, which we show is difficult for generic models like Transformers. We approach this task by developing an interpreter-inspired architecture with an inductive bias towards mimicking program executions, which models exception handling and \"learns to execute\" descriptions of external resources. Surprisingly, we show that the model can also predict the locations of errors, despite being trained only on labels indicating error presence or absence and kind. In total, we present a practical and difficult-yet-approachable challenge problem related to learning program execution behavior and we demonstrate promising new capabilities of interpreter-inspired machine learning models for code.",
    "authors": [
      "David Bieber",
      "Rishab Goel",
      "Dan Zheng",
      "Hugo Larochelle",
      "Daniel Tarlow"
    ],
    "keywords": [
      "program analysis",
      "graph neural networks",
      "recurrent networks",
      "attention mechanisms",
      "source code",
      "program execution"
    ],
    "real_all_scores": [
      3,
      8,
      3
    ],
    "real_confidences": [
      3,
      5,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Matching receptor to odorant with protein language and graph neural networks": {
    "paper_pk": null,
    "title": "Matching receptor to odorant with protein language and graph neural networks",
    "abstract": "Odor perception in mammals is triggered by interactions between volatile organic compounds and a subset of hundreds of proteins called olfactory receptors (ORs). Molecules activate these receptors in a complex combinatorial coding allowing mammals to discriminate a vast number of chemical stimuli. Recently, ORs have gained attention as new therapeutic targets following the discovery of their involvement in other physiological processes and diseases. To date, predicting molecule-induced activation for ORs is highly challenging since $43\\%$ of ORs have no identified active compound. In this work, we combine [CLS] token from protBERT with a molecular graph and propose a tailored GNN architecture incorporating inductive biases from the protein-molecule binding. We abstract the biological process of protein-molecule activation as the injection of a molecule into a protein-specific environment. On a newly gathered dataset of $46$ $700$ OR-molecule pairs, this model outperforms state-of-the-art models on drug-target interaction prediction as well as standard GNN baselines. Moreover, by incorporating non-bonded interactions the model is able to work with mixtures of compounds. Finally, our predictions reveal a similar activation pattern for molecules within a given odor family, which is in agreement with the theory of combinatorial coding in olfaction.",
    "authors": [
      "Matej Hladi\u0161",
      "Maxence Lalis",
      "Sebastien Fiorucci",
      "J\u00e9r\u00e9mie Topin"
    ],
    "keywords": [
      "Olfaction",
      "protein-ligand binding",
      "olfactory receptors",
      "computational biology",
      "protein language modelling",
      "graph neural networks"
    ],
    "real_all_scores": [
      6,
      8,
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      4,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Energy-based Out-of-Distribution Detection for Graph Neural Networks": {
    "paper_pk": null,
    "title": "Energy-based Out-of-Distribution Detection for Graph Neural Networks",
    "abstract": "Representation learning on semi-structured data, e.g., graphs, has become a central problem in deep learning community as relational structures are pervasive in real situations and induce data inter-dependence that hinders trivial adaptation of existing approaches in other domains where the inputs are assumed to be i.i.d. sampled. However, current models in this regime mostly focus on improving testing performance of in-distribution data and largely ignores the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the model is overconfident in prediction on them. In this paper, we identify a provably effective OOD discriminator based on an energy function directly extracted from a graph neural network trained with standard supervised classification loss. This paves a way for a simple and efficient OOD detection model for GNN-based semi-supervised learning on graphs, which we call GNN-Safe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a non-learning-based structured propagation scheme. Extensive experiments over five real-world datasets validate the practical efficacy of the proposed model for detecting various OOD instances that are inter-connected in a graph with up to 17.0% improvement on average AUROC over competitive peer models and without sacrificing in-distribution testing accuracy.",
    "authors": [
      "Qitian Wu",
      "Yiting Chen",
      "Chenxiao Yang",
      "Junchi Yan"
    ],
    "keywords": [
      "graph neural networks",
      "out-of-distribution detection",
      "semi-supervised node classification",
      "energy model"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      6
    ],
    "real_confidences": [
      3,
      4,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "GOING BEYOND 1-WL EXPRESSIVE POWER WITH 1-LAYER GRAPH NEURAL NETWORKS": {
    "paper_pk": null,
    "title": "GOING BEYOND 1-WL EXPRESSIVE POWER WITH 1-LAYER GRAPH NEURAL NETWORKS",
    "abstract": "Graph neural networks have become the \\textit{de facto} standard for representational learning in graphs, and have achieved SOTA in many graph-related tasks such as node classification, graph classification and link prediction. However, it has been shown that the expressive power is equivalent maximally to Weisfeiler-Lehman Test. Recently, there is a line of work aiming to enhance the expressive power of graph neural networks. In this work, we propose a more generalized variant of neural Weisfeiler-Lehman test to enhance structural representation for each node in a graph to uplift the expressive power of any graph neural network. It is shown theoretically our method is strictly more powerful than 1\\&2-WL test. The Numerical experiments also demonstrate that our proposed method outperforms the standard GNNs on almost all the benchmark datasets by a large margin in most cases with significantly lower running time and memory consumption compared with other more powerful GNNs. ",
    "authors": [
      "Tianjun Yao",
      "Yingxu Wang",
      "Shangsong Liang"
    ],
    "keywords": [
      "graph neural networks",
      "expressivity",
      "memory-efficient"
    ],
    "real_all_scores": [
      3,
      3,
      5
    ],
    "real_confidences": [
      4,
      5,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Scalable and Privacy-enhanced Graph Generative Model for Graph Neural Networks": {
    "paper_pk": null,
    "title": "Scalable and Privacy-enhanced Graph Generative Model for Graph Neural Networks",
    "abstract": "As the field of Graph Neural Networks (GNN) continues to grow, it experiences a corresponding increase in the need for large, real-world datasets to train and test new GNN models on challenging, realistic problems. Unfortunately, such graph datasets are often generated from online, highly privacy-restricted ecosystems, which makes research and development on these datasets hard, if not impossible. This greatly reduces the amount of benchmark graphs available to researchers, causing the field to rely only on a handful of publicly-available datasets. To address this dilemma, we introduce a novel graph generative model, Computation Graph Transformer (CGT) that can learn and reproduce the distribution of real-world graphs in a privacy-enhanced way. Our proposed model (1) generates effective benchmark graphs on which GNNs show similar task performance as on the source graphs, (2) scales to process large-scale real-world graphs, (3) guarantees privacy for end-users. Extensive experiments across a vast body of graph generative models show that only our model can successfully generate privacy-controlled, synthetic substitutes of large-scale real-world graphs that can be effectively used to evaluate GNN models.",
    "authors": [
      "Minji Yoon",
      "Yue Wu",
      "John Palowitch",
      "Bryan Perozzi",
      "Russ Salakhutdinov"
    ],
    "keywords": [
      "graph generative model",
      "graph neural networks",
      "graph convolutional networks",
      "benchmark graph generation"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      6
    ],
    "real_confidences": [
      4,
      2,
      5,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Diffusing Graph Attention": {
    "paper_pk": null,
    "title": "Diffusing Graph Attention",
    "abstract": "The dominant paradigm for machine learning on graphs uses Message Passing Graph Neural Networks~(MP-GNNs), in which node representations are updated by aggregating information in their local neighborhood. Recently, there have been increasingly more attempts to adapt the Transformer architecture to graphs in an effort to solve some known limitations of MP-GNN. A challenging aspect of designing Graph Transformers is integrating the arbitrary graph structure into the architecture. We propose \\emph{Graph Diffuser}~(GD) to address this challenge. GD learns to extract structural and positional relationships between distant nodes in the graph, which it then uses to direct the Transformer's attention and node representation. We demonstrate that existing GNNs and Graph Transformers struggle to capture long-range interactions and how Graph Diffuser does so while admitting intuitive visualizations. Experiments on eight benchmarks show Graph Diffuser to be a highly competitive model, outperforming the state-of-the-art in a diverse set of domains.",
    "authors": [
      "Daniel Glickman",
      "Eran Yahav"
    ],
    "keywords": [
      "Graph Transformer",
      "graph neural networks",
      "transformers",
      "long-range context"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Low-Rank Graph Neural Networks Inspired by the Weak-balance Theory in Social Networks": {
    "paper_pk": null,
    "title": "Low-Rank Graph Neural Networks Inspired by the Weak-balance Theory in Social Networks",
    "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art performance on node classification tasks by exploiting both the graph structures and node features. Generally, most existing GNNs depend on the implicit homophily assumption that nodes belonging to the same class are more likely to be connected. However, GNNs may fail to model heterophilious graphs where nodes with different labels tend to be linked, as shown in recent studies.  To address this issue, we propose a generic GNN applicable to both homophilious and heterophilious graphs, namely Low-Rank Graph Neural Network (LRGNN). In detail, we aim at computing a coefficient matrix such that the sign of each coefficient reveals whether the corresponding two nodes belong to the same class, which is similar to the sign inference problem. In Signed Social Networks (SSNs), the sign inference problem can be modeled as a low-rank matrix factorization (LRMF) problem due to the global low-rank structure described by the weak balance theory. In this paper, we show that signed graphs are naturally generalized weakly-balanced when considering node classification tasks. Motivated by this observation, we propose to leverage LRMF to recover a coefficient matrix from a partially observed signed adjacency matrix. To effectively capture the node similarity, we further incorporate the low-rank representation (LRR) method. Our theoretical result shows that under our update rule of node representations, LRR obtained by solving a subspace clustering problem can recover the subspace structure of node representations. To solve the corresponding optimization problem, we utilize an iterative optimization algorithm with a convergence guarantee and develop a neural-style initialization manner that enables fast convergence. Finally, extensive experimental evaluation on both real-world and synthetic graphs has validated the superior performance of LRGNN over various state-of-the-art GNNs. In particular, LRGNN can offer clear performance gains in a scenario when the node features are not informative enough.",
    "authors": [
      "Langzhang Liang",
      "Xiangjing Hu",
      "Zenglin Xu",
      "Zixing Song",
      "Irwin King"
    ],
    "keywords": [
      "graph neural networks",
      "heterophily",
      "social theory",
      "low rank"
    ],
    "real_all_scores": [
      6,
      6,
      6,
      5
    ],
    "real_confidences": [
      4,
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Conditional Invariances for Conformer Invariant Protein Representations": {
    "paper_pk": null,
    "title": "Conditional Invariances for Conformer Invariant Protein Representations",
    "abstract": "Representation learning for proteins is an emerging area in geometric deep learning. Recent works have factored in both the relational (atomic bonds) and the geometric aspects (atomic positions) of the task, notably bringing together graph neural networks (GNNs) with neural networks for point clouds. The equivariances and invariances to geometric transformations (group actions such as rotations and translations) so far treats large molecules as rigid structures. However, in many important settings, proteins can co-exist as an ensemble of multiple stable conformations. The conformations of a protein, however, cannot be described as input-independent transformations of the protein: Two proteins may require different sets of transformations in order to describe their set of viable conformations. To address this limitation, we introduce the concept of conditional transformations (CT). CT can capture protein structure, while respecting the restrictions posed by constraints on dihedral (torsion) angles and steric repulsions between atoms. We then introduce a Markov chain Monte Carlo framework to learn representations that are invariant to these conditional transformations. Our results show that endowing existing baseline models with these conditional transformations helps improve their performance without sacrificing computational cost.",
    "authors": [
      "Balasubramaniam Srinivasan",
      "Vassilis N. Ioannidis",
      "Soji Adeshina",
      "Mayank Kakodkar",
      "George Karypis",
      "Bruno Ribeiro"
    ],
    "keywords": [
      "Invariances",
      "Conditional Invariances",
      "input dependent invariances",
      "proteins",
      "protein representation learning",
      "conformer invariant representations",
      "graph neural networks",
      "group invariant neural networks"
    ],
    "real_all_scores": [
      6,
      3,
      5,
      6
    ],
    "real_confidences": [
      4,
      4,
      1,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Can GNNs Learn Heuristic Information for Link Prediction?": {
    "paper_pk": null,
    "title": "Can GNNs Learn Heuristic Information for Link Prediction?",
    "abstract": "Graph Neural Networks (GNNs) have shown superior performance in Link Prediction (LP). Especially, SEAL and its successors address the LP problem by classifying the subgraphs extracted specifically for candidate links, gaining state-of-the-art results. Nevertheless, we question whether these methods can effectively learn the information equivalent to link heuristics such as Common Neighbors, Katz index, etc. (we refer to such information as heuristic information in this work). We show that link heuristics and GNNs capture different information. Link heuristics usually collect pair-specific information by counting the involved neighbors or paths between two nodes in a candidate link, while GNNs learn node-wise representations through a neighborhood aggregation algorithm in which two nodes in the candidate link do not pay special attention to each other. Our further analysis shows that SEAL-type methods only use a GNN to model the pair-specific subgraphs and also cannot effectively capture heuristic information. To verify our analysis, a straightforward way is to compare the LP performance between existing methods and a model that learns heuristic information independently of the GNN learning. To this end, we present a simple yet light framework ComHG by directly Combining the embeddings of link Heuristics and the representations produced by a GNN. Experiments on OGB LP benchmarks show that ComHG outperforms all top competitors by a large margin, empirically confirming our propositions. Our experimental study also indicates that the contributions of link heuristics and the GNN to LP are sensitive to the graph degree, where the former is powerful on sparse graphs while the latter becomes dominant on dense graphs.",
    "authors": [
      "Shuming Liang",
      "Yu Ding",
      "Zhidong Li",
      "Bin Liang",
      "siqi zhang",
      "Yang Wang",
      "Fang Chen"
    ],
    "keywords": [
      "link prediction",
      "graph neural networks",
      "heuristics"
    ],
    "real_all_scores": [
      5,
      5,
      6
    ],
    "real_confidences": [
      4,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Deep Ensembles for Graphs with Higher-order Dependencies": {
    "paper_pk": null,
    "title": "Deep Ensembles for Graphs with Higher-order Dependencies",
    "abstract": "Graph neural networks (GNNs) continue to achieve state-of-the-art performance on many graph learning tasks, but rely on the assumption that a given graph is a sufficient approximation of the true neighborhood structure. In the presence of higher-order sequential dependencies, we show that the tendency of traditional graph representations to underfit each node's neighborhood causes existing GNNs to generalize poorly. To address this, we propose a novel Deep Graph Ensemble (DGE), which captures neighborhood variance by training an ensemble of GNNs on different neighborhood subspaces of the same node within a higher-order network structure. We show that DGE consistently outperforms existing GNNs on semisupervised and supervised tasks on six real-world data sets with known higher-order dependencies, even under a similar parameter budget. We demonstrate that learning diverse and accurate base classifiers is central to DGE's success, and discuss the implications of these findings for future work on GNNs.",
    "authors": [
      "Steven Krieg",
      "William Burgis",
      "Patrick Soga",
      "Nitesh Chawla"
    ],
    "keywords": [
      "graph neural networks",
      "higher order networks",
      "deep ensembles",
      "representation learning",
      "semisupervised learning"
    ],
    "real_all_scores": [
      6,
      5,
      5
    ],
    "real_confidences": [
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "MEGAN: Multi Explanation Graph Attention Network": {
    "paper_pk": null,
    "title": "MEGAN: Multi Explanation Graph Attention Network",
    "abstract": "Explainable artificial intelligence (XAI) methods are expected to improve trust during human-AI interactions, provide tools for model analysis and extend human understanding of complex problems. Attention-based models are an important subclass of XAI methods, partly due to their full differentiability and the potential to improve explanations by means of explanation-supervised training. We propose the novel multi-explanation graph attention network (MEGAN). Our graph regression and classification model features multiple explanation channels, which can be chosen independently of the task specifications. We first validate our model on a synthetic graph regression dataset, where our model produces single-channel explanations with quality similar to GNNExplainer. Furthermore, we demonstrate the advantages of multi-channel explanations on one synthetic and two real-world datasets: The prediction of water solubility of molecular graphs and sentiment classification of movie reviews. We find that our model produces explanations consistent with human intuition, opening the way to learning from our model in less well-understood tasks.",
    "authors": [
      "Jonas Teufel",
      "Luca Torresi",
      "Patrick Nicholas Reiser",
      "Pascal Friederich"
    ],
    "keywords": [
      "explainable artificial intelligence",
      "interpretable machine learning",
      "graph neural networks",
      "attention network",
      "graph regression",
      "graph classification"
    ],
    "real_all_scores": [
      3,
      3,
      6
    ],
    "real_confidences": [
      3,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs": {
    "paper_pk": null,
    "title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
    "abstract": "Despite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.",
    "authors": [
      "Yi-Lun Liao",
      "Tess Smidt"
    ],
    "keywords": [
      "equivariant neural networks",
      "graph neural networks",
      "computational physics",
      "transformer networks"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      5,
      4,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Towards Equivariant Graph Contrastive Learning via Cross-Graph Augmentation": {
    "paper_pk": null,
    "title": "Towards Equivariant Graph Contrastive Learning via Cross-Graph Augmentation",
    "abstract": "Leading graph contrastive learning (GCL) frameworks conform to the invariance mechanism by encouraging insensitivity to different augmented views of the same graph. Despite the promising performance, invariance worsens representation when augmentations cause aggressive semantics shifts. For example, dropping the super-node can dramatically change a social network's topology. In this case, encouraging invariance to the original graph can bring together dissimilar patterns and hurt the task of instance discrimination. To resolve the problem, we get inspiration from equivariant self-supervised learning and propose Equivariant Graph Contrastive Learning (E-GCL) to encourage the sensitivity to global semantic shifts. Viewing each graph as a transformation to others, we ground the equivariance principle as a cross-graph augmentation -- graph interpolation -- to simulate global semantic shifts. Without using annotation, we supervise the representation of cross-graph augmented views by linearly combining the representations of their original samples. This simple but effective equivariance principle empowers E-GCL with the ability of cross-graph discrimination. It shows significant improvements over the state-of-the-art GCL models in unsupervised learning and transfer learning. Further experiments demonstrate E-GCL's generalization to various graph pre-training frameworks. Code is available at \\url{https://anonymous.4open.science/r/E-GCL/}",
    "authors": [
      "Zhiyuan Liu",
      "An Zhang",
      "Yu Sun",
      "Yicong Li",
      "Yaorui Shi",
      "Sihang Li",
      "Xiang Wang",
      "Xiangnan He",
      "Tat-Seng Chua"
    ],
    "keywords": [
      "equivariant",
      "self-supervised learning",
      "contrastive learning",
      "graph neural networks"
    ],
    "real_all_scores": [
      3,
      3,
      6,
      3
    ],
    "real_confidences": [
      4,
      3,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "A Graph Neural Network Approach to Automated Model Building in Cryo-EM Maps": {
    "paper_pk": null,
    "title": "A Graph Neural Network Approach to Automated Model Building in Cryo-EM Maps",
    "abstract": "Electron cryo-microscopy (cryo-EM) produces three-dimensional (3D) maps of the electrostatic potential of biological macromolecules, including proteins. At sufficient resolution, the cryo-EM maps, along with some knowledge about the imaged molecules, allow de novo atomic modelling. Typically, this is done through a laborious manual process. Recent advances in machine learning applications to protein structure prediction show potential for automating this process. Taking inspiration from these techniques, we have built ModelAngelo for automated model building of proteins in cryo-EM maps. ModelAngelo first uses a residual convolutional neural network (CNN) to initialize a graph representation with nodes assigned to individual amino acids of the proteins in the map and edges representing the protein chain. The graph is then refined with a graph neural network (GNN) that combines the cryo-EM data, the amino acid sequence data and prior knowledge about protein geometries. The GNN refines the geometry of the protein chain and classifies the amino acids for each of its nodes. The final graph is post-processed with a hidden Markov model (HMM) search to map each protein chain to entries in a user provided sequence file. Application to 28 test cases shows that ModelAngelo outperforms state-of-the-art and approximates manual building for cryo-EM maps with resolutions better than 3.5 A.",
    "authors": [
      "Kiarash Jamali",
      "Dari Kimanius",
      "Sjors HW Scheres"
    ],
    "keywords": [
      "cryo-em",
      "model building",
      "graph neural networks",
      "attention networks",
      "proteins"
    ],
    "real_all_scores": [
      6,
      6,
      8,
      6
    ],
    "real_confidences": [
      4,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Confidence-Based Feature Imputation for Graphs with Partially Known Features": {
    "paper_pk": null,
    "title": "Confidence-Based Feature Imputation for Graphs with Partially Known Features",
    "abstract": "This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting the certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\\%) and it achieves state-of-the-art accuracy for both semi-supervised node classification and link prediction on various datasets containing a high rate of missing features. Codes are available at https://github.com/daehoum1/pcfi.",
    "authors": [
      "Daeho Um",
      "Jiwoong Park",
      "Seulki Park",
      "Jin young Choi"
    ],
    "keywords": [
      "Graph neural networks",
      "Graphs",
      "Missing features"
    ],
    "real_all_scores": [
      8,
      8,
      10
    ],
    "real_confidences": [
      3,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Exphormer: Scaling Graph Transformers with Expander Graphs": {
    "paper_pk": null,
    "title": "Exphormer: Scaling Graph Transformers with Expander Graphs",
    "abstract": "Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer,  a framework for building powerful and scalable graph transformers.  Exphormer consists of a sparse attention mechanism based on expander graphs, whose mathematical characteristics, such as spectral expansion, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that Exphormer can scale to datasets on larger graphs than shown in previous graph transformer architectures.",
    "authors": [
      "Hamed Shirzad",
      "Ameya Velingker",
      "Balaji Venkatachalam",
      "Danica J. Sutherland",
      "Ali Kemal Sinop"
    ],
    "keywords": [
      "Graph neural networks",
      "Transformers"
    ],
    "real_all_scores": [
      3,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization": {
    "paper_pk": null,
    "title": "Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization",
    "abstract": "Graph Neural Networks (GNNs) have achieved state-of-the-art results on a variety of graph learning tasks, however, it has been demonstrated that they are vulnerable to adversarial attacks, raising serious security concerns. A lot of studies have been developed to train GNNs in a noisy environment and increase their robustness against adversarial attacks. However, existing methods have not uncovered a principled difficulty: the convoluted mixture distribution between clean and attacked data samples, which leads to sub-optimal model design and limits their frameworks\u2019 robustness. In this work, we first begin by identifying the root cause of mixture distribution, then, for tackling it, we propose a novel method GAME - Graph Adversarial Mixture of Experts to enlarge the model capacity and enrich the representation diversity of adversarial samples, from three perspectives of model, training, and optimization. Specifically, we first propose a plug-and- play GAME layer that can be easily incorporated into any GNNs and enhance their adversarial learning capabilities. Second, we design a decoupling-based graph adversarial training in which the component of the model used to generate adversarial graphs is separated from the component used to update weights. Third, we introduce a graph diversity regularization that enables the model to learn diverse representation and further improves model performance. Extensive experiments demonstrate the effectiveness and advantages of GAME over the state-of-the-art adversarial training methods across various datasets given different attacks.",
    "authors": [
      "Chunhui Zhang",
      "Yijun Tian",
      "Mingxuan Ju",
      "Zheyuan Liu",
      "Yanfang Ye",
      "Nitesh Chawla",
      "Chuxu Zhang"
    ],
    "keywords": [
      "Graph neural networks",
      "Mixture of experts",
      "Graph adversarial learning"
    ],
    "real_all_scores": [
      5,
      5,
      3,
      5
    ],
    "real_confidences": [
      4,
      4,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Affinity-Aware Graph Networks": {
    "paper_pk": null,
    "title": "Affinity-Aware Graph Networks",
    "abstract": "Graph Neural Networks (GNNs) have emerged as a powerful technique for learning on relational data. Owing to the relatively limited number of message passing steps they perform\u2014and hence a smaller receptive field\u2014there has been significant interest in improving their expressivity by incorporating structural aspects of the underlying graph. In this paper, we explore the use of affinity measures as features in graph neural networks, in particular measures arising from random walks, including effective resistance, hitting and commute times. We propose message passing networks based on these features and evaluate their performance on a variety of node and graph property prediction tasks.",
    "authors": [
      "Ameya Velingker",
      "Ali Kemal Sinop",
      "Ira Ktena",
      "Petar Veli\u010dkovi\u0107",
      "Sreenivas Gollapudi"
    ],
    "keywords": [
      "Graph neural networks",
      "message passing networks",
      "effective resistance"
    ],
    "real_all_scores": [
      6,
      3,
      8,
      6
    ],
    "real_confidences": [
      4,
      3,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "Localized Randomized Smoothing for Collective Robustness Certification": {
    "paper_pk": null,
    "title": "Localized Randomized Smoothing for Collective Robustness Certification",
    "abstract": "Models for image segmentation, node classification and many other tasks map a single input to multiple labels. By perturbing this single shared input (e.g. the image) an adversary can manipulate several predictions (e.g. misclassify several pixels). Collective robustness certification is the task of provably bounding the number of robust predictions under this threat model. The only dedicated method that goes beyond certifying each output independently is limited to strictly local models, where each prediction is associated with a small receptive field. We propose a more general collective robustness certificate for all types of models. We further show that this approach is beneficial for the larger class of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on our novel localized randomized smoothing approach, where the random perturbation strength for different input regions is proportional to their importance for the outputs. Localized smoothing Pareto-dominates existing certificates on both image segmentation and node classification tasks, simultaneously offering higher accuracy and stronger certificates.",
    "authors": [
      "Jan Schuchardt",
      "Tom Wollschl\u00e4ger",
      "Aleksandar Bojchevski",
      "Stephan G\u00fcnnemann"
    ],
    "keywords": [
      "Robustness",
      "Certification",
      "Verification",
      "Trustworthiness",
      "Graph neural networks"
    ],
    "real_all_scores": [
      5,
      8,
      3,
      5
    ],
    "real_confidences": [
      3,
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs": {
    "paper_pk": null,
    "title": "Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs",
    "abstract": "Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node's representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substructures like ring systems with more than four atoms. To overcome this limitation, we propose I$^2$-GNNs to extend Subgraph MPNNs by assigning different identifiers for the root node and its neighbors in each subgraph. I$^2$-GNNs' discriminative power is shown to be strictly stronger than Subgraph MPNNs and partially stronger than the 3-WL test. More importantly, I$^2$-GNNs are proven capable of counting all 3, 4, 5 and 6-cycles, covering common substructures like benzene rings in organic chemistry, while still keeping linear complexity. To the best of our knowledge, it is the first linear-time GNN model that can count 6-cycles with theoretical guarantees. We validate its counting power in cycle counting tasks and demonstrate its competitive performance in molecular prediction benchmarks.",
    "authors": [
      "Yinan Huang",
      "Xingang Peng",
      "Jianzhu Ma",
      "Muhan Zhang"
    ],
    "keywords": [
      "Graph neural networks"
    ],
    "real_all_scores": [
      8,
      6,
      6,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: poster"
  },
  "CUTS: Neural Causal Discovery from Irregular Time-Series Data": {
    "paper_pk": null,
    "title": "CUTS: Neural Causal Discovery from Irregular Time-Series Data",
    "abstract": "Causal discovery from time-series data has been a central task in machine learning. Recently, Granger causality inference is gaining momentum due to its good explainability and high compatibility with emerging deep neural networks. However, most existing methods assume structured input data and degenerate greatly when encountering data with randomly missing entries or non-uniform sampling frequencies, which hampers their applications in real scenarios. To address this issue, here we present CUTS, a neural Granger causal discovery algorithm to jointly impute unobserved data points and build causal graphs, via plugging in two mutually boosting modules in an iterative framework: (i) Latent data prediction stage: designs a Delayed Supervision Graph Neural Network (DSGNN) to hallucinate and register unstructured data which might be of high dimension and with complex distribution; (ii) Causal graph fitting stage: builds a causal adjacency matrix with imputed data under sparse penalty. Experiments show that CUTS effectively infers causal graphs from irregular time-series data, with significantly superior performance to existing methods. Our approach constitutes a promising step towards applying causal discovery to real applications with non-ideal observations.",
    "authors": [
      "Yuxiao Cheng",
      "Runzhao Yang",
      "Tingxiong Xiao",
      "Zongren Li",
      "Jinli Suo",
      "Kunlun He",
      "Qionghai Dai"
    ],
    "keywords": [
      "Time series",
      "Granger causality",
      "Causal discovery",
      "Neural networks",
      "Graph neural networks"
    ],
    "real_all_scores": [
      5,
      3,
      3
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Equivariant Descriptor Fields: SE(3)-Equivariant Energy-Based Models for End-to-End Visual Robotic Manipulation Learning": {
    "paper_pk": null,
    "title": "Equivariant Descriptor Fields: SE(3)-Equivariant Energy-Based Models for End-to-End Visual Robotic Manipulation Learning",
    "abstract": "End-to-end learning for visual robotic manipulation is known to suffer from sample inefficiency, requiring large numbers of demonstrations. The spatial roto-translation equivariance, or the SE(3)-equivariance can be exploited to improve the sample efficiency for learning robotic manipulation. In this paper, we present SE(3)-equivariant models for visual robotic manipulation from point clouds that can be trained fully end-to-end. By utilizing the representation theory of the Lie group, we construct novel SE(3)-equivariant energy-based models that allow highly sample efficient end-to-end learning. We show that our models can learn from scratch without prior knowledge and yet are highly sample efficient (5~10 demonstrations are enough). Furthermore, we show that our models can generalize to tasks with (i) previously unseen target object poses, (ii) previously unseen target object instances of the category, and (iii) previously unseen visual distractors. We experiment with 6-DoF robotic manipulation tasks to validate our models' sample efficiency and generalizability. Codes are available at: https://github.com/tomato1mule/edf",
    "authors": [
      "Hyunwoo Ryu",
      "Hong-in Lee",
      "Jeong-Hoon Lee",
      "Jongeun Choi"
    ],
    "keywords": [
      "Robotics",
      "Manipulation",
      "Robotic manipulation",
      "Equivariance",
      "SE(3)",
      "SO(3)",
      "Energy-based model",
      "Lie group",
      "Representation theory",
      "Equivariant robotics",
      "Roto-translation equivariance",
      "End-to-end",
      "Point clouds",
      "Graph neural networks",
      "Imitation learning",
      "Learning from demonstration",
      "Sample efficient",
      "Few shot",
      "Unseen object",
      "Category-level manipulation",
      "MCMC",
      "Langevin dynamics"
    ],
    "real_all_scores": [
      6,
      5,
      5
    ],
    "real_confidences": [
      2,
      3,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Distributional Signals for Node Classification in Graph Neural Networks": {
    "paper_pk": null,
    "title": "Distributional Signals for Node Classification in Graph Neural Networks",
    "abstract": "In graph neural networks (GNNs), both node features and labels are examples of graph signals, a key notion in graph signal processing (GSP). While it is common in GSP to impose signal smoothness constraints in learning and estimation tasks, it is unclear how this can be done for discrete node labels. We bridge this gap by introducing the concept of distributional graph signals. In our framework, we work with the distributions of node labels instead of their values and propose notions of smoothness and non-uniformity of such distributional graph signals. We then propose a general regularization method for GNNs that allows us to encode distributional smoothness and non-uniformity of the model output in semi-supervised node classification tasks. Numerical experiments demonstrate that our method can significantly improve the performance of most base GNN models in different problem settings. ",
    "authors": [
      "Feng Ji",
      "See Hian Lee",
      "Zhao Kai",
      "Wee Peng Tay",
      "Jielong Yang"
    ],
    "keywords": [
      "Graph neural networks",
      "graph signal processing",
      "regularization",
      "smoothness",
      "node classification"
    ],
    "real_all_scores": [
      6,
      5,
      3,
      5
    ],
    "real_confidences": [
      3,
      2,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success": {
    "paper_pk": null,
    "title": "SlenderGNN: Accurate, Robust, and Interpretable GNN, and the Reasons for its Success",
    "abstract": "Can we design a GNN that is accurate and interpretable at the same time? Could it also be robust to handle the case of homophily, heterophily, or even noisy edges without network effects? We propose SlenderGNN that has all desirable properties: (a) accurate, (b) robust, and (c) interpretable. For the reasons of its success, we had to dig deeper: The result is our GNNLIN framework which highlights the fundamental differences among popular GNN models (e.g., feature combination, structural normalization, etc.) and thus reveals the reasons for the success of our SlenderGNN, as well as the reasons for occasional failures of other GNN variants. Thanks to our careful design, SlenderGNN passes all the 'sanity checks' we propose, and it achieves the highest overall accuracy on 9 real-world datasets of both homophily and heterophily graphs, when compared against 10 recent GNN models. Specifically, SlenderGNN exceeds the accuracy of linear GNNs and matches or exceeds the accuracy of nonlinear models with up to 64 times fewer parameters.",
    "authors": [
      "Jaemin Yoo",
      "Meng-Chieh Lee",
      "Shubhranshu Shekhar",
      "Christos Faloutsos"
    ],
    "keywords": [
      "Graph neural networks",
      "Linear models",
      "Node classification",
      "Heterophily graphs",
      "Lightweight models"
    ],
    "real_all_scores": [
      1,
      6,
      6,
      5,
      3
    ],
    "real_confidences": [
      5,
      3,
      1,
      3,
      4
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Wide Graph Neural Network": {
    "paper_pk": null,
    "title": "Wide Graph Neural Network",
    "abstract": "Usually, graph neural networks (GNNs) suffer from several problems, e.g., over-smoothing (in the spatial domain), poor flexibility (in the spectral domain), and low performance on heterophily (in both domains). In this paper, we provide a new GNN framework, called Wide Graph Neural Networks (WGNN) to solve these problems. It is motivated by our proposed unified view of GNNs from the perspective of dictionary learning. In light of this view, we formulate the graph learning in GNNs as learning representations from the dictionaries, where the fixed graph information is regarded as the dictionary and the trainable parameters are representations. Then, the dictionaries of spatial GNNs encode the adjacency matrix multiplication, while spectral ones sum its polynomials. Differently, WGNN directly concatenates all polynomials as the dictionary, where each polynomial is a sub-dictionary. Beyond polynomials, WGNN allows sub-dictionaries with an arbitrary size, for instance, the principal components of the adjacency matrix. This wide concatenation structure enjoys the great capability of avoiding over-smoothing and promoting flexibility, while the supplement of principal components can significantly improve the representation of heterophilic graphs. We provide a detailed theoretical analysis and conduct extensive experiments on eight datasets to demonstrate the superiority of the proposed WGNN. ",
    "authors": [
      "Jiaqi Sun",
      "Lin Zhang",
      "Guangyi Chen",
      "Kun Zhang",
      "Peng XU",
      "Yujiu Yang"
    ],
    "keywords": [
      "Graph neural networks",
      "represenation learning",
      "dictionary learning"
    ],
    "real_all_scores": [
      5,
      6,
      6,
      6
    ],
    "real_confidences": [
      4,
      3,
      2,
      3
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Relational Curriculum Learning for Graph Neural Networks": {
    "paper_pk": null,
    "title": "Relational Curriculum Learning for Graph Neural Networks",
    "abstract": "Graph neural networks have achieved great success in representing structured data and its downstream tasks such as node classification. The key idea is to recursively propagate and aggregate information along the edges of a given graph topology. However, edges in real-world graphs often have varying degrees of difficulty, and some edges may even be noisy to the downstream tasks. Therefore, existing graph neural network models may lead to suboptimal learned representations because they usually consider every edge in a given graph topology equally. On the other hand, curriculum learning, which mimics the human learning principle of learning data samples in a meaningful order, has been shown to be effective in improving the generalization ability of representation learners by gradually proceeding from easy to more difficult samples during training. Unfortunately, most existing curriculum learning strategies are designed for i.i.d data samples and cannot be trivially generalized to handle structured data with dependencies. In order to address these issues, in this paper we propose a novel curriculum learning method for structured data to leverage the various underlying difficulties of data dependencies to improve the quality of learned representations on structured data. Specifically, we design a learning strategy that gradually incorporates edges in a given graph topology into training according to their difficulty from easy to hard, where the degree of difficulty is measured by a self-supervised learning paradigm. We demonstrate the strength of our proposed method in improving the generalization ability of learned representations through extensive experiments on nine synthetic datasets and seven real-world datasets with different commonly used graph neural network models as backbone models.",
    "authors": [
      "Zheng Zhang",
      "Junxiang Wang",
      "Liang Zhao"
    ],
    "keywords": [
      "Graph neural networks",
      "Curriculum learning"
    ],
    "real_all_scores": [
      5,
      8,
      3
    ],
    "real_confidences": [
      4,
      3,
      5
    ],
    "real_contents": [],
    "real_decision": "Reject"
  },
  "Cross-modal Graph Contrastive Learning with Cellular Images": {
    "paper_pk": null,
    "title": "Cross-modal Graph Contrastive Learning with Cellular Images",
    "abstract": "Constructing discriminative representations of molecules lies at the core of a number of domains such as drug discovery, material science, and chemistry. State-of-the-art methods employ graph neural networks (GNNs) and self-supervised learning (SSL) to learn the structural representations from unlabeled data, which can then be fine-tuned for downstream tasks. Albeit powerful, these methods that are pre-trained solely on molecular structures cannot generalize well to the tasks involved in intricate biological processes. To cope with this challenge, we propose using high-content cell microscopy images to assist in learning molecular representation. The fundamental rationale of our method is to leverage the correspondence between molecular topological structures and the caused perturbations at the phenotypic level. By including cross-modal pre-training with different types of contrastive loss functions in a unified framework, our model can efficiently learn generic and informative representations from cellular images, which are complementary to molecular structures. Empirical experiments demonstrated that the model transfers non-trivially to a variety of downstream tasks and is often competitive with the existing SSL baselines, e.g., a 15.4\\% absolute Hit@10 gains in graph-image retrieval task and a 4.0\\% absolute AUC improvements in clinical outcome predictions. Further zero-shot case studies show the potential of the approach to be applied to real-world drug discovery. ",
    "authors": [
      "Shuangjia Zheng",
      "Jiahua Rao",
      "Jixian Zhang",
      "Cohen Ethan",
      "Chengtao Li",
      "Yuedong Yang"
    ],
    "keywords": [
      "Celluar Image",
      "Drug discovery",
      "Graph neural networks",
      "Self-supervised learning",
      "Cross-modal learning"
    ],
    "real_all_scores": [
      5,
      6,
      5,
      3
    ],
    "real_confidences": [
      4,
      2,
      4,
      5
    ],
    "real_contents": [],
    "real_decision": ""
  },
  "Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks": {
    "paper_pk": null,
    "title": "Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks",
    "abstract": "Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include graph sparsification that samples a subgraph to reduce the amount of data aggregation and model sparsification that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.",
    "authors": [
      "Shuai Zhang",
      "Meng Wang",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Songtao Lu",
      "Miao Liu"
    ],
    "keywords": [
      "Learning theory",
      "Graph neural networks",
      "Generalization analysis",
      "Graph sparisification"
    ],
    "real_all_scores": [
      8,
      8,
      8
    ],
    "real_confidences": [
      4,
      4,
      4
    ],
    "real_contents": [],
    "real_decision": "Accept: notable-top-5%"
  },
  "Your Neighbors Are Communicating: Towards Powerful and Scalable Graph Neural Networks": {
    "paper_pk": null,
    "title": "Your Neighbors Are Communicating: Towards Powerful and Scalable Graph Neural Networks",
    "abstract": "Message passing graph neural networks (GNNs) are known to have their expressiveness upper-bounded by 1-dimensional Weisfeiler-Lehman (1-WL) algorithm. To achieve more powerful GNNs, existing attempts either require ad hoc features, or involve operations that incur high time and space complexities. In this work, we propose a general and provably powerful GNN framework that preserves the scalability of message passing scheme. In particular, we first propose to empower 1-WL for graph isomorphism test by considering edges among neighbors, giving rise to NC-1-WL. The expressiveness of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically. Further, we propose the NC-GNN framework as a differentiable neural version of NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as NC-1-WL. Experiments demonstrate that our NC-GNN achieves remarkable performance on various benchmarks.",
    "authors": [
      "Meng Liu",
      "Haiyang Yu",
      "Shuiwang Ji"
    ],
    "keywords": [
      "Graph neural networks",
      "expressiveness",
      "graph isomorphism test"
    ],
    "real_all_scores": [
      5,
      5,
      5,
      5,
      5
    ],
    "real_confidences": [
      4,
      4,
      3,
      4,
      3
    ],
    "real_contents": [],
    "real_decision": ""
  }
}
