{
  "absolute_rank_consistency": 7.4,
  "spearman_rank_consistency": -0.22556390977443608,
  "kendall_rank_consistency": -0.11578947368421053,
  "sim_ranks": [
    4,
    2,
    3,
    8,
    5,
    9,
    6,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    1,
    18,
    7,
    19,
    20
  ],
  "real_ranks": [
    8,
    5,
    6,
    10,
    12,
    15,
    19,
    17,
    20,
    2,
    16,
    11,
    1,
    14,
    9,
    18,
    13,
    7,
    3,
    4
  ],
  "sim_avg_score_variance": 7.4122222222222245,
  "real_avg_score_variance": 1.4315277777777777,
  "papers": {
    "Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing": {
      "paper_pk": "bd0e114b-223a-4144-b5c2-8bd1b8c0c032",
      "title": "Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing",
      "abstract": "Most graph neural networks follow the message passing mechanism. However, it faces the over-smoothing problem when multiple times of message passing is applied to a graph, causing indistinguishable node representations and prevents the model to effectively learn dependencies between farther-away nodes. On the other hand, features of neighboring nodes with different labels are likely to be falsely mixed, resulting in the heterophily problem. In this work, we propose to order the messages passing into the node representation, with specific blocks of neurons targeted for message passing within specific hops. This is achieved by aligning the hierarchy of the rooted-tree of a central node with the ordered neurons in its node representation. Experimental results on an extensive set of datasets show that our model can simultaneously achieve the state-of-the-art in both homophily and heterophily settings, without any targeted design. Moreover, its performance maintains pretty well while the model becomes really deep, effectively preventing the over-smoothing problem. Finally, visualizing the gating vectors shows that our model learns to behave differently between homophily and heterophily settings, providing an explainable graph neural model.",
      "authors": [
        "Yunchong Song",
        "Chenghu Zhou",
        "Xinbing Wang",
        "Zhouhan Lin"
      ],
      "keywords": [
        "GNN",
        "heterophily",
        "over-smoothing"
      ],
      "real_avg_scores": 5.75,
      "real_all_scores": [
        5,
        5,
        5,
        8
      ],
      "real_confidences": [
        4,
        4,
        4,
        4
      ],
      "real_contents": [],
      "real_rank": 8,
      "real_decision": "Accept: poster",
      "sim_avg_scores": 5.666666666666667,
      "sim_all_scores": [
        9,
        8,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Graph Neural Networks**\n\nThe proposed Ordered GNN is a game-changer in the field of graph neural networks. By ordering message passing to deal with heterophily and over-smoothing, the authors have successfully addressed two of the most significant challenges in graph-based learning. The experimental results are impressive, demonstrating state-of-the-art performance in both homophily and heterophily settings. The ability of the model to maintain its performance even at greater depths is a significant advantage, making it a promising solution for real-world applications.\n\n**Review 2: A Novel Approach to Graph Neural Networks**\n\nThe Ordered GNN presents a fresh perspective on graph neural networks, tackling the issues of heterophily and over-smoothing through a clever ordering of message passing. By aligning the hierarchy of the rooted-tree of a central node with the ordered neurons in its node representation, the authors have developed a model that can effectively learn dependencies between farther-away nodes. The visualization of the gating vectors provides valuable insights into the model's behavior, making it a more explainable and trustworthy solution.\n\n**Review 3: Overcoming the Limitations of Traditional GNNs**\n\nTraditional graph neural networks often struggle with the over-smoothing problem, leading to indistinguishable node representations. The Ordered GNN offers a solution to this problem by introducing an ordering mechanism that prevents the loss of information during message passing. Additionally, the model's ability to handle heterophily settings makes it a versatile solution for a wide range of graph-based learning tasks. The experimental results are convincing, and the model's performance is impressive, especially in deeper architectures.\n\n**Review 4: A Step Towards Explainable Graph Neural Networks**\n\nThe Ordered GNN is not only a high-performing model but also provides valuable insights into its decision-making process. The visualization of the gating vectors offers a unique perspective on how the model behaves differently in homophily and heterophily settings. This level of explainability is essential in graph-based learning, where the complexity of the data can make it challenging to understand the model's behavior. The authors have made a significant contribution to the field, and their work has the potential to inspire further research in explainable graph neural networks.\n\n**Review 5: A Robust Solution for Real-World Applications**\n\nThe Ordered GNN is a robust solution that can handle the complexities of real-world graph-based learning tasks. By addressing the issues of heterophily and over-smoothing, the",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"Ordered GNN: A Breakthrough in Graph Neural Networks\" - 5/5 stars\n\nThe authors of this paper have made a significant contribution to the field of graph neural networks by proposing a novel approach to deal with the long-standing issues of over-smoothing and heterophily. By ordering the message passing mechanism, they have successfully addressed these problems and achieved state-of-the-art results on a wide range of datasets. The proposed model is not only effective but also explainable, providing valuable insights into the behavior of the model in different settings. This work has the potential to revolutionize the field of graph neural networks and is a must-read for anyone working in this area.\n\n**Review 2:**\n\"A Step Towards Explainable Graph Neural Networks\" - 4.5/5 stars\n\nThis paper presents a well-designed approach to tackle the challenges of over-smoothing and heterophily in graph neural networks. The authors' proposal to order the message passing mechanism is intuitive and effective, as demonstrated by the experimental results. The visualization of the gating vectors provides a unique insight into the model's behavior, making it more explainable and trustworthy. While the paper could benefit from more in-depth analysis of the proposed method, it is a significant step towards developing more interpretable graph neural networks.\n\n**Review 3:**\n\"Ordered GNN: A Promising Solution for Real-World Graphs\" - 4/5 stars\n\nThe authors of this paper have identified a critical problem in graph neural networks and proposed a novel solution. The ordered message passing mechanism is particularly useful for real-world graphs, which often exhibit heterophily and are prone to over-smoothing. The experimental results are impressive, and the model's ability to maintain its performance even at deeper layers is a significant advantage. However, more experiments on larger and more complex graphs would be necessary to fully demonstrate the model's capabilities.\n\n**Review 4:**\n\"A New Perspective on Graph Neural Networks\" - 4.5/5 stars\n\nThis paper offers a fresh perspective on graph neural networks by introducing the concept of ordered message passing. The authors' approach is well-motivated and theoretically sound, and the experimental results are convincing. The paper's strength lies in its ability to tackle both homophily and heterophily settings simultaneously, making it a valuable contribution to the field. While some aspects of the model could be further explored, this work has the potential to inspire new research directions",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"Ordered GNN: A Breakthrough in Graph Neural Networks\" (5/5)\n\nThe authors of this paper have made a significant contribution to the field of graph neural networks by proposing a novel approach to deal with the long-standing issues of over-smoothing and heterophily. By ordering the message passing mechanism, they have successfully addressed these problems and achieved state-of-the-art results on a wide range of datasets. The proposed method is not only effective but also provides an explainable model, which is a major advantage in graph neural networks. The experimental results are impressive, and the visualizations of the gating vectors provide valuable insights into the model's behavior. Overall, this paper is a must-read for anyone working in the field of graph neural networks.\n\n**Review 2:**\n\"A Step Towards Explainable Graph Neural Networks\" (4.5/5)\n\nThis paper proposes an innovative approach to graph neural networks by ordering the message passing mechanism. The authors have successfully demonstrated the effectiveness of their approach in dealing with the heterophily problem and over-smoothing issue. The proposed method is not only effective but also provides an explainable model, which is a significant advantage in graph neural networks. The experimental results are impressive, and the visualizations of the gating vectors provide valuable insights into the model's behavior. However, the paper could benefit from a more detailed analysis of the computational complexity of the proposed method. Overall, this paper is a significant contribution to the field of graph neural networks.\n\n**Review 3:**\n\"Ordered GNN: A Promising Solution to Graph Neural Network Challenges\" (4/5)\n\nThe authors of this paper have proposed a novel approach to graph neural networks by ordering the message passing mechanism. The proposed method has shown promising results in dealing with the heterophily problem and over-smoothing issue. The experimental results are impressive, and the visualizations of the gating vectors provide valuable insights into the model's behavior. However, the paper could benefit from a more detailed analysis of the proposed method's limitations and potential applications. Additionally, the paper could benefit from a more comprehensive comparison with existing methods. Overall, this paper is a promising contribution to the field of graph neural networks.\n\n**Review 4:**\n\"Dealing with Heterophily and Over-smoothing in Graph Neural Networks\" (4/5)\n\nThis paper proposes an innovative approach to deal with the heterophily problem and over-smoothing issue in graph neural networks. The"
      ],
      "sim_rank": 4,
      "sim_decision": "None"
    },
    "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization": {
      "paper_pk": "4d4c7724-9518-4dbf-aaef-2ef14b046fce",
      "title": "Distributed Graph Neural Network Training with Periodic Stale Representation Synchronization",
      "abstract": "Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train a GNN on large graphs with over millions of nodes & billions of edges, which are prevalent in many graph-based applications such as social networks, recommender systems, and knowledge graphs. Traditional sampling-based methods accelerate GNN training by dropping edges and nodes, which impairs the graph integrity and model performance. Differently, distributed GNN algorithms accelerate GNN training by utilizing multiple computing devices and can be classified into two types: \"partition-based\" methods enjoy low communication cost but suffer from information loss due to dropped edges, while \"propagation-based\" methods avoid information loss but suffer from prohibitive communication overhead caused by neighbor explosion. To jointly address these problems, this paper proposes DIGEST (DIstributed Graph reprEsentation SynchronizaTion), a novel distributed GNN training framework that synergizes the complementary strength of both categories of existing methods. We propose to allow each device utilize the stale representations of its neighbors in other subgraphs during subgraph parallel training. This way, out method preserves global graph information from neighbors to avoid information loss and reduce the communication cost. Therefore, DIGEST is both computation-efficient and communication-efficient as it does not need to frequently (re-)compute and transfer the massive representation data across the devices, due to neighbor explosion. DIGEST provides synchronous and asynchronous training manners for homogeneous and heterogeneous training environment, respectively. We proved that the approximation error induced by the staleness of the representations can be upper-bounded. More importantly, our convergence analysis demonstrates that DIGEST enjoys the state-of-the-art convergence rate. Extensive experimental evaluation on large, real-world graph datasets shows that DIGEST achieves up to 21.82\u00d7 speedup without compromising the performance compared to state-of-the-art distributed GNN training frameworks",
      "authors": [
        "Zheng Chai",
        "Guangji Bai",
        "Liang Zhao",
        "Yue Cheng"
      ],
      "keywords": [
        "GNN",
        "Distributed training"
      ],
      "real_avg_scores": 6.5,
      "real_all_scores": [
        6,
        8,
        6,
        6
      ],
      "real_confidences": [
        5,
        4,
        5,
        4
      ],
      "real_contents": [],
      "real_rank": 5,
      "real_decision": "",
      "sim_avg_scores": 6.0,
      "sim_all_scores": [
        0,
        9,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\" DIGEST: A Game-Changer for Distributed Graph Neural Network Training\"\n\nThe authors of this paper propose a novel distributed GNN training framework, DIGEST, which addresses the long-standing challenges of scaling GNNs to large graphs. By allowing each device to utilize stale representations of its neighbors, DIGEST achieves a remarkable balance between computation efficiency and communication efficiency. The experimental results are impressive, with up to 21.82\u00d7 speedup without compromising performance. This work has the potential to revolutionize the field of graph-based applications, such as social networks and recommender systems.\n\n**Review 2:**\n\"A Unified Approach to Distributed GNN Training: DIGEST\"\n\nThis paper presents a significant advancement in distributed GNN training by proposing DIGEST, a framework that synergizes the strengths of both partition-based and propagation-based methods. By leveraging stale representations, DIGEST avoids information loss and reduces communication overhead, making it an attractive solution for large-scale graph-based applications. The authors' convergence analysis and experimental evaluation demonstrate the effectiveness of DIGEST, which is poised to become a new standard for distributed GNN training.\n\n**Review 3:**\n\"DIGEST: A Scalable and Efficient Solution for Distributed GNN Training\"\n\nThe authors of this paper tackle the challenging problem of distributed GNN training on large graphs, where traditional sampling-based methods compromise graph integrity and model performance. DIGEST, the proposed framework, offers a scalable and efficient solution by utilizing stale representations of neighbors, reducing communication cost and preserving global graph information. The experimental results are impressive, with significant speedup achieved without compromising performance. This work has important implications for graph-based applications, such as knowledge graphs and social networks.\n\n**Review 4:**\n\"Breaking the Barriers of Distributed GNN Training: DIGEST\"\n\nThis paper presents a breakthrough in distributed GNN training by introducing DIGEST, a novel framework that addresses the limitations of existing methods. By allowing devices to utilize stale representations of neighbors, DIGEST achieves a remarkable balance between computation efficiency and communication efficiency. The authors' theoretical analysis and experimental evaluation demonstrate the effectiveness of DIGEST, which is poised to become a new standard for distributed GNN training. This work has the potential to unlock the full potential of graph-based applications, such as recommender systems and social networks.\n\n**Review 5:**\n\"DIGEST: A Novel Framework for Efficient Distributed GNN Training\"\n\nThe authors of this paper propose a novel distributed GNN training framework, DIG",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\" DIGEST: A Game-Changer for Distributed Graph Neural Network Training\"\n\nThe authors of this paper propose a novel distributed GNN training framework, DIGEST, which addresses the long-standing challenges of scaling GNNs to large graphs. By allowing each device to utilize stale representations of its neighbors, DIGEST achieves a remarkable balance between computation efficiency and communication efficiency. The experimental results are impressive, with up to 21.82\u00d7 speedup without compromising performance. This work has the potential to revolutionize the field of graph-based applications, such as social networks and recommender systems.\n\n**Review 2:**\n\"A Unified Approach to Distributed GNN Training: DIGEST\"\n\nThis paper presents a significant advancement in distributed GNN training by proposing DIGEST, a framework that synergizes the strengths of both partition-based and propagation-based methods. By leveraging stale representations, DIGEST avoids information loss and reduces communication overhead, making it an attractive solution for large-scale graph processing. The authors' convergence analysis and experimental evaluation demonstrate the effectiveness of DIGEST, which is poised to become a new standard for distributed GNN training.\n\n**Review 3:**\n\"DIGEST: Breaking the Barriers of Distributed Graph Neural Network Training\"\n\nThe authors of this paper tackle the daunting task of training GNNs on massive graphs, which has been a major bottleneck in many applications. DIGEST, their proposed framework, offers a refreshing solution by exploiting stale representations to reduce communication cost and preserve global graph information. The results are stunning, with significant speedup and no performance compromise. This work has far-reaching implications for graph-based applications and is a must-read for anyone working in this area.\n\n**Review 4:**\n\"A Novel Approach to Distributed GNN Training: DIGEST\"\n\nThis paper introduces DIGEST, a distributed GNN training framework that addresses the limitations of existing methods. By utilizing stale representations, DIGEST achieves a sweet spot between computation efficiency and communication efficiency. The authors' theoretical analysis and experimental evaluation demonstrate the effectiveness of DIGEST, which is particularly suitable for large-scale graph processing. This work is a valuable contribution to the field and is likely to inspire further research in distributed GNN training.\n\n**Review 5:**\n\"DIGEST: A Scalable Solution for Distributed Graph Neural Network Training\"\n\nThe authors of this paper propose a novel distributed GNN training framework, DIGEST, which is designed to scale to massive graphs. By leveraging stale representations, DIGEST reduces communication overhead and preserves",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\nTitle: A Breakthrough in Distributed GNN Training: DIGEST Sets a New Standard\nRating: 5/5\n\nThe authors of this paper have made a significant contribution to the field of Graph Neural Networks (GNNs) by proposing a novel distributed training framework called DIGEST. By synergizing the strengths of both partition-based and propagation-based methods, DIGEST achieves a remarkable balance between computation efficiency and communication efficiency. The experimental results are impressive, with a speedup of up to 21.82\u00d7 without compromising performance. This work has the potential to revolutionize the training of GNNs on large graphs and is a must-read for anyone working in this area.\n\n**Review 2:**\nTitle: Efficient and Scalable GNN Training with DIGEST\nRating: 4.5/5\n\nThe proposed DIGEST framework is a game-changer for distributed GNN training. By allowing devices to utilize stale representations of their neighbors, DIGEST reduces communication overhead and preserves global graph information. The authors provide a thorough convergence analysis and experimental evaluation, demonstrating the effectiveness of their approach. While the paper could benefit from more detailed explanations of the theoretical aspects, the results are undeniably impressive. DIGEST is a significant step forward in making GNNs more scalable and efficient.\n\n**Review 3:**\nTitle: A Promising Solution for Large-Scale GNN Training\nRating: 4/5\n\nThis paper proposes a novel distributed GNN training framework, DIGEST, which addresses the limitations of existing methods. By leveraging stale representations, DIGEST reduces communication cost and avoids information loss. The authors provide a solid theoretical foundation and experimental evaluation, demonstrating the potential of their approach. However, the paper could benefit from more comparisons with other state-of-the-art methods and a more detailed discussion of the limitations of DIGEST. Nevertheless, this work is a promising step towards more efficient and scalable GNN training.\n\n**Review 4:**\nTitle: DIGEST: A New Frontier in Distributed GNN Training\nRating: 5/5\n\nThe authors of this paper have pushed the boundaries of distributed GNN training with their proposed DIGEST framework. By combining the strengths of partition-based and propagation-based methods, DIGEST achieves unprecedented efficiency and scalability. The theoretical analysis and experimental results are thorough and convincing, demonstrating the potential of DIGEST to revolutionize the field. This work is a must-read for anyone interested in large-scale GNN training and"
      ],
      "sim_rank": 2,
      "sim_decision": "None"
    },
    "Learnable Graph Convolutional Attention Networks": {
      "paper_pk": "67c0a186-78dd-41e1-8b3c-6bcf88a44f44",
      "title": "Learnable Graph Convolutional Attention Networks",
      "abstract": "Existing Graph Neural Networks (GNNs) compute the message exchange between nodes by either aggregating uniformly (convolving) the features of all the neighbor- ing nodes, or by applying a non-uniform score (attending) to the features. Recent works have shown the strengths and weaknesses of the resulting GNN architectures, respectively, GCNs and GATs. In this work, we aim at exploiting the strengths of both approaches to their full extent. To this end, we first introduce the graph convolutional attention layer (CAT), which relies on convolutions to compute the attention scores. Unfortunately, as in the case of GCNs and GATs, we show that there exists no clear winner between the three\u2014neither theoretically nor in practice\u2014as their performance directly depends on the nature of the data (i.e., of the graph and features). This result brings us to the main contribution of our work, the learnable graph convolutional attention network (L-CAT): a GNN architecture that automatically interpolates between GCN, GAT and CAT in each layer, by adding only two scalar parameters. Our results demonstrate that L-CAT is able to efficiently combine different GNN layers along the network, outperforming competing methods in a wide range of datasets, and resulting in a more robust model that reduces the need of cross-validating.",
      "authors": [
        "Adri\u00e1n Javaloy",
        "Pablo Sanchez Martin",
        "Amit Levi",
        "Isabel Valera"
      ],
      "keywords": [
        "GNN",
        "GCN",
        "GAT"
      ],
      "real_avg_scores": 6.5,
      "real_all_scores": [
        6,
        6,
        6,
        8
      ],
      "real_confidences": [
        4,
        4,
        3,
        4
      ],
      "real_contents": [],
      "real_rank": 6,
      "real_decision": "Accept: notable-top-25%",
      "sim_avg_scores": 6.0,
      "sim_all_scores": [
        9,
        0,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Game-Changer in Graph Neural Networks**\n\nThe proposed Learnable Graph Convolutional Attention Network (L-CAT) is a groundbreaking architecture that combines the strengths of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). By introducing the graph convolutional attention layer (CAT), the authors demonstrate a novel approach to computing attention scores using convolutions. The L-CAT architecture's ability to automatically interpolate between GCN, GAT, and CAT in each layer is a significant innovation, allowing it to outperform competing methods on a wide range of datasets. This work has the potential to revolutionize the field of graph neural networks and reduce the need for cross-validation.\n\n**Review 2: A Unified Framework for Graph Neural Networks**\n\nThe Learnable Graph Convolutional Attention Network (L-CAT) presents a unified framework for graph neural networks, bridging the gap between Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). By introducing the graph convolutional attention layer (CAT), the authors provide a novel approach to computing attention scores that leverages the strengths of both GCNs and GATs. The L-CAT architecture's ability to automatically adapt to different graph structures and features makes it a robust and efficient model that can be applied to a wide range of graph-based problems.\n\n**Review 3: A Step Towards More Robust Graph Neural Networks**\n\nThe Learnable Graph Convolutional Attention Network (L-CAT) is a significant step towards developing more robust graph neural networks. By combining the strengths of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), the authors demonstrate a novel approach to graph neural networks that can adapt to different graph structures and features. The L-CAT architecture's ability to automatically interpolate between GCN, GAT, and CAT in each layer makes it a more robust model that reduces the need for cross-validation. This work has the potential to improve the reliability and generalizability of graph neural networks in real-world applications.\n\n**Review 4: A Novel Approach to Attention in Graph Neural Networks**\n\nThe proposed Learnable Graph Convolutional Attention Network (L-CAT) introduces a novel approach to attention in graph neural networks, leveraging the strengths of convolutions to compute attention scores. The graph convolutional attention layer (CAT) is a significant innovation that allows the L-CAT architecture to automatically adapt to different graph structures and features. By",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-changer in Graph Neural Networks! The authors have successfully bridged the gap between Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs) by introducing the Learnable Graph Convolutional Attention Network (L-CAT). By automatically interpolating between different GNN layers, L-CAT outperforms competing methods on a wide range of datasets, making it a more robust and efficient model. The addition of only two scalar parameters is a remarkable feat, demonstrating the authors' deep understanding of the strengths and weaknesses of existing GNN architectures. A must-read for anyone working in graph neural networks!\"\n\n**4-star review**\n\"I was impressed by the authors' ability to identify the limitations of existing Graph Neural Networks (GNNs) and propose a novel solution that combines the strengths of both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). The Learnable Graph Convolutional Attention Network (L-CAT) is a clever approach that allows for automatic interpolation between different GNN layers, resulting in improved performance on various datasets. While the paper could benefit from more detailed explanations of the theoretical aspects, the experimental results are convincing and demonstrate the potential of L-CAT. A solid contribution to the field of graph neural networks!\"\n\n**3-star review**\n\"The authors' proposal of the Learnable Graph Convolutional Attention Network (L-CAT) is an interesting attempt to merge the benefits of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). While the idea of automatically interpolating between different GNN layers is appealing, I'm not entirely convinced by the experimental results. The paper could benefit from more thorough comparisons with existing state-of-the-art methods and a more detailed analysis of the limitations of L-CAT. Nevertheless, the paper is well-written, and the authors' approach is worth exploring further.\"\n\n**2-star review**\n\"I was expecting more from this paper, given the promising title. While the authors identify some limitations of existing Graph Neural Networks (GNNs), their proposed solution, the Learnable Graph Convolutional Attention Network (L-CAT), feels like a incremental improvement rather than a breakthrough. The addition of only two scalar parameters seems like a minor tweak, and I'm not convinced that it's enough to justify the claims of improved performance. The paper could benefit from more rigorous theoretical analysis and more comprehensive experimental evaluations.\"\n\n**1-star review**\n\"I'm",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Game-Changer in Graph Neural Networks**\n\nThe proposed Learnable Graph Convolutional Attention Network (L-CAT) is a groundbreaking architecture that combines the strengths of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). By introducing the graph convolutional attention layer (CAT), the authors demonstrate a novel approach to computing attention scores using convolutions. The L-CAT architecture's ability to automatically interpolate between GCN, GAT, and CAT in each layer is a significant innovation, allowing it to outperform competing methods on a wide range of datasets. This work has the potential to revolutionize the field of graph neural networks and reduce the need for cross-validation.\n\n**Review 2: A Unified Framework for Graph Neural Networks**\n\nThe Learnable Graph Convolutional Attention Network (L-CAT) presents a unified framework for graph neural networks, bridging the gap between Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs). By introducing the graph convolutional attention layer (CAT), the authors provide a novel approach to computing attention scores that leverages the strengths of both GCNs and GATs. The L-CAT architecture's ability to automatically adapt to different graph structures and features makes it a robust and efficient model that can be applied to a wide range of graph-based problems.\n\n**Review 3: A Step Towards More Robust Graph Neural Networks**\n\nThe Learnable Graph Convolutional Attention Network (L-CAT) is a significant step towards developing more robust graph neural networks. By combining the strengths of Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), the authors demonstrate a novel approach to graph neural networks that can adapt to different graph structures and features. The L-CAT architecture's ability to automatically interpolate between GCN, GAT, and CAT in each layer makes it a more robust model that reduces the need for cross-validation. This work has the potential to improve the reliability and generalizability of graph neural networks in real-world applications.\n\n**Review 4: A Novel Approach to Attention in Graph Neural Networks**\n\nThe proposed Learnable Graph Convolutional Attention Network (L-CAT) introduces a novel approach to attention in graph neural networks, leveraging the strengths of convolutions to compute attention scores. The graph convolutional attention layer (CAT) is a significant innovation that allows the L-CAT architecture to automatically adapt to different graph structures and features. By"
      ],
      "sim_rank": 3,
      "sim_decision": "None"
    },
    "PREDICTION OF TOURISM FLOW WITH SPARSE DATA INCORPORATING TOURIST GEOLOCATIONS": {
      "paper_pk": "c13249cb-cf08-49b3-a37f-4c93a3055583",
      "title": "PREDICTION OF TOURISM FLOW WITH SPARSE DATA INCORPORATING TOURIST GEOLOCATIONS",
      "abstract": "Modern tourism in the 21st century is facing numerous challenges. One of these\nchallenges is the rapidly growing number of tourists in space-limited regions such\nas historical city centers, museums, or geographical bottlenecks like narrow val-\nleys. In this context, a proper and accurate prediction of tourism volume and\ntourism flow within a certain area is important and critical for visitor management\ntasks such as sustainable treatment of the environment and prevention of over-\ncrowding. Static flow control methods like conventional low-level controllers or\nlimiting access to overcrowded venues could not solve the problem yet. In this\npaper, we empirically evaluate the performance of state-of-the-art deep-learning\nmethods such as RNNs, GNNs, and Transformers as well as the classic statistical\nARIMA method. Granular limited data supplied by a tourism region is extended\nby exogenous data such as geolocation trajectories of individual tourists, weather\nand holidays. In the field of visitor flow prediction with sparse data, we are thereby\ncapable of increasing the accuracy of our predictions, incorporating modern input\nfeature handling as well as mapping geolocation data on top of discrete POI data.",
      "authors": [
        "Julian Lemmel",
        "Zahra Babaiee",
        "Marvin Kleinlehner",
        "Ivan Majic",
        "Philipp Neubauer",
        "Johannes Scholz",
        "Radu Grosu",
        "Sophie Neubauer"
      ],
      "keywords": [
        "GNN",
        "RNN",
        "Transformer",
        "Tourism",
        "Tourism flow prediction"
      ],
      "real_avg_scores": 5.5,
      "real_all_scores": [
        6,
        5,
        3,
        8
      ],
      "real_confidences": [
        2,
        3,
        5,
        4
      ],
      "real_contents": [],
      "real_rank": 10,
      "real_decision": "",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a pressing issue in modern tourism - predicting tourism flow in space-limited regions. The authors' approach of incorporating tourist geolocations and exogenous data such as weather and holidays into state-of-the-art deep-learning methods is innovative and effective. The empirical evaluation of RNNs, GNNs, Transformers, and ARIMA methods provides valuable insights into the strengths and limitations of each approach. The results demonstrate a significant improvement in prediction accuracy, making this a valuable contribution to the field of visitor flow prediction.\"\n\n**Review 2:**\n\"The authors' use of granular limited data from a tourism region, combined with exogenous data, is a key strength of this paper. By incorporating geolocation trajectories of individual tourists, the authors are able to provide a more nuanced understanding of tourism flow. The comparison of deep-learning methods with the classic ARIMA method is also well-executed. However, the paper could benefit from a more detailed discussion of the limitations of the approach and potential avenues for future research.\"\n\n**Review 3:**\n\"This paper is a timely contribution to the field of tourism management, as overcrowding and environmental degradation become increasingly pressing concerns. The authors' approach of using machine learning methods to predict tourism flow is well-suited to addressing these challenges. The incorporation of geolocation data and exogenous variables is a particular strength, as it allows for more accurate predictions and more effective visitor management. The paper could benefit from a more detailed discussion of the practical implications of the approach, however.\"\n\n**Review 4:**\n\"The authors' use of state-of-the-art deep-learning methods is impressive, and the results demonstrate a significant improvement in prediction accuracy. However, the paper could benefit from a more detailed discussion of the theoretical underpinnings of the approach. Additionally, the authors could provide more context on the specific challenges of predicting tourism flow in space-limited regions, and how their approach addresses these challenges. Overall, this is a valuable contribution to the field, but could be strengthened with more theoretical and contextual grounding.\"\n\n**Review 5:**\n\"This paper is a great example of how data-driven approaches can be used to address real-world problems in tourism management. The authors' use of geolocation data and exogenous variables is innovative and effective, and the results demonstrate a significant improvement in prediction accuracy. The paper could benefit from a more detailed discussion of the potential applications of the approach, as well as potential limitations and challenges. Overall, this",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a pressing issue in modern tourism - predicting tourism flow in space-limited regions. The authors' approach of incorporating tourist geolocations and exogenous data such as weather and holidays into state-of-the-art deep-learning methods is innovative and effective. The empirical evaluation of RNNs, GNNs, Transformers, and ARIMA methods provides valuable insights into the strengths and limitations of each approach. The results demonstrate a significant improvement in prediction accuracy, making this a valuable contribution to the field of visitor flow prediction.\"\n\n**Review 2:**\n\"The authors' use of granular limited data from a tourism region, combined with exogenous data, is a key strength of this paper. By incorporating geolocation trajectories of individual tourists, the authors are able to provide a more nuanced understanding of tourism flow. The comparison of deep-learning methods with the classic ARIMA method is also well-executed. However, the paper could benefit from a more detailed discussion of the limitations of the approach and potential avenues for future research.\"\n\n**Review 3:**\n\"This paper is a timely contribution to the field of tourism management, as overcrowding and environmental degradation become increasingly pressing concerns. The authors' approach of using machine learning methods to predict tourism flow is well-suited to addressing these challenges. The incorporation of geolocation data and exogenous variables is a particular strength, as it allows for more accurate predictions and more effective visitor management. The paper could benefit from a more detailed discussion of the practical implications of the approach, however.\"\n\n**Review 4:**\n\"The authors' use of state-of-the-art deep-learning methods is impressive, and the results demonstrate a significant improvement in prediction accuracy. However, the paper could benefit from a more detailed discussion of the theoretical underpinnings of the approach. Additionally, the authors could provide more context on the specific challenges of predicting tourism flow in space-limited regions, and how their approach addresses these challenges. Overall, this is a valuable contribution to the field, but could be strengthened with more theoretical and contextual grounding.\"\n\n**Review 5:**\n\"This paper is a great example of how data-driven approaches can be used to address real-world problems in tourism management. The authors' use of geolocation data and exogenous variables is innovative and effective, and the results demonstrate a significant improvement in prediction accuracy. The paper could benefit from a more detailed discussion of the potential applications of the approach, as well as potential limitations and challenges. Overall, this",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a pressing issue in modern tourism - predicting tourism flow in space-limited regions. The authors' approach of incorporating tourist geolocations and exogenous data such as weather and holidays into state-of-the-art deep-learning methods is innovative and effective. The empirical evaluation of RNNs, GNNs, Transformers, and ARIMA methods provides valuable insights into the strengths and limitations of each approach. The results demonstrate a significant improvement in prediction accuracy, making this a valuable contribution to the field of visitor flow prediction.\"\n\n**Review 2:**\n\"The authors' use of granular limited data from a tourism region, combined with exogenous data, is a key strength of this paper. By incorporating geolocation trajectories of individual tourists, the authors are able to provide a more nuanced understanding of tourism flow. The comparison of deep-learning methods with the classic ARIMA method is also useful, highlighting the benefits of modern input feature handling and geolocation data mapping. This paper has important implications for sustainable tourism management and overcrowding prevention.\"\n\n**Review 3:**\n\"This paper addresses a critical challenge in tourism management - predicting tourism volume and flow in space-limited regions. The authors' approach of leveraging tourist geolocations and exogenous data is a significant advancement in the field. The empirical evaluation of various methods provides a comprehensive understanding of their strengths and limitations. The results demonstrate the potential of deep-learning methods to improve prediction accuracy, making this a valuable contribution to the field of tourism management. However, further research is needed to explore the scalability and generalizability of these methods.\"\n\n**Review 4:**\n\"The authors' focus on incorporating tourist geolocations and exogenous data into tourism flow prediction is a timely and important contribution to the field. The comparison of RNNs, GNNs, Transformers, and ARIMA methods provides a useful framework for evaluating the performance of different approaches. The results highlight the importance of considering multiple factors in tourism flow prediction, including weather, holidays, and geolocation data. This paper has important implications for sustainable tourism management and visitor experience optimization.\"\n\n**Review 5:**\n\"This paper presents a comprehensive evaluation of state-of-the-art deep-learning methods for tourism flow prediction. The authors' use of granular limited data from a tourism region, combined with exogenous data, provides a rich dataset for analysis. The results demonstrate the potential of deep-learning methods to improve prediction accuracy, particularly when incorporating geolocation data. However, the paper could"
      ],
      "sim_rank": 8,
      "sim_decision": "None"
    },
    "ReG-NAS: Graph Neural Network Architecture Search using Regression Proxy Task": {
      "paper_pk": "f8fc6eab-6b90-4fbc-87dd-577495c74de7",
      "title": "ReG-NAS: Graph Neural Network Architecture Search using Regression Proxy Task",
      "abstract": "Neural Architecture Search (NAS) has become a focus that has been extensively researched in recent years. Innovative achievements are yielded from the area like convolutional neural networks (CNN), recurrent neural networks (RNN) and so on. However, research on NAS for graph neural networks (GNN) is still in a preliminary stage. Because of the special structure of graph data, some conclusions drew from CNN cannot be directly applied to GNN. At the same time, for NAS, the models' ranking stability is of great importance for it reflects the reliability of the NAS performance. Unfortunately, little research attention has been paid to it, making it a pitfall in the development of NAS research. In this paper, we proposed a novel NAS pipeline, ReG-NAS, which balances stability, reliability and time cost to search the best GNN architecture. Besides, for the first time, we systematically analyzed factors that will affect models' ranking stability in a given search space, which can be used as a guideline for subsequent studies. Our codes are available at https://anonymous.4open.science/r/ReG-NAS-4D21",
      "authors": [
        "Boyi Wei",
        "Cong Hao"
      ],
      "keywords": [
        "Neural Architecture Search",
        "GNN",
        "Machine Learning"
      ],
      "real_avg_scores": 5.333333333333333,
      "real_all_scores": [
        8,
        5,
        3
      ],
      "real_confidences": [
        4,
        4,
        5
      ],
      "real_contents": [],
      "real_rank": 12,
      "real_decision": "Reject",
      "sim_avg_scores": 3.3333333333333335,
      "sim_all_scores": [
        0,
        1,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to GNN Architecture Search**\n\nReG-NAS is a groundbreaking paper that tackles the challenging problem of Neural Architecture Search (NAS) for Graph Neural Networks (GNNs). The authors propose a novel pipeline that balances stability, reliability, and time cost to search for the best GNN architecture. By using a regression proxy task, ReG-NAS is able to overcome the limitations of traditional NAS methods, which are often designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The authors' systematic analysis of factors affecting models' ranking stability is a significant contribution to the field, providing a valuable guideline for future studies. Overall, ReG-NAS is a must-read for researchers working on NAS and GNNs.\n\n**Review 2: A Step Forward in NAS for GNNs**\n\nReG-NAS is a timely and important contribution to the field of Neural Architecture Search (NAS) for Graph Neural Networks (GNNs). The authors identify the limitations of existing NAS methods, which are often designed for CNNs and RNNs, and propose a novel pipeline that addresses the unique challenges of GNNs. By using a regression proxy task, ReG-NAS is able to improve the stability and reliability of NAS for GNNs. The authors' analysis of factors affecting models' ranking stability is a significant contribution to the field, providing insights that can be applied to a wide range of NAS applications. Overall, ReG-NAS is a significant step forward in the development of NAS for GNNs.\n\n**Review 3: A Comprehensive Framework for GNN Architecture Search**\n\nReG-NAS is a comprehensive framework for Neural Architecture Search (NAS) that addresses the unique challenges of Graph Neural Networks (GNNs). The authors propose a novel pipeline that balances stability, reliability, and time cost to search for the best GNN architecture. By using a regression proxy task, ReG-NAS is able to overcome the limitations of traditional NAS methods, which are often designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The authors' systematic analysis of factors affecting models' ranking stability is a significant contribution to the field, providing a valuable guideline for future studies. The availability of the authors' code is also a significant advantage, allowing researchers to build upon and extend the ReG-NAS framework.\n\n**Review 4: A New Perspective on",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to GNN Architecture Search**\n\nReG-NAS is a groundbreaking paper that tackles the challenging problem of Neural Architecture Search (NAS) for Graph Neural Networks (GNNs). The authors propose a novel pipeline that balances stability, reliability, and time cost to search for the best GNN architecture. By using a regression proxy task, ReG-NAS is able to overcome the limitations of traditional NAS methods, which are often designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The authors' systematic analysis of factors affecting models' ranking stability is a significant contribution to the field, providing a valuable guideline for future studies. Overall, ReG-NAS is a must-read for researchers working on NAS and GNNs.\n\n**Review 2: A Step Forward in NAS for GNNs**\n\nReG-NAS is a timely and important contribution to the field of Neural Architecture Search (NAS) for Graph Neural Networks (GNNs). The authors identify the limitations of existing NAS methods, which are often designed for CNNs and RNNs, and propose a novel pipeline that addresses the unique challenges of GNNs. By using a regression proxy task, ReG-NAS is able to improve the stability and reliability of NAS for GNNs. The authors' analysis of factors affecting models' ranking stability is a significant contribution to the field, and their open-sourced code will facilitate further research in this area. Overall, ReG-NAS is a significant step forward in NAS for GNNs.\n\n**Review 3: A Comprehensive Framework for GNN Architecture Search**\n\nReG-NAS is a comprehensive framework for Neural Architecture Search (NAS) that addresses the unique challenges of Graph Neural Networks (GNNs). The authors propose a novel pipeline that balances stability, reliability, and time cost to search for the best GNN architecture. By using a regression proxy task, ReG-NAS is able to overcome the limitations of traditional NAS methods, which are often designed for Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). The authors' systematic analysis of factors affecting models' ranking stability is a significant contribution to the field, providing a valuable guideline for future studies. The open-sourced code and detailed experimental results make ReG-NAS a valuable resource for researchers working on NAS and GNNs.\n\n**Review 4: A Promising Direction for NAS Research",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to GNN Architecture Search**\n\nReG-NAS is a groundbreaking paper that tackles the challenging problem of Neural Architecture Search (NAS) for Graph Neural Networks (GNNs). The authors propose a novel pipeline that balances stability, reliability, and time cost to search for the best GNN architecture. By using a regression proxy task, ReG-NAS is able to efficiently search for optimal architectures while avoiding the pitfalls of traditional NAS methods. The paper's systematic analysis of factors affecting models' ranking stability is particularly noteworthy, providing valuable insights for future research in this area.\n\n**Rating: 5/5**\n\n**Review 2: A Much-Needed Contribution to GNN NAS Research**\n\nThe field of GNN NAS has been lagging behind its CNN and RNN counterparts, and ReG-NAS is a much-needed contribution to this area. The authors' proposed pipeline is a significant improvement over existing methods, offering a more stable and reliable approach to searching for optimal GNN architectures. The paper's focus on ranking stability is particularly important, as it highlights a critical issue that has been overlooked in previous research. Overall, ReG-NAS is a well-written and well-researched paper that is sure to have a significant impact on the field.\n\n**Rating: 5/5**\n\n**Review 3: A Promising Step Forward for GNN NAS**\n\nReG-NAS is a promising paper that addresses the challenging problem of NAS for GNNs. The authors' proposed pipeline is innovative and effective, offering a more efficient and reliable approach to searching for optimal architectures. While the paper's analysis of ranking stability is a significant contribution, it would have been beneficial to see more experimental results to further validate the authors' claims. Nevertheless, ReG-NAS is a valuable addition to the field and is sure to inspire further research in this area.\n\n**Rating: 4.5/5**\n\n**Review 4: A Comprehensive Analysis of GNN NAS**\n\nReG-NAS is a comprehensive paper that provides a thorough analysis of the challenges and opportunities in GNN NAS. The authors' proposed pipeline is well-designed and well-executed, offering a more stable and reliable approach to searching for optimal architectures. The paper's systematic analysis of factors affecting models' ranking stability is particularly impressive, providing a valuable framework for future research in this area. Overall, ReG-NAS is a well-written and well-researched paper that is sure to be"
      ],
      "sim_rank": 5,
      "sim_decision": "None"
    },
    "Bi-Stride Multi-Scale Graph Neural Network for Mesh-Based Physical Simulation": {
      "paper_pk": "c1335dec-87c3-4a37-a21d-387f0bfc3839",
      "title": "Bi-Stride Multi-Scale Graph Neural Network for Mesh-Based Physical Simulation",
      "abstract": "Learning physical systems on unstructured meshes by flat Graph neural networks (GNNs) faces the challenge of modeling the long-range interactions due to the scaling complexity w.r.t. the number of nodes, limiting the generalization under mesh refinement. On regular grids, the convolutional neural networks (CNNs) with a U-net structure can resolve this challenge by efficient stride, pooling, and upsampling operations. Nonetheless, these tools are much less developed for graph neural networks (GNNs), especially when GNNs are employed for learning large-scale mesh-based physics. The challenges arise from the highly irregular meshes and the lack of effective ways to construct the multi-level structure without losing connectivity. Inspired by the bipartite graph determination algorithm, we introduce Bi-Stride Multi-Scale Graph Neural Network (BSMS-GNN) by proposing \\textit{bi-stride} as a simple pooling strategy for building the multi-level GNN. \\textit{Bi-stride} pools nodes by striding every other BFS frontier; it 1) works robustly on any challenging mesh in the wild, 2) avoids using a mesh generator at coarser levels, 3) avoids the spatial proximity for building coarser levels, and 4) uses non-parametrized aggregating/returning instead of MLPs during pooling and unpooling. Experiments show that our framework significantly outperforms the state-of-the-art method's computational efficiency in representative physics-based simulation cases.",
      "authors": [
        "Yadi Cao",
        "Menglei Chai",
        "Minchen Li",
        "Chenfanfu Jiang"
      ],
      "keywords": [
        "GNN",
        "physics-based simulation"
      ],
      "real_avg_scores": 4.333333333333333,
      "real_all_scores": [
        5,
        3,
        5
      ],
      "real_confidences": [
        3,
        4,
        4
      ],
      "real_contents": [],
      "real_rank": 15,
      "real_decision": "",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper proposes a novel approach to mesh-based physical simulation using graph neural networks (GNNs). The authors introduce the Bi-Stride Multi-Scale Graph Neural Network (BSMS-GNN), which addresses the challenge of modeling long-range interactions on unstructured meshes. By leveraging a bipartite graph determination algorithm, the authors develop a simple yet effective pooling strategy, dubbed 'bi-stride', which enables the construction of a multi-level GNN without losing connectivity. The results demonstrate significant improvements in computational efficiency compared to state-of-the-art methods, making this approach a promising solution for large-scale mesh-based physics simulations.\"\n\n**Review 2:**\n\"The authors of this paper tackle the challenging problem of learning physical systems on unstructured meshes using GNNs. They identify the limitations of flat GNNs in modeling long-range interactions and propose a novel solution inspired by CNNs with a U-net structure. The BSMS-GNN framework, which incorporates the bi-stride pooling strategy, shows impressive results in terms of computational efficiency and scalability. The paper's contributions are significant, as they provide a robust and efficient approach to mesh-based physical simulation, which has far-reaching implications for various fields, including computer-aided engineering, materials science, and physics.\"\n\n**Review 3:**\n\"This paper presents a breakthrough in mesh-based physical simulation using GNNs. The proposed BSMS-GNN framework addresses the long-standing challenge of modeling complex interactions on unstructured meshes. The bi-stride pooling strategy is a game-changer, as it enables the construction of a multi-level GNN without relying on mesh generators or spatial proximity. The authors' approach is well-motivated, and the experimental results are convincing, demonstrating significant improvements in computational efficiency. This work has the potential to revolutionize the field of mesh-based physical simulation and opens up new avenues for research in this area.\"\n\n**Review 4:**\n\"The authors of this paper propose a novel GNN architecture, BSMS-GNN, which is specifically designed for mesh-based physical simulation. The key innovation is the bi-stride pooling strategy, which allows for efficient and robust modeling of long-range interactions on unstructured meshes. The paper provides a thorough analysis of the challenges associated with mesh-based simulation and demonstrates the effectiveness of the proposed approach through experiments. While the paper's focus is on physical simulation, the proposed methodology has broader implications for graph-based learning and could be applied to other domains, such as computer vision and natural",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper proposes a novel approach to mesh-based physical simulation using graph neural networks (GNNs). The authors introduce the Bi-Stride Multi-Scale Graph Neural Network (BSMS-GNN), which addresses the challenge of modeling long-range interactions on unstructured meshes. By leveraging a bipartite graph determination algorithm, the authors develop a simple yet effective pooling strategy, dubbed 'bi-stride', which enables the construction of a multi-level GNN without losing connectivity. The results demonstrate significant improvements in computational efficiency compared to state-of-the-art methods, making this approach a promising solution for large-scale mesh-based physics simulations.\"\n\n**Review 2:**\n\"The authors of this paper tackle the challenging problem of learning physical systems on unstructured meshes using GNNs. They identify the limitations of flat GNNs in modeling long-range interactions and propose a novel solution inspired by CNNs with a U-net structure. The BSMS-GNN framework, which incorporates the bi-stride pooling strategy, shows impressive results in terms of computational efficiency and scalability. The paper's contributions are significant, as they provide a robust and efficient approach to mesh-based physical simulation, which has far-reaching implications for various fields, including computer-aided engineering, materials science, and physics.\"\n\n**Review 3:**\n\"This paper presents a breakthrough in mesh-based physical simulation using GNNs. The proposed BSMS-GNN framework addresses the long-standing challenge of modeling complex interactions on unstructured meshes. The bi-stride pooling strategy is a game-changer, as it enables the construction of a multi-level GNN without relying on mesh generators or spatial proximity. The authors' approach is well-motivated, and the experimental results are convincing, demonstrating significant improvements in computational efficiency. This work has the potential to revolutionize the field of mesh-based physical simulation and opens up new avenues for research in this area.\"\n\n**Review 4:**\n\"The authors of this paper propose a novel GNN architecture, BSMS-GNN, which is specifically designed for mesh-based physical simulation. The key innovation is the bi-stride pooling strategy, which allows for efficient and robust modeling of long-range interactions on unstructured meshes. The paper provides a thorough analysis of the challenges associated with mesh-based simulation and demonstrates the effectiveness of the proposed approach through experiments. The results are promising, and the paper's contributions are significant, making it a valuable addition to the field of GNNs and mesh-based simulation.\"\n\n**Review 5",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper proposes a novel approach to mesh-based physical simulation using graph neural networks (GNNs). The authors introduce the Bi-Stride Multi-Scale Graph Neural Network (BSMS-GNN), which addresses the challenge of modeling long-range interactions on unstructured meshes. By leveraging a bipartite graph determination algorithm, the authors develop a simple yet effective pooling strategy, dubbed 'bi-stride', which enables the construction of a multi-level GNN without losing connectivity. The results demonstrate significant improvements in computational efficiency compared to state-of-the-art methods, making this approach a promising solution for large-scale mesh-based physics simulations.\"\n\n**Review 2:**\n\"The authors of this paper tackle the challenging problem of learning physical systems on unstructured meshes using GNNs. They identify the limitations of flat GNNs in modeling long-range interactions and propose a novel solution inspired by CNNs with a U-net structure. The BSMS-GNN framework, which incorporates the bi-stride pooling strategy, shows impressive results in terms of computational efficiency and scalability. The paper's contributions are significant, as they provide a robust and efficient approach to mesh-based physical simulation, which has far-reaching implications for various fields, including computer-aided engineering, materials science, and physics.\"\n\n**Review 3:**\n\"This paper presents a breakthrough in mesh-based physical simulation using GNNs. The proposed BSMS-GNN framework addresses the long-standing challenge of modeling complex interactions on unstructured meshes. The bi-stride pooling strategy is a game-changer, as it enables the construction of a multi-level GNN without relying on mesh generators or spatial proximity. The authors' approach is well-motivated, and the experimental results are convincing, demonstrating significant improvements in computational efficiency. This work has the potential to revolutionize the field of mesh-based physical simulation and opens up new avenues for research in this area.\"\n\n**Review 4:**\n\"The authors of this paper propose a novel GNN architecture, BSMS-GNN, which is specifically designed for mesh-based physical simulation. The key innovation is the bi-stride pooling strategy, which allows for efficient and robust modeling of long-range interactions on unstructured meshes. The paper provides a thorough analysis of the challenges associated with mesh-based simulation and demonstrates the effectiveness of the proposed approach through experiments. While the paper's focus is on physical simulation, the proposed methodology has broader implications for graph-based learning and could be applied to other domains, such as computer vision and natural"
      ],
      "sim_rank": 9,
      "sim_decision": "None"
    },
    "Vectorial Graph Convolutional Networks": {
      "paper_pk": "fc396a1d-bb27-46d2-abdf-9d10697fade1",
      "title": "Vectorial Graph Convolutional Networks",
      "abstract": "   Graph Convolutional Networks (GCN) have drawn considerable attention recently due to their outstanding performance in processing graph-structured data. However, GCNs still limited to the undirected graph because they theoretically require a symmetric matrix as the basis for the Laplacian transform. This causes the isotropic problem of the operator and reduced sensitivity in response to different information. In order to solve the problem, we generalize the spectral convolution operator to directed graphs by field extension, which improves the edge representations from scalars to vectors. Therefore, it brings in the concept of direction. That is to say, and even homogeneous information can become distinguishable by its differences in directions.In this paper, we propose the Vectorial Graph Convolutional Network(VecGCN) and the experimental evidence showing the advantages of a variety of directed graph node classification and link prediction tasks. ",
      "authors": [
        "ZhongYu Li",
        "Geng Zhao",
        "Hao Ning"
      ],
      "keywords": [
        "GNN",
        "GCN"
      ],
      "real_avg_scores": 3.5,
      "real_all_scores": [
        5,
        3,
        3,
        3
      ],
      "real_confidences": [
        4,
        5,
        5,
        4
      ],
      "real_contents": [],
      "real_rank": 19,
      "real_decision": "Reject",
      "sim_avg_scores": 1.0,
      "sim_all_scores": [
        1,
        1,
        1
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"VecGCN: A Breakthrough in Directed Graph Processing\"\n\nThe authors of this paper have made a significant contribution to the field of graph convolutional networks by proposing a novel approach to handle directed graphs. By generalizing the spectral convolution operator to directed graphs through field extension, VecGCN is able to capture the direction of edges, leading to improved edge representations and enhanced sensitivity to different information. The experimental results demonstrate the superiority of VecGCN in various directed graph node classification and link prediction tasks. This work has the potential to open up new avenues for graph-based learning in domains where directionality is crucial.\n\n**Review 2:**\n\"Overcoming the Limitations of Traditional GCNs\"\n\nTraditional graph convolutional networks (GCNs) have been limited by their requirement for symmetric matrices, restricting their applicability to undirected graphs. VecGCN addresses this limitation by introducing vectorial edge representations, enabling the processing of directed graphs. This innovation allows for the capture of directional information, making it possible to distinguish between homogeneous information based on direction. The authors provide compelling evidence of VecGCN's advantages in node classification and link prediction tasks, making this a must-read for researchers working with graph-structured data.\n\n**Review 3:**\n\"A New Frontier in Graph Neural Networks\"\n\nVecGCN represents a significant advancement in the field of graph neural networks, enabling the effective processing of directed graphs. By extending the spectral convolution operator to directed graphs, the authors have developed a powerful tool for capturing complex relationships in graph-structured data. The experimental results demonstrate the versatility of VecGCN in various tasks, including node classification and link prediction. This work has far-reaching implications for applications such as social network analysis, traffic prediction, and recommender systems, where directionality is a critical aspect of the data.\n\n**Review 4:**\n\"VecGCN: A Step Towards More Realistic Graph Modeling\"\n\nReal-world graphs are often directed, yet traditional GCNs are limited to undirected graphs. VecGCN addresses this mismatch by introducing a vectorial graph convolutional network that can handle directed graphs. This innovation enables the capture of directional information, leading to more realistic graph modeling. The authors provide a thorough evaluation of VecGCN, demonstrating its advantages in various tasks. This work has the potential to inspire new research directions in graph-based learning, particularly in domains where directionality is essential.\n\n**Review 5:**\n\"Unleashing the Power of Directed Graphs",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"VecGCN: A Breakthrough in Directed Graph Processing\"\n\nThe authors of this paper have made a significant contribution to the field of graph convolutional networks by proposing a novel approach to handle directed graphs. By generalizing the spectral convolution operator to directed graphs through field extension, VecGCN is able to capture the direction of edges, leading to improved edge representations and enhanced sensitivity to different information. The experimental results demonstrate the superiority of VecGCN in various directed graph node classification and link prediction tasks. This work has the potential to revolutionize the way we process graph-structured data and is a must-read for researchers in the field.\n\n**Review 2:**\n\"Overcoming the Limitations of Traditional GCNs\"\n\nTraditional Graph Convolutional Networks (GCNs) have been limited by their requirement for symmetric matrices, making them unsuitable for directed graphs. VecGCN addresses this limitation by introducing vectorial edge representations, enabling the capture of directional information. This innovation allows VecGCN to outperform traditional GCNs in directed graph node classification and link prediction tasks. The authors' approach is well-motivated and thoroughly evaluated, making this paper a valuable addition to the graph neural network literature.\n\n**Review 3:**\n\"A New Frontier in Graph Neural Networks\"\n\nVecGCN represents a significant advancement in the field of graph neural networks, enabling the effective processing of directed graphs. By extending the spectral convolution operator to directed graphs, the authors have opened up new possibilities for graph-based machine learning. The experimental results demonstrate the power of VecGCN in capturing complex relationships in directed graphs, and its potential applications are vast. This paper is a must-read for anyone interested in the future of graph neural networks.\n\n**Review 4:**\n\"VecGCN: A Step Towards More Realistic Graph Modeling\"\n\nReal-world graphs are often directed, yet traditional GCNs are limited to undirected graphs. VecGCN addresses this limitation by introducing a vectorial graph convolutional network that can effectively process directed graphs. The authors' approach is well-grounded in theory and is supported by comprehensive experimental evaluations. VecGCN has the potential to enable more realistic graph modeling and is a significant contribution to the field of graph neural networks.\n\n**Review 5:**\n\"Unleashing the Power of Directed Graphs with VecGCN\"\n\nVecGCN is a game-changer in the field of graph neural networks, enabling the effective processing of directed graphs. By capturing the",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"VecGCN: A Breakthrough in Directed Graph Processing\"\n\nThe authors of this paper have made a significant contribution to the field of graph convolutional networks by proposing a novel approach to handle directed graphs. By generalizing the spectral convolution operator to directed graphs through field extension, VecGCN is able to capture the direction of edges, leading to improved edge representations and enhanced sensitivity to different information. The experimental results demonstrate the superiority of VecGCN in various directed graph node classification and link prediction tasks. This work has the potential to open up new avenues for graph-based learning in domains where directionality is crucial.\n\n**Review 2:**\n\"Overcoming the Limitations of Traditional GCNs\"\n\nTraditional graph convolutional networks (GCNs) have been limited by their requirement for symmetric matrices, restricting their applicability to undirected graphs. VecGCN addresses this limitation by introducing vectorial edge representations, enabling the processing of directed graphs. This innovation allows for the capture of directional information, making it possible to distinguish between homogeneous information based on direction. The authors provide compelling evidence of VecGCN's advantages in node classification and link prediction tasks, making this a must-read for researchers and practitioners working with graph-structured data.\n\n**Review 3:**\n\"VecGCN: A Step Towards More Realistic Graph Modeling\"\n\nReal-world graphs are often directed, yet traditional GCNs are ill-equipped to handle these structures. VecGCN fills this gap by proposing a vectorial graph convolutional network that can effectively process directed graphs. By incorporating directionality into the edge representations, VecGCN enables more realistic graph modeling, which is critical in many applications, such as social network analysis and traffic flow prediction. The authors' experimental results demonstrate the power of VecGCN in capturing nuanced patterns in directed graphs, making this a significant contribution to the field.\n\n**Review 4:**\n\"Unleashing the Power of Directed Graphs with VecGCN\"\n\nVecGCN represents a major breakthrough in graph convolutional networks, as it enables the effective processing of directed graphs. By moving beyond the limitations of traditional GCNs, VecGCN unlocks the potential of directed graphs, allowing for more accurate modeling of complex relationships and patterns. The authors' work has far-reaching implications for various domains, including computer vision, natural language processing, and recommender systems, where directed graphs are ubiquitous. This paper is a must-read for anyone interested in harnessing the power of graph-based learning"
      ],
      "sim_rank": 6,
      "sim_decision": "None"
    },
    "Graph Contrastive Learning Under Heterophily: Utilizing Graph Filters to Generate Graph Views": {
      "paper_pk": "997684b5-d773-469e-9f8b-de5121de5a6d",
      "title": "Graph Contrastive Learning Under Heterophily: Utilizing Graph Filters to Generate Graph Views",
      "abstract": "Graph Neural Networks have achieved tremendous success in (semi-)supervised tasks for which task-specific node labels are available. However, obtaining labels is expensive in many domains, specially as the graphs grow larger in size. Hence, there has been a growing interest in the application of self-supervised techniques, in particular contrastive learning (CL), to graph data. In general, CL methods work by maximizing the agreement between encoded augmentations of the same example, and minimizing agreement between encoded augmentations of different examples. However, we show that existing graph CL methods perform very poorly on graphs with heterophily, in which connected nodes tend to belong to different classes. First, we show that this is attributed to the ineffectiveness of existing graph augmentation methods. Then, we leverage graph filters to directly generate augmented graph views for graph CL under heterophily. In particular, instead of explicitly augmenting the graph topology and encoding the augmentations, we use a high-pass filter in the encoder to generate node representations only based on high-frequency graph signals. Then, we contrast the high-pass filtered representations with their low-pass counterparts produced by the same encoder, to generate representations. Our experimental results confirm that our proposed method, HLCL, outperforms state-of-the-art CL methods on benchmark graphs with heterophily, by up to 10%.",
      "authors": [
        "Wenhan Yang",
        "Baharan Mirzasoleiman"
      ],
      "keywords": [
        "GNN",
        "Contrastive learning",
        "Heterophily",
        "Graph Representation Learning"
      ],
      "real_avg_scores": 4.0,
      "real_all_scores": [
        3,
        5,
        5,
        3
      ],
      "real_confidences": [
        5,
        3,
        3,
        4
      ],
      "real_contents": [],
      "real_rank": 17,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a crucial problem in graph contrastive learning, namely the poor performance of existing methods on graphs with heterophily. The authors' proposed method, HLCL, leverages graph filters to generate augmented graph views, which is a clever and effective approach. The experimental results are impressive, with HLCL outperforming state-of-the-art methods by up to 10%. This work has significant implications for self-supervised learning on large graphs, where labeling is often impractical. (Related papers: [1], [2])\"\n\n**Review 2:**\n\"The authors identify a key limitation of existing graph contrastive learning methods, which is their inability to handle heterophily in graphs. By using graph filters to generate high-pass and low-pass filtered representations, HLCL is able to effectively capture the underlying structure of the graph. The results demonstrate a significant improvement over existing methods, and the approach is well-motivated by the theoretical analysis. This work contributes to the growing body of research on self-supervised learning for graphs. (Related papers: [3], [4])\"\n\n**Review 3:**\n\"HLCL is a well-designed method that addresses a critical challenge in graph contrastive learning. The use of graph filters to generate augmented views is a novel and effective approach, and the experimental results are convincing. The authors' analysis of the limitations of existing methods is thorough and insightful, and the proposed method is well-supported by theoretical and empirical evidence. This work has the potential to impact a wide range of applications, from social network analysis to bioinformatics. (Related papers: [5], [6])\"\n\n**Review 4:**\n\"This paper presents a significant advancement in graph contrastive learning, particularly in the context of heterophily. The authors' proposed method, HLCL, is a clever combination of graph filters and contrastive learning, which enables the model to capture the underlying structure of the graph. The results are impressive, and the approach is well-motivated by the theoretical analysis. This work has important implications for self-supervised learning on large graphs, and is a valuable contribution to the field. (Related papers: [7], [8])\"\n\nNote: The related papers listed are fictional and used only for demonstration purposes. In a real review, the related papers would be actual papers that are relevant to the topic and methodology of the paper being reviewed.",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a crucial problem in graph contrastive learning, namely the poor performance of existing methods on graphs with heterophily. The authors' proposed method, HLCL, leverages graph filters to generate augmented graph views, which is a clever and effective approach. The experimental results are impressive, with HLCL outperforming state-of-the-art methods by up to 10%. This work has significant implications for self-supervised learning on large graphs, where labeling is often impractical. (Related papers: [1], [2])\"\n\n**Review 2:**\n\"The authors identify a key limitation of existing graph contrastive learning methods, which is their inability to handle heterophily in graphs. By using graph filters to generate high-pass and low-pass filtered representations, HLCL is able to effectively capture the underlying structure of the graph. The results are convincing, and the proposed method has the potential to be widely applicable to various graph-based tasks. One potential direction for future work is to explore the use of other types of graph filters or augmentations. (Related papers: [3], [4])\"\n\n**Review 3:**\n\"This paper makes a significant contribution to the field of graph contrastive learning by proposing a novel method that can handle heterophily in graphs. The use of graph filters to generate augmented views is a clever idea, and the experimental results demonstrate the effectiveness of HLCL. One of the strengths of this paper is its thorough analysis of the limitations of existing methods, which provides a clear motivation for the proposed approach. Overall, this work has the potential to have a significant impact on the development of self-supervised learning methods for graph data. (Related papers: [5], [6])\"\n\nReferences:\n\n[1] \"Graph Contrastive Learning with Augmented Graphs\" by XYZ et al.\n\n[2] \"Self-Supervised Graph Learning with Graph Autoencoders\" by ABC et al.\n\n[3] \"Graph Filter Banks for Graph Neural Networks\" by DEF et al.\n\n[4] \"Graph Augmentation for Graph Neural Networks\" by GHI et al.\n\n[5] \"Heterophily in Graphs: A Survey\" by JKL et al.\n\n[6] \"Graph Neural Networks for Semi-Supervised Learning\" by MNO et al.",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a crucial problem in graph contrastive learning, namely the poor performance of existing methods on graphs with heterophily. The authors' proposed method, HLCL, leverages graph filters to generate augmented graph views, which is a clever and effective approach. The experimental results are impressive, with HLCL outperforming state-of-the-art methods by up to 10%. This work has significant implications for self-supervised learning on large graphs, where labeling is often impractical. (Related papers: [1], [2])\"\n\n**Review 2:**\n\"The authors identify a key limitation of existing graph contrastive learning methods, which is their inability to handle heterophily in graphs. By using graph filters to generate high-pass and low-pass filtered representations, HLCL is able to effectively capture the underlying structure of the graph. The results demonstrate a significant improvement over existing methods, and the approach is well-motivated by the theoretical analysis. This work contributes to the growing body of research on self-supervised learning for graphs. (Related papers: [3], [4])\"\n\n**Review 3:**\n\"HLCL is a well-designed method that addresses a critical challenge in graph contrastive learning. The use of graph filters to generate augmented views is a novel and effective approach, and the experimental results are convincing. The authors' analysis of the limitations of existing methods is thorough and well-supported by the literature. This work has the potential to impact a wide range of applications, from social network analysis to bioinformatics. (Related papers: [5], [6])\"\n\n**Review 4:**\n\"This paper presents a significant advancement in graph contrastive learning, particularly in the context of heterophily. The authors' proposed method, HLCL, is well-motivated by the theoretical analysis and is supported by strong experimental results. The use of graph filters to generate augmented views is a clever and effective approach, and the results demonstrate a significant improvement over existing methods. This work is a valuable contribution to the field of self-supervised learning for graphs. (Related papers: [7], [8])\"\n\nNote: The related papers listed are fictional and used only for demonstration purposes. In a real review, the related papers would be actual papers that are relevant to the topic and support the review."
      ],
      "sim_rank": 10,
      "sim_decision": "None"
    },
    "FoSR: First-order spectral rewiring for addressing oversquashing in GNNs": {
      "paper_pk": "0354584f-57bc-4b80-89d8-8b664eba7b88",
      "title": "FoSR: First-order spectral rewiring for addressing oversquashing in GNNs",
      "abstract": "Graph neural networks (GNNs) are able to leverage the structure of graph data by passing messages along the edges of the graph. While this allows GNNs to learn features depending on the graph structure, for certain graph topologies it leads to inefficient information propagation and a problem known as oversquashing. This has recently been linked with the curvature and spectral gap of the graph. On the other hand, adding edges to the message-passing graph can lead to increasingly similar node representations and a problem known as oversmoothing. We propose a computationally efficient algorithm that prevents oversquashing by systematically adding edges to the graph based on spectral expansion. We combine this with a relational architecture, which lets the GNN preserve the original graph structure and provably prevents oversmoothing. We find experimentally that our algorithm outperforms existing graph rewiring methods in several graph classification tasks.",
      "authors": [
        "Kedar Karhadkar",
        "Pradeep Kr. Banerjee",
        "Guido Montufar"
      ],
      "keywords": [
        "oversquashing",
        "oversmoothing",
        "graph rewiring",
        "graph neural networks",
        "GNN",
        "relational GNN",
        "spectral expansion"
      ],
      "real_avg_scores": 3.5,
      "real_all_scores": [
        3,
        3,
        3,
        5
      ],
      "real_confidences": [
        4,
        4,
        5,
        4
      ],
      "real_contents": [],
      "real_rank": 20,
      "real_decision": "",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FoSR: A Breakthrough in Graph Neural Networks\" (5/5)\n\nThe authors of FoSR have made a significant contribution to the field of graph neural networks (GNNs) by addressing the long-standing problem of oversquashing. By leveraging spectral expansion to add edges to the message-passing graph, FoSR prevents inefficient information propagation and achieves state-of-the-art performance in several graph classification tasks. The proposed relational architecture is a clever solution to prevent oversmoothing, and the experimental results are impressive. FoSR is a must-read for anyone working with GNNs.\n\n**Review 2:**\n\"A Novel Approach to Graph Rewiring\" (4.5/5)\n\nFoSR presents a novel and computationally efficient algorithm for graph rewiring, which tackles the problem of oversquashing in GNNs. The authors' use of spectral expansion to add edges to the graph is a clever idea, and the experimental results demonstrate the effectiveness of their approach. While the relational architecture is an interesting concept, it would be beneficial to see more theoretical analysis to support its claims. Nevertheless, FoSR is a valuable contribution to the field of graph neural networks.\n\n**Review 3:**\n\"Oversquashing in GNNs: A Solution at Last?\" (4/5)\n\nFoSR addresses a critical problem in graph neural networks, namely oversquashing, which has been linked to the curvature and spectral gap of the graph. The proposed algorithm is computationally efficient and shows promising results in several graph classification tasks. However, it would be beneficial to see more comparisons with existing graph rewiring methods to fully understand the advantages of FoSR. Additionally, the authors could provide more insights into the theoretical implications of their approach. Nevertheless, FoSR is a step in the right direction towards addressing oversquashing in GNNs.\n\n**Review 4:**\n\"A Promising Solution to Oversquashing and Oversmoothing\" (4.5/5)\n\nFoSR presents a comprehensive solution to two major problems in graph neural networks: oversquashing and oversmoothing. The authors' approach is well-motivated, and the experimental results demonstrate the effectiveness of their algorithm. The relational architecture is an interesting concept that allows the GNN to preserve the original graph structure while preventing oversmoothing. While more theoretical analysis would be beneficial, FoSR is a promising solution that has the potential to improve the performance of GNNs in various applications",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FoSR: A Breakthrough in Graph Neural Networks\" (5/5)\n\nThe authors of FoSR have made a significant contribution to the field of graph neural networks (GNNs) by addressing the long-standing problem of oversquashing. By leveraging spectral expansion to add edges to the message-passing graph, FoSR prevents inefficient information propagation and achieves state-of-the-art performance in several graph classification tasks. The proposed relational architecture is a clever solution to prevent oversmoothing, and the experimental results are impressive. FoSR is a must-read for anyone working with GNNs.\n\n**Review 2:**\n\"A Novel Approach to Graph Rewiring\" (4.5/5)\n\nFoSR presents a novel and computationally efficient algorithm for graph rewiring, which addresses the problem of oversquashing in GNNs. The authors' use of spectral expansion to add edges to the graph is a clever idea, and the experimental results demonstrate the effectiveness of their approach. While the relational architecture is an interesting solution to prevent oversmoothing, it may benefit from further exploration. Overall, FoSR is a valuable contribution to the field of GNNs and is worth reading for its innovative approach to graph rewiring.\n\n**Review 3:**\n\"Oversquashing in GNNs: A Critical Review of FoSR\" (4/5)\n\nFoSR tackles the important problem of oversquashing in GNNs, which has been linked to the curvature and spectral gap of the graph. The authors' proposed algorithm is computationally efficient and shows promising results in several graph classification tasks. However, the paper could benefit from a more in-depth analysis of the theoretical foundations of FoSR and its limitations. Additionally, the relational architecture, while interesting, may require further exploration to fully understand its implications. Overall, FoSR is a valuable contribution to the field, but it is not without its limitations.\n\n**Review 4:**\n\"Graph Neural Networks: A New Era with FoSR\" (5/5)\n\nFoSR marks a significant milestone in the development of graph neural networks. By addressing the problem of oversquashing, the authors have opened up new possibilities for GNNs to learn more effective representations of graph-structured data. The proposed algorithm is not only computationally efficient but also theoretically grounded, making it a valuable contribution to the field. The experimental results are impressive, and the relational architecture is a clever solution to prevent oversmoothing",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FoSR: A Breakthrough in Graph Neural Networks\" (5/5)\n\nThe authors of FoSR have made a significant contribution to the field of graph neural networks (GNNs) by addressing the long-standing problem of oversquashing. By leveraging spectral expansion to add edges to the message-passing graph, FoSR prevents inefficient information propagation and achieves state-of-the-art performance in several graph classification tasks. The proposed relational architecture is a clever solution to prevent oversmoothing, allowing the GNN to preserve the original graph structure. FoSR is a must-read for anyone working with GNNs.\n\n**Review 2:**\n\"A Novel Approach to Graph Rewiring\" (4.5/5)\n\nFoSR presents a novel and computationally efficient algorithm for graph rewiring, which addresses the problem of oversquashing in GNNs. The authors' use of spectral expansion to add edges to the graph is a clever idea, and the experimental results demonstrate the effectiveness of FoSR in several graph classification tasks. While the relational architecture is an interesting approach to prevent oversmoothing, it may benefit from further exploration and analysis. Overall, FoSR is a valuable contribution to the field of GNNs.\n\n**Review 3:**\n\"Oversquashing and Oversmoothing: A Tale of Two Problems\" (4/5)\n\nFoSR tackles two of the most pressing problems in GNNs: oversquashing and oversmoothing. The authors' approach to addressing oversquashing through spectral rewiring is well-motivated and effective, and the experimental results demonstrate the benefits of FoSR in several graph classification tasks. However, the relational architecture, while interesting, may not be the most effective solution to prevent oversmoothing. Further analysis and comparison with other approaches would be beneficial to fully understand the strengths and limitations of FoSR.\n\n**Review 4:**\n\"A Step Forward in Graph Neural Networks\" (4.5/5)\n\nFoSR is a significant step forward in the development of GNNs, addressing the problem of oversquashing and providing a novel approach to graph rewiring. The authors' use of spectral expansion is a clever idea, and the experimental results demonstrate the effectiveness of FoSR in several graph classification tasks. While FoSR is not a panacea for all the problems in GNNs, it is a valuable contribution to the field and a must-read for anyone working with graph-structured data.\n\n**"
      ],
      "sim_rank": 11,
      "sim_decision": "None"
    },
    "Agent-based Graph Neural Networks": {
      "paper_pk": "bbb70046-6fbb-47c5-af3b-f4189c9b5965",
      "title": "Agent-based Graph Neural Networks",
      "abstract": "We present a novel graph neural network we call AgentNet, which is designed specifically for graph-level tasks. AgentNet is inspired by sublinear algorithms, featuring a computational complexity that is independent of the graph size. The architecture of AgentNet differs fundamentally from the architectures of traditional graph neural networks. In AgentNet, some trained \\textit{neural agents} intelligently walk the graph, and then collectively decide on the output. We provide an extensive theoretical analysis of AgentNet: We show that the agents can learn to systematically explore their neighborhood and that AgentNet can distinguish some structures that are even indistinguishable by 2-WL. Moreover, AgentNet is able to separate any two graphs which are sufficiently different in terms of subgraphs. We confirm these theoretical results with synthetic experiments on hard-to-distinguish graphs and real-world graph classification tasks. In both cases, we compare favorably not only to standard GNNs but also to computationally more expensive GNN extensions.",
      "authors": [
        "Karolis Martinkus",
        "P\u00e1l Andr\u00e1s Papp",
        "Benedikt Schesch",
        "Roger Wattenhofer"
      ],
      "keywords": [
        "Graph Neural Networks",
        "GNN",
        "Graph Classification",
        "Expressive Graph Neural Networks",
        "Sublinear algorithms"
      ],
      "real_avg_scores": 7.0,
      "real_all_scores": [
        6,
        8,
        6,
        8
      ],
      "real_confidences": [
        3,
        3,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 2,
      "real_decision": "Accept: poster",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer in Graph Neural Networks! AgentNet is a revolutionary approach to graph-level tasks, offering a sublinear computational complexity that's independent of graph size. The neural agents' intelligent walking and collective decision-making process is a stroke of genius. The theoretical analysis is thorough, and the experimental results on hard-to-distinguish graphs and real-world tasks are impressive. AgentNet outperforms standard GNNs and even computationally expensive extensions. A must-read for anyone working in graph neural networks!\"\n\n**4-star review**\n\"AgentNet: A Fresh Perspective on Graph Neural Networks. This paper presents a novel architecture that diverges from traditional GNNs, leveraging neural agents to explore the graph and make collective decisions. The theoretical analysis is solid, and the experiments demonstrate AgentNet's ability to distinguish structures that are indistinguishable by 2-WL. While the results are promising, I'd like to see more comparisons with other state-of-the-art methods. Nonetheless, AgentNet is an exciting development in the field of graph neural networks.\"\n\n**3-star review**\n\"AgentNet: An Interesting Approach, But Room for Improvement. The idea of using neural agents to walk the graph and make collective decisions is intriguing, and the sublinear computational complexity is a significant advantage. However, I'm not entirely convinced by the experimental results, which seem to focus on specific graph structures. I'd like to see more diverse experiments and comparisons with other methods. Additionally, the paper could benefit from more detailed explanations of the neural agents' decision-making process. Still, AgentNet is a promising direction for graph neural networks.\"\n\n**2-star review**\n\"AgentNet: A Novelty, But Not Without Its Flaws. While the concept of AgentNet is innovative, I'm not convinced that it's a game-changer. The theoretical analysis is thorough, but the experimental results are limited, and the comparisons with other methods are not comprehensive. I'm also concerned about the scalability of AgentNet, as the number of neural agents increases with the graph size. More work is needed to demonstrate the practicality and effectiveness of AgentNet in real-world scenarios.\"\n\n**1-star review**\n\"AgentNet: An Overhyped Approach. I'm underwhelmed by the novelty of AgentNet, which seems to be more of a theoretical exercise than a practical solution. The paper's claims about sublinear computational complexity are exaggerated, and the experimental results are unimpressive. The neural agents'",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer in Graph Neural Networks! AgentNet is a revolutionary approach to graph-level tasks, offering a sublinear computational complexity that's independent of graph size. The neural agents' intelligent walking and collective decision-making process is a stroke of genius. The theoretical analysis is thorough, and the experimental results on hard-to-distinguish graphs and real-world tasks are impressive. AgentNet outperforms standard GNNs and even computationally expensive extensions. A must-read for anyone working in graph neural networks!\"\n\n**4-star review**\n\"AgentNet: A Fresh Perspective on Graph Neural Networks. This paper presents a novel architecture that diverges from traditional GNNs, leveraging neural agents to explore the graph and make collective decisions. The theoretical analysis is solid, and the results on synthetic and real-world datasets are promising. While the idea of neural agents is intriguing, I'd like to see more exploration of their limitations and potential applications. Nonetheless, AgentNet is a significant contribution to the field, and I'm excited to see its future developments.\"\n\n**3-star review**\n\"AgentNet: An Interesting Approach, But Room for Improvement. The authors propose a unique graph neural network architecture, AgentNet, which boasts sublinear computational complexity. The neural agents' walking mechanism is an innovative idea, but I'm not entirely convinced of its effectiveness in all scenarios. The theoretical analysis is thorough, but the experimental results, while promising, could be more comprehensive. AgentNet shows potential, but it's not yet clear whether it can generalize to diverse graph structures and tasks. Further research is needed to fully realize its potential.\"\n\n**2-star review**\n\"AgentNet: A Novelty, But Not Without Its Drawbacks. The paper introduces AgentNet, a graph neural network that uses neural agents to explore the graph. While the idea is intriguing, I'm concerned about the scalability and interpretability of this approach. The theoretical analysis is dense, and the experimental results, although promising, are limited to specific graph classification tasks. I'd like to see more discussion on the limitations and potential pitfalls of AgentNet, as well as more extensive evaluations on diverse graph datasets.\"\n\n**1-star review**\n\"AgentNet: An Overhyped Approach. The authors claim that AgentNet is a game-changer in graph neural networks, but I'm not convinced. The neural agents' walking mechanism seems overly complex, and the theoretical analysis is too focused on specific graph structures. The experimental results are underwhelming,",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer in Graph Neural Networks! AgentNet is a revolutionary approach to graph-level tasks, offering a sublinear computational complexity that's independent of graph size. The neural agents' intelligent walking and collective decision-making process is a stroke of genius. The theoretical analysis is thorough, and the experimental results on hard-to-distinguish graphs and real-world tasks are impressive. AgentNet outperforms standard GNNs and even computationally expensive extensions. A must-read for anyone working in graph neural networks!\"\n\n**4-star review**\n\"AgentNet: A Fresh Perspective on Graph Neural Networks. This paper presents a novel architecture that diverges from traditional GNNs, leveraging neural agents to explore the graph and make collective decisions. The theoretical analysis is solid, and the experiments demonstrate AgentNet's ability to distinguish structures that are indistinguishable by 2-WL. While the results are promising, I'd like to see more comparisons with other state-of-the-art methods. Nonetheless, AgentNet is an exciting development in the field of graph neural networks.\"\n\n**3-star review**\n\"AgentNet: An Interesting Approach, But Room for Improvement. The idea of using neural agents to walk the graph and make collective decisions is intriguing, and the sublinear computational complexity is a significant advantage. However, I'm not entirely convinced by the experimental results, which seem to focus on specific graph structures. I'd like to see more diverse experiments and comparisons with other methods. Additionally, the paper could benefit from more detailed explanations of the neural agents' decision-making process. Still, AgentNet is a promising direction for graph neural networks.\"\n\n**2-star review**\n\"AgentNet: A Novelty, But Not Without Its Flaws. While the concept of AgentNet is innovative, I'm not convinced that it's a game-changer. The theoretical analysis is thorough, but the experimental results are limited, and the comparisons with other methods are not comprehensive. I'm also concerned about the scalability of AgentNet, as the number of neural agents increases with the graph size. More work is needed to demonstrate the practicality and effectiveness of AgentNet in real-world scenarios.\"\n\n**1-star review**\n\"AgentNet: An Overhyped Approach. I'm underwhelmed by the novelty of AgentNet, which seems to be more of a theoretical exercise than a practical solution. The paper's claims about sublinear computational complexity are exaggerated, and the experimental results are unimpressive. The neural agents'"
      ],
      "sim_rank": 12,
      "sim_decision": "None"
    },
    "DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks": {
      "paper_pk": "f9b6a95a-139f-45d5-86ee-5f1edb1ade00",
      "title": "DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks",
      "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure \u2013 GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its\u2019 reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.",
      "authors": [
        "Wenqian Li",
        "Yinchuan Li",
        "Zhigang Li",
        "Jianye HAO",
        "Yan Pang"
      ],
      "keywords": [
        "GNN",
        "Interpretability"
      ],
      "real_avg_scores": 4.333333333333333,
      "real_all_scores": [
        5,
        3,
        5
      ],
      "real_confidences": [
        3,
        5,
        3
      ],
      "real_contents": [],
      "real_rank": 16,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"DAG Matters! indeed! The authors of this paper have made a significant breakthrough in explaining graph neural networks (GNNs) by proposing a novel generative approach, GFlowNets-based GNN Explainer (GFlowExplainer). By transforming the optimization problem into a step-by-step generative problem, they have successfully eliminated the need for pre-training strategies and made the approach scalable to large-scale GNNs. The experimental results on both synthetic and real datasets are impressive, demonstrating the superiority of GFlowExplainer over existing methods. This paper is a must-read for anyone working on explainability in GNNs.\"\n\n**4-star review**\n\"This paper presents an innovative approach to explaining graph neural networks (GNNs) using GFlowNets-based GNN Explainer (GFlowExplainer). By learning a policy that generates a distribution of subgraphs, the authors have developed a more efficient and effective method for uncovering rationales behind GNN predictions. The proposed cut vertex matrix is a clever solution to efficiently explore parent states for GFlowNets structure. While the experimental results are promising, I would have liked to see more comparisons with other state-of-the-art methods to further validate the approach. Nonetheless, this paper is a valuable contribution to the field of explainable AI.\"\n\n**3-star review**\n\"I was excited to dive into this paper, given the growing importance of explainability in graph neural networks (GNNs). The authors' proposal of using GFlowNets-based GNN Explainer (GFlowExplainer) is an interesting one, and the idea of transforming the optimization problem into a generative problem is novel. However, I found the paper to be somewhat lacking in terms of clarity and detail. The experimental results, while promising, could be more comprehensive and include more baselines for comparison. Additionally, I would have liked to see more discussion on the limitations and potential applications of GFlowExplainer. Overall, this paper shows promise, but could benefit from further refinement.\"\n\n**2-star review**\n\"While I appreciate the authors' effort to tackle the challenging problem of explaining graph neural networks (GNNs), I was underwhelmed by the proposed approach. The use of GFlowNets-based GNN Explainer (GFlowExplainer) seems to be a complex solution to a problem that may not require such complexity. The paper could benefit from a more thorough comparison with existing methods, as well as more detailed explanations",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"DAG Matters! indeed! The authors of this paper have made a significant breakthrough in explaining graph neural networks (GNNs) by proposing a novel generative approach, GFlowNets-based GNN Explainer (GFlowExplainer). By transforming the optimization problem into a step-by-step generative problem, they have successfully eliminated the need for pre-training strategies and made the approach scalable to large-scale GNNs. The experimental results on both synthetic and real datasets are impressive, demonstrating the superiority of GFlowExplainer over existing methods. This paper is a must-read for anyone working on explainability in GNNs.\"\n\n**4-star review**\n\"This paper presents an innovative approach to explaining graph neural networks (GNNs) using GFlowNets-based GNN Explainer (GFlowExplainer). By learning a policy that generates a distribution of subgraphs, the authors have developed a more efficient and effective method for identifying rationales behind GNN predictions. The elimination of node sequence influence and the introduction of a new cut vertex matrix are particularly noteworthy contributions. While the experimental results are promising, I would have liked to see more comparisons with existing methods to further validate the approach. Nonetheless, this paper is a valuable addition to the field of GNN explainability.\"\n\n**3-star review**\n\"I was intrigued by the idea of using GFlowNets to explain graph neural networks (GNNs), but I'm not entirely convinced by the authors' approach. While the GFlowExplainer method is novel and shows promise, I think it's still early days for this technique. The experimental results are encouraging, but I'd like to see more thorough evaluations and comparisons with other state-of-the-art methods. Additionally, I'm not sure if the proposed approach can generalize well to more complex GNN architectures. Overall, this paper is a good starting point, but more work is needed to fully realize the potential of GFlowNets in GNN explainability.\"\n\n**2-star review**\n\"I was disappointed by this paper's lack of depth in exploring the theoretical foundations of GFlowNets-based GNN Explainer (GFlowExplainer). While the authors claim to have developed a novel approach, I found the explanations and justifications for their method to be superficial. The experimental results, while promising, are not sufficient to convince me of the approach's validity. Furthermore, I'm concerned about the scalability of the method, as the authors' claims about its applic",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-changer for GNN explainability! The authors of DAG Matters! have successfully tackled the long-standing issue of scalability in graph neural network (GNN) explainers. By introducing GFlowNets-based GNN Explainer (GFlowExplainer), they have transformed the optimization problem into a generative one, making it possible to handle large-scale GNNs. The proposed approach is not only efficient but also eliminates the need for pre-training strategies. The experimental results on both synthetic and real datasets are impressive, demonstrating the superiority of GFlowExplainer. A must-read for anyone working on GNN explainability!\"\n\n**4-star review**\n\"I was impressed by the innovative approach taken by the authors in DAG Matters! to address the limitations of existing GNN explainers. By leveraging GFlowNets, they have developed a generative model that can learn a policy to generate a distribution of subgraphs, making it possible to provide faithful explanations for large-scale GNNs. The introduction of the cut vertex matrix is also a clever move, enabling efficient exploration of parent states. While the experimental results are promising, I would have liked to see more comparisons with other state-of-the-art methods. Nevertheless, GFlowExplainer is a significant contribution to the field of GNN explainability.\"\n\n**3-star review**\n\"DAG Matters! presents an interesting approach to GNN explainability, but it's not without its limitations. The authors' proposal to use GFlowNets to generate a distribution of subgraphs is novel, but it's unclear how well this approach will generalize to more complex graph structures. The elimination of node sequence influence is a plus, but I'm not convinced that this approach is scalable to extremely large graphs. The experimental results are encouraging, but more thorough evaluations are needed to fully assess the effectiveness of GFlowExplainer. Still, this paper is a valuable contribution to the ongoing discussion on GNN explainability.\"\n\n**2-star review**\n\"I was underwhelmed by DAG Matters! The authors' claim that their approach is more scalable than existing methods is not entirely convincing, given the lack of thorough comparisons with other state-of-the-art methods. While the use of GFlowNets is an interesting idea, it's not clear how well this approach will work in practice, especially for graphs with complex structures. The experimental results are somewhat promising, but more work is needed to demonstrate the robustness and generalizability of GFlowExplainer"
      ],
      "sim_rank": 13,
      "sim_decision": "None"
    },
    "Graph Spline Networks for Efficient Continuous Simulation of Dynamical Systems": {
      "paper_pk": "69498623-3bfe-4df0-be20-02f94b36d411",
      "title": "Graph Spline Networks for Efficient Continuous Simulation of Dynamical Systems",
      "abstract": "While complex simulations of physical systems have been widely studied in engineering and scientific computing, lowering their often prohibitive computational requirements has only recently been tackled by deep learning approaches. In this paper, we present GraphSplineNets, a novel deep learning approach to speed up simulation of physical systems with spatio-temporal continuous outputs by exploiting the synergy between graph neural networks (GNN) and orthogonal spline collocation (OSC). Two differentiable time-oriented OSC and spatial-oriented OSC are applied to bridge the gap between discrete GNN outputs and generate continuous solutions at any location in space and time without explicit prior knowledge of underlying differential equations. Moreover, we introduce an adaptive collocation strategy in space to enable the model to sample from the most important regions. Our model improves on widely used graph neural networks for physics simulation on both efficiency and solution accuracy. We demonstrate SplineGraphNets in predicting complex dynamical systems such as the heat equation, damped wave propagation and the Navier-Stokes equations for incompressible flow, where they improve accuracy of more than 25% while providing at least 60% speedup. ",
      "authors": [
        "Chuanbo Hua",
        "Federico Berto",
        "Michael Poli",
        "Stefano Massaroli",
        "Jinkyoo Park"
      ],
      "keywords": [
        "Graph",
        "Spline Collocation Method",
        "Graph Neural Networks",
        "Simulation",
        "Partial Differential Equations",
        "PDEs",
        "Physics",
        "Scientific Computing"
      ],
      "real_avg_scores": 5.5,
      "real_all_scores": [
        6,
        5,
        8,
        3
      ],
      "real_confidences": [
        5,
        3,
        2,
        4
      ],
      "real_contents": [],
      "real_rank": 11,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-changer for physics simulations! The GraphSplineNets approach presented in this paper is a masterclass in combining the strengths of graph neural networks and orthogonal spline collocation. The authors' innovative use of differentiable time-oriented and spatial-oriented OSC bridges the gap between discrete GNN outputs and continuous solutions, making it possible to simulate complex physical systems with unprecedented efficiency and accuracy. The results speak for themselves: 25% improvement in accuracy and at least 60% speedup compared to traditional GNN methods. A must-read for anyone working in scientific computing or engineering.\"\n\n**4-star review**\n\"I was impressed by the authors' ability to tackle the long-standing problem of computational requirements in physical system simulations. The GraphSplineNets approach is a clever combination of GNNs and OSC, and the introduction of an adaptive collocation strategy in space is a nice touch. The results are promising, with significant improvements in accuracy and speedup compared to traditional methods. However, I would have liked to see more discussion on the limitations and potential applications of this approach. Nevertheless, a solid contribution to the field of deep learning for physics simulations.\"\n\n**3-star review**\n\"While the GraphSplineNets approach is an interesting idea, I'm not entirely convinced by the authors' claims. The paper could benefit from more detailed explanations of the methodology and more extensive experiments to demonstrate the robustness of the approach. The results are promising, but I'd like to see more comparisons with other state-of-the-art methods to fully appreciate the advantages of GraphSplineNets. Additionally, the authors could have explored more real-world applications of their approach. Still, a worthwhile read for those interested in the intersection of deep learning and scientific computing.\"\n\n**2-star review**\n\"I was underwhelmed by this paper. While the authors' idea of combining GNNs and OSC is intriguing, the execution falls short. The paper lacks a clear explanation of the underlying mathematics, making it difficult to follow for non-experts. The results, while promising, are not sufficiently convincing to justify the authors' claims. Moreover, the paper could have benefited from more discussion on the potential limitations and challenges of the approach. Overall, a decent effort, but not a game-changer in the field of physics simulations.\"\n\n**1-star review**\n\"I struggled to understand the authors' approach and was disappointed by the lack of clarity in the paper. The writing is dense and assumes a high level of",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer for Physics Simulation! GraphSplineNets is a revolutionary approach that combines the strengths of graph neural networks and orthogonal spline collocation to simulate complex physical systems with unprecedented efficiency and accuracy. The authors' innovative use of differentiable time-oriented and spatial-oriented OSC bridges the gap between discrete GNN outputs and continuous solutions, making it possible to generate accurate predictions at any location in space and time without prior knowledge of underlying differential equations. The results are stunning, with improvements of over 25% in accuracy and at least 60% speedup compared to traditional graph neural networks. This paper is a must-read for anyone working in scientific computing, engineering, or physics simulation.\"\n\n**4-star review**\n\"A Promising Approach to Efficient Physics Simulation. GraphSplineNets offers a novel solution to the long-standing problem of computationally expensive simulations of physical systems. By leveraging the strengths of graph neural networks and orthogonal spline collocation, the authors demonstrate significant improvements in efficiency and accuracy compared to traditional methods. The adaptive collocation strategy in space is a particularly clever innovation, allowing the model to focus on the most important regions. While the results are impressive, I would have liked to see more extensive comparisons with other state-of-the-art methods. Nevertheless, this paper is a valuable contribution to the field and has the potential to inspire further research.\"\n\n**3-star review**\n\"An Interesting but Not Fully Convincing Approach. GraphSplineNets is an innovative attempt to combine graph neural networks and orthogonal spline collocation for efficient physics simulation. The authors' use of differentiable OSC is an interesting idea, and the results show some promise. However, I have some reservations about the methodology and the scope of the experiments. The paper could benefit from more detailed explanations of the underlying mathematics and more extensive validation of the results. Additionally, the comparisons with other methods are limited, making it difficult to fully assess the advantages of GraphSplineNets. While this paper is a good starting point, more work is needed to fully establish the effectiveness of this approach.\"\n\n**2-star review**\n\"A Novel but Not Fully Developed Idea. GraphSplineNets is an ambitious attempt to tackle the challenging problem of efficient physics simulation. The authors' idea of combining graph neural networks and orthogonal spline collocation is intriguing, but the execution falls short. The paper lacks a clear explanation of the underlying theory, and the experiments are limited in scope and depth. The results, while promising,",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer for Physics Simulation! GraphSplineNets is a revolutionary approach that combines the strengths of graph neural networks and orthogonal spline collocation to simulate complex physical systems with unprecedented efficiency and accuracy. The authors' innovative use of differentiable time-oriented and spatial-oriented OSC bridges the gap between discrete GNN outputs and continuous solutions, making it possible to generate accurate predictions at any location in space and time without prior knowledge of underlying differential equations. The results are stunning, with improvements of over 25% in accuracy and at least 60% speedup compared to traditional graph neural networks. This paper is a must-read for anyone working in scientific computing, engineering, or physics simulation.\"\n\n**4-star review**\n\"A Promising Approach to Efficient Physics Simulation. GraphSplineNets offers a novel solution to the long-standing problem of computationally expensive simulations of physical systems. By leveraging the synergy between graph neural networks and orthogonal spline collocation, the authors demonstrate significant improvements in both efficiency and accuracy. The adaptive collocation strategy in space is a particularly clever innovation, enabling the model to focus on the most important regions. While the results are impressive, I would have liked to see more extensive comparisons with other state-of-the-art methods. Nevertheless, this paper is a valuable contribution to the field and has the potential to inspire further research.\"\n\n**3-star review**\n\"An Interesting but Not Fully Convincing Approach. GraphSplineNets is an intriguing idea that attempts to combine the strengths of graph neural networks and orthogonal spline collocation to simulate physical systems. While the authors demonstrate some promising results, I have some reservations about the methodology. The use of differentiable time-oriented and spatial-oriented OSC is innovative, but I'm not convinced that it's the most effective way to bridge the gap between discrete GNN outputs and continuous solutions. Additionally, the evaluation could be more comprehensive, with more detailed comparisons to other methods and a more thorough analysis of the limitations of the approach. Overall, this paper is a good starting point, but more work is needed to fully realize the potential of GraphSplineNets.\"\n\n**2-star review**\n\"A Novel but Not Fully Developed Idea. GraphSplineNets is an ambitious attempt to speed up simulation of physical systems using deep learning. While the authors propose an interesting combination of graph neural networks and orthogonal spline collocation, the execution falls short. The paper lacks a thorough evaluation of the method, with limited comparisons to other approaches and no"
      ],
      "sim_rank": 14,
      "sim_decision": "None"
    },
    "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks": {
      "paper_pk": "597263f2-6bde-4c76-b66e-a2e78d584e63",
      "title": "GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks",
      "abstract": "Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation.",
      "authors": [
        "Xiaoqi Wang",
        "Han Wei Shen"
      ],
      "keywords": [
        "AI Interpretability",
        "Graph Neural Networks",
        "Model-Level Explanation of Neural Networks"
      ],
      "real_avg_scores": 7.5,
      "real_all_scores": [
        8,
        6,
        8,
        8
      ],
      "real_confidences": [
        4,
        3,
        5,
        4
      ],
      "real_contents": [],
      "real_rank": 1,
      "real_decision": "Accept: notable-top-5%",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"GNNInterpreter is a game-changer in the field of graph neural networks. The proposed model-agnostic explanation method provides unparalleled insights into the decision-making process of GNNs, which is crucial in high-stakes applications like biomedicine. The probabilistic generative graph distribution approach is both flexible and efficient, allowing for the generation of explanation graphs with varying node and edge features without introducing additional complexity. The experimental results are impressive, demonstrating the effectiveness of GNNInterpreter in revealing desired graph patterns and potential model pitfalls. A must-read for anyone working with GNNs!\"\n\n**4-star review**\n\"I was impressed by the novelty of the objective function designed specifically for model-level explanation in GNNs. GNNInterpreter's ability to learn a probabilistic generative graph distribution that captures the most discriminative graph patterns is a significant contribution to the field. The flexibility and computational efficiency of the method are notable advantages over existing works. However, I would have liked to see more extensive experiments on diverse datasets to further validate the approach. Nevertheless, GNNInterpreter is a valuable tool for demystifying GNNs and promoting trust in their predictions.\"\n\n**3-star review**\n\"While GNNInterpreter is an interesting approach to explaining GNNs, I'm not entirely convinced of its practical applicability. The method's reliance on a novel objective function and probabilistic generative graph distribution may make it challenging to implement and interpret for non-experts. Additionally, the experimental results, although promising, are limited to four datasets and may not generalize well to other domains. That being said, the idea of model-agnostic explanation for GNNs is intriguing, and I look forward to seeing further developments and refinements of this approach.\"\n\n**2-star review**\n\"I was expecting more from GNNInterpreter, given the hype surrounding explainable AI. While the authors propose a novel method for explaining GNNs, it feels like an incremental improvement over existing works rather than a breakthrough. The probabilistic generative graph distribution approach is not particularly innovative, and the experimental results, although decent, don't blow me away. Furthermore, the method's flexibility and efficiency come at the cost of increased complexity, which may deter practitioners from adopting it. Overall, GNNInterpreter is a solid but not spectacular contribution to the field.\"\n\n**1-star review**\n\"I'm underwhelmed by GNNInterpreter. The authors' claims about the method's flexibility and efficiency are overstated, and the experimental results",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"GNNInterpreter is a game-changer in the field of graph neural networks. The proposed model-agnostic explanation method provides unparalleled insights into the decision-making process of GNNs, which is crucial in high-stakes applications like biomedicine. The probabilistic generative graph distribution approach is both flexible and efficient, allowing for the generation of explanation graphs with varying node and edge features without introducing additional complexity. The experimental results are impressive, demonstrating the effectiveness of GNNInterpreter in revealing model pitfalls and identifying desired graph patterns. A must-read for anyone working with GNNs!\"\n\n**4-star review**\n\"I was impressed by the novelty of the objective function designed specifically for model-level explanation in GNNs. GNNInterpreter's ability to learn a probabilistic generative graph distribution that captures the most discriminative graph patterns is a significant contribution to the field. The flexibility and efficiency of the approach are notable advantages over existing methods. However, I would have liked to see more extensive experiments on diverse datasets to further validate the method's generalizability. Nevertheless, GNNInterpreter is a valuable tool for demystifying GNNs and promoting trust in their predictions.\"\n\n**3-star review**\n\"While GNNInterpreter presents an interesting approach to explaining GNNs, I'm not entirely convinced of its practical applicability. The method's reliance on optimizing a novel objective function may require significant computational resources, which could be a limitation in real-world scenarios. Additionally, the assumption that the generated explanation graphs will always match the desired pattern if the model is ideal seems overly optimistic. More thorough evaluations of the method's robustness and sensitivity to model imperfections are needed to fully assess its utility. That being said, GNNInterpreter is a promising step towards more transparent and trustworthy GNNs.\"\n\n**2-star review**\n\"I'm skeptical about the novelty of GNNInterpreter, as it builds upon existing ideas in probabilistic generative models and graph neural networks. The paper's claims about the method's flexibility and efficiency seem exaggerated, and I'm not convinced that it can handle complex graph structures and diverse node/edge features as seamlessly as claimed. Furthermore, the experimental results, while promising, are limited to a few datasets and don't provide a comprehensive evaluation of the method's performance. More rigorous testing and comparison to existing methods are needed to establish GNNInterpreter's value.\"\n\n**1-star review**\n\"I was disappointed by the lack of clarity in the paper's presentation of GNNInterpreter.",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"GNNInterpreter is a game-changer in the field of graph neural networks. The proposed model-agnostic explanation method provides unparalleled insights into the decision-making process of GNNs, which is crucial in high-stakes applications like biomedicine. The probabilistic generative graph distribution approach is both flexible and efficient, allowing for the generation of explanation graphs with varying node and edge features without introducing additional complexity. The experimental results are impressive, demonstrating the effectiveness of GNNInterpreter in revealing model pitfalls and identifying desired graph patterns. A must-read for anyone working with GNNs!\"\n\n**4-star review**\n\"I was impressed by the novelty of the objective function designed specifically for model-level explanation in GNNs. GNNInterpreter's ability to generate explanation graphs without relying on manually specified domain-specific rules is a significant advantage over existing methods. The paper's experimental studies on four different datasets provide convincing evidence of the method's effectiveness. However, I would have liked to see more discussion on the limitations and potential extensions of the approach. Nevertheless, GNNInterpreter is a valuable contribution to the field of GNN interpretability.\"\n\n**3-star review**\n\"While GNNInterpreter shows promise in explaining the decision-making process of GNNs, I'm not entirely convinced by the approach. The paper's claim of being model-agnostic is appealing, but I'm concerned about the method's applicability to more complex GNN architectures. The experimental results are encouraging, but I would have liked to see more rigorous evaluation and comparison with existing methods. Additionally, the paper could benefit from more detailed explanations of the probabilistic generative graph distribution and its optimization process. Overall, GNNInterpreter is an interesting contribution, but more work is needed to fully realize its potential.\"\n\n**2-star review**\n\"I was underwhelmed by GNNInterpreter's approach to explaining GNNs. While the idea of generating explanation graphs is intriguing, I'm not convinced that the method is significantly better than existing approaches. The paper's reliance on a novel objective function and probabilistic generative graph distribution makes it difficult to understand and implement. The experimental results are limited, and I would have liked to see more comprehensive evaluation and comparison with other methods. Overall, GNNInterpreter feels like a incremental improvement rather than a breakthrough in GNN interpretability.\"\n\n**1-star review**\n\"I'm disappointed by the lack of clarity and rigor in GNNInterpreter's approach. The paper's abstract and introduction are overly ambitious, promising more"
      ],
      "sim_rank": 15,
      "sim_decision": "None"
    },
    "Topology Matters in Fair Graph Learning: a Theoretical Pilot Study": {
      "paper_pk": "a2ad9aa9-2503-418e-b115-7deb458fa3ee",
      "title": "Topology Matters in Fair Graph Learning: a Theoretical Pilot Study",
      "abstract": "Recent advances in fair graph learning observe that graph neural networks (GNNs) further amplify prediction bias compared with multilayer perception (MLP), while the reason behind this is unknown. In this paper, we conduct a theoretical analysis of the bias amplification mechanism in GNNs. This is a challenging task since GNNs are difficult to be interpreted, and real-world networks are complex. To bridge the gap, we theoretically and experimentally demonstrate that aggregation operation in representative GNNs accumulates bias in node representation due to topology bias induced by graph topology. We provide a sufficient condition identifying the statistical information of graph data, so that graph aggregation enhances prediction bias in GNNs. \n Motivated by this data-centric finding, we propose a fair graph refinement algorithm, named \\textit{FairGR}, to rewire graph topology to reduce sensitive homophily coefficient while preserving useful graph topology. Experiments on node classification tasks demonstrate that \\textit{FairGR} can mitigate the prediction bias with comparable performance on three real-world datasets. Additionally, \\textit{FairGR} is compatible with many state-of-the-art methods, such as adding regularization, adversarial debiasing, and Fair mixup via refining graph topology. Therefore, \\textit{FairGR} is a plug-in fairness method and can be adapted to improve existing fair graph learning strategies. ",
      "authors": [
        "Zhimeng Jiang",
        "Xiaotian Han",
        "Chao Fan",
        "Zirui Liu",
        "Xiao Huang",
        "Na Zou",
        "Ali Mostafavi",
        "Xia Hu"
      ],
      "keywords": [
        "Graph Neural Networks",
        "Fairness",
        "Topology"
      ],
      "real_avg_scores": 5.0,
      "real_all_scores": [
        5,
        5,
        5,
        5
      ],
      "real_confidences": [
        3,
        3,
        3,
        4
      ],
      "real_contents": [],
      "real_rank": 14,
      "real_decision": "",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper takes a crucial step towards understanding the bias amplification mechanism in graph neural networks (GNNs). By theoretically and experimentally demonstrating the role of aggregation operation in accumulating bias, the authors provide a much-needed explanation for the observed phenomenon. The proposed FairGR algorithm is a promising solution to mitigate prediction bias, and its compatibility with existing methods makes it a valuable addition to the fair graph learning toolkit. Overall, this work contributes significantly to the development of fair and unbiased GNNs.\"\n\n**Review 2:**\n\"The authors of this paper tackle a pressing issue in fair graph learning, namely the bias amplification in GNNs. Their theoretical analysis and experimental results convincingly show that graph topology plays a critical role in bias accumulation. The FairGR algorithm, which refines graph topology to reduce sensitive homophily coefficient, is a clever solution to this problem. The fact that it can be easily integrated with existing methods makes it a practical and effective tool for fair graph learning. This work has the potential to inspire further research in this area.\"\n\n**Review 3:**\n\"This paper presents a thorough investigation into the bias amplification mechanism in GNNs, providing a data-centric explanation for the observed phenomenon. The authors' theoretical and experimental results are convincing, and the proposed FairGR algorithm is a promising solution to mitigate prediction bias. What's more, the algorithm's compatibility with existing methods makes it a valuable addition to the fair graph learning toolkit. One potential area for future research is to explore the applicability of FairGR to other graph-based tasks beyond node classification. Overall, this work is a significant contribution to the field of fair graph learning.\"\n\n**Review 4:**\n\"The authors of this paper make a compelling case for the importance of considering graph topology in fair graph learning. Their theoretical analysis and experimental results demonstrate that aggregation operation in GNNs can accumulate bias due to topology bias. The proposed FairGR algorithm is a clever solution to this problem, and its ability to refine graph topology to reduce sensitive homophily coefficient is a significant advancement. The fact that FairGR can be easily integrated with existing methods makes it a practical and effective tool for fair graph learning. This work has the potential to inspire further research into the interplay between graph topology and bias in GNNs.\"\n\n**Review 5:**\n\"This paper provides a timely and important contribution to the field of fair graph learning. The authors' theoretical and experimental results convincingly demonstrate the role",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper takes a crucial step towards understanding the bias amplification mechanism in graph neural networks (GNNs). By theoretically and experimentally demonstrating the role of aggregation operation in accumulating bias, the authors provide a much-needed explanation for the observed phenomenon. The proposed FairGR algorithm is a promising solution to mitigate prediction bias, and its compatibility with existing methods makes it a valuable addition to the fair graph learning toolkit. Overall, this work contributes significantly to the development of fair and unbiased GNNs.\"\n\n**Review 2:**\n\"The authors of this paper tackle a pressing issue in fair graph learning, namely the bias amplification in GNNs. Their theoretical analysis and experimental results convincingly show that graph topology plays a critical role in bias accumulation. The FairGR algorithm, which refines graph topology to reduce sensitive homophily coefficient, is a clever solution to this problem. The fact that it can be easily integrated with existing methods makes it a practical and effective tool for fair graph learning. This work has the potential to inspire further research in this area.\"\n\n**Review 3:**\n\"This paper presents a thorough investigation into the bias amplification mechanism in GNNs, providing a data-centric explanation for the observed phenomenon. The authors' theoretical and experimental results are convincing, and the proposed FairGR algorithm is a promising solution to mitigate prediction bias. What's more, the algorithm's compatibility with existing methods makes it a valuable addition to the fair graph learning toolkit. One potential area for future research is to explore the applicability of FairGR to other graph-based tasks beyond node classification. Overall, this work is a significant contribution to the field of fair graph learning.\"\n\n**Review 4:**\n\"The authors of this paper make a compelling case for the importance of considering graph topology in fair graph learning. Their theoretical analysis and experimental results demonstrate that aggregation operation in GNNs can accumulate bias due to topology bias. The proposed FairGR algorithm is a clever solution to this problem, and its ability to refine graph topology to reduce sensitive homophily coefficient is a significant advancement. The fact that FairGR can be easily integrated with existing methods makes it a practical and effective tool for fair graph learning. This work has the potential to inspire further research into the interplay between graph topology and bias in GNNs.\"\n\n**Review 5:**\n\"This paper provides a timely and important contribution to the field of fair graph learning. The authors' theoretical and experimental results convincingly demonstrate the role",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper takes a crucial step towards understanding the bias amplification mechanism in graph neural networks (GNNs). By theoretically and experimentally demonstrating the role of aggregation operation in accumulating bias, the authors provide a much-needed explanation for the observed phenomenon. The proposed FairGR algorithm is a promising solution to mitigate prediction bias, and its compatibility with existing methods makes it a valuable addition to the fair graph learning toolkit. Overall, this work contributes significantly to the development of fair and unbiased GNNs.\"\n\n**Review 2:**\n\"The authors tackle a challenging problem in fair graph learning by investigating the impact of graph topology on bias amplification in GNNs. Their theoretical analysis and experimental results provide a compelling case for the importance of considering topology in fair graph learning. The FairGR algorithm is a clever solution that refines graph topology to reduce sensitive homophily coefficient while preserving useful information. The experiments on real-world datasets demonstrate the effectiveness of FairGR, and its plug-in nature makes it a versatile tool for improving existing fair graph learning strategies.\"\n\n**Review 3:**\n\"This paper offers a refreshing perspective on fair graph learning by highlighting the critical role of graph topology in bias amplification. The authors' theoretical and experimental findings provide a solid foundation for understanding the mechanisms underlying bias amplification in GNNs. The proposed FairGR algorithm is an innovative solution that addresses the root cause of bias amplification by refining graph topology. The results on node classification tasks are promising, and the compatibility of FairGR with existing methods makes it a valuable contribution to the field of fair graph learning.\"\n\n**Review 4:**\n\"The authors of this paper make a significant contribution to the field of fair graph learning by identifying the statistical information of graph data that enhances prediction bias in GNNs. Their theoretical analysis and experimental results provide a comprehensive understanding of the bias amplification mechanism in GNNs. The proposed FairGR algorithm is a well-designed solution that refines graph topology to reduce bias while preserving useful information. The experiments on real-world datasets demonstrate the effectiveness of FairGR, and its adaptability to existing methods makes it a valuable tool for improving fair graph learning strategies.\"\n\n**Review 5:**\n\"This paper presents a thorough investigation of the bias amplification mechanism in graph neural networks (GNNs) and proposes a novel solution to mitigate prediction bias. The authors' theoretical analysis and experimental results provide a solid understanding of the role of graph topology in bias amplification. The FairGR algorithm"
      ],
      "sim_rank": 16,
      "sim_decision": "None"
    },
    "CktGNN:  Circuit Graph Neural Network for Electronic Design Automation": {
      "paper_pk": "462ff939-beef-488a-a447-89d9078c04dc",
      "title": "CktGNN:  Circuit Graph Neural Network for Electronic Design Automation",
      "abstract": "The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have only been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves efficiency by reducing the number of subgraphs to perform message passing.\n\nNonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains $10$K distinct operational amplifiers with carefully-extracted circuit specifications from physical implementations. OCB also equips with communicative circuit generation and evaluation capabilities such that it can be used to generalize the applicability of CktGNN to design various analog circuits by efficiently producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and manual design from human experts. Our work paves the way toward a learning-based open-sourced design automation flow for analog circuits.",
      "authors": [
        "Zehao Dong",
        "Weidong Cao",
        "Muhan Zhang",
        "Dacheng Tao",
        "Yixin Chen",
        "Xuan Zhang"
      ],
      "keywords": [
        "Graph Neural Networks",
        "Electronic Design Automation",
        "Benchmark Graph Dataset"
      ],
      "real_avg_scores": 5.75,
      "real_all_scores": [
        8,
        6,
        6,
        3
      ],
      "real_confidences": [
        4,
        3,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 9,
      "real_decision": "Accept: poster",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"CktGNN is a game-changer in electronic design automation! The authors' innovative approach to circuit topology generation and device sizing using a two-level GNN framework has significantly improved efficiency and accuracy. The introduction of Open Circuit Benchmark (OCB) is a major milestone, providing a much-needed public dataset for reproducible research and canonical assessment. The experimental results are impressive, outperforming recent GNN baselines and manual design from human experts. This work has the potential to revolutionize the field of analog circuit design automation.\"\n\n**4-star review**\n\"I was impressed by the authors' recognition of the graph nature of circuits and their development of CktGNN, a Circuit Graph Neural Network that automates circuit topology generation and device sizing. The use of a two-level GNN framework to encode circuit graphs is clever and efficient. The Open Circuit Benchmark (OCB) dataset is a valuable resource for the community, providing a standardized platform for evaluating and comparing different design automation approaches. While the experimental results are promising, I would have liked to see more detailed comparisons with other state-of-the-art methods.\"\n\n**3-star review**\n\"CktGNN is an interesting approach to electronic design automation, but it's not without its limitations. The authors' use of a two-level GNN framework is novel, but it's unclear how well it will generalize to more complex circuit designs. The introduction of Open Circuit Benchmark (OCB) is a positive step, but the dataset is still relatively small and may not be representative of all analog circuit design scenarios. The experimental results are encouraging, but more thorough evaluations are needed to fully assess the effectiveness of CktGNN.\"\n\n**Related paper connections**\n\n* \"Graph Neural Networks for Analog Circuit Design Automation\" by [Author et al.]: This paper explores the use of graph neural networks for analog circuit design automation, which is closely related to the CktGNN approach. A review could discuss how CktGNN builds upon or differs from this work.\n* \"Analog Circuit Design Automation using Deep Learning\" by [Author et al.]: This paper presents a deep learning-based approach to analog circuit design automation, which could be compared and contrasted with the CktGNN method. A review could discuss the strengths and weaknesses of each approach.\n* \"Benchmarking Analog Circuit Design Automation Tools\" by [Author et al.]: This paper provides a comprehensive benchmarking of various analog circuit design automation tools, which could be",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"CktGNN is a game-changer in electronic design automation! The authors' innovative approach to circuit topology generation and device sizing using a two-level GNN framework has significantly improved efficiency and accuracy. The introduction of Open Circuit Benchmark (OCB) is a major milestone, providing a much-needed public dataset for reproducible research and canonical assessment. The experimental results are impressive, outperforming recent GNN baselines and manual design from human experts. This work has the potential to revolutionize the field of analog circuit design automation.\"\n\n**4-star review**\n\"I was impressed by the authors' recognition of the graph nature of circuits and their development of CktGNN, a Circuit Graph Neural Network that tackles both circuit topology generation and device sizing. The use of a nested GNN framework to encode circuit graphs is clever and efficient. The Open Circuit Benchmark (OCB) dataset is a valuable resource for the community, providing a standardized platform for evaluating and comparing different design automation approaches. While the experimental results are promising, I would have liked to see more comparisons with other state-of-the-art methods.\"\n\n**3-star review**\n\"CktGNN is an interesting approach to electronic design automation, but it's not without its limitations. The authors' use of a two-level GNN framework is novel, but it's unclear how well it will generalize to more complex circuits. The introduction of Open Circuit Benchmark (OCB) is a positive step, but the dataset is still relatively small and may not be representative of all analog circuits. The experimental results are encouraging, but more thorough evaluations are needed to fully assess the effectiveness of CktGNN. Overall, this is a promising direction, but more work is needed to fully realize its potential.\"\n\n**Related paper connections**\n\n* \"Graph Neural Networks for Analog Circuit Design Automation\" by [Author et al.]: This paper explores the use of graph neural networks for analog circuit design automation, which is closely related to the CktGNN approach. A review could discuss how CktGNN builds upon or differs from this work.\n* \"Analog Circuit Synthesis using Deep Learning\" by [Author et al.]: This paper presents a deep learning-based approach to analog circuit synthesis, which shares similarities with CktGNN's goal of automating circuit design. A review could compare and contrast the two approaches.\n* \"Benchmarking Analog Circuit Design Automation Tools\" by [Author et al.]: This paper discusses the need for",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"CktGNN is a game-changer in electronic design automation! The authors' innovative approach to circuit topology generation and device sizing using a two-level GNN framework has significantly improved efficiency and accuracy. The introduction of Open Circuit Benchmark (OCB) is a major milestone, providing a much-needed public dataset for reproducible research and canonical assessment. The experimental results are impressive, outperforming recent GNN baselines and manual design from human experts. This work has the potential to revolutionize the field of analog circuit design automation.\"\n\n**4-star review**\n\"I was impressed by the authors' recognition of the graph nature of circuits and their development of CktGNN, a Circuit Graph Neural Network that tackles both circuit topology generation and device sizing. The use of a nested GNN framework to encode circuit graphs is clever and efficient. The Open Circuit Benchmark (OCB) dataset is a valuable resource for the community, providing a standardized platform for evaluating and comparing different design automation approaches. While the experimental results are promising, I would have liked to see more comparisons with other state-of-the-art methods.\"\n\n**3-star review**\n\"CktGNN is an interesting approach to electronic design automation, but it's not without its limitations. The authors' use of a two-level GNN framework is novel, but it's unclear how well it will generalize to more complex circuit designs. The introduction of Open Circuit Benchmark (OCB) is a positive step, but the dataset is still relatively small and may not be representative of all analog circuit design challenges. The experimental results are encouraging, but more thorough evaluations are needed to fully assess the effectiveness of CktGNN.\"\n\n**Related paper connections**\n\n* \"Graph Neural Networks for Analog Circuit Design Automation\" by [Author et al.]: This paper explores the use of graph neural networks for analog circuit design automation, which is closely related to the CktGNN approach. A review could discuss how CktGNN builds upon or differs from this earlier work.\n* \"Analog Circuit Design Automation using Deep Learning\" by [Author et al.]: This paper presents a deep learning-based approach to analog circuit design automation, which could be compared and contrasted with the CktGNN method. A review could discuss the strengths and weaknesses of each approach.\n* \"Benchmarking Analog Circuit Design Automation Tools\" by [Author et al.]: This paper provides a comprehensive benchmarking of various analog circuit design automation tools, which could be used"
      ],
      "sim_rank": 17,
      "sim_decision": "None"
    },
    "Faster Hyperparameter Search for GNNs via Calibrated Dataset Condensation": {
      "paper_pk": "c2434b7a-5239-47e9-b632-956ce4120d28",
      "title": "Faster Hyperparameter Search for GNNs via Calibrated Dataset Condensation",
      "abstract": "Dataset condensation aims to reduce the computational cost of training multiple models on a large dataset by condensing the training dataset into a small synthetic set. State-of-the-art approaches rely on matching the model gradients for the real and synthetic data and have recently been applied to condense large-scale graphs for node classification tasks. Although dataset condensation may be efficient when training multiple models for hyperparameter optimization, there is no theoretical guarantee on the generalizability of the condensed data: data condensation often generalizes poorly across hyperparameters/architectures in practice, while we find and prove this overfitting is much more severe on graphs. In this paper, we consider a different condensation objective specifically geared towards hyperparameter search. We aim to generate the synthetic dataset so that the validation-performance rankings of the models, with different hyperparameters, on the condensed and original datasets are comparable. We propose a novel hyperparameter-calibrated dataset condensation algorithm, which obtains the synthetic validation data by matching the hyperparameter gradients computed via implicit differentiation and efficient inverse Hessian approximation. HCDC employs a supernet with differentiable hyperparameters, making it suitable for modeling GNNs with widely different convolution filters. Experiments demonstrate that the proposed framework effectively maintains the validation-performance rankings of GNNs and speeds up hyperparameter/architecture search on graphs.",
      "authors": [
        "Mucong Ding",
        "Xiaoyu Liu",
        "Tahseen Rabbani",
        "Teresa Ranadive",
        "Tai-Ching Tuan",
        "Furong Huang"
      ],
      "keywords": [
        "Graph Condensation",
        "Dataset Condensation",
        "Hyperparameter Optimization",
        "Graph Neural Networks",
        "Graph Compression"
      ],
      "real_avg_scores": 4.0,
      "real_all_scores": [
        5,
        3,
        5,
        3
      ],
      "real_confidences": [
        3,
        4,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 18,
      "real_decision": "Reject",
      "sim_avg_scores": 9.0,
      "sim_all_scores": [
        9,
        9,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles the crucial problem of dataset condensation for hyperparameter search in Graph Neural Networks (GNNs). The authors identify the limitations of existing approaches, which often fail to generalize across hyperparameters and architectures, and propose a novel hyperparameter-calibrated dataset condensation algorithm (HCDC). By matching hyperparameter gradients via implicit differentiation and efficient inverse Hessian approximation, HCDC generates synthetic datasets that maintain validation-performance rankings of GNNs. The experimental results demonstrate the effectiveness of HCDC in speeding up hyperparameter and architecture search on graphs. This work has significant implications for efficient GNN design and optimization.\"\n\n**Review 2:**\n\"The authors of this paper address a critical challenge in dataset condensation for GNNs: the lack of generalizability across hyperparameters and architectures. By formulating a new condensation objective that prioritizes hyperparameter search, they develop a calibrated dataset condensation algorithm (HCDC) that outperforms existing methods. The use of a supernet with differentiable hyperparameters enables HCDC to model GNNs with diverse convolution filters, making it a versatile tool for graph-based applications. The experimental results are convincing, and the proposed approach has the potential to significantly accelerate hyperparameter tuning and architecture search in GNNs.\"\n\n**Review 3:**\n\"This paper presents a significant advancement in dataset condensation for GNNs, particularly in the context of hyperparameter search. The authors' observation that existing approaches often overfit to specific hyperparameters and architectures is well-supported by their theoretical analysis and experimental results. The proposed HCDC algorithm addresses this limitation by generating synthetic datasets that preserve validation-performance rankings of GNNs across different hyperparameters. The use of implicit differentiation and efficient inverse Hessian approximation enables efficient computation of hyperparameter gradients, making HCDC a practical solution for large-scale graph-based applications.\"\n\n**Review 4:**\n\"The authors of this paper identify a critical gap in existing dataset condensation methods for GNNs, which often fail to generalize across hyperparameters and architectures. By formulating a novel condensation objective that prioritizes hyperparameter search, they develop a calibrated dataset condensation algorithm (HCDC) that effectively maintains validation-performance rankings of GNNs. The experimental results demonstrate the effectiveness of HCDC in speeding up hyperparameter and architecture search on graphs, making it a valuable contribution to the field of GNN optimization. The proposed approach has significant implications for efficient GNN design and optimization in various applications.\"\n\n",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles the crucial problem of dataset condensation for hyperparameter search in Graph Neural Networks (GNNs). The authors identify the limitations of existing approaches, which often fail to generalize across hyperparameters and architectures, especially on graph-structured data. The proposed Hyperparameter-Calibrated Dataset Condensation (HCDC) algorithm addresses this issue by matching hyperparameter gradients via implicit differentiation and efficient inverse Hessian approximation. The experiments demonstrate the effectiveness of HCDC in maintaining validation-performance rankings of GNNs and accelerating hyperparameter search on graphs. This work has significant implications for efficient GNN design and optimization.\"\n\n**Review 2:**\n\"The authors present a novel approach to dataset condensation, specifically designed for hyperparameter search in GNNs. By calibrating the condensed dataset to the hyperparameter space, HCDC ensures that the validation-performance rankings of models with different hyperparameters are preserved. This is a significant improvement over existing methods, which often suffer from poor generalizability. The use of a supernet with differentiable hyperparameters is a clever idea, allowing HCDC to model GNNs with diverse convolution filters. The experimental results are convincing, and I believe this work will have a substantial impact on the field of GNN optimization.\"\n\n**Review 3:**\n\"While dataset condensation has shown promise in reducing the computational cost of training multiple models, its application to GNNs has been limited by the lack of theoretical guarantees on generalizability. This paper addresses this gap by proposing a hyperparameter-calibrated dataset condensation algorithm, HCDC. By matching hyperparameter gradients, HCDC ensures that the condensed dataset is representative of the original data across different hyperparameters and architectures. The experiments demonstrate the effectiveness of HCDC in accelerating hyperparameter search on graphs, making it a valuable contribution to the field of GNN optimization.\"\n\n**Review 4:**\n\"The authors identify a critical limitation of existing dataset condensation approaches, which is their poor generalizability across hyperparameters and architectures, particularly on graph-structured data. To address this, they propose HCDC, a novel algorithm that calibrates the condensed dataset to the hyperparameter space. The use of implicit differentiation and efficient inverse Hessian approximation is a clever idea, allowing HCDC to capture the complex relationships between hyperparameters and model performance. The experimental results are impressive, and I believe this work will have a significant impact on the development of efficient GNN optimization methods.\"\n\n**Review 5:",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles the crucial problem of dataset condensation for hyperparameter search in Graph Neural Networks (GNNs). The authors identify the limitations of existing approaches, which often fail to generalize across hyperparameters and architectures. By proposing a novel hyperparameter-calibrated dataset condensation algorithm, HCDC, they demonstrate significant improvements in maintaining validation-performance rankings of GNNs. The use of implicit differentiation and efficient inverse Hessian approximation is a clever innovation. Overall, this work has the potential to accelerate hyperparameter search in GNNs and deserves further exploration.\"\n\n**Review 2:**\n\"The authors of this paper take a fresh look at dataset condensation, recognizing that existing methods may not be suitable for hyperparameter search in GNNs. By formulating a new condensation objective that prioritizes comparable validation-performance rankings across different hyperparameters, they develop a more effective approach. The proposed HCDC algorithm is well-designed, leveraging a supernet with differentiable hyperparameters to model GNNs with diverse convolution filters. The experimental results are convincing, and this work could have a significant impact on the efficiency of hyperparameter search in GNNs.\"\n\n**Review 3:**\n\"While dataset condensation has shown promise in reducing computational costs, its application to hyperparameter search in GNNs has been limited by poor generalizability. This paper addresses this challenge by introducing a hyperparameter-calibrated dataset condensation algorithm, HCDC. The authors' approach is theoretically grounded and empirically validated, demonstrating improved maintenance of validation-performance rankings across different hyperparameters. The use of implicit differentiation and inverse Hessian approximation is a notable contribution. Overall, this work is a valuable addition to the field of GNN hyperparameter search and warrants further investigation.\"\n\n**Review 4:**\n\"The problem of dataset condensation for hyperparameter search in GNNs is a pressing one, and this paper offers a promising solution. By recognizing the limitations of existing approaches and formulating a new condensation objective, the authors develop a more effective algorithm, HCDC. The experimental results are impressive, and the proposed method has the potential to significantly accelerate hyperparameter search in GNNs. However, further exploration is needed to fully understand the theoretical guarantees and limitations of this approach. Nonetheless, this work is a significant step forward in the field.\"\n\n**Review 5:**\n\"This paper presents a timely contribution to the field of GNN hyperparameter search, addressing the critical issue of dataset condensation. The"
      ],
      "sim_rank": 1,
      "sim_decision": "None"
    },
    "3D-IntPhys: Learning 3D Visual Intuitive Physics for Fluids, Rigid Bodies, and Granular Materials": {
      "paper_pk": "f528fc59-ee56-43ba-aeb7-bb689f3b00b2",
      "title": "3D-IntPhys: Learning 3D Visual Intuitive Physics for Fluids, Rigid Bodies, and Granular Materials",
      "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models purely from unlabeled images. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, in which we impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We evaluate the models on three challenging scenarios involving fluid, granular materials, and rigid objects, where standard detection and tracking methods are not applicable. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that, once trained, our model can achieve strong generalization in complex scenarios under extrapolate settings.",
      "authors": [
        "Haotian Xue",
        "Antonio Torralba",
        "Daniel LK Yamins",
        "Joshua B. Tenenbaum",
        "Yunzhu Li",
        "Hsiao-Yu Tung"
      ],
      "keywords": [
        "Visual Intuitive Physics",
        "Neural Implicit Representations",
        "Graph Neural Networks",
        "Learning-Based Dynamics Modeling",
        "Particle-Based Dynamics"
      ],
      "real_avg_scores": 5.333333333333333,
      "real_all_scores": [
        5,
        6,
        5
      ],
      "real_confidences": [
        4,
        3,
        5
      ],
      "real_contents": [],
      "real_rank": 13,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper presents a groundbreaking framework for learning 3D visual intuitive physics models from unlabeled images. The authors' approach, which combines a conditional NeRF-style visual frontend with a 3D point-based dynamics prediction backend, is a significant departure from existing methods that rely on dense point trajectory supervision from simulators. The results are impressive, with the model demonstrating strong performance in challenging scenarios involving fluids, granular materials, and rigid objects. The ability to make long-horizon future predictions from raw images and generalize to complex scenarios under extrapolate settings is a major achievement. This work has the potential to revolutionize the field of intuitive physics and has far-reaching implications for robotics, computer vision, and machine learning.\"\n\n**Review 2:**\n\"The authors of this paper tackle the challenging problem of learning 3D visual intuitive physics models from raw images. By imposing strong relational and structural inductive bias on the model, they are able to capture the underlying structure of the environment and make accurate predictions about future states. The use of a 3D point-based dynamics prediction backend is a key innovation, allowing the model to handle scenarios where accurate point estimation and tracking are difficult or impossible. The results are impressive, with the model outperforming baselines that do not employ an explicit 3D representation space. This work is a significant contribution to the field of intuitive physics and has the potential to enable more effective planning and decision-making in complex environments.\"\n\n**Review 3:**\n\"This paper presents a novel framework for learning 3D visual intuitive physics models from multi-view RGB images and instance masks. The authors' approach is notable for its ability to handle scenarios where standard detection and tracking methods are not applicable. The use of a conditional NeRF-style visual frontend and a 3D point-based dynamics prediction backend allows the model to capture the underlying structure of the environment and make accurate predictions about future states. The results are promising, with the model demonstrating strong performance in challenging scenarios involving fluids, granular materials, and rigid objects. However, further work is needed to fully explore the capabilities and limitations of this approach. Nevertheless, this work is an important step forward in the development of intuitive physics models that can operate in complex, real-world environments.\"\n\n**Review 4:**\n\"The authors of this paper propose a framework for learning 3D visual intuitive physics models from raw images, which is a critical ability for effective planning and decision-making in complex environments. The use",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper presents a groundbreaking framework for learning 3D visual intuitive physics models from unlabeled images. The authors' approach, which combines a conditional NeRF-style visual frontend with a 3D point-based dynamics prediction backend, is a significant departure from existing methods that rely on dense point trajectory supervision from simulators. The results are impressive, with the model demonstrating strong performance in challenging scenarios involving fluids, granular materials, and rigid objects. The ability to make long-horizon future predictions from raw images and generalize to complex scenarios under extrapolate settings is a major achievement. This work has the potential to revolutionize the field of intuitive physics and has far-reaching implications for robotics, computer vision, and machine learning.\"\n\n**Review 2:**\n\"The authors of this paper tackle the challenging problem of learning 3D visual intuitive physics models from raw images. By imposing strong relational and structural inductive bias on the model, they are able to capture the underlying structure of the environment and make accurate predictions about future states. The use of a 3D point-based dynamics prediction backend is a key innovation, allowing the model to handle scenarios where accurate point estimation and tracking are difficult or impossible. The results are impressive, with the model outperforming baselines that do not employ an explicit 3D representation space. This work is a significant contribution to the field of intuitive physics and has the potential to enable more effective planning and decision-making in complex environments.\"\n\n**Review 3:**\n\"This paper presents a novel framework for learning 3D visual intuitive physics models from multi-view RGB images and instance masks. The authors' approach is notable for its ability to handle scenarios where standard detection and tracking methods are not applicable. The use of a conditional NeRF-style visual frontend and a 3D point-based dynamics prediction backend allows the model to capture the underlying structure of the environment and make accurate predictions about future states. The results are promising, with the model demonstrating strong performance in challenging scenarios involving fluids, granular materials, and rigid objects. However, further evaluation is needed to fully understand the limitations and potential applications of this approach.\"\n\n**Review 4:**\n\"The authors of this paper propose a framework for learning 3D visual intuitive physics models that is capable of handling complex scenarios involving fluids, granular materials, and rigid objects. The use of a 3D point-based dynamics prediction backend is a key innovation, allowing the model to capture the underlying structure of the environment",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper presents a groundbreaking framework for learning 3D visual intuitive physics models from unlabeled images. The authors' approach, which combines a conditional NeRF-style visual frontend with a 3D point-based dynamics prediction backend, is a significant departure from existing methods that rely on dense point trajectory supervision from simulators. The results are impressive, with the model demonstrating strong performance in challenging scenarios involving fluids, granular materials, and rigid objects. The ability to make long-horizon future predictions from raw images and generalize to complex scenarios under extrapolate settings is a major achievement. This work has the potential to revolutionize the field of intuitive physics and has far-reaching implications for robotics, computer vision, and machine learning.\"\n\n**Review 2:**\n\"The authors of this paper tackle the challenging problem of learning 3D visual intuitive physics models from raw images. By imposing strong relational and structural inductive bias on the model, they are able to capture the underlying structure of the environment and make accurate predictions about future states. The use of a 3D point-based dynamics prediction backend is a key innovation, allowing the model to handle scenarios where accurate point estimation and tracking are difficult or impossible. The results are impressive, with the model outperforming baselines that do not employ an explicit 3D representation space. This work is a significant contribution to the field of intuitive physics and has the potential to enable more effective planning and decision-making in complex environments.\"\n\n**Review 3:**\n\"This paper presents a novel framework for learning 3D visual intuitive physics models from multi-view RGB images and instance masks. The authors' approach is notable for its ability to handle scenarios where standard detection and tracking methods are not applicable. The use of a conditional NeRF-style visual frontend and a 3D point-based dynamics prediction backend allows the model to capture the underlying structure of the environment and make accurate predictions about future states. The results are promising, with the model demonstrating strong performance in challenging scenarios involving fluids, granular materials, and rigid objects. However, further work is needed to fully explore the capabilities and limitations of this approach. Nevertheless, this work is an important step forward in the development of intuitive physics models that can operate in complex, real-world environments.\"\n\n**Review 4:**\n\"The authors of this paper propose a framework for learning 3D visual intuitive physics models that is capable of handling complex scenarios involving fluids, granular materials, and rigid objects. The use of"
      ],
      "sim_rank": 18,
      "sim_decision": "None"
    },
    "Graph Neural Networks as Multi-View Learning": {
      "paper_pk": "a1673522-6811-4bfe-a061-e01f966d675d",
      "title": "Graph Neural Networks as Multi-View Learning",
      "abstract": "Graph Neural Networks (GNNs) have demonstrated powerful representation capability in semi-supervised node classification. In this task, there are often three types of information  -- graph structure, node features, and node labels. Existing GNNs usually leverage both node features and graph structure by feature transformation and aggregation, following end-to-end training via node labels. In this paper, we change our perspective by considering these three types of information as three views of nodes. This perspective motivates us to design a new GNN framework as multi-view learning which enables alternating optimization training instead of end-to-end training,  resulting in significantly improved computation and memory efficiency. Extensive experiments with different settings demonstrate the effectiveness and efficiency of the proposed method. \n",
      "authors": [
        "Haoyu Han",
        "Xiaorui Liu",
        "Haitao Mao",
        "MohamadAli Torkamani",
        "Feng Shi",
        "Victor Lee",
        "Jiliang Tang"
      ],
      "keywords": [
        "Graph Neural Networks",
        "Alternating Optimization",
        "Semi-Supervised Learning",
        "Multi-View Learning"
      ],
      "real_avg_scores": 6.0,
      "real_all_scores": [
        8,
        3,
        5,
        8
      ],
      "real_confidences": [
        3,
        4,
        3,
        4
      ],
      "real_contents": [],
      "real_rank": 7,
      "real_decision": "Reject",
      "sim_avg_scores": 0.3333333333333333,
      "sim_all_scores": [
        0,
        1,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper offers a fresh perspective on Graph Neural Networks (GNNs) by reframing the node classification task as a multi-view learning problem. By considering graph structure, node features, and node labels as three views of nodes, the authors propose a novel GNN framework that enables alternating optimization training. This approach leads to significant improvements in computation and memory efficiency, making it a promising solution for large-scale graph-based problems. The experimental results demonstrate the effectiveness of the proposed method, and I believe this work has the potential to inspire new research directions in the field of graph neural networks.\"\n\n**Review 2:**\n\"I was impressed by the authors' creative approach to GNNs, which leverages the concept of multi-view learning to improve the efficiency of node classification. By decoupling the feature transformation and aggregation steps, the proposed framework achieves better computation and memory efficiency compared to traditional end-to-end training methods. The experimental results are convincing, and I appreciate the authors' efforts to provide a thorough evaluation of their method. One potential direction for future work is to explore the application of this framework to other graph-based tasks, such as graph classification and clustering.\"\n\n**Review 3:**\n\"This paper presents an innovative solution to the node classification problem in graph neural networks. By treating graph structure, node features, and node labels as three views of nodes, the authors develop a multi-view learning framework that enables alternating optimization training. The proposed method is not only more efficient but also achieves competitive performance compared to state-of-the-art GNN models. I particularly appreciate the authors' discussion on the connections between their work and existing research in multi-view learning and graph neural networks. This paper is a valuable contribution to the field, and I expect it to have a significant impact on future research in graph-based machine learning.\"\n\n**Review 4:**\n\"I found this paper to be a thought-provoking contribution to the field of graph neural networks. The authors' idea of reframing node classification as a multi-view learning problem is both intuitive and powerful. The proposed framework is well-motivated, and the experimental results demonstrate its effectiveness in improving computation and memory efficiency. One potential area for improvement is to provide more theoretical analysis of the proposed method, particularly in terms of its convergence properties and robustness to different graph structures. Nevertheless, this paper is a valuable addition to the literature, and I expect it to inspire further research in this direction.\"\n\n**Review 5:**\n\"This paper",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper offers a fresh perspective on Graph Neural Networks (GNNs) by reframing the node classification problem as a multi-view learning task. By considering graph structure, node features, and node labels as three distinct views, the authors propose a novel GNN framework that leverages alternating optimization training. The results are impressive, with significant improvements in computation and memory efficiency. This work has the potential to open up new avenues for efficient GNN design and deployment.\"\n\n**Review 2:**\n\"The authors of this paper make a compelling case for rethinking the way we approach Graph Neural Networks. By recognizing the inherent multi-view nature of node classification, they develop a framework that can efficiently integrate graph structure, node features, and node labels. The experimental results are convincing, and the proposed method's ability to reduce computational overhead is a significant advantage. This work is a valuable contribution to the field of graph learning and has important implications for real-world applications.\"\n\n**Review 3:**\n\"This paper presents a clever reinterpretation of Graph Neural Networks as multi-view learning models. By treating graph structure, node features, and node labels as separate views, the authors are able to design a more efficient and effective GNN framework. The use of alternating optimization training is a key innovation, allowing for faster and more memory-efficient training. The experimental results demonstrate the power of this approach, and I expect this work to have a significant impact on the development of GNNs for node classification tasks.\"\n\n**Review 4:**\n\"The authors of this paper take a step back and reexamine the fundamental assumptions underlying Graph Neural Networks. By recognizing the multi-view nature of node classification, they develop a novel framework that can better leverage the strengths of each view. The resulting model is not only more efficient but also more effective, as demonstrated by the extensive experimental results. This work is a testament to the power of rethinking established approaches and has the potential to inspire new innovations in the field of graph learning.\"\n\n**Review 5:**\n\"This paper offers a timely and important contribution to the field of Graph Neural Networks. By reframing node classification as a multi-view learning problem, the authors are able to develop a more efficient and effective GNN framework. The use of alternating optimization training is a key innovation, and the experimental results demonstrate the significant advantages of this approach. I expect this work to have a lasting impact on the development of GNNs and to inspire further research in this area.\"",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper offers a fresh perspective on Graph Neural Networks (GNNs) by reframing the node classification problem as a multi-view learning task. By considering graph structure, node features, and node labels as three distinct views, the authors propose a novel GNN framework that leverages alternating optimization training. The results are impressive, with significant improvements in computation and memory efficiency. This work has the potential to open up new avenues for efficient GNN design and deployment.\"\n\n**Review 2:**\n\"The authors of this paper make a compelling case for rethinking the way we approach Graph Neural Networks. By recognizing the inherent multi-view nature of node classification, they develop a framework that can efficiently integrate graph structure, node features, and node labels. The experimental results are convincing, and the proposed method's ability to reduce computational overhead is a significant advantage. This work is a valuable contribution to the field of graph learning and has important implications for real-world applications.\"\n\n**Review 3:**\n\"This paper presents a clever reinterpretation of Graph Neural Networks as multi-view learning models. By treating graph structure, node features, and node labels as separate views, the authors are able to design a more efficient and effective GNN framework. The use of alternating optimization training is a key innovation, allowing for faster and more memory-efficient training. The experimental results demonstrate the power of this approach, and I expect this work to have a significant impact on the development of GNNs for node classification tasks.\"\n\n**Review 4:**\n\"I was impressed by the authors' ability to identify the limitations of traditional GNN approaches and propose a novel solution that addresses these shortcomings. By framing node classification as a multi-view learning problem, they are able to develop a more efficient and scalable GNN framework. The experimental results are thorough and convincing, and I believe this work has the potential to influence the direction of future GNN research. The authors' contribution is a significant one, and I look forward to seeing how their approach is applied in real-world settings.\"\n\n**Review 5:**\n\"This paper offers a timely and important contribution to the field of graph learning. By recognizing the multi-view nature of node classification, the authors are able to develop a more efficient and effective GNN framework. The use of alternating optimization training is a key innovation, and the experimental results demonstrate the power of this approach. I expect this work to have a significant impact on the development of GNNs for node classification tasks, and"
      ],
      "sim_rank": 7,
      "sim_decision": "None"
    },
    "Interpretable Geometric Deep Learning via Learnable Randomness Injection": {
      "paper_pk": "192c7300-bf91-4f68-9aa0-428a63584770",
      "title": "Interpretable Geometric Deep Learning via Learnable Randomness Injection",
      "abstract": "Point cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at https://github.com/Graph-COM/LRI.",
      "authors": [
        "Siqi Miao",
        "Yunan Luo",
        "Mia Liu",
        "Pan Li"
      ],
      "keywords": [
        "Geometric Deep Learning",
        "Interpretation",
        "Graph Neural Networks"
      ],
      "real_avg_scores": 7.0,
      "real_all_scores": [
        6,
        8,
        6,
        8
      ],
      "real_confidences": [
        5,
        4,
        3,
        5
      ],
      "real_contents": [],
      "real_rank": 3,
      "real_decision": "Accept: poster",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper presents a significant breakthrough in interpretable geometric deep learning (GDL) for point cloud data. The proposed learnable randomness injection (LRI) mechanism allows for building inherently interpretable models that can detect informative points in the data, aligning well with ground-truth patterns. The authors' evaluation on four real-world datasets from high-energy physics and biochemistry demonstrates the effectiveness of LRI over previous post-hoc interpretation methods. The information bottleneck principle grounding LRI also ensures robustness to distribution shifts. A must-read for scientists and researchers working with point cloud data.\"\n\n**Review 2:**\n\"I was impressed by the authors' innovative approach to interpretable GDL using learnable randomness injection. By injecting randomness into the model, they enable the detection of informative points in point cloud data, providing valuable insights for scientific analysis and experiments. The proposed mechanism is generalizable to various GDL backbones, making it a versatile tool for the community. The evaluation on real-world datasets and the availability of code and datasets on GitHub further strengthen the paper's impact. A great contribution to the field of interpretable machine learning.\"\n\n**Review 3:**\n\"This paper tackles a crucial challenge in geometric deep learning: interpretability. The authors' LRI mechanism offers a promising solution, allowing models to identify informative points in point cloud data that are indicative of the prediction label. The evaluation results are convincing, showing that LRI-induced models outperform previous post-hoc interpretation methods in terms of alignment with ground-truth patterns. The paper's focus on scientific applications and the provision of datasets from high-energy physics and biochemistry make it a valuable resource for researchers in these domains. Overall, a well-written and impactful paper.\"\n\n**Review 4:**\n\"I appreciate the authors' effort to develop an interpretable GDL framework that can be applied to various scientific domains. The learnable randomness injection mechanism is an interesting approach that enables the detection of informative points in point cloud data. The paper's evaluation on four real-world datasets demonstrates the effectiveness of LRI, and the authors' discussion of the information bottleneck principle provides a solid theoretical foundation for their approach. While some technical aspects of the paper may require further clarification, the overall contribution is significant, and I expect this work to inspire further research in interpretable machine learning.\"\n\nLet me know if you'd like me to generate more reviews or modify these ones in any way!",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper presents a significant breakthrough in interpretable geometric deep learning (GDL) for point cloud data. The proposed learnable randomness injection (LRI) mechanism allows for building inherently interpretable models that can detect informative points in the data, aligning well with ground-truth patterns. The authors' evaluation on four real-world datasets from high-energy physics and biochemistry demonstrates the effectiveness of LRI in improving model interpretability and robustness. The connection to the information bottleneck principle provides a solid theoretical foundation for LRI. Overall, this work has the potential to revolutionize the field of GDL by providing scientists with more trustworthy and explainable models.\"\n\n**Review 2:**\n\"I was impressed by the authors' innovative approach to interpretable GDL using learnable randomness injection. By injecting randomness into the model, they are able to identify the most informative points in the point cloud data, which is crucial for scientific applications. The experiments on real-world datasets show that LRI outperforms previous post-hoc interpretation methods, providing more stable and meaningful results. The availability of the code and datasets on GitHub is also a significant plus. One potential area for improvement is to explore the applicability of LRI to other types of data beyond point clouds. Nevertheless, this paper is a must-read for anyone working in GDL or interpretable machine learning.\"\n\n**Review 3:**\n\"The authors tackle the critical issue of interpretability in geometric deep learning models, which is essential for building trust in scientific applications. The proposed LRI mechanism is well-motivated and grounded in the information bottleneck principle. The experimental results on four datasets demonstrate the effectiveness of LRI in improving model interpretability and robustness. I particularly appreciate the authors' effort to provide a comprehensive evaluation of LRI, including comparisons with previous methods and ablation studies. One potential direction for future work is to explore the integration of LRI with other interpretability techniques, such as feature importance or attention mechanisms. Overall, this paper is a significant contribution to the field of interpretable GDL.\"\n\n**Review 4:**\n\"This paper presents a novel approach to interpretable geometric deep learning using learnable randomness injection. The authors' idea of injecting randomness into the model to identify informative points is intriguing and well-executed. The evaluation on real-world datasets shows that LRI can provide more accurate and stable results compared to previous methods. I appreciate the authors' attention to detail in providing a thorough analysis of the",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper presents a significant breakthrough in interpretable geometric deep learning (GDL) for point cloud data. The proposed learnable randomness injection (LRI) mechanism is a game-changer for scientists who require transparent and trustworthy models for their analysis and experiments. By injecting learnable randomness into GDL backbones, the authors demonstrate improved interpretability and robustness to distribution shifts. The evaluation on four real-world datasets from high-energy physics and biochemistry showcases the effectiveness of LRI in detecting informative points in point cloud data. A must-read for anyone working in GDL and scientific applications.\"\n\n**Review 2:**\n\"I was impressed by the authors' ability to tackle the long-standing issue of interpretability in geometric deep learning. The LRI mechanism is elegantly simple yet powerful, allowing for the detection of informative points in point cloud data that align with ground-truth patterns. The information bottleneck principle provides a solid theoretical foundation for LRI, and the experimental results demonstrate its superiority over post-hoc interpretation methods. The availability of code and datasets will facilitate further research and adoption in the scientific community. Overall, a well-written and impactful paper that advances the field of GDL.\"\n\n**Review 3:**\n\"This work addresses a critical challenge in geometric deep learning: building models that are both accurate and interpretable. The proposed LRI mechanism is a step in the right direction, enabling the development of inherently interpretable models that can identify informative points in point cloud data. The evaluation on diverse datasets from high-energy physics and biochemistry demonstrates the versatility and effectiveness of LRI. While some may argue that the approach is not universally applicable, the authors provide a compelling case for the importance of interpretability in scientific applications. A valuable contribution to the field of GDL and its applications.\"\n\n**Review 4:**\n\"I appreciate the authors' effort to bridge the gap between geometric deep learning and scientific applications. The LRI mechanism is an innovative solution to the interpretability problem, and the experimental results are promising. However, I would have liked to see a more in-depth analysis of the limitations and potential pitfalls of LRI. Additionally, the connection to the information bottleneck principle could be further explored. Nevertheless, this paper is a significant step forward in making GDL more accessible and trustworthy for scientists. The open-sourced code and datasets will facilitate further research and development in this area.\"\n\n**Review 5:**\n\"This paper presents a timely and important contribution to the field of geometric"
      ],
      "sim_rank": 19,
      "sim_decision": "None"
    },
    "Fair Graph Message Passing with Transparency": {
      "paper_pk": "de1b1721-945d-423f-b819-20581eb6584e",
      "title": "Fair Graph Message Passing with Transparency",
      "abstract": "Recent advanced works achieve fair representations and predictions through regularization, adversarial debiasing, and contrastive learning in graph neural networks (GNNs). These methods \\textit{implicitly} encode the sensitive attribute information in the well-trained model weight via \\textit{backward propagation}. In practice, we not only pursue a fair machine learning model but also lend such fairness perception to the public. For current fairness methods,\nhow the sensitive attribute information usage makes the model achieve fair prediction still remains a black box. In this work, we first propose the concept \\textit{transparency} to describe \\textit{whether} the model embraces the ability of lending fairness perception to the public \\textit{or not}. Motivated by the fact that current fairness models lack of transparency, we aim to pursue a fair machine learning model with transparency via \\textit{explicitly} rendering sensitive attribute usage for fair prediction in \\textit{forward propagation} . Specifically, we develop an effective and transparent \\textsf{F}air \\textsf{M}essage \\textsf{P}assing (FMP) scheme adopting sensitive attribute information in forward propagation. In this way, FMP explicitly uncovers how sensitive attributes influence final prediction. Additionally, FMP scheme can aggregate useful information from neighbors and mitigate bias in a unified framework to simultaneously achieve graph smoothness and fairness objectives. An acceleration approach is also adopted to improve the efficiency of FMP. Experiments on node classification tasks demonstrate that the proposed FMP outperforms the state-of-the-art baselines in terms of fairness and accuracy on three real-world datasets. The code is available in {\\color{blue}\\url{https://anonymous.4open.science/r/FMP-AD84}}.",
      "authors": [
        "Zhimeng Jiang",
        "Xiaotian Han",
        "Chao Fan",
        "Zirui Liu",
        "Na Zou",
        "Ali Mostafavi",
        "Xia Hu"
      ],
      "keywords": [
        "Fairness",
        "Transparency",
        "Graph Neural Networks"
      ],
      "real_avg_scores": 7.0,
      "real_all_scores": [
        8,
        8,
        6,
        6
      ],
      "real_confidences": [
        4,
        4,
        4,
        4
      ],
      "real_contents": [],
      "real_rank": 4,
      "real_decision": "Accept: notable-top-5%",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\nTitle: A Step Towards Transparent Fairness in Graph Neural Networks\nRating: 4.5/5\n\nThis paper proposes a novel approach to achieving fairness in graph neural networks (GNNs) by introducing the concept of transparency. The authors argue that current fairness methods lack transparency, making it difficult to understand how sensitive attribute information is used to achieve fair predictions. The proposed Fair Message Passing (FMP) scheme addresses this issue by explicitly rendering sensitive attribute usage in forward propagation. The experiments demonstrate the effectiveness of FMP in achieving fairness and accuracy on three real-world datasets. The code availability is a plus. However, the paper could benefit from a more in-depth analysis of the trade-offs between fairness and accuracy.\n\n**Review 2:**\nTitle: Fairness Meets Transparency in Graph Neural Networks\nRating: 4.8/5\n\nThis paper is a significant contribution to the field of fair machine learning, particularly in the context of graph neural networks. The authors' proposal of the FMP scheme is a game-changer, as it provides a transparent and interpretable way to achieve fairness in GNNs. The idea of explicitly rendering sensitive attribute usage in forward propagation is innovative and well-executed. The experiments are thorough, and the results are impressive. The paper is well-written, and the authors provide a clear motivation for their work. The only suggestion is to explore the applicability of FMP to other fairness metrics and datasets.\n\n**Review 3:**\nTitle: A New Frontier in Fair Graph Neural Networks: Transparency and Interpretability\nRating: 4.2/5\n\nThis paper tackles an important issue in fair machine learning, namely the lack of transparency in current fairness methods. The authors propose a novel approach, FMP, which explicitly incorporates sensitive attribute information in forward propagation. The idea is promising, and the experiments demonstrate the effectiveness of FMP in achieving fairness and accuracy. However, the paper could benefit from a more detailed analysis of the theoretical foundations of FMP and its connections to other fairness methods. Additionally, the authors could explore the potential applications of FMP in other domains, such as computer vision or natural language processing.\n\n**Review 4:**\nTitle: Fairness and Transparency in Graph Neural Networks: A Promising Approach\nRating: 4.0/5\n\nThis paper proposes an interesting approach to achieving fairness in graph neural networks by introducing transparency. The authors argue that current fairness methods lack transparency, and their",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\nTitle: A Step Towards Transparent Fairness in Graph Neural Networks\nRating: 4.5/5\n\nThis paper proposes a novel approach to achieving fairness in graph neural networks (GNNs) by introducing the concept of transparency. The authors argue that current fairness methods lack transparency, making it difficult to understand how sensitive attribute information is used to achieve fair predictions. The proposed Fair Message Passing (FMP) scheme addresses this issue by explicitly rendering sensitive attribute usage in forward propagation. The experiments demonstrate the effectiveness of FMP in achieving fairness and accuracy on three real-world datasets. The code availability is a plus. However, the paper could benefit from a more in-depth analysis of the trade-offs between fairness and accuracy.\n\n**Review 2:**\nTitle: Fairness Meets Transparency in Graph Neural Networks\nRating: 4.8/5\n\nThis paper is a significant contribution to the field of fair machine learning, particularly in the context of graph neural networks. The authors' proposal of the FMP scheme is a game-changer, as it provides a transparent and interpretable way to achieve fairness in GNNs. The idea of explicitly rendering sensitive attribute usage in forward propagation is innovative and well-executed. The experiments are thorough, and the results are impressive. The paper is well-written, and the authors provide a clear motivation for their work. The only suggestion is to explore the applicability of FMP to other fairness metrics and datasets.\n\n**Review 3:**\nTitle: A New Frontier in Fair Graph Neural Networks: Transparency and Interpretability\nRating: 4.2/5\n\nThis paper tackles an important issue in fair machine learning, namely the lack of transparency in current fairness methods. The authors propose a novel approach, FMP, which explicitly incorporates sensitive attribute information in forward propagation. The idea is promising, and the experiments demonstrate the effectiveness of FMP in achieving fairness and accuracy. However, the paper could benefit from a more detailed analysis of the theoretical foundations of FMP and its connections to other fairness methods. Additionally, the authors could explore the potential applications of FMP in other domains, such as computer vision or natural language processing.\n\n**Review 4:**\nTitle: Fairness and Transparency in Graph Neural Networks: A Promising Approach\nRating: 4.0/5\n\nThis paper proposes an interesting approach to achieving fairness in graph neural networks by introducing transparency. The authors argue that current fairness methods lack transparency, and their",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\nTitle: A Step Towards Transparent Fairness in Graph Neural Networks\nRating: 4.5/5\n\nThis paper proposes a novel approach to achieving fairness in graph neural networks (GNNs) by introducing the concept of transparency. The authors argue that current fairness methods lack transparency, making it difficult to understand how sensitive attribute information is used to achieve fair predictions. The proposed Fair Message Passing (FMP) scheme addresses this issue by explicitly rendering sensitive attribute usage in forward propagation. The experiments demonstrate the effectiveness of FMP in achieving fairness and accuracy on three real-world datasets. The code availability is a plus. However, the paper could benefit from a more in-depth analysis of the trade-offs between fairness and accuracy.\n\n**Review 2:**\nTitle: Fairness Meets Transparency in Graph Neural Networks\nRating: 4.8/5\n\nThis paper is a significant contribution to the field of fair machine learning, particularly in the context of graph neural networks. The authors' proposal of the FMP scheme is a game-changer, as it provides a transparent and interpretable way to achieve fairness in GNNs. The idea of explicitly rendering sensitive attribute usage in forward propagation is innovative and well-executed. The experiments are thorough, and the results are impressive. The paper is well-written, and the authors provide a clear motivation for their work. The only suggestion is to explore the applicability of FMP to other fairness metrics and datasets.\n\n**Review 3:**\nTitle: A New Frontier in Fair Graph Neural Networks: Transparency and Interpretability\nRating: 4.2/5\n\nThis paper tackles an important problem in fair machine learning: the lack of transparency in current fairness methods. The authors propose a novel approach, FMP, which explicitly incorporates sensitive attribute information in forward propagation. The idea is promising, and the experiments demonstrate the effectiveness of FMP in achieving fairness and accuracy. However, the paper could benefit from a more detailed analysis of the theoretical foundations of FMP and its connections to other fairness methods. Additionally, the authors could explore the potential applications of FMP in other domains, such as computer vision or natural language processing.\n\n**Review 4:**\nTitle: Fairness and Transparency in Graph Neural Networks: A Promising Approach\nRating: 4.0/5\n\nThis paper proposes an interesting approach to achieving fairness in graph neural networks by introducing transparency. The FMP scheme is a step in the right direction, as"
      ],
      "sim_rank": 20,
      "sim_decision": "None"
    }
  }
}