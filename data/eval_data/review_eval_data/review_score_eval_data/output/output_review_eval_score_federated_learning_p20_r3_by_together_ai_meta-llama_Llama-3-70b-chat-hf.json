{
  "absolute_rank_consistency": 4.7,
  "spearman_rank_consistency": 0.3533834586466165,
  "kendall_rank_consistency": 0.2947368421052632,
  "sim_ranks": [
    14,
    11,
    3,
    13,
    15,
    16,
    9,
    17,
    4,
    18,
    19,
    10,
    1,
    5,
    2,
    7,
    12,
    6,
    20,
    8
  ],
  "real_ranks": [
    10,
    12,
    5,
    14,
    11,
    18,
    7,
    2,
    6,
    17,
    13,
    3,
    19,
    8,
    4,
    15,
    9,
    1,
    20,
    16
  ],
  "sim_avg_score_variance": 9.236388888888888,
  "real_avg_score_variance": 2.291579861111111,
  "papers": {
    "Towards scalable and non-IID robust Hierarchical Federated Learning via Label-driven Knowledge Aggregator": {
      "paper_pk": "1af82ccc-c318-4699-aaf7-8495afa02cb7",
      "title": "Towards scalable and non-IID robust Hierarchical Federated Learning via Label-driven Knowledge Aggregator",
      "abstract": "In real-world applications, Federated Learning (FL) meets two challenges: (1) scalability, especially when applied to massive IoT networks, and (2) how to be robust against an environment with heterogeneous data. Realizing the first problem, we aim to design a novel FL framework named Full-stack FL (F2L). More specifically, F2L utilizes a hierarchical network architecture, making extending the FL network accessible without reconstructing the whole network system. Moreover, leveraging the advantages of hierarchical network design, we propose a new label-driven knowledge distillation (LKD) technique at the global server to address the second problem. As opposed to current knowledge distillation techniques, LKD is capable of training a student model, which consists of good knowledge from all teachers' models. Therefore, our proposed algorithm can effectively extract the knowledge of the regions' data distribution (i.e., the regional aggregated models) to reduce the divergence between clients' models when operating under the FL system with non-independent identically distributed data. Extensive experiment results reveal that: (i) our F2L method can significantly improve the overall FL efficiency in all global distillations, and (ii) F2L rapidly achieves convergence as global distillation stages occur instead of increasing on each communication cycle.",
      "authors": [
        "Duong Minh Nguyen",
        "Viet Quoc Pham",
        "Hoang Thai Dinh",
        "Diep Nguyen",
        "Long Tran-Thanh",
        "Won-Joo Hwang"
      ],
      "keywords": [
        "Federated Learning",
        "Knowledge Distillation",
        "non-IID"
      ],
      "real_avg_scores": 5.666666666666667,
      "real_all_scores": [
        6,
        3,
        8
      ],
      "real_confidences": [
        3,
        3,
        2
      ],
      "real_contents": [],
      "real_rank": 10,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper proposes a novel Hierarchical Federated Learning (HFL) framework, Full-stack FL (F2L), which tackles two major challenges in real-world FL applications: scalability and robustness against heterogeneous data. The authors' innovative label-driven knowledge distillation (LKD) technique at the global server effectively addresses the non-IID issue, leading to improved overall FL efficiency and rapid convergence. The experimental results demonstrate the superiority of F2L over existing methods, making it a promising solution for large-scale IoT networks.\"\n\n**Review 2:**\n\"The authors present a well-structured and comprehensive approach to Hierarchical Federated Learning, which is essential for addressing the scalability and robustness issues in FL. The proposed F2L framework, combined with the LKD technique, shows significant improvements in FL efficiency and convergence speed. The paper's strength lies in its ability to handle non-IID data, a common problem in real-world FL applications. The experimental results are convincing, and the proposed method has the potential to be widely adopted in IoT networks and other large-scale FL scenarios.\"\n\n**Review 3:**\n\"This paper makes a significant contribution to the field of Federated Learning by proposing a novel hierarchical framework, F2L, which is designed to address the challenges of scalability and non-IID robustness. The authors' LKD technique is a key innovation that enables the global server to effectively aggregate knowledge from regional models, leading to improved FL efficiency and convergence. The experimental results are thorough and demonstrate the effectiveness of F2L in various scenarios. This paper is a must-read for researchers and practitioners working on large-scale FL applications.\"\n\n**Review 4:**\n\"The authors' proposed F2L framework is a promising solution for large-scale IoT networks, where scalability and robustness are critical. The LKD technique is a clever approach to addressing the non-IID issue, and the experimental results demonstrate its effectiveness. However, the paper could benefit from a more detailed comparison with existing HFL methods and a more in-depth analysis of the trade-offs between FL efficiency and convergence speed. Nevertheless, this paper is a valuable contribution to the field of FL and has the potential to inspire further research in this area.\"\n\n**Review 5:**\n\"This paper presents a well-designed and well-executed study on Hierarchical Federated Learning, with a focus on scalability and non-IID robustness. The proposed F2L framework and LKD technique are innovative and effective, as",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper proposes a novel Hierarchical Federated Learning (HFL) framework, Full-stack FL (F2L), which tackles two major challenges in real-world FL applications: scalability and robustness against heterogeneous data. The authors' innovative label-driven knowledge distillation (LKD) technique at the global server effectively addresses the non-IID issue, leading to improved overall FL efficiency and rapid convergence. The experimental results demonstrate the superiority of F2L over existing methods, making it a promising solution for large-scale IoT networks.\"\n\n**Review 2:**\n\"The authors present a well-structured and comprehensive approach to Hierarchical Federated Learning, which is essential for addressing the scalability and robustness issues in FL. The proposed F2L framework, combined with the LKD technique, shows significant improvements in FL efficiency and convergence speed. The paper's strengths lie in its thorough analysis of the challenges in FL and its innovative solutions. However, further experiments on diverse datasets would strengthen the paper's claims.\"\n\n**Review 3:**\n\"This paper makes a valuable contribution to the field of Federated Learning by introducing a hierarchical network architecture and a novel knowledge distillation technique. The authors' approach, F2L, demonstrates impressive results in terms of scalability and robustness against non-IID data. The paper's writing is clear, and the experimental results are well-presented. However, the authors could provide more insights into the theoretical aspects of their approach and its connections to existing FL methods.\"\n\n**Review 4:**\n\"The proposed F2L framework and LKD technique are innovative solutions to the challenges of scalability and non-IID data in FL. The authors' experimental results demonstrate the effectiveness of their approach, particularly in terms of FL efficiency and convergence speed. However, the paper could benefit from a more detailed comparison with existing HFL methods and a discussion on the potential limitations and future directions of their approach.\"\n\n**Review 5:**\n\"This paper presents a thorough investigation of the challenges in Federated Learning and proposes a novel solution, F2L, which combines a hierarchical network architecture with a label-driven knowledge distillation technique. The authors' approach shows promising results, but the paper could be strengthened by providing more insights into the theoretical foundations of their method and its relationships with other FL techniques. Additionally, the authors could explore the applicability of their approach to other domains and datasets.\"",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper proposes a novel Hierarchical Federated Learning (HFL) framework, Full-stack FL (F2L), which tackles two major challenges in real-world FL applications: scalability and robustness against heterogeneous data. The authors' innovative label-driven knowledge distillation (LKD) technique at the global server effectively addresses the non-IID issue, leading to improved overall FL efficiency and rapid convergence. The experimental results demonstrate the superiority of F2L over existing methods, making it a promising solution for large-scale IoT networks.\"\n\n**Review 2:**\n\"The authors present a well-structured and comprehensive approach to Hierarchical Federated Learning, which is essential for addressing the scalability and robustness issues in FL. The proposed F2L framework, combined with the LKD technique, shows significant improvements in FL efficiency and convergence speed. The paper's strength lies in its ability to handle non-IID data, a common problem in real-world FL applications. The experimental results are convincing, and the proposed method has the potential to be widely adopted in IoT networks and other large-scale FL scenarios.\"\n\n**Review 3:**\n\"This paper makes a significant contribution to the field of Federated Learning by proposing a novel hierarchical framework, F2L, which is designed to address the challenges of scalability and non-IID robustness. The authors' LKD technique is a key innovation that enables the global server to effectively aggregate knowledge from regional models, leading to improved FL efficiency and convergence. The experimental results are thorough and demonstrate the effectiveness of F2L in various scenarios. This paper is a must-read for researchers and practitioners working on large-scale FL applications.\"\n\n**Review 4:**\n\"The authors' proposed F2L framework is a promising solution for large-scale IoT networks, where scalability and robustness are critical. The LKD technique is a clever approach to addressing the non-IID issue, and the experimental results demonstrate its effectiveness. However, the paper could benefit from a more detailed comparison with existing HFL methods and a more in-depth analysis of the trade-offs between FL efficiency and convergence speed. Nevertheless, this paper is a valuable contribution to the field of FL and has the potential to inspire further research in this area.\"\n\n**Review 5:**\n\"This paper presents a well-designed and well-executed study on Hierarchical Federated Learning, with a focus on scalability and non-IID robustness. The proposed F2L framework and LKD technique are innovative and effective, as"
      ],
      "sim_rank": 14,
      "sim_decision": "None"
    },
    "Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning": {
      "paper_pk": "e5f109a5-ef1d-4234-add9-5dd889345169",
      "title": "Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning",
      "abstract": "Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy.  Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free.  Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it).  We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \\DPFEDREP\\ converges to a ball centered around the \\emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget.  With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\\sqrt{d}$, where $d$ is the input dimension.  We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link, https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.",
      "authors": [
        "Zebang Shen",
        "Jiayuan Ye",
        "Anmin Kang",
        "Hamed Hassani",
        "Reza Shokri"
      ],
      "keywords": [
        "Differential Privacy",
        "Representation Learning",
        "Federated Learning"
      ],
      "real_avg_scores": 4.75,
      "real_all_scores": [
        3,
        5,
        6,
        5
      ],
      "real_confidences": [
        4,
        5,
        3,
        3
      ],
      "real_contents": [],
      "real_rank": 12,
      "real_decision": "Reject",
      "sim_avg_scores": 1.0,
      "sim_all_scores": [
        1,
        1,
        1
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\nThe authors of \"Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning\" have made a significant breakthrough in the field of federated learning. By proposing a novel representation federated learning objective, they have successfully mitigated the risk of information leakage while ensuring differential privacy guarantees. The convergence analysis of their algorithm, DPFEDREP, is impressive, and the empirical results on image classification tasks demonstrate a significant performance improvement over prior work. This paper is a must-read for anyone working in federated learning and privacy-preserving machine learning.\n\n**Review 2: A New Frontier in Privacy-Preserving Representation Learning**\n\nThis paper presents a game-changing approach to representation learning in federated settings. By decoupling the consensus part of the model from local personalization, the authors have achieved a remarkable improvement in the utility-privacy tradeoff. The theoretical guarantees provided by the authors are impressive, and the empirical results demonstrate the effectiveness of their approach. The open-source implementation of DPFEDREP is a welcome addition, making it easier for researchers to build upon this work. Overall, this paper opens up new avenues for research in privacy-preserving representation learning.\n\n**Review 3: A Step Towards Practical Federated Learning**\n\nThe authors of \"Share Your Representation Only\" have taken a significant step towards making federated learning practical for real-world applications. By addressing the critical issue of information leakage, they have developed a framework that balances privacy and utility. The linear rate of convergence of DPFEDREP is impressive, and the empirical results demonstrate the effectiveness of their approach. This paper has the potential to impact a wide range of applications, from healthcare to finance, where data privacy is paramount. Overall, this paper is a significant contribution to the field of federated learning.\n\n**Review 4: A Theoretically Grounded Approach to Federated Representation Learning**\n\nThis paper presents a theoretically grounded approach to federated representation learning, which is a significant departure from previous work in this area. The authors' use of differential privacy guarantees and convergence analysis provides a solid foundation for their approach. The empirical results demonstrate the effectiveness of DPFEDREP, and the open-source implementation makes it easier for researchers to build upon this work. Overall, this paper is a significant contribution to the field of federated learning, and its theoretical foundations make it a valuable resource for researchers.\n\n**Review 5: A Promising",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\nThe authors of \"Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning\" have made a significant breakthrough in the field of federated learning. By proposing a novel representation federated learning objective, they have successfully mitigated the risk of information leakage while ensuring differential privacy guarantees. The proposed algorithm, DPFEDREP, converges to a global optimal solution at a linear rate, outperforming existing state-of-the-art methods. The empirical evaluation on image classification tasks demonstrates a significant performance improvement under the same small privacy budget. This work has the potential to revolutionize the field of federated learning and its applications in real-world scenarios.\n\n**Review 2: A Novel Approach to Differential Privacy in Federated Learning**\n\nThis paper presents a novel approach to differential privacy in federated learning, which has been a long-standing challenge in the field. By encouraging collaborative refinement of the consensus part of the model while allowing for local personalization, the authors have successfully improved the utility-privacy tradeoff by a factor of \u221ad. The proposed algorithm, DPFEDREP, is shown to converge to a global optimal solution at a linear rate, making it a promising solution for real-world applications. The empirical evaluation on image classification tasks demonstrates the effectiveness of the proposed method. This work is a significant contribution to the field of federated learning and has the potential to inspire further research in this area.\n\n**Review 3: A Game-Changer for Federated Learning with Differential Privacy**\n\nThe authors of \"Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning\" have developed a game-changing solution for federated learning with differential privacy. By proposing a novel representation federated learning objective, they have successfully addressed the issue of information leakage while ensuring differential privacy guarantees. The proposed algorithm, DPFEDREP, is shown to converge to a global optimal solution at a linear rate, making it a highly effective solution for real-world applications. The empirical evaluation on image classification tasks demonstrates a significant performance improvement over prior work under the same small privacy budget. This work has the potential to transform the field of federated learning and its applications in areas such as healthcare, finance, and education.\n\n**Review 4: A Theoretical Breakthrough with Practical Implications**\n\nThis paper presents a theoretical breakthrough in the field of federated learning with differential privacy. The authors have developed a novel representation feder",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\nThe authors of \"Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning\" have made a significant breakthrough in the field of federated learning. By proposing a novel representation federated learning objective, they have successfully mitigated the risk of information leakage while ensuring differential privacy guarantees. The convergence analysis of their algorithm, DPFEDREP, is impressive, and the empirical results on image classification tasks demonstrate a significant performance improvement over prior work. This paper is a must-read for anyone working in federated learning and privacy-preserving machine learning.\n\n**Review 2: A New Frontier in Privacy-Preserving Representation Learning**\n\nThis paper presents a game-changing approach to representation learning in federated settings. By decoupling the consensus part of the model from local personalization, the authors have achieved a remarkable improvement in the utility-privacy tradeoff. The theoretical guarantees provided by the authors are impressive, and the empirical results demonstrate the effectiveness of their approach. The open-source implementation of DPFEDREP is a welcome addition, making it easier for researchers to build upon this work. Overall, this paper opens up new avenues for research in privacy-preserving representation learning.\n\n**Review 3: A Step Towards Practical Federated Learning**\n\nThe authors of \"Share Your Representation Only\" have taken a significant step towards making federated learning practical for real-world applications. By addressing the critical issue of information leakage, they have developed a framework that balances privacy and utility. The linear rate of convergence of DPFEDREP is impressive, and the empirical results demonstrate the effectiveness of their approach. This paper has the potential to impact a wide range of applications, from healthcare to finance, where data privacy is paramount. Overall, this paper is a significant contribution to the field of federated learning.\n\n**Review 4: A Theoretically Grounded Approach to Federated Representation Learning**\n\nThis paper presents a theoretically grounded approach to federated representation learning, which is a significant departure from existing methods. The authors' use of differential privacy guarantees and convergence analysis provides a solid foundation for their approach. The empirical results demonstrate the effectiveness of DPFEDREP, and the open-source implementation makes it easier for researchers to build upon this work. Overall, this paper is a valuable contribution to the field of federated learning, and its theoretical foundations make it a compelling read for researchers and practitioners alike.\n\n**Review 5: A Promising"
      ],
      "sim_rank": 11,
      "sim_decision": "None"
    },
    "FedGC: An Accurate and Efficient Federated Learning under Gradient Constraint for Heterogeneous Data": {
      "paper_pk": "9b897dc7-481a-4884-8f34-d69e86d77aa7",
      "title": "FedGC: An Accurate and Efficient Federated Learning under Gradient Constraint for Heterogeneous Data",
      "abstract": "Federated Learning (FL) is an important paradigm in large-scale distributed machine learning, which enables multiple clients to jointly learn a unified global model without transmitting their local data to a central server. FL has attracted growing attentions in many real-world applications, such as multi-center cardiovascular disease diagnosis and autonomous driving. Practically, the data across clients are always heterogeneous, i.e., not independently and identically distributed (Non-IID), making the local models suffer from catastrophic forgetting of the initial (or global) model. To mitigate this forgetting issue, existing FL methods may require additional regularization terms or generates pseudo data, resulting to 1) limited accuracy; 2) long training time and slow convergence rate for real-time applications; and 3) high communication cost. In this work, an accurate and efficient Federated Learning algorithm under Gradient Constraints (FedGC) is proposed, which provides three advantages: i) High accuracy is achieved by the proposed Client-Gradient-Constraint based projection method (CGC) to alleviate the forgetting issue occurred in clients, and the proposed Server-Gradient-Constraint based projection method (SGC) to effectively aggregate the gradients of clients; ii) Short training time and fast convergence rate are enabled by the proposed fast Pseudo-gradient-based mini-batch Gradient Descent (PGD) method and SGC; iii) Low communication cost is required due to the fast convergence rate and only gradients are necessary to be transmitted between server and clients. In the experiments, four real-world image datasets with three Non-IID types are evaluated, and five popular FL methods are used for comparison. The experimental results demonstrate that our FedGC not only significantly improves the accuracy and convergence rate on Non-IID data, but also drastically decreases the training time. Compared to the state-of-art FedReg, our FedGC improves the accuracy by up to 14.28% and speeds up the local training time by 15.5 times while decreasing 23% of the communication cost.",
      "authors": [
        "Peng Liu",
        "Jie Du",
        "Chi Man VONG"
      ],
      "keywords": [
        "Federated Learning",
        "Non-IID data"
      ],
      "real_avg_scores": 6.0,
      "real_all_scores": [
        6,
        6,
        6
      ],
      "real_confidences": [
        4,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 5,
      "real_decision": "Reject",
      "sim_avg_scores": 6.0,
      "sim_all_scores": [
        9,
        9,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FedGC: A Game-Changer for Federated Learning on Heterogeneous Data\"\n\nIn this paper, the authors propose FedGC, a novel federated learning algorithm that tackles the challenging problem of heterogeneous data distribution across clients. By introducing client-gradient-constraint and server-gradient-constraint based projection methods, FedGC achieves high accuracy and fast convergence rates, outperforming state-of-the-art methods like FedReg. The experimental results on four real-world image datasets demonstrate the effectiveness of FedGC in improving accuracy and reducing training time and communication cost. This work has significant implications for real-world applications like multi-center cardiovascular disease diagnosis and autonomous driving.\n\n**Review 2:**\n\"Efficient and Accurate Federated Learning with FedGC\"\n\nFederated learning has gained popularity in recent years, but it still faces significant challenges, particularly when dealing with heterogeneous data. The authors of this paper address this issue by proposing FedGC, a federated learning algorithm that leverages gradient constraints to improve accuracy and efficiency. The proposed method, which includes client-gradient-constraint and server-gradient-constraint based projection methods, as well as a fast pseudo-gradient-based mini-batch gradient descent method, demonstrates superior performance compared to existing methods. The results show that FedGC can improve accuracy by up to 14.28% and reduce training time by 15.5 times, making it a promising solution for real-world applications.\n\n**Review 3:**\n\"FedGC: A Step Towards Scalable and Efficient Federated Learning\"\n\nFederated learning has the potential to revolutionize the way we train machine learning models, but it requires efficient and scalable algorithms to handle heterogeneous data. The authors of this paper take a significant step towards achieving this goal by proposing FedGC, a federated learning algorithm that combines gradient constraints with efficient optimization methods. The experimental results demonstrate that FedGC can achieve high accuracy and fast convergence rates, while reducing communication cost and training time. This work has important implications for large-scale distributed machine learning and real-world applications like autonomous driving and healthcare.\n\n**Review 4:**\n\"Overcoming Catastrophic Forgetting in Federated Learning with FedGC\"\n\nCatastrophic forgetting is a major challenge in federated learning, particularly when dealing with heterogeneous data. The authors of this paper propose FedGC, a novel algorithm that addresses this issue by introducing client-gradient-constraint and server-gradient-constraint based projection methods. The results show that FedGC can effectively alleviate catastrophic",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FedGC: A Game-Changer for Federated Learning on Heterogeneous Data\"\n\nIn this paper, the authors propose FedGC, a novel federated learning algorithm that tackles the challenging problem of heterogeneous data distribution across clients. By introducing client-gradient-constraint and server-gradient-constraint based projection methods, FedGC achieves high accuracy and fast convergence rates, outperforming state-of-the-art methods like FedReg. The experimental results on four real-world image datasets demonstrate the effectiveness of FedGC in improving accuracy and reducing training time and communication cost. This work has significant implications for real-world applications like multi-center cardiovascular disease diagnosis and autonomous driving.\n\n**Review 2:**\n\"Efficient and Accurate Federated Learning with FedGC\"\n\nFederated learning has gained popularity in recent years, but it still faces significant challenges, particularly when dealing with heterogeneous data. The authors of this paper address this issue by proposing FedGC, a federated learning algorithm that leverages gradient constraints to improve accuracy and efficiency. The proposed method, which includes client-gradient-constraint and server-gradient-constraint based projection methods, as well as a fast pseudo-gradient-based mini-batch gradient descent method, demonstrates superior performance compared to existing methods. The results show that FedGC can improve accuracy by up to 14.28% and reduce training time by 15.5 times, making it a promising solution for real-world applications.\n\n**Review 3:**\n\"FedGC: A Step Towards Scalable and Efficient Federated Learning\"\n\nFederated learning has the potential to revolutionize the way we train machine learning models, but it requires efficient and scalable solutions to handle heterogeneous data. The authors of this paper take a significant step towards achieving this goal by proposing FedGC, a federated learning algorithm that combines gradient constraints with efficient optimization methods. The experimental results demonstrate that FedGC can achieve high accuracy and fast convergence rates, while reducing communication cost and training time. This work has important implications for large-scale distributed machine learning applications and paves the way for further research in this area.\n\n**Review 4:**\n\"Overcoming Catastrophic Forgetting in Federated Learning with FedGC\"\n\nCatastrophic forgetting is a major challenge in federated learning, particularly when dealing with heterogeneous data. The authors of this paper propose FedGC, a novel algorithm that addresses this issue by introducing client-gradient-constraint and server-gradient-constraint based projection methods. The results show that FedGC can",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FedGC: A Game-Changer for Federated Learning on Heterogeneous Data\"\n\nThe authors of FedGC have made a significant breakthrough in the field of Federated Learning (FL) by proposing an accurate and efficient algorithm that tackles the notorious problem of heterogeneous data. By introducing the Client-Gradient-Constraint (CGC) and Server-Gradient-Constraint (SGC) based projection methods, FedGC achieves high accuracy and fast convergence rates, outperforming state-of-the-art methods like FedReg. The experimental results on four real-world image datasets with three Non-IID types are impressive, demonstrating the potential of FedGC to revolutionize FL applications in areas like multi-center cardiovascular disease diagnosis and autonomous driving.\n\n**Review 2:**\n\"Efficient Federated Learning for Real-World Applications\"\n\nFedGC is a timely and impactful contribution to the field of Federated Learning, addressing the critical issues of accuracy, training time, and communication cost in FL. By leveraging the proposed fast Pseudo-gradient-based mini-batch Gradient Descent (PGD) method and SGC, FedGC enables short training times and fast convergence rates, making it suitable for real-time applications. The experimental results show that FedGC significantly improves accuracy and convergence rate on Non-IID data, while reducing training time and communication cost. This work has far-reaching implications for FL applications in areas like healthcare and autonomous driving.\n\n**Review 3:**\n\"A Novel Approach to Mitigating Catastrophic Forgetting in Federated Learning\"\n\nFedGC offers a fresh perspective on mitigating catastrophic forgetting in Federated Learning, a long-standing problem that has plagued FL applications. By introducing the CGC and SGC based projection methods, FedGC effectively alleviates the forgetting issue, achieving high accuracy and fast convergence rates. The authors' approach is well-motivated and well-executed, with thorough experimental evaluations on four real-world image datasets with three Non-IID types. FedGC is a significant step forward in FL research, with potential applications in areas like multi-center cardiovascular disease diagnosis and autonomous driving.\n\n**Review 4:**\n\"FedGC: A Step Towards Scalable and Efficient Federated Learning\"\n\nFedGC is an important contribution to the field of Federated Learning, addressing the critical issues of scalability and efficiency in FL. By proposing an accurate and efficient algorithm that tackles heterogeneous data, FedGC enables fast convergence rates and low communication cost, making it suitable for large-scale FL applications."
      ],
      "sim_rank": 3,
      "sim_decision": "None"
    },
    "A Hierarchical Bayesian Approach to Federated Learning": {
      "paper_pk": "42690bfd-15d4-4237-b4e4-57fde88fba2f",
      "title": "A Hierarchical Bayesian Approach to Federated Learning",
      "abstract": "We propose a novel hierarchical Bayesian approach to Federated learning (FL), where our models reasonably describe the generative process of clients' local data via hierarchical Bayesian modeling: constituting random variables of local models for clients that are governed by a higher-level global variate. Interestingly, the variational inference in our Bayesian model leads to an optimization problem whose block-coordinate descent  solution becomes a distributed algorithm that is separable over clients and allows them not to reveal their own private data at all, thus fully compatible with FL. We also highlight that our block-coordinate algorithm has particular forms that subsume the well-known FL algorithms including Fed-Avg and Fed-Prox as special cases. That is, we not only justify the previous Fed-Avg and Fed-Prox algorithms whose learning protocols look intuitive but theoretically less underpinned, but also generalise them even further via principled Bayesian approaches. Beyond introducing novel modeling and derivations, we also offer convergence analysis showing that our block-coordinate FL algorithm converges to an (local) optimum of the objective at the rate of $O(1/\\sqrt{t})$, the same rate as regular (centralised) SGD, as well as the generalisation error analysis where we prove that the test error of our model on unseen data is guaranteed to vanish as we increase the training data size, thus asymptotically optimal.",
      "authors": [
        "Minyoung Kim",
        "Timothy Hospedales"
      ],
      "keywords": [
        "Federated Learning",
        "Bayesian Methods",
        "Probabilistic Models"
      ],
      "real_avg_scores": 4.25,
      "real_all_scores": [
        8,
        3,
        3,
        3
      ],
      "real_confidences": [
        4,
        4,
        4,
        5
      ],
      "real_contents": [],
      "real_rank": 14,
      "real_decision": "Reject",
      "sim_avg_scores": 0.6666666666666666,
      "sim_all_scores": [
        1,
        0,
        1
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel and Theoretically Sound Approach to Federated Learning**\n\nThis paper presents a groundbreaking hierarchical Bayesian approach to federated learning (FL), which addresses the long-standing issue of privacy preservation in distributed machine learning. By modeling the generative process of clients' local data via hierarchical Bayesian modeling, the authors derive a novel optimization problem that leads to a distributed algorithm that is separable over clients and fully compatible with FL. The authors' approach not only justifies the well-known Fed-Avg and Fed-Prox algorithms but also generalizes them further via principled Bayesian approaches. The convergence analysis and generalization error analysis provided in the paper demonstrate the theoretical soundness of the proposed approach. Overall, this paper is a significant contribution to the field of FL and has the potential to inspire new research directions.\n\n**Review 2: A Unifying Framework for Federated Learning Algorithms**\n\nThis paper proposes a hierarchical Bayesian approach to federated learning, which provides a unifying framework for understanding and generalizing existing FL algorithms. The authors' approach is based on a principled Bayesian modeling of the generative process of clients' local data, which leads to a distributed algorithm that subsumes Fed-Avg and Fed-Prox as special cases. The paper's convergence analysis and generalization error analysis demonstrate the theoretical soundness of the proposed approach. Furthermore, the authors' approach provides a new perspective on FL, highlighting the importance of Bayesian modeling in distributed machine learning. Overall, this paper is a valuable contribution to the field of FL and has the potential to inspire new research directions.\n\n**Review 3: A Theoretically Principled Approach to Federated Learning with Convergence Guarantees**\n\nThis paper presents a novel hierarchical Bayesian approach to federated learning, which provides a theoretically principled approach to distributed machine learning. The authors' approach is based on a Bayesian modeling of the generative process of clients' local data, which leads to a distributed algorithm that is separable over clients and fully compatible with FL. The paper's convergence analysis demonstrates that the proposed algorithm converges to an (local) optimum of the objective at the rate of $O(1/\\sqrt{t})$, which is the same rate as regular (centralized) SGD. Furthermore, the authors provide a generalization error analysis that guarantees the test error of the model on unseen data will vanish as the training data size increases. Overall, this paper is a significant contribution to the field of FL and provides a new",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel and Theoretically Sound Approach to Federated Learning**\n\nThis paper presents a groundbreaking hierarchical Bayesian approach to federated learning (FL), which addresses the long-standing issue of privacy preservation in distributed machine learning. By modeling the generative process of clients' local data using hierarchical Bayesian modeling, the authors derive a novel optimization problem that leads to a distributed algorithm that is fully compatible with FL. The proposed approach not only justifies the well-known Fed-Avg and Fed-Prox algorithms but also generalizes them further via principled Bayesian approaches. The convergence analysis and generalization error analysis provided in the paper demonstrate the theoretical soundness of the proposed approach. Overall, this paper is a significant contribution to the field of FL and has the potential to inspire new research directions.\n\n**Review 2: A Unifying Framework for Federated Learning Algorithms**\n\nThis paper proposes a hierarchical Bayesian approach to federated learning, which provides a unifying framework for understanding and generalizing existing FL algorithms. The authors show that their approach subsumes well-known algorithms such as Fed-Avg and Fed-Prox as special cases, providing a theoretical justification for their learning protocols. The proposed approach is not only theoretically sound but also leads to a distributed algorithm that is separable over clients, ensuring privacy preservation. The paper's convergence analysis and generalization error analysis demonstrate the effectiveness of the proposed approach. Overall, this paper is a valuable contribution to the field of FL, providing a new perspective on existing algorithms and opening up new avenues for research.\n\n**Review 3: A Principled Bayesian Approach to Federated Learning with Theoretical Guarantees**\n\nThis paper presents a novel hierarchical Bayesian approach to federated learning, which provides a principled way to model the generative process of clients' local data. The authors derive a distributed algorithm that is fully compatible with FL and demonstrate its theoretical soundness through convergence analysis and generalization error analysis. The proposed approach is not only theoretically justified but also leads to a distributed algorithm that converges to an (local) optimum of the objective at the rate of $O(1/\\sqrt{t})$, which is comparable to regular (centralized) SGD. Overall, this paper is a significant contribution to the field of FL, providing a new approach that is both theoretically sound and practically effective.\n\n**Review 4: A Hierarchical Bayesian Approach to Federated Learning: A New Perspective on Privacy Preservation**\n\nThis paper proposes a novel hierarchical Bayesian approach to federated learning",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel and Theoretically Sound Approach to Federated Learning**\n\nThis paper presents a groundbreaking hierarchical Bayesian approach to federated learning (FL), which addresses the long-standing issue of privacy preservation in distributed machine learning. By modeling the generative process of clients' local data via hierarchical Bayesian modeling, the authors derive a novel optimization problem that leads to a distributed algorithm that is fully compatible with FL. The proposed approach not only justifies the well-known Fed-Avg and Fed-Prox algorithms but also generalizes them further via principled Bayesian approaches. The convergence analysis and generalization error analysis provided in the paper demonstrate the theoretical soundness of the approach. Overall, this paper is a significant contribution to the field of FL and has the potential to inspire new research directions.\n\n**Review 2: A Unifying Framework for Federated Learning Algorithms**\n\nThe authors of this paper propose a hierarchical Bayesian approach to federated learning that provides a unifying framework for understanding and generalizing existing FL algorithms. By casting the FL problem in a Bayesian framework, the authors are able to derive a block-coordinate descent algorithm that subsumes Fed-Avg and Fed-Prox as special cases. This framework provides a principled way to analyze and compare different FL algorithms, and the authors' convergence and generalization error analyses provide a solid theoretical foundation for the approach. The paper's contributions are significant, and the proposed framework has the potential to accelerate research in FL by providing a common language and set of tools for analyzing and developing new algorithms.\n\n**Review 3: A Bayesian Perspective on Federated Learning**\n\nThis paper offers a fresh perspective on federated learning by casting the problem in a Bayesian framework. The authors' hierarchical Bayesian approach provides a novel way to model the generative process of clients' local data, which leads to a distributed algorithm that is fully compatible with FL. The paper's main contribution is the derivation of a block-coordinate descent algorithm that is separable over clients, which allows for private data to remain on-device. The authors' analysis of the algorithm's convergence and generalization error properties provides a solid theoretical foundation for the approach. Overall, this paper is a valuable contribution to the field of FL, and its Bayesian perspective has the potential to inspire new research directions.\n\n**Review 4: A Theoretically Principled Approach to Federated Learning**\n\nThe authors of this paper propose a hierarchical Bayesian approach to federated learning that is theoretically principled and well-motivated. By modeling the"
      ],
      "sim_rank": 13,
      "sim_decision": "None"
    },
    "The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation": {
      "paper_pk": "5960db34-91e2-4a2d-93a0-1c9fd3093857",
      "title": "The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation",
      "abstract": "Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model's accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge\". The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on a public dataset nor it deploys a generative model at the server. We analyze convergence of FedHKD and conduct extensive experiments on visual datasets in a variety of scenarios, demonstrating that FedHKD provides significant improvement in both personalized as well as global model performance compared to state-of-the-art FL methods designed for heterogeneous data settings.",
      "authors": [
        "Huancheng Chen",
        "Chaining Wang",
        "Haris Vikalo"
      ],
      "keywords": [
        "Federated Learning",
        "Representation Learning",
        "Knowledge Distillation"
      ],
      "real_avg_scores": 5.25,
      "real_all_scores": [
        6,
        6,
        3,
        6
      ],
      "real_confidences": [
        3,
        2,
        2,
        3
      ],
      "real_contents": [],
      "real_rank": 11,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review: A Breakthrough in Federated Learning**\n\n\"FedHKD is a game-changer in the field of federated learning. The authors have successfully addressed the long-standing issue of heterogeneous data distribution across clients, which has limited the performance of global models. By leveraging knowledge distillation and hyper-knowledge, FedHKD achieves the best of both worlds - accurate global models and personalized local models. The experimental results are impressive, demonstrating significant improvements over state-of-the-art methods. This paper is a must-read for anyone working in federated learning, especially those dealing with imbalanced class distributions.\"\n\n**4-star review: A Promising Approach to Personalized Federated Learning**\n\n\"FedHKD offers a novel and promising approach to personalized federated learning. By extracting and aggregating hyper-knowledge from local data representations and soft predictions, the authors have developed a method that can improve both global and local model performance. The fact that FedHKD does not require a public dataset or a generative model at the server makes it a more practical solution. While the experimental results are encouraging, I would have liked to see more analysis on the convergence properties of FedHKD. Nevertheless, this paper is a valuable contribution to the field of federated learning.\"\n\n**3-star review: An Interesting Idea, But More Work Needed**\n\n\"FedHKD presents an interesting idea for addressing the challenges of heterogeneous data in federated learning. The use of knowledge distillation and hyper-knowledge is a novel approach that shows promise. However, I have some reservations about the practicality of this method. The authors assume that clients can extract and send hyper-knowledge to the server, which may not be feasible in all scenarios. Additionally, the experimental results, while encouraging, are limited to visual datasets. More work is needed to demonstrate the effectiveness of FedHKD in other domains and to address potential issues with scalability and communication overhead.\"\n\n**2-star review: Not a Significant Improvement Over Existing Methods**\n\n\"I was expecting more from FedHKD, but unfortunately, it doesn't seem to offer a significant improvement over existing personalized federated learning methods. While the idea of using hyper-knowledge is interesting, the experimental results don't demonstrate a substantial advantage over state-of-the-art methods. The authors' claim that FedHKD provides the best of both worlds - accurate global and personalized models - is not entirely convincing. More analysis and experimentation are needed to demonstrate the effectiveness of FedHKD in a wider range of",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review: A Breakthrough in Federated Learning**\n\n\"FedHKD is a game-changer in the field of federated learning. The authors have successfully addressed the long-standing issue of heterogeneous data distribution across clients, which has limited the performance of global models. By leveraging knowledge distillation and hyper-knowledge, FedHKD achieves the best of both worlds - accurate global models and personalized local models. The experimental results are impressive, demonstrating significant improvements over state-of-the-art methods. This paper is a must-read for anyone working in federated learning, especially those dealing with imbalanced class distributions.\"\n\n**4-star review: A Promising Approach to Personalized Federated Learning**\n\n\"FedHKD offers a novel and promising approach to personalized federated learning. By extracting and sharing hyper-knowledge, clients can improve their local models without sacrificing the accuracy of the global model. The authors' convergence analysis and experimental results are convincing, and the method's ability to handle heterogeneous data settings is a significant advantage. While FedHKD may not be applicable to all scenarios, it is a valuable contribution to the field and warrants further exploration.\"\n\n**3-star review: An Interesting Idea, But Room for Improvement**\n\n\"FedHKD presents an intriguing idea for addressing the challenges of heterogeneous data in federated learning. The use of hyper-knowledge distillation is a clever approach, and the authors' experimental results show promise. However, the method's reliance on clients sending means of local data representations and soft predictions may raise concerns about data privacy and security. Additionally, the paper could benefit from more detailed comparisons with other personalized federated learning methods. Overall, FedHKD is an interesting contribution, but it requires further refinement and evaluation.\"\n\n**Related papers:**\n\n* \"Personalized Federated Learning: A Survey\" by [Author et al.], which provides a comprehensive overview of personalized federated learning methods and their applications.\n* \"Knowledge Distillation for Federated Learning\" by [Author et al.], which explores the use of knowledge distillation in federated learning settings.\n* \"Federated Learning with Heterogeneous Data\" by [Author et al.], which discusses the challenges and opportunities of federated learning in heterogeneous data settings.\n\nThese reviews and related papers can be used to provide a more comprehensive understanding of the paper's contributions and limitations, as well as its position within the broader field of federated learning.",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review: A Breakthrough in Federated Learning**\n\n\"FedHKD is a game-changer in the field of federated learning. The authors have successfully addressed the long-standing issue of heterogeneous data distribution across clients, which has limited the performance of global models. By leveraging knowledge distillation and hyper-knowledge, FedHKD achieves the best of both worlds - accurate global models and personalized local models. The experimental results are impressive, demonstrating significant improvements over state-of-the-art methods. This paper is a must-read for anyone working in federated learning, especially those dealing with imbalanced class distributions.\"\n\n**4-star review: A Promising Approach to Personalized Federated Learning**\n\n\"FedHKD offers a novel and promising approach to personalized federated learning. By extracting and aggregating hyper-knowledge from local data representations and soft predictions, the authors have developed a method that can improve both global and local model performance. The fact that FedHKD does not require a public dataset or a generative model at the server makes it a more practical solution. While the experimental results are encouraging, I would have liked to see more analysis on the convergence properties of FedHKD. Nevertheless, this paper is a valuable contribution to the field of federated learning.\"\n\n**3-star review: An Interesting Idea, But More Work Needed**\n\n\"FedHKD presents an interesting idea for addressing the challenges of heterogeneous data in federated learning. The use of knowledge distillation and hyper-knowledge is a novel approach that shows promise. However, I have some reservations about the practicality of this method. The authors assume that clients can extract and send hyper-knowledge to the server, which may not be feasible in all scenarios. Additionally, the experimental results, while encouraging, are limited to visual datasets and may not generalize to other domains. More work is needed to fully explore the potential of FedHKD and to address its limitations.\"\n\n**2-star review: Not a Significant Improvement Over Existing Methods**\n\n\"I was expecting more from FedHKD, but unfortunately, it doesn't seem to offer a significant improvement over existing personalized federated learning methods. The use of knowledge distillation and hyper-knowledge is not new, and the authors' approach doesn't seem to provide a substantial advantage over other methods. The experimental results are not convincing, and the authors' claims about the superiority of FedHKD are not fully supported by the data. While FedHKD may be a useful addition to the field, it"
      ],
      "sim_rank": 15,
      "sim_decision": "None"
    },
    "Kuiper: Moderated Asynchronous Federated Learning on Heterogeneous Mobile Devices with Non-IID Data": {
      "paper_pk": "40454ef7-785c-437d-8e1a-f7ac14b47dc3",
      "title": "Kuiper: Moderated Asynchronous Federated Learning on Heterogeneous Mobile Devices with Non-IID Data",
      "abstract": "Federated learning allows multiple clients to jointly learn an ML model while keeping their data private. While synchronous federated learning (Sync-FL) requires the devices to share local gradients synchronously to provide better guarantees, it suffers from the problem of stragglers. This is the scenario where the faster clients have to wait for the slower ones, slowing the entire training process. Conventional techniques completely drop the updates from the stragglers and lose the opportunity to learn from the data they hold, which is especially important in a non-iid setting. Asynchronous learning (Async-FL) provides a potential solution to allow the clients to function at their own pace, which typically achieves faster convergence. Since edge devices have a low compute, it is hard to train a video action recognition task on them. We present Kuiper, a variant of Async-FL, to help heterogeneous edge devices with limited resources learn a heavy model on video-action-recognition tasks with data distributed non-IID. Kuiper introduces a novel aggregation scheme, which solves the straggler problem while considering the different data distribution at different clients. Kuiper shows a 11% faster convergence compared to Oort15 [OSDI-21], up to 12% and 9% improvement in test accuracy compared to FedBuff16 [AISTAT-22] and Oort [OSDI-21] on HMDB51, and 10% and 9% on UCF101.",
      "authors": [
        "Dipesh Tamboli",
        "Pranjal Jain",
        "Atul Sharma",
        "Biplab Banerjee",
        "Saurabh Bagchi",
        "Somali Chaterji"
      ],
      "keywords": [
        "Federated Learning",
        "Edge devices",
        "Non-IID",
        "Video action recognition"
      ],
      "real_avg_scores": 2.5,
      "real_all_scores": [
        3,
        1,
        3,
        3
      ],
      "real_confidences": [
        4,
        5,
        5,
        5
      ],
      "real_contents": [],
      "real_rank": 18,
      "real_decision": "",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Kuiper is a game-changer for federated learning on heterogeneous mobile devices. The authors have successfully addressed the straggler problem in asynchronous federated learning, which is particularly important when dealing with non-IID data. The novel aggregation scheme introduced in Kuiper shows impressive results, with a 11% faster convergence rate compared to Oort and significant improvements in test accuracy on HMDB51 and UCF101 datasets. This work has the potential to enable video action recognition tasks on edge devices with limited resources, which is a crucial step towards deploying AI models in real-world scenarios. Highly recommended for anyone working in federated learning and edge AI.\"\n\n**4-star review**\n\"I was impressed by the Kuiper paper's approach to addressing the straggler problem in asynchronous federated learning. The authors have done a great job in designing a novel aggregation scheme that takes into account the different data distributions at each client. The results are promising, with significant improvements in test accuracy compared to FedBuff and Oort. However, I would have liked to see more analysis on the robustness of Kuiper to varying levels of heterogeneity in the edge devices. Nevertheless, this is a solid contribution to the field of federated learning and I look forward to seeing more work in this direction.\"\n\n**3-star review**\n\"Kuiper is an interesting approach to asynchronous federated learning on heterogeneous mobile devices. The authors have identified an important problem in the field and proposed a novel solution. However, I'm not entirely convinced by the results. While the 11% faster convergence rate is impressive, I would have liked to see more experiments to demonstrate the robustness of Kuiper to different scenarios. Additionally, the comparison to Oort and FedBuff could be more comprehensive. Overall, Kuiper is a good starting point, but more work is needed to fully realize its potential.\"\n\n**2-star review**\n\"I was underwhelmed by the Kuiper paper. While the authors have identified an important problem in asynchronous federated learning, their solution seems to be a minor variation of existing approaches. The novel aggregation scheme is not particularly innovative, and the results are not significantly better than existing methods. I would have liked to see more analysis on the theoretical guarantees of Kuiper, as well as more experiments to demonstrate its effectiveness in different scenarios. Overall, Kuiper feels like a incremental contribution to the field, rather than a breakthrough.\"",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Kuiper is a game-changer for federated learning on heterogeneous mobile devices. The authors have successfully addressed the straggler problem in asynchronous federated learning, which is particularly important when dealing with non-IID data. The novel aggregation scheme introduced in Kuiper shows impressive results, with a 11% faster convergence rate compared to Oort and significant improvements in test accuracy on HMDB51 and UCF101 datasets. This work has the potential to enable video action recognition tasks on edge devices with limited resources, which is a crucial step towards deploying AI models in real-world scenarios. Highly recommended for anyone working in federated learning and edge AI.\"\n\n**4-star review**\n\"I was impressed by the Kuiper paper's approach to addressing the straggler problem in asynchronous federated learning. The authors' solution is well-motivated and well-executed, and the results are promising. The 11% faster convergence rate and improvements in test accuracy on HMDB51 and UCF101 datasets are significant, and the comparison to Oort and FedBuff is helpful in understanding the advantages of Kuiper. However, I would have liked to see more discussion on the limitations of the approach and potential avenues for future work. Nevertheless, Kuiper is a valuable contribution to the field of federated learning and edge AI.\"\n\n**3-star review**\n\"Kuiper is an interesting paper that tackles the straggler problem in asynchronous federated learning. The authors' approach is novel and shows some promising results, particularly in terms of convergence rate and test accuracy on certain datasets. However, I'm not entirely convinced that Kuiper is a significant improvement over existing methods like Oort and FedBuff. The comparison to these methods is not always apples-to-apples, and I would have liked to see more rigorous evaluation and analysis. Additionally, the paper could benefit from more discussion on the practical implications of Kuiper and how it can be deployed in real-world scenarios. Overall, Kuiper is a solid paper, but it's not a breakthrough in the field.\"\n\n**2-star review**\n\"I was underwhelmed by the Kuiper paper's approach to asynchronous federated learning. While the authors identify the straggler problem as an important issue, their solution feels like a incremental improvement over existing methods rather than a fundamental breakthrough. The results are not particularly impressive, and the comparison to Oort and FedBuff is not convincing. I would have liked to see more analysis of the",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Kuiper is a game-changer for federated learning on heterogeneous mobile devices. The authors have successfully addressed the straggler problem in asynchronous federated learning, which is particularly important when dealing with non-IID data. The novel aggregation scheme introduced in Kuiper shows impressive results, with a 11% faster convergence rate compared to Oort and significant improvements in test accuracy on HMDB51 and UCF101 datasets. This work has the potential to enable video action recognition tasks on edge devices with limited resources, which is a crucial step towards deploying AI models in real-world scenarios. Highly recommended for anyone working in federated learning and edge AI.\"\n\n**4-star review**\n\"I was impressed by the Kuiper paper's approach to addressing the straggler problem in asynchronous federated learning. The authors' solution is well-motivated and well-executed, and the results are promising. The 11% faster convergence rate and improvements in test accuracy on HMDB51 and UCF101 datasets are significant, and the comparison to Oort and FedBuff is helpful in understanding the advantages of Kuiper. However, I would have liked to see more discussion on the limitations of the approach and potential avenues for future work. Nevertheless, Kuiper is a valuable contribution to the field of federated learning and edge AI.\"\n\n**3-star review**\n\"Kuiper is an interesting paper that tackles the straggler problem in asynchronous federated learning. The authors' approach is novel and shows some promise, but I'm not entirely convinced by the results. The 11% faster convergence rate is notable, but the improvements in test accuracy on HMDB51 and UCF101 datasets are not as significant as I would have expected. Additionally, I would have liked to see more comparison to other state-of-the-art methods and a more detailed analysis of the trade-offs between Kuiper and other approaches. Overall, Kuiper is a solid contribution to the field, but it's not a game-changer.\"\n\n**2-star review**\n\"I was underwhelmed by the Kuiper paper. While the authors' approach to addressing the straggler problem in asynchronous federated learning is novel, the results are not as impressive as I would have expected. The 11% faster convergence rate is modest, and the improvements in test accuracy on HMDB51 and UCF101 datasets are not significant enough to justify the complexity of the approach. Additionally, I found the writing to be unclear"
      ],
      "sim_rank": 16,
      "sim_decision": "None"
    },
    "Convergence Analysis of Split Learning on Non-IID Data": {
      "paper_pk": "4108a671-1088-49f7-9f83-c4f3a14b6ba0",
      "title": "Convergence Analysis of Split Learning on Non-IID Data",
      "abstract": "Split Learning (SL) is one promising variant of Federated Learning (FL), where the AI model is split and trained at the clients and the server collaboratively. By offloading the computation-intensive portions to the server, SL enables efficient model training on resource-constrained clients. Despite its booming applications, SL still lacks rigorous convergence analysis on non-IID data, which is critical for hyperparameter selection. In this paper, we first prove that SL exhibits an $\\mathcal{O}(1/\\sqrt{T})$ convergence rate for non-convex objectives on non-IID data, where $T$ is the number of total steps. By comparing the convergence analysis and experimental results, SL can outperform FL in terms of convergence rate (w.r.t. per-client training/communication rounds, and hence, the computation efficiency) and exhibit comparable accuracy to FL on mildly non-IID data. In contrast, FL prevails on highly non-IID data.",
      "authors": [
        "Yipeng Li",
        "Xinchen Lyu"
      ],
      "keywords": [
        "Federated Learning",
        "Split Leanring",
        "Convergence analysis"
      ],
      "real_avg_scores": 5.75,
      "real_all_scores": [
        6,
        6,
        3,
        8
      ],
      "real_confidences": [
        3,
        4,
        4,
        2
      ],
      "real_contents": [],
      "real_rank": 7,
      "real_decision": "Reject",
      "sim_avg_scores": 3.0,
      "sim_all_scores": [
        9,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper provides a much-needed convergence analysis of Split Learning (SL) on non-IID data, a critical gap in the literature. The authors' proof of an $\\mathcal{O}(1/\\sqrt{T})$ convergence rate for non-convex objectives is a significant contribution, enabling more informed hyperparameter selection. The comparison with Federated Learning (FL) is also insightful, highlighting SL's advantages in terms of convergence rate and computation efficiency on mildly non-IID data. Overall, a well-written and impactful paper that advances our understanding of SL.\"\n\n**Review 2:**\n\"The authors tackle a crucial problem in Split Learning, namely the lack of convergence analysis on non-IID data. Their theoretical results, supported by experimental evidence, demonstrate that SL can outperform FL in certain scenarios. The paper's strengths lie in its rigorous mathematical analysis and the provision of practical insights for hyperparameter tuning. However, the authors could further explore the limitations of SL on highly non-IID data, where FL appears to be more effective. Nevertheless, a valuable contribution to the field of distributed machine learning.\"\n\n**Review 3:**\n\"This paper is a timely contribution to the growing body of research on Federated Learning and its variants. The authors' convergence analysis of Split Learning on non-IID data is a significant step forward, providing a deeper understanding of the algorithm's behavior. The comparison with FL is particularly interesting, highlighting the trade-offs between convergence rate, computation efficiency, and accuracy. While the paper could benefit from more extensive experimental results, the theoretical foundations are solid, and the implications for real-world applications are substantial. A well-structured and thought-provoking paper.\"\n\n**Review 4:**\n\"The convergence analysis of Split Learning on non-IID data presented in this paper is a crucial milestone in the development of distributed machine learning algorithms. The authors' results have important implications for the design of efficient and effective AI systems, particularly in resource-constrained environments. The paper's strengths lie in its theoretical rigor and the provision of practical insights for hyperparameter selection. However, the authors could further explore the relationships between SL and other variants of FL, such as Federated Averaging, to provide a more comprehensive understanding of the landscape. Overall, a valuable and impactful contribution to the field.\"",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper provides a much-needed convergence analysis of Split Learning (SL) on non-IID data, a critical gap in the literature. The authors' proof of an $\\mathcal{O}(1/\\sqrt{T})$ convergence rate for non-convex objectives is a significant contribution, enabling more informed hyperparameter selection. The comparison with Federated Learning (FL) is also insightful, highlighting SL's advantages in terms of convergence rate and computation efficiency on mildly non-IID data. Overall, a well-written and impactful paper that advances our understanding of SL.\"\n\n**Review 2:**\n\"The authors tackle a crucial problem in Split Learning, namely the lack of convergence analysis on non-IID data. Their theoretical results, supported by experimental evidence, demonstrate that SL can outperform FL in certain scenarios. The paper's strengths lie in its rigorous mathematical analysis and the provision of practical insights for hyperparameter tuning. However, the authors could further explore the limitations of SL on highly non-IID data, where FL appears to be more effective. Nevertheless, a valuable contribution to the field of distributed machine learning.\"\n\n**Review 3:**\n\"This paper is a timely contribution to the growing body of research on Federated Learning and its variants. The authors' convergence analysis of Split Learning on non-IID data is a significant step forward, providing a deeper understanding of the algorithm's behavior. The comparison with FL is particularly interesting, highlighting the trade-offs between convergence rate, computation efficiency, and accuracy. While the paper could benefit from more extensive experimental results, the theoretical foundations are solid, and the implications for real-world applications are substantial. A well-structured and thought-provoking paper.\"\n\n**Review 4:**\n\"The convergence analysis of Split Learning on non-IID data presented in this paper is a crucial milestone in the development of distributed machine learning algorithms. The authors' results have important implications for the design of efficient and effective AI systems, particularly in resource-constrained environments. The paper's strengths lie in its theoretical rigor and the provision of practical insights for hyperparameter selection. However, the authors could further explore the relationships between SL and other variants of FL, such as horizontal and vertical FL. Nevertheless, a valuable and impactful contribution to the field.\"",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper provides a much-needed convergence analysis of Split Learning (SL) on non-IID data, a critical gap in the literature. The authors' proof of an $\\mathcal{O}(1/\\sqrt{T})$ convergence rate for non-convex objectives is a significant contribution, enabling more informed hyperparameter selection. The comparison with Federated Learning (FL) is also insightful, highlighting SL's advantages in terms of convergence rate and computation efficiency on mildly non-IID data. Overall, a well-written and impactful paper that advances our understanding of SL.\"\n\n**Review 2:**\n\"The authors tackle a crucial problem in Split Learning, namely the lack of convergence analysis on non-IID data. Their theoretical results, supported by experimental evidence, demonstrate that SL can outperform FL in certain scenarios. The paper's strengths lie in its rigorous mathematical analysis and the provision of practical insights for hyperparameter tuning. However, the authors could further explore the limitations of SL on highly non-IID data, where FL appears to be more effective. Nevertheless, a valuable contribution to the field of distributed machine learning.\"\n\n**Review 3:**\n\"This paper is a timely contribution to the growing body of research on Split Learning and Federated Learning. The authors' convergence analysis sheds light on the performance of SL on non-IID data, a common challenge in real-world applications. The comparison with FL is particularly interesting, as it highlights the trade-offs between convergence rate, computation efficiency, and accuracy. While the paper could benefit from more extensive experimental results, the theoretical foundations are solid, and the implications for hyperparameter selection are significant. A recommended read for researchers and practitioners in the field.\"\n\n**Review 4:**\n\"The convergence analysis of Split Learning on non-IID data presented in this paper is a crucial step forward in understanding the behavior of this promising variant of Federated Learning. The authors' results provide a theoretical foundation for selecting hyperparameters and optimizing the performance of SL in real-world applications. The paper's strengths lie in its mathematical rigor and the provision of practical insights. However, the authors could further explore the implications of their results for more complex scenarios, such as heterogeneous client data or varying levels of non-IID-ness. Nevertheless, a valuable contribution to the field of distributed machine learning.\"\n\nLet me know if you'd like me to generate more reviews or modify these in any way!"
      ],
      "sim_rank": 9,
      "sim_decision": "None"
    },
    "Bias Propagation in Federated Learning": {
      "paper_pk": "8e14751d-9f02-4369-a23b-f1a1fcc135a8",
      "title": "Bias Propagation in Federated Learning",
      "abstract": "We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.\n",
      "authors": [
        "Hongyan Chang",
        "Reza Shokri"
      ],
      "keywords": [
        "Fairness",
        "Algorithmic Bias",
        "Federated Learning"
      ],
      "real_avg_scores": 6.5,
      "real_all_scores": [
        8,
        6,
        6,
        6
      ],
      "real_confidences": [
        5,
        3,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 2,
      "real_decision": "Accept: poster",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Critical Examination of Federated Learning**\n\nTitle: \"The Dark Side of Federated Learning: Bias Propagation and its Consequences\"\n\nThis paper sheds light on a critical issue in federated learning, namely the propagation of bias from a few parties to the entire network. The authors demonstrate that even unintentional bias can be encoded in model parameters, leading to a higher level of bias than in centralized training. This is a sobering reminder that federated learning is not a panacea for fairness concerns. The authors' call to action for auditing group fairness and designing robust algorithms is timely and necessary.\n\n**Review 2: Unintended Consequences of Decentralized Learning**\n\nTitle: \"The Unseen Dangers of Federated Learning: Bias Propagation and its Implications\"\n\nThis paper reveals a disturbing phenomenon in federated learning, where bias from a few parties can spread to the entire network, exacerbating existing fairness issues. The authors' analysis of real-world datasets provides a stark reminder of the importance of fairness auditing in decentralized learning. The paper's findings have significant implications for the development of fair and transparent AI systems, and highlight the need for more research into bias mitigation strategies in federated learning.\n\n**Review 3: A Wake-Up Call for Fairness in AI**\n\nTitle: \"Federated Learning's Dirty Little Secret: Bias Propagation and its Consequences\"\n\nThis paper is a wake-up call for the AI community, highlighting the potential for federated learning to perpetuate and even amplify existing biases. The authors' work is a crucial step towards understanding the mechanisms of bias propagation in decentralized learning, and their recommendations for auditing and designing fair algorithms are essential reading for anyone working in this field. The paper's findings have far-reaching implications for the development of fair and trustworthy AI systems.\n\n**Review 4: A Call to Action for Fair and Transparent AI**\n\nTitle: \"Bias Propagation in Federated Learning: A Call to Action for Fairness and Transparency\"\n\nThis paper is a timely reminder of the importance of fairness and transparency in AI development. The authors' analysis of bias propagation in federated learning highlights the need for more research into fairness auditing and bias mitigation strategies. The paper's findings have significant implications for the development of fair and trustworthy AI systems, and the authors' call to action for the AI community is a necessary step towards creating more equitable and just AI systems.\n\nLet me know if you'd like me to generate more reviews or if you",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Critical Examination of Federated Learning**\n\nTitle: \"The Dark Side of Federated Learning: Bias Propagation and its Consequences\"\n\nThis paper sheds light on a critical issue in federated learning, namely the propagation of bias from a few parties to the entire network. The authors demonstrate that even unintentional bias can be encoded in model parameters, leading to a higher level of bias than in centralized training. This is a sobering reminder that federated learning is not a panacea for fairness concerns. The authors' call to action for auditing group fairness and designing robust algorithms is timely and necessary.\n\n**Review 2: Unintended Consequences of Decentralized Learning**\n\nTitle: \"The Unseen Dangers of Federated Learning: Bias Propagation and its Implications\"\n\nThis paper reveals a disturbing phenomenon in federated learning, where bias from a few parties can spread to the entire network, exacerbating existing fairness issues. The authors' analysis of real-world datasets provides a stark reminder of the importance of fairness auditing in decentralized learning. The paper's findings have significant implications for the development of fair and transparent AI systems, and highlight the need for more research into bias mitigation strategies in federated learning.\n\n**Review 3: A Wake-Up Call for Fairness in AI**\n\nTitle: \"Federated Learning's Dirty Little Secret: Bias Propagation and its Consequences\"\n\nThis paper is a wake-up call for the AI community, highlighting the potential for federated learning to perpetuate and even amplify existing biases. The authors' work is a crucial contribution to the ongoing conversation about fairness and transparency in AI, and serves as a reminder that decentralized learning is not inherently fair or unbiased. The paper's findings have far-reaching implications for the development of fair and inclusive AI systems, and underscore the need for more research into bias mitigation strategies in federated learning.\n\n**Review 4: A Call to Action for Fairer AI**\n\nTitle: \"The Hidden Dangers of Federated Learning: Bias Propagation and the Need for Fairer AI\"\n\nThis paper is a timely reminder of the importance of fairness and transparency in AI development. The authors' work highlights the potential for federated learning to perpetuate and amplify existing biases, and underscores the need for more research into bias mitigation strategies. The paper's findings have significant implications for the development of fair and inclusive AI systems, and serve as a call to action for the AI community to prioritize fairness and transparency in decentralized learning.\n\n",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Critical Examination of Federated Learning**\n\nTitle: \"The Dark Side of Federated Learning: Bias Propagation and its Consequences\"\n\nThis paper sheds light on a critical issue in federated learning, namely the propagation of bias from a few parties to the entire network. The authors demonstrate that even unintentional bias can be encoded in model parameters, leading to a higher level of bias than in centralized training. This is a sobering reminder that federated learning is not a panacea for fairness concerns. The authors' call to action for auditing group fairness and designing robust algorithms is timely and necessary.\n\n**Review 2: Unintended Consequences of Decentralized Learning**\n\nTitle: \"The Unseen Dangers of Federated Learning: Bias Propagation and its Implications\"\n\nThis paper reveals a disturbing phenomenon in federated learning, where bias from a few parties can spread to the entire network, exacerbating existing fairness issues. The authors' analysis of real-world datasets provides a stark reminder of the importance of fairness auditing in decentralized learning. The paper's findings have significant implications for the development of fair and transparent AI systems, and highlight the need for more research into bias mitigation strategies in federated learning.\n\n**Review 3: A Wake-Up Call for Fairness in AI**\n\nTitle: \"Federated Learning's Dirty Little Secret: Bias Propagation and its Consequences\"\n\nThis paper is a wake-up call for the AI community, highlighting the potential for federated learning to perpetuate and even amplify existing biases. The authors' work is a crucial step towards understanding the mechanisms of bias propagation in decentralized learning, and their recommendations for auditing and mitigating bias are essential reading for anyone working in this field. The paper's findings have far-reaching implications for the development of fair and trustworthy AI systems.\n\n**Review 4: A Call to Action for Fair and Transparent AI**\n\nTitle: \"Bias Propagation in Federated Learning: A Call to Action for Fairness and Transparency\"\n\nThis paper is a timely reminder of the importance of fairness and transparency in AI development. The authors' analysis of bias propagation in federated learning highlights the need for more research into bias mitigation strategies and fairness auditing in decentralized learning. The paper's findings have significant implications for the development of fair and trustworthy AI systems, and the authors' call to action is a necessary step towards creating a more equitable AI landscape.\n\nLet me know if you'd like me to generate more reviews or if you have any"
      ],
      "sim_rank": 17,
      "sim_decision": "None"
    },
    "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models": {
      "paper_pk": "0daaa973-cd62-4dc9-ae65-0147f443c57e",
      "title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models",
      "abstract": "Privacy is a central tenet of Federated learning (FL), in which a central server trains models without centralizing user data. However, gradient updates used in FL can leak user information.  While the most industrial uses of FL are for text applications (e.g. keystroke prediction), the majority of attacks on user privacy in FL have focused on simple image classifiers and threat models that assume honest execution of the FL protocol from the server. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. We argue that the threat model of malicious server states is highly relevant from a user-centric perspective, and show that in this scenario, text applications using transformer models are much more vulnerable than previously thought. ",
      "authors": [
        "Liam H Fowl",
        "Jonas Geiping",
        "Steven Reich",
        "Yuxin Wen",
        "Wojciech Czaja",
        "Micah Goldblum",
        "Tom Goldstein"
      ],
      "keywords": [
        "Federated Learning",
        "Attack",
        "Privacy",
        "Transformers",
        "Gradient Inversion"
      ],
      "real_avg_scores": 6.0,
      "real_all_scores": [
        5,
        5,
        8
      ],
      "real_confidences": [
        5,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 6,
      "real_decision": "Reject",
      "sim_avg_scores": 6.0,
      "sim_all_scores": [
        9,
        0,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Critical Look at Federated Learning's Privacy Guarantees**\n\nThe paper \"Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models\" raises important questions about the privacy guarantees of Federated Learning (FL) in language models. While FL is touted as a privacy-preserving approach to machine learning, the authors demonstrate a novel attack that can reveal private user text even with mini-batches, multiple users, and long sequences. This work highlights the vulnerability of text applications using transformer models and challenges the assumption of honest server execution in FL protocols. The findings have significant implications for the development of secure FL systems, particularly in industrial settings where text applications are prevalent.\n\n**Review 2: A New Frontier in Federated Learning Attacks**\n\nThis paper presents a groundbreaking attack on Federated Learning (FL) systems, exploiting the characteristics of both the Transformer architecture and token embedding to retrieve high-fidelity text. The authors' approach, dubbed \"Decepticons,\" demonstrates the potential for malicious parameter vectors to breach user privacy in FL. The work is particularly noteworthy for its focus on text applications, which are increasingly common in industrial settings. The results underscore the need for more robust threat models that account for malicious server states, and highlight the importance of developing countermeasures to protect user privacy in FL systems.\n\n**Review 3: The Dark Side of Federated Learning: A Call to Action**\n\nThe \"Decepticons\" attack presented in this paper is a sobering reminder of the potential risks associated with Federated Learning (FL). By demonstrating the vulnerability of text applications using transformer models, the authors highlight the need for a more nuanced understanding of FL's privacy guarantees. The work is a call to action for the research community to develop more secure FL protocols that can withstand malicious server states. The findings also have significant implications for industry practitioners, who must re-evaluate their reliance on FL for text applications and consider alternative approaches that prioritize user privacy.\n\n**Review 4: A Novel Attack on Federated Learning: Implications and Future Directions**\n\nThis paper introduces a novel attack on Federated Learning (FL) systems, leveraging the characteristics of transformer models to breach user privacy. The \"Decepticons\" attack is a significant contribution to the field, as it highlights the potential risks associated with FL in text applications. The work raises important questions about the design of FL protocols and the need for more robust threat models. Future research directions may include the development of counterme",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Critical Look at Federated Learning's Privacy Guarantees**\n\nThe paper \"Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models\" raises important questions about the privacy guarantees of Federated Learning (FL) in language models. While FL is touted as a privacy-preserving approach to machine learning, the authors demonstrate a novel attack that can reveal private user text even with mini-batches, multiple users, and long sequences. This work highlights the vulnerability of text applications using transformer models and challenges the assumption of honest server execution in FL protocols. The findings have significant implications for the development of secure FL systems, particularly in industrial settings where text applications are prevalent.\n\n**Review 2: A New Frontier in Federated Learning Attacks**\n\nThis paper presents a groundbreaking attack on Federated Learning (FL) systems, exploiting the characteristics of both the Transformer architecture and token embedding to retrieve high-fidelity text. The authors' approach, dubbed \"Decepticons,\" demonstrates the feasibility of malicious parameter vectors in breaching user privacy. The work builds upon previous attacks on FL, but its focus on text applications and transformer models sets it apart. The results underscore the need for more robust threat models that account for malicious server states, particularly in user-centric scenarios. This research opens up new avenues for exploring FL vulnerabilities and developing more secure protocols.\n\n**Review 3: The Dark Side of Federated Learning: A Call to Action**\n\nThe \"Decepticons\" paper is a wake-up call for the machine learning community, exposing the privacy risks associated with Federated Learning (FL) in language models. By demonstrating a novel attack that can extract private user text, the authors highlight the limitations of current FL protocols and the need for more stringent security measures. The work's focus on transformer models and text applications underscores the vulnerability of industrial FL systems, which are increasingly being used in keystroke prediction and other applications. This research serves as a reminder that FL's privacy guarantees are not absolute and that more research is needed to develop secure and trustworthy FL systems.\n\n**Review 4: A Step Backward for Federated Learning's Privacy Claims**\n\nThe \"Decepticons\" paper is a sobering reminder that Federated Learning (FL) is not the panacea for privacy concerns in machine learning. The authors' attack, which exploits the characteristics of transformer models and token embedding, reveals the fragility of FL's privacy guarantees. The work's findings are particularly concerning given the widespread",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Critical Look at Federated Learning's Privacy Guarantees**\n\nThe paper \"Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models\" raises important questions about the privacy guarantees of Federated Learning (FL) in language models. While FL is touted as a privacy-preserving approach to machine learning, the authors demonstrate a novel attack that can reveal private user text even with mini-batches, multiple users, and long sequences. This work highlights the vulnerability of text applications using transformer models and challenges the assumption of honest server execution in FL protocols. The findings have significant implications for the development of secure FL systems, particularly in industrial settings where text applications are prevalent.\n\n**Review 2: A New Frontier in Federated Learning Attacks**\n\nThis paper presents a groundbreaking attack on Federated Learning (FL) for language models, leveraging the characteristics of Transformer architectures and token embeddings to extract high-fidelity text from user data. The authors' approach is a significant departure from previous attacks on FL, which have focused on image classifiers and assumed honest server execution. The \"Decepticons\" attack demonstrates the feasibility of malicious server states in FL, highlighting the need for more robust threat models and security measures in FL protocols. The work has far-reaching implications for the development of secure FL systems, particularly in applications where user privacy is paramount.\n\n**Review 3: The Dark Side of Federated Learning: A Call to Action**\n\nThe \"Decepticons\" paper is a wake-up call for the machine learning community, exposing the vulnerabilities of Federated Learning (FL) in language models to malicious attacks. By exploiting the characteristics of Transformer architectures and token embeddings, the authors demonstrate a powerful attack that can compromise user privacy even in industrial settings. The work underscores the importance of considering malicious server states in FL threat models and highlights the need for more secure FL protocols. The findings have significant implications for the development of trustworthy AI systems, and the authors' call to action should resonate with researchers and practitioners alike.\n\n**Review 4: A Novel Attack on Federated Learning: Implications and Future Directions**\n\nThis paper introduces a novel attack on Federated Learning (FL) for language models, leveraging the strengths of Transformer architectures and token embeddings to extract private user text. The \"Decepticons\" attack is a significant contribution to the field, demonstrating the feasibility of malicious server states in FL and highlighting the need for more robust security measures. The work has important implications for the development of"
      ],
      "sim_rank": 4,
      "sim_decision": "None"
    },
    "Benchmarking Algorithms for Domain Generalization in Federated Learning": {
      "paper_pk": "09b30ef3-2e82-4554-b3b6-65563cd74c49",
      "title": "Benchmarking Algorithms for Domain Generalization in Federated Learning",
      "abstract": "In this paper, we present a unified platform to study domain generalization in the federated learning (FL) context and conduct extensive empirical evaluations of the current state-of-the-art domain generalization algorithms adapted to FL. In particular, we perform a fair comparison of nine existing algorithms in solving domain generalization {either centralized domain generalization algorithms adapted to the FL context or existing FL domain generalization algorithms } to comprehensively explore the challenges introduced by FL. These challenges include statistical heterogeneity among clients, the number of clients, the number of communication rounds, etc. The evaluations are conducted on three diverse datasets including PACS (image dataset covering photo, sketch, cartoon, and painting domains), iWildCam (image dataset with 323 domains), and Py150 (natural language processing dataset with 8421 domains). The experiments show that the challenges brought by federated learning stay unsolved in the realistic experiment setting. Furthermore, the code base supports fair and reproducible new algorithm evaluation with easy implementation.",
      "authors": [
        "Ruqi Bai",
        "Saurabh Bagchi",
        "David I. Inouye"
      ],
      "keywords": [
        "Domain Generalization",
        "Federated Learning",
        "Benchmark."
      ],
      "real_avg_scores": 3.5,
      "real_all_scores": [
        5,
        3,
        3,
        3
      ],
      "real_confidences": [
        4,
        5,
        5,
        3
      ],
      "real_contents": [],
      "real_rank": 17,
      "real_decision": "",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer for Federated Learning! This paper provides a much-needed unified platform for benchmarking domain generalization algorithms in federated learning. The authors' comprehensive evaluation of 9 state-of-the-art algorithms on 3 diverse datasets (PACS, iWildCam, and Py150) sheds light on the unsolved challenges introduced by FL, including statistical heterogeneity and communication rounds. The open-source code base is a bonus, enabling fair and reproducible evaluation of new algorithms. A must-read for anyone working in FL!\"\n\n**4-star review**\n\"A Valuable Contribution to the Field of Federated Learning. This paper takes a significant step towards advancing domain generalization in FL by providing a thorough comparison of existing algorithms. The authors' use of three diverse datasets helps to identify the challenges that remain to be addressed in FL, such as the impact of client number and communication rounds. While the results may not be surprising, the paper's value lies in its systematic evaluation and open-source code base, which will facilitate future research in this area.\"\n\n**3-star review**\n\"A Solid, but Not Groundbreaking, Study. This paper presents a useful benchmarking platform for domain generalization algorithms in FL, but the results may not be particularly surprising or innovative. The evaluation of 9 algorithms on 3 datasets provides a comprehensive overview of the current state-of-the-art, but the paper's findings on the challenges of FL are not particularly new or insightful. Nevertheless, the open-source code base is a welcome contribution to the research community.\"\n\n**2-star review**\n\"A Missed Opportunity. While this paper attempts to provide a unified platform for benchmarking domain generalization algorithms in FL, it falls short in several areas. The evaluation of 9 algorithms is limited to three datasets, which may not be representative of the diversity of FL applications. Furthermore, the paper's findings on the challenges of FL are not particularly novel or actionable. The open-source code base is a positive aspect, but it is not enough to redeem the paper's overall lack of impact.\"\n\n**1-star review**\n\"Underwhelming Experience. This paper's attempt to benchmark domain generalization algorithms in FL is marred by its lack of depth and insight. The evaluation of 9 algorithms is superficial, and the results are predictable and uninformative. The paper's discussion of the challenges of FL is cursory and lacks concrete recommendations for addressing them. Overall, this paper fails to contribute meaningfully to the field of",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer for Federated Learning! This paper provides a much-needed unified platform for benchmarking domain generalization algorithms in federated learning. The authors' comprehensive evaluation of 9 state-of-the-art algorithms on 3 diverse datasets (PACS, iWildCam, and Py150) sheds light on the unsolved challenges of FL, including statistical heterogeneity and communication rounds. The open-source code base is a bonus, enabling fair and reproducible evaluation of new algorithms. A must-read for anyone working in FL!\"\n\n**4-star review**\n\"A Valuable Contribution to FL Research. This paper takes a significant step forward in advancing our understanding of domain generalization in federated learning. The authors' thorough comparison of centralized and FL-specific algorithms on multiple datasets provides valuable insights into the challenges of FL. While the results may not be surprising (i.e., FL challenges remain unsolved), the paper's methodology and code base will undoubtedly facilitate future research in this area. Well done!\"\n\n**3-star review**\n\"A Solid, but Not Groundbreaking, Study. This paper presents a useful benchmarking platform for domain generalization in federated learning, but the results may not be particularly surprising or innovative. The evaluation of 9 algorithms on 3 datasets is thorough, but the conclusions drawn from the experiments are somewhat expected (e.g., FL challenges persist). Nevertheless, the open-source code base is a welcome contribution to the research community. A solid, if not spectacular, addition to the FL literature.\"\n\n**2-star review**\n\"More of the Same in FL Research. Unfortunately, this paper doesn't break new ground in the field of federated learning. The authors' benchmarking platform is a useful tool, but the evaluation of 9 algorithms on 3 datasets yields few surprises. The challenges of FL are well-known, and this paper doesn't offer many new insights or solutions. The code base is a positive aspect, but overall, this paper feels like more of the same in FL research.\"\n\n**1-star review**\n\"Underwhelming Experience. I was expecting more from this paper, but it failed to deliver. The benchmarking platform is overly simplistic, and the evaluation of algorithms is lacking in depth. The results are predictable and don't offer any meaningful contributions to the field of federated learning. The code base is the only redeeming quality, but even that is not enough to save this paper from its overall lack of impact.\"",
        "Here are some potential reviews based on the input and external data:\n\n**5-star review**\n\"Game-Changer for Federated Learning! This paper provides a much-needed unified platform for benchmarking domain generalization algorithms in federated learning. The authors' comprehensive evaluation of 9 state-of-the-art algorithms on 3 diverse datasets (PACS, iWildCam, and Py150) sheds light on the unsolved challenges introduced by FL, including statistical heterogeneity and communication rounds. The open-source code base is a bonus, enabling fair and reproducible evaluation of new algorithms. A must-read for anyone working in FL!\"\n\n**4-star review**\n\"A Valuable Contribution to Federated Learning Research. This paper takes a significant step towards advancing domain generalization in federated learning by providing a unified platform for evaluation and comparison of existing algorithms. The authors' experiments on three diverse datasets highlight the difficulties in solving domain generalization in FL settings. While the paper could benefit from more in-depth analysis of the results, the provided code base is a valuable resource for the research community. Recommended for those interested in FL and domain generalization.\"\n\n**3-star review**\n\"A Good Starting Point, But Room for Improvement. This paper presents a useful platform for evaluating domain generalization algorithms in federated learning, but the analysis could be more thorough. The authors' comparison of 9 algorithms on three datasets provides some insights, but the paper could benefit from more discussion on the implications of the results and potential avenues for future research. The open-source code base is a positive aspect, but more guidance on its usage would be helpful. A good starting point for those new to FL and domain generalization.\"\n\n**2-star review**\n\"More of a Survey Than a Breakthrough. While this paper provides a comprehensive overview of existing domain generalization algorithms in federated learning, it doesn't offer many new insights or contributions. The evaluation of 9 algorithms on three datasets is useful, but the results are not particularly surprising or groundbreaking. The code base is a nice addition, but it's not clear how much it adds to the paper's overall value. Recommended for those looking for a brief introduction to the field, but not for those seeking novel research or ideas.\"\n\n**1-star review**\n\"Underwhelming Experience. Unfortunately, this paper fails to deliver on its promise of a unified platform for benchmarking domain generalization algorithms in federated learning. The evaluation of 9 algorithms is superficial, and the results are not well-analyzed or discussed. The code base is the only redeeming aspect,"
      ],
      "sim_rank": 18,
      "sim_decision": "None"
    },
    "FedX: Federated Learning for Compositional Pairwise Risk Optimization": {
      "paper_pk": "63f036da-f286-4e88-82fa-7bf4e67c4586",
      "title": "FedX: Federated Learning for Compositional Pairwise Risk Optimization",
      "abstract": "In this paper, we tackle a novel federated learning (FL) problem for optimizing a family of compositional pairwise risks, to which no existing FL algorithms are applicable. In particular, the objective has the form of $\\E_{\\z\\sim \\mathcal S_1} f(\\E_{\\z'\\sim\\mathcal S_2} \\ell(\\w; \\z, \\z'))$, where two sets of data $\\mathcal S_1, \\mathcal S_2$ are distributed over multiple machines, $\\ell(\\cdot; \\cdot,\\cdot)$ is a pairwise loss that only depends on the prediction outputs of the input data pairs $(\\z, \\z')$, and $f(\\cdot)$ is possibly a non-linear non-convex function. This problem has important applications in machine learning, e.g., AUROC maximization with a pairwise loss, and partial AUROC maximization with a compositional loss, etc. The challenges for designing a FL algorithm lie at the non-decomposability of the objective over multiple machines and the interdependency between different machines. We propose two provable FL algorithms (FedX) for handling linear and nolinear $f$, respectively. To tackle the challenges, we decouple the gradient's components with two types namely active parts and  lazy parts, where the {\\it active} parts depend on local data that can be computed with the local model  and the {\\it lazy} parts depend on other machines that are communicated/computed based on historical models. We develop a novel theoretical analysis to address the issue of latency of lazy parts and interdependency between the local gradient estimators and the involved data. We establish both iteration and communication complexities and exhibit that using the historical models for computing the lazy parts do not degrade the complexity results. We conduct empirical studies of FedX for AUROC and  partial AUROC maximization, and demonstrate their performance compared with multiple baselines.",
      "authors": [
        "Zhishuai Guo",
        "Rong Jin",
        "Jiebo Luo",
        "Tianbao Yang"
      ],
      "keywords": [
        "Federated Learning",
        "Pairwise Loss",
        "Compositional Optimization"
      ],
      "real_avg_scores": 4.75,
      "real_all_scores": [
        3,
        5,
        8,
        3
      ],
      "real_confidences": [
        5,
        4,
        4,
        4
      ],
      "real_contents": [],
      "real_rank": 13,
      "real_decision": "",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FedX: A Breakthrough in Federated Learning for Compositional Pairwise Risk Optimization\"\n\nThis paper tackles a novel and challenging problem in federated learning, where the objective function involves a compositional pairwise risk. The authors propose two provable algorithms, FedX, which can handle both linear and non-linear functions. The key innovation lies in decoupling the gradient components into active and lazy parts, allowing for efficient computation and communication. The theoretical analysis is thorough, and the empirical results demonstrate the effectiveness of FedX in AUROC and partial AUROC maximization tasks. This work has significant implications for machine learning applications and paves the way for further research in federated learning.\n\n**Review 2:**\n\"A Novel Approach to Federated Learning: FedX for Compositional Pairwise Risk Optimization\"\n\nFedX addresses a critical gap in federated learning by proposing a novel algorithm for optimizing compositional pairwise risks. The authors' approach is innovative, as they decompose the gradient into active and lazy parts, enabling efficient computation and communication. The theoretical guarantees are strong, and the empirical results demonstrate the superiority of FedX over existing baselines. This work has important applications in machine learning, particularly in AUROC and partial AUROC maximization tasks. The authors' contribution is significant, and FedX is likely to become a benchmark for future research in federated learning.\n\n**Review 3:**\n\"FedX: A Theoretically-Grounded Approach to Federated Learning for Compositional Pairwise Risk Optimization\"\n\nThis paper presents a rigorous and theoretically-grounded approach to federated learning for compositional pairwise risk optimization. The authors' proposal, FedX, is based on a novel decomposition of the gradient into active and lazy parts, which enables efficient computation and communication. The theoretical analysis is thorough, and the authors provide strong guarantees on the iteration and communication complexities. The empirical results demonstrate the effectiveness of FedX in AUROC and partial AUROC maximization tasks. This work is a significant contribution to the field of federated learning and provides a solid foundation for future research.\n\n**Review 4:**\n\"FedX: A Step Forward in Federated Learning for Real-World Applications\"\n\nFedX tackles a critical problem in federated learning, where the objective function involves a compositional pairwise risk. The authors' approach is practical and effective, as demonstrated by the empirical results in AUROC and partial AUROC maximization tasks. The key innovation lies",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FedX: A Breakthrough in Federated Learning for Compositional Pairwise Risk Optimization\"\n\nThis paper tackles a novel and challenging problem in federated learning (FL) by optimizing a family of compositional pairwise risks. The proposed FedX algorithm is a significant breakthrough in this area, as it can handle non-decomposable objectives and interdependent data across multiple machines. The authors' decoupling of gradient components into active and lazy parts is a clever approach to address the challenges of latency and interdependency. The theoretical analysis and empirical studies demonstrate the effectiveness of FedX in AUROC and partial AUROC maximization tasks. This work has important implications for machine learning applications and paves the way for further research in FL.\n\n**Review 2:**\n\"A Novel Approach to Federated Learning for Compositional Pairwise Risk Optimization\"\n\nThe authors of FedX propose a novel FL algorithm for optimizing compositional pairwise risks, which is a significant extension of existing FL algorithms. The key innovation lies in the decoupling of gradient components into active and lazy parts, which enables the algorithm to handle non-decomposable objectives and interdependent data. The theoretical analysis provides a solid foundation for the algorithm, and the empirical studies demonstrate its effectiveness in AUROC and partial AUROC maximization tasks. This work has important implications for machine learning applications, particularly in areas where pairwise losses are used. However, further research is needed to explore the applicability of FedX to other FL scenarios.\n\n**Review 3:**\n\"FedX: A Promising Solution for Federated Learning with Compositional Pairwise Risks\"\n\nFedX addresses a critical gap in FL research by proposing a novel algorithm for optimizing compositional pairwise risks. The authors' approach to decoupling gradient components into active and lazy parts is an innovative solution to the challenges of latency and interdependency. The theoretical analysis and empirical studies demonstrate the potential of FedX in AUROC and partial AUROC maximization tasks. However, further research is needed to explore the scalability and robustness of FedX in more complex FL scenarios. Nevertheless, this work is a promising step forward in the development of FL algorithms for compositional pairwise risks.\n\n**Review 4:**\n\"FedX: A Theoretically Grounded Approach to Federated Learning for Compositional Pairwise Risk Optimization\"\n\nThe authors of FedX provide a theoretically grounded approach to FL for compositional pairwise risk optimization. The decoupling of gradient",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"FedX: A Breakthrough in Federated Learning for Compositional Pairwise Risk Optimization\"\n\nThis paper tackles a novel and challenging problem in federated learning, optimizing compositional pairwise risks with non-linear and non-convex objectives. The proposed FedX algorithm is a significant breakthrough, offering a provable solution to this complex problem. The authors' decoupling of gradient components into active and lazy parts is a clever approach to addressing the challenges of non-decomposability and interdependency between machines. The theoretical analysis and empirical studies demonstrate the effectiveness of FedX in AUROC and partial AUROC maximization, outperforming multiple baselines. This work has important implications for machine learning applications and is a must-read for researchers in the field.\n\n**Review 2:**\n\"FedX: A Novel Approach to Federated Learning for Pairwise Risk Optimization\"\n\nThe authors of FedX propose a novel federated learning algorithm for optimizing compositional pairwise risks, a problem that has not been addressed by existing FL algorithms. The paper's contribution lies in its ability to handle non-linear and non-convex objectives, which is a significant advancement in the field. The decoupling of gradient components into active and lazy parts is an innovative approach to addressing the challenges of non-decomposability and interdependency between machines. The empirical studies demonstrate the effectiveness of FedX in AUROC and partial AUROC maximization, and the theoretical analysis provides a solid foundation for the algorithm's performance. This work is a valuable contribution to the field of federated learning and has important implications for machine learning applications.\n\n**Review 3:**\n\"FedX: Overcoming the Challenges of Compositional Pairwise Risk Optimization in Federated Learning\"\n\nFedX is a significant contribution to the field of federated learning, offering a provable solution to the challenging problem of optimizing compositional pairwise risks. The authors' approach to decoupling gradient components into active and lazy parts is a clever way to address the challenges of non-decomposability and interdependency between machines. The paper's theoretical analysis and empirical studies demonstrate the effectiveness of FedX in AUROC and partial AUROC maximization, outperforming multiple baselines. The work's significance lies in its ability to handle non-linear and non-convex objectives, which is a major advancement in the field. This paper is a must-read for researchers interested in federated learning and its applications in machine learning.\n\n**Review"
      ],
      "sim_rank": 19,
      "sim_decision": "None"
    },
    "Federated Representation Learning via Maximal Coding Rate Reduction": {
      "paper_pk": "21cdb6f7-1de9-4220-9ccb-39eabbf1f147",
      "title": "Federated Representation Learning via Maximal Coding Rate Reduction",
      "abstract": "We propose a federated methodology to learn low-dimensional representations from a dataset that is distributed among several clients. In particular, we move away from the commonly-used cross-entropy loss in federated learning, and seek to learn shared low-dimensional representations of the data in a decentralized manner via the principle of maximal coding rate reduction (MCR2). Our proposed method, which we refer to as FLOW, utilizes MCR2 as the objective of choice, hence resulting in representations that are both between-class discriminative and within-class compressible. We theoretically show that our distributed algorithm achieves a first-order stationary point. Moreover, we demonstrate, via numerical experiments, the utility of the learned low-dimensional representations.",
      "authors": [
        "Juan Cervino",
        "Navid Naderializadeh",
        "Alejandro Ribeiro"
      ],
      "keywords": [
        "Federated Learning",
        "Representation Learning",
        "Information Theory"
      ],
      "real_avg_scores": 6.5,
      "real_all_scores": [
        6,
        8,
        6,
        6
      ],
      "real_confidences": [
        2,
        4,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 3,
      "real_decision": "Accept: poster",
      "sim_avg_scores": 3.0,
      "sim_all_scores": [
        9,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"Federated Representation Learning via Maximal Coding Rate Reduction\" presents a novel approach to learning low-dimensional representations in a decentralized manner. By leveraging the principle of maximal coding rate reduction (MCR2), the authors propose a federated methodology that moves away from traditional cross-entropy loss. The resulting representations are both discriminative and compressible, making them highly useful for downstream tasks. The theoretical guarantees and empirical results demonstrate the effectiveness of the proposed method, FLOW. This work has significant implications for federated learning and representation learning research.\n\n**Review 2:**\nIn \"Federated Representation Learning via Maximal Coding Rate Reduction\", the authors tackle the challenging problem of learning shared representations from decentralized data. By formulating the problem as a maximal coding rate reduction (MCR2) objective, they develop a federated algorithm that achieves a first-order stationary point. The proposed method, FLOW, is shown to learn representations that are both discriminative and compressible, outperforming traditional federated learning approaches. This work contributes to the growing body of research on federated learning and representation learning, and has potential applications in areas such as computer vision and natural language processing.\n\n**Review 3:**\nThis paper proposes a federated representation learning methodology that leverages the principle of maximal coding rate reduction (MCR2) to learn low-dimensional representations from decentralized data. The authors' approach, FLOW, is theoretically grounded and empirically validated, demonstrating the effectiveness of MCR2 as an objective for federated learning. The learned representations are shown to be both discriminative and compressible, making them suitable for a range of downstream tasks. This work builds upon recent advances in federated learning and representation learning, and has the potential to impact a wide range of applications.\n\n**Review 4:**\n\"Federated Representation Learning via Maximal Coding Rate Reduction\" presents a significant contribution to the field of federated learning. By moving away from traditional cross-entropy loss and instead leveraging the principle of maximal coding rate reduction (MCR2), the authors develop a federated algorithm that learns shared low-dimensional representations of decentralized data. The proposed method, FLOW, is shown to achieve a first-order stationary point and learn representations that are both discriminative and compressible. This work has important implications for areas such as healthcare, finance, and education, where data is often decentralized and sensitive.\n\n**Review 5:**\nThis paper proposes a novel federated representation learning",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"Federated Representation Learning via Maximal Coding Rate Reduction\" presents a novel approach to learning low-dimensional representations in a decentralized manner. By leveraging the principle of maximal coding rate reduction (MCR2), the authors propose a federated methodology that moves away from traditional cross-entropy loss. The resulting representations are both discriminative and compressible, making them highly useful for downstream tasks. The theoretical guarantees and empirical results demonstrate the effectiveness of the proposed method, FLOW. This work has significant implications for federated learning and representation learning research.\n\n**Review 2:**\nIn \"Federated Representation Learning via Maximal Coding Rate Reduction\", the authors tackle the challenging problem of learning shared representations from decentralized data. By formulating the problem as a maximal coding rate reduction (MCR2) objective, they develop a federated algorithm that achieves a first-order stationary point. The proposed method, FLOW, is shown to learn representations that are both discriminative and compressible, outperforming traditional federated learning approaches. This work contributes to the growing body of research on federated learning and has potential applications in areas such as computer vision and natural language processing.\n\n**Review 3:**\nThis paper proposes a federated representation learning approach that leverages the principle of maximal coding rate reduction (MCR2) to learn low-dimensional representations from decentralized data. The authors' method, FLOW, is theoretically grounded and empirically validated, demonstrating its effectiveness in learning discriminative and compressible representations. The work builds upon recent advances in federated learning and representation learning, and its contributions have the potential to impact a wide range of applications, from healthcare to finance. Overall, \"Federated Representation Learning via Maximal Coding Rate Reduction\" is a significant contribution to the field.\n\n**Review 4:**\nThe authors of \"Federated Representation Learning via Maximal Coding Rate Reduction\" present a compelling case for moving beyond traditional cross-entropy loss in federated learning. By adopting the principle of maximal coding rate reduction (MCR2) as the objective, they develop a federated algorithm that learns shared low-dimensional representations of decentralized data. The proposed method, FLOW, is shown to achieve a first-order stationary point and outperform existing approaches in numerical experiments. This work has important implications for federated learning and representation learning research, and its contributions are likely to be of interest to researchers and practitioners alike.\n\n**Review 5:**\nIn \"Federated Representation",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"Federated Representation Learning via Maximal Coding Rate Reduction\" presents a novel approach to learning low-dimensional representations in a decentralized manner. By leveraging the principle of maximal coding rate reduction (MCR2), the authors propose a federated methodology that moves away from traditional cross-entropy loss. The resulting representations are both discriminative and compressible, making them highly useful for downstream tasks. The theoretical guarantees and empirical results demonstrate the effectiveness of the proposed method, FLOW. This work has significant implications for federated learning and representation learning research.\n\n**Review 2:**\nIn \"Federated Representation Learning via Maximal Coding Rate Reduction\", the authors tackle the challenging problem of learning shared representations from decentralized data. By formulating the problem as a maximal coding rate reduction (MCR2) objective, they develop a federated algorithm that achieves a first-order stationary point. The proposed method, FLOW, is shown to learn representations that are both discriminative and compressible, outperforming traditional cross-entropy based methods. This work builds upon recent advances in federated learning and representation learning, and has the potential to enable new applications in areas such as edge computing and distributed machine learning.\n\n**Review 3:**\nThis paper presents a significant contribution to the field of federated learning by introducing a novel objective function based on maximal coding rate reduction (MCR2). The proposed method, FLOW, is theoretically grounded and empirically validated, demonstrating its ability to learn low-dimensional representations that are both discriminative and compressible. The authors' approach has important implications for decentralized data analysis and machine learning, and is likely to inspire further research in this area. The paper is well-written and easy to follow, making it accessible to a broad audience.\n\n**Review 4:**\n\"Federated Representation Learning via Maximal Coding Rate Reduction\" is a timely and important contribution to the field of federated learning. By moving away from traditional cross-entropy loss, the authors develop a federated methodology that is better suited to decentralized data analysis. The proposed method, FLOW, is shown to be effective in learning low-dimensional representations that are both discriminative and compressible. The paper's theoretical guarantees and empirical results demonstrate the potential of this approach for real-world applications. This work has significant implications for areas such as healthcare, finance, and edge computing, where decentralized data analysis is increasingly important.\n\n**Review 5:**\nThis paper proposes a novel federated methodology for learning"
      ],
      "sim_rank": 10,
      "sim_decision": "None"
    },
    "Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation": {
      "paper_pk": "1a88d2c3-db5a-49a1-bf5f-99f5aece0708",
      "title": "Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation",
      "abstract": "As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase \"my credit card number is ...\". In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.",
      "authors": [
        "Hong-Min Chu",
        "Jonas Geiping",
        "Liam H Fowl",
        "Micah Goldblum",
        "Tom Goldstein"
      ],
      "keywords": [
        "Federated Learning",
        "Privacy",
        "Security",
        "Privacy attack"
      ],
      "real_avg_scores": 2.0,
      "real_all_scores": [
        3,
        1,
        1,
        3
      ],
      "real_confidences": [
        2,
        4,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 19,
      "real_decision": "Reject",
      "sim_avg_scores": 9.0,
      "sim_all_scores": [
        9,
        9,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to Targeted Text Extraction in Federated Learning**\n\nThis paper presents a groundbreaking attack on federated learning (FL) systems, specifically designed to extract targeted sequences containing sensitive personal information. Unlike existing attacks that aim to recover as much data as possible, this approach focuses on extracting specific phrases, such as credit card numbers, from aggregated user data. The authors' use of maliciously modified parameters to filter relevant sequences and encode them in the gradient update is a clever and effective technique. The results demonstrate the attack's ability to extract sequences of interest even under large-scale aggregation, making it a significant threat to FL systems.\n\n**Review 2: A Step Forward in Federated Learning Privacy Attacks**\n\nThe authors of this paper have made a significant contribution to the field of federated learning (FL) privacy attacks. By proposing the first attack that achieves targeted extraction of sequences containing privacy-critical phrases, they have demonstrated a new level of sophistication in FL attacks. The attack's ability to filter relevant sequences from aggregated user data and encode them in the gradient update is a notable advancement. This work highlights the need for FL systems to prioritize privacy and develop more robust defenses against such targeted attacks.\n\n**Review 3: A Wake-Up Call for Federated Learning Systems**\n\nThis paper serves as a wake-up call for federated learning (FL) systems, which have been increasingly vulnerable to privacy attacks. The authors' proposed attack, which targets specific sequences containing sensitive personal information, is a stark reminder of the need for FL systems to prioritize privacy and security. The attack's effectiveness under large-scale aggregation is particularly concerning, as it highlights the potential for attackers to extract meaningful data even in realistic settings. This work underscores the importance of developing more robust defenses against targeted attacks in FL systems.\n\n**Review 4: A New Frontier in Federated Learning Privacy Research**\n\nThis paper marks a significant shift in federated learning (FL) privacy research, as it proposes the first attack that achieves targeted extraction of sequences containing privacy-critical phrases. The authors' approach, which employs maliciously modified parameters to filter relevant sequences and encode them in the gradient update, opens up new avenues for research in FL privacy. The results demonstrate the attack's effectiveness under large-scale aggregation, highlighting the need for FL systems to develop more sophisticated defenses against targeted attacks. This work has the potential to inspire new research directions in FL privacy and security.\n\n**Review 5: A Call to Action for Federated Learning Practition",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to Targeted Text Extraction in Federated Learning**\n\nThis paper presents a groundbreaking attack on federated learning (FL) systems, specifically designed to extract targeted sequences containing sensitive personal information. Unlike existing sequence-agnostic attacks, the proposed method leverages maliciously modified parameters to enable the transformer to filter and encode relevant sequences in the gradient update. The authors demonstrate the effectiveness of their approach even under extremely large-scale aggregation, making it a significant threat to FL systems. The paper's contribution lies in its ability to extract specific phrases, such as \"my credit card number is ...\", which is a crucial aspect of privacy attacks. Overall, this work highlights the need for more robust privacy measures in FL.\n\n**Review 2: A Step Forward in Federated Learning Privacy Attacks**\n\nThe authors of this paper take a significant step forward in the development of privacy attacks against federated language models. By proposing a targeted text extraction attack, they demonstrate the potential to extract sensitive information from user data, even under large-scale aggregation. The use of maliciously modified parameters to filter and encode relevant sequences is a clever approach that sets this work apart from existing attacks. The paper's results are concerning, as they show that current FL systems are vulnerable to such targeted attacks. This work serves as a wake-up call for the FL community to develop more effective privacy-preserving mechanisms.\n\n**Review 3: Targeted Extraction in Federated Learning: A New Frontier in Privacy Attacks**\n\nThis paper marks a significant shift in the landscape of federated learning (FL) privacy attacks. By focusing on targeted text extraction, the authors demonstrate the ability to extract specific sequences containing sensitive information, such as credit card numbers. The proposed attack is particularly concerning, as it can operate effectively even under extremely large-scale aggregation. The use of transformer-based models to filter and encode relevant sequences is a novel approach that highlights the need for more robust privacy measures in FL. This work has significant implications for the development of privacy-preserving FL systems and serves as a warning to practitioners and researchers in the field.\n\n**Review 4: A Critical Examination of Federated Learning Privacy**\n\nThis paper presents a critical examination of the privacy vulnerabilities in federated learning (FL) systems. The authors propose a targeted text extraction attack that can extract sensitive information from user data, even under large-scale aggregation. The attack's effectiveness highlights the need for more robust privacy measures in FL, particularly in the context of language models. The paper's results",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to Targeted Text Extraction in Federated Learning**\n\nThis paper presents a groundbreaking attack on federated learning (FL) systems, specifically designed to extract targeted sequences containing sensitive personal information. Unlike existing sequence-agnostic attacks, the proposed method leverages maliciously modified parameters to enable the transformer to filter and encode relevant sequences in the gradient update. The authors demonstrate the effectiveness of their approach even under extremely large-scale aggregation, making it a significant threat to FL systems. The paper's contribution lies in its ability to extract specific phrases, such as \"my credit card number is ...\", which is a crucial aspect of privacy attacks. Overall, this work highlights the need for more robust privacy measures in FL.\n\n**Review 2: A Step Forward in Federated Learning Privacy Attacks**\n\nThe authors of this paper have made a significant contribution to the field of federated learning (FL) privacy attacks. By proposing a targeted text extraction attack, they have demonstrated the potential to extract sensitive personal information from FL updates, even under large-scale aggregation. The use of maliciously modified parameters to filter and encode relevant sequences is a novel approach that sets this work apart from existing attacks. The paper's results are concerning, as they highlight the vulnerability of FL systems to targeted attacks. This work serves as a wake-up call for the development of more secure FL systems that can protect user data.\n\n**Review 3: Targeted Extraction in Federated Learning: A New Frontier in Privacy Attacks**\n\nThis paper marks a significant shift in the field of federated learning (FL) privacy attacks, moving from sequence-agnostic attacks to targeted extraction of sensitive information. The authors' approach, which employs maliciously modified parameters to filter and encode relevant sequences, is both innovative and concerning. The ability to extract specific phrases, such as those containing credit card numbers, raises serious privacy concerns. The paper's results demonstrate the potential for attackers to exploit FL systems, even under large-scale aggregation. This work highlights the need for more research into FL privacy and the development of robust countermeasures to protect user data.\n\n**Review 4: A Critical Examination of Federated Learning Privacy**\n\nThis paper presents a critical examination of federated learning (FL) privacy, highlighting the vulnerability of FL systems to targeted attacks. The authors' proposed attack, which leverages maliciously modified parameters to extract sensitive information, is a significant contribution to the field. The paper's results demonstrate the potential for attackers to extract specific phrases, such"
      ],
      "sim_rank": 1,
      "sim_decision": "None"
    },
    "DIGEST: FAST AND COMMUNICATION EFFICIENT DECENTRALIZED LEARNING WITH LOCAL UPDATES": {
      "paper_pk": "85c07402-2deb-4229-ad82-2626959cb2db",
      "title": "DIGEST: FAST AND COMMUNICATION EFFICIENT DECENTRALIZED LEARNING WITH LOCAL UPDATES",
      "abstract": "Decentralized learning advocates the elimination of centralized parameter servers\n(aggregation points) for potentially better utilization of underlying resources, de-\nlay reduction, and resiliency against parameter server unavailability and catas-\ntrophic failures. Gossip based decentralized algorithms, where each node in a net-\nwork has its own locally kept model on which it effectuates the learning by talking\nto its neighbors, received a lot of attention recently. Despite their potential, Gossip\nalgorithms introduce huge communication costs. In this work, we show that nodes\ndo not need to communicate as frequently as in Gossip for fast convergence; in\nfact, a sporadic exchange of a digest of a trained model is sufficient. Thus, we\ndesign a fast and communication-efficient decentralized learning mechanism; DI-\nGEST by particularly focusing on stochastic gradient descent (SGD). DIGEST is\na decentralized algorithm building on local-SGD algorithms, which are originally\ndesigned for communication efficient centralized learning. We show through anal-\nysis and experiments that DIGEST significantly reduces the communication cost\nwithout hurting convergence time for both iid and non-iid data.",
      "authors": [
        "Peyman Gholami",
        "Hulya Seferoglu"
      ],
      "keywords": [
        "Decentralized Learning",
        "Distributed Optimization",
        "Communication Efficient Learning",
        "Local SGD",
        "Federated Learning"
      ],
      "real_avg_scores": 5.75,
      "real_all_scores": [
        6,
        6,
        3,
        8
      ],
      "real_confidences": [
        3,
        4,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 8,
      "real_decision": "Reject",
      "sim_avg_scores": 6.0,
      "sim_all_scores": [
        0,
        9,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Decentralized Learning**\n\nDIGEST, a novel decentralized learning algorithm, marks a significant breakthrough in the field of decentralized learning. By introducing a sporadic exchange of a digest of a trained model, DIGEST reduces communication costs while maintaining fast convergence rates. This approach is particularly impressive when compared to traditional gossip-based decentralized algorithms, which often introduce huge communication costs. The authors' focus on stochastic gradient descent (SGD) and local-SGD algorithms has led to a decentralized mechanism that is both efficient and effective. With its ability to handle both iid and non-iid data, DIGEST has the potential to revolutionize the way we approach decentralized learning.\n\n**Review 2: A Communication-Efficient Solution for Decentralized Learning**\n\nDecentralized learning has long been hindered by the need for frequent communication between nodes. DIGEST, a decentralized algorithm that builds on local-SGD algorithms, offers a communication-efficient solution to this problem. By exchanging a digest of a trained model, nodes can achieve fast convergence rates while reducing communication costs. The authors' analysis and experiments demonstrate the effectiveness of DIGEST in both iid and non-iid data settings. This work has significant implications for the development of decentralized learning systems, particularly in resource-constrained environments.\n\n**Review 3: A New Era for Decentralized Learning**\n\nThe advent of DIGEST marks a new era for decentralized learning. By eliminating the need for frequent communication, DIGEST enables fast and efficient decentralized learning. The algorithm's ability to handle both iid and non-iid data makes it a versatile solution for a wide range of applications. The authors' focus on stochastic gradient descent (SGD) and local-SGD algorithms has led to a decentralized mechanism that is both scalable and effective. With its potential to reduce communication costs and improve convergence rates, DIGEST is poised to have a significant impact on the field of decentralized learning.\n\n**Review 4: A Promising Approach to Decentralized Learning**\n\nDIGEST, a decentralized learning algorithm that leverages local-SGD algorithms, offers a promising approach to decentralized learning. By reducing communication costs, DIGEST enables fast and efficient decentralized learning. The authors' analysis and experiments demonstrate the effectiveness of DIGEST in both iid and non-iid data settings. While further research is needed to fully explore the potential of DIGEST, this work represents a significant step forward in the development of decentralized learning systems.\n\n**Review 5: A Step Towards Scalable Dec",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Decentralized Learning**\n\nDIGEST, a novel decentralized learning algorithm, marks a significant breakthrough in the field of decentralized learning. By introducing a sporadic exchange of a digest of a trained model, DIGEST reduces communication costs while maintaining fast convergence rates. This approach is particularly impressive when compared to traditional gossip-based decentralized algorithms, which often introduce huge communication costs. The authors' focus on stochastic gradient descent (SGD) and local-SGD algorithms has led to a decentralized mechanism that is both efficient and effective. With its ability to handle both iid and non-iid data, DIGEST has the potential to revolutionize the way we approach decentralized learning.\n\n**Review 2: A Communication-Efficient Solution for Decentralized Learning**\n\nDecentralized learning has long been hindered by the need for frequent communication between nodes. DIGEST, a decentralized algorithm that builds on local-SGD algorithms, offers a communication-efficient solution to this problem. By exchanging a digest of a trained model, nodes can achieve fast convergence rates while reducing communication costs. The authors' analysis and experiments demonstrate the effectiveness of DIGEST in both iid and non-iid data settings. This work has significant implications for the development of decentralized learning systems, particularly in resource-constrained environments.\n\n**Review 3: A New Era for Decentralized Learning**\n\nThe advent of DIGEST marks a new era for decentralized learning. By eliminating the need for frequent communication, DIGEST enables fast and efficient decentralized learning. The algorithm's ability to handle both iid and non-iid data makes it a versatile solution for a wide range of applications. The authors' focus on stochastic gradient descent (SGD) and local-SGD algorithms has led to a decentralized mechanism that is both scalable and effective. With its potential to reduce communication costs and improve convergence rates, DIGEST is poised to have a significant impact on the field of decentralized learning.\n\n**Review 4: A Step Towards Resilient Decentralized Learning Systems**\n\nDecentralized learning systems are often vulnerable to parameter server unavailability and catastrophic failures. DIGEST, a decentralized algorithm that eliminates the need for centralized parameter servers, offers a step towards more resilient decentralized learning systems. By reducing communication costs and improving convergence rates, DIGEST enables fast and efficient decentralized learning. The authors' analysis and experiments demonstrate the effectiveness of DIGEST in both iid and non-iid data settings. This work has significant implications for the development of decentralized learning systems that can operate",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Decentralized Learning**\n\nDIGEST, a novel decentralized learning algorithm, marks a significant breakthrough in the field of decentralized learning. By introducing a sporadic exchange of a digest of a trained model, DIGEST reduces communication costs while maintaining fast convergence rates. This approach is particularly impressive when compared to traditional gossip-based decentralized algorithms, which often introduce huge communication costs. The authors' focus on stochastic gradient descent (SGD) and local-SGD algorithms has led to a decentralized mechanism that is both efficient and effective. With its ability to handle both iid and non-iid data, DIGEST has the potential to revolutionize the way we approach decentralized learning.\n\n**Review 2: A Communication-Efficient Solution for Decentralized Learning**\n\nDecentralized learning has long been hindered by the need for frequent communication between nodes. DIGEST, a decentralized algorithm that builds on local-SGD algorithms, offers a communication-efficient solution to this problem. By exchanging a digest of a trained model, nodes can achieve fast convergence rates while reducing communication costs. The authors' analysis and experiments demonstrate the effectiveness of DIGEST in both iid and non-iid data settings. This work has significant implications for the development of decentralized learning systems, particularly in resource-constrained environments.\n\n**Review 3: A New Era for Decentralized Learning**\n\nThe advent of DIGEST marks a new era for decentralized learning. By eliminating the need for frequent communication, DIGEST enables fast and efficient decentralized learning. The algorithm's ability to handle both iid and non-iid data makes it a versatile solution for a wide range of applications. The authors' focus on stochastic gradient descent (SGD) and local-SGD algorithms has led to a decentralized mechanism that is both scalable and effective. With its potential to reduce communication costs and improve convergence rates, DIGEST is poised to have a significant impact on the field of decentralized learning.\n\n**Review 4: A Step Towards Resilient Decentralized Learning Systems**\n\nDecentralized learning systems are often vulnerable to parameter server unavailability and catastrophic failures. DIGEST, a decentralized algorithm that eliminates the need for centralized parameter servers, offers a step towards more resilient decentralized learning systems. By reducing communication costs and improving convergence rates, DIGEST enables fast and efficient decentralized learning. The authors' analysis and experiments demonstrate the effectiveness of DIGEST in both iid and non-iid data settings. This work has significant implications for the development of decentralized learning systems that can operate"
      ],
      "sim_rank": 5,
      "sim_decision": "None"
    },
    "FedorAS: Federated Architecture Search under system heterogeneity": {
      "paper_pk": "84bd66a3-f682-462e-9804-89d8a48318d4",
      "title": "FedorAS: Federated Architecture Search under system heterogeneity",
      "abstract": "Federated learning (FL) has recently gained considerable attention due to its ability to learn on decentralised data while preserving client privacy. However, it also poses additional challenges related to the heterogeneity of the participating devices, both in terms of their computational capabilities and contributed data. Meanwhile, Neural Architecture Search (NAS) has been successfully used with centralised datasets, producing state-of-the-art results in constrained or unconstrained settings. However, such centralised datasets may not be always available for training. Most recent work at the intersection of NAS and FL attempts to alleviate this issue in a cross-silo federated setting, which assumes homogeneous compute environments with datacenter-grade hardware. \nIn this paper we explore the question of whether we can design architectures of different footprints in a cross-device federated setting, where the device landscape, availability and scale are very different. To this end, we design our system, FedorAS, to discover and train promising architectures in a resource-aware manner when dealing with devices of varying capabilities holding non-IID distributed data. We present empirical evidence of its effectiveness across different settings, spanning across three different modalities (vision, speech, text), and showcase its better performance compared to state-of-the-art federated solutions, while maintaining resource efficiency.",
      "authors": [
        "\u0141ukasz Dudziak",
        "Stefanos Laskaridis",
        "Javier Fernandez-Marques"
      ],
      "keywords": [
        "Federated Learning",
        "Neural Architecture Search",
        "Deep Learning",
        "Efficient DNN Training"
      ],
      "real_avg_scores": 6.333333333333333,
      "real_all_scores": [
        5,
        8,
        6
      ],
      "real_confidences": [
        3,
        4,
        3
      ],
      "real_contents": [],
      "real_rank": 4,
      "real_decision": "Accept: poster",
      "sim_avg_scores": 9.0,
      "sim_all_scores": [
        9,
        9,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\n\"FedorAS is a game-changer in the field of federated learning. By tackling the long-standing issue of system heterogeneity, the authors have successfully demonstrated the feasibility of federated architecture search in a cross-device setting. The proposed system's ability to discover and train promising architectures in a resource-aware manner is a significant breakthrough, especially considering the non-IID distributed data and varying device capabilities. The empirical evidence presented across three different modalities is convincing, and the comparison with state-of-the-art federated solutions is impressive. This work has the potential to unlock new possibilities in federated learning and neural architecture search.\"\n\n**Review 2: A Step Towards Practical Federated Learning**\n\n\"FedorAS addresses a critical gap in the existing federated learning literature by considering the heterogeneity of participating devices. The authors' approach to designing architectures of different footprints in a cross-device federated setting is both novel and practical. The system's ability to maintain resource efficiency while achieving better performance compared to state-of-the-art federated solutions is a significant advantage. This work takes a crucial step towards making federated learning more practical and applicable to real-world scenarios, where device capabilities and data distributions can vary significantly. The results presented in this paper are promising, and I look forward to seeing further developments in this area.\"\n\n**Review 3: A New Frontier in Neural Architecture Search**\n\n\"The intersection of neural architecture search and federated learning is an exciting area of research, and FedorAS is a significant contribution to this field. By exploring the possibility of designing architectures of different footprints in a cross-device federated setting, the authors have opened up new avenues for research. The system's ability to adapt to varying device capabilities and non-IID distributed data is a testament to its robustness and flexibility. The empirical evidence presented in this paper is convincing, and the potential applications of FedorAS are vast. This work has the potential to inspire new research directions in neural architecture search and federated learning.\"\n\n**Review 4: A Comprehensive Solution for Federated Learning**\n\n\"FedorAS is a comprehensive solution that addresses several challenges in federated learning, including system heterogeneity, non-IID distributed data, and resource efficiency. The authors' approach to federated architecture search is well-designed and well-executed, and the results presented in this paper are impressive. The system's ability to discover and train promising architectures in a resource-aware manner is a significant",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\n\"FedorAS is a game-changer in the field of federated learning. By tackling the long-standing issue of system heterogeneity, the authors have successfully demonstrated the feasibility of federated architecture search in a cross-device setting. The proposed system's ability to discover and train promising architectures in a resource-aware manner is a significant breakthrough, especially considering the non-IID distributed data and varying device capabilities. The empirical results across three modalities are impressive, and the comparison to state-of-the-art federated solutions is convincing. This work has the potential to unlock new possibilities in federated learning and neural architecture search.\"\n\n**Review 2: A Step Towards Practical Federated Learning**\n\n\"FedorAS addresses a critical gap in the existing federated learning literature by considering the heterogeneity of participating devices. The authors' approach to designing architectures of different footprints in a cross-device federated setting is both novel and practical. The system's ability to maintain resource efficiency while achieving better performance compared to state-of-the-art federated solutions is a significant advantage. This work takes us a step closer to deploying federated learning in real-world scenarios, where device heterogeneity is a common challenge. The empirical results are promising, and I look forward to seeing further developments in this direction.\"\n\n**Review 3: A New Frontier in Neural Architecture Search**\n\n\"The intersection of neural architecture search and federated learning is an exciting area of research, and FedorAS is a significant contribution to this field. By exploring the possibility of designing architectures of different footprints in a cross-device federated setting, the authors have opened up new avenues for research. The system's ability to handle non-IID distributed data and varying device capabilities is a notable achievement. The empirical results across three modalities demonstrate the effectiveness of FedorAS, and I believe this work will inspire further research in this direction. The potential applications of this technology are vast, and I am excited to see where this research will lead.\"\n\n**Review 4: A Well-Designed System for Federated Architecture Search**\n\n\"FedorAS is a well-designed system that tackles the complex problem of federated architecture search in a cross-device setting. The authors' approach to resource-aware architecture search is both novel and effective. The system's ability to discover and train promising architectures in a decentralized manner is a significant achievement. The empirical results are convincing, and the comparison to state-of-the-art federated solutions is thorough. I",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\n\"FedorAS is a game-changer in the field of federated learning. By tackling the long-standing issue of system heterogeneity, the authors have successfully demonstrated the feasibility of federated architecture search in a cross-device setting. The proposed system's ability to discover and train promising architectures in a resource-aware manner is a significant breakthrough, especially considering the non-IID distributed data and varying device capabilities. The empirical evidence presented across three different modalities is convincing, and the comparison with state-of-the-art federated solutions is impressive. This work has the potential to unlock new possibilities in federated learning and neural architecture search.\"\n\n**Review 2: A Step Towards Practical Federated Learning**\n\n\"FedorAS addresses a critical gap in the existing federated learning literature by considering the heterogeneity of participating devices. The authors' approach to designing architectures of different footprints in a cross-device federated setting is both novel and practical. The system's ability to maintain resource efficiency while achieving better performance compared to state-of-the-art federated solutions is a significant advantage. This work takes a crucial step towards making federated learning more practical and applicable to real-world scenarios, where device capabilities and data distributions can vary significantly. The results presented in this paper are promising, and I look forward to seeing further developments in this area.\"\n\n**Review 3: A New Frontier in Neural Architecture Search**\n\n\"The intersection of neural architecture search and federated learning is an exciting area of research, and FedorAS is a significant contribution to this field. By exploring the possibility of designing architectures of different footprints in a cross-device federated setting, the authors have opened up new avenues for research. The system's ability to adapt to varying device capabilities and non-IID distributed data is a testament to its flexibility and robustness. The empirical evidence presented in this paper is convincing, and the potential applications of FedorAS are vast. This work has the potential to inspire new research directions in neural architecture search and federated learning.\"\n\n**Review 4: A Comprehensive Solution for Federated Learning**\n\n\"FedorAS is a comprehensive solution that addresses several challenges in federated learning, including system heterogeneity, non-IID distributed data, and resource efficiency. The authors' approach to federated architecture search is well-designed and well-executed, and the results presented in this paper are impressive. The system's ability to discover and train promising architectures in a resource-aware manner is a significant"
      ],
      "sim_rank": 2,
      "sim_decision": "None"
    },
    "SuperFed: Weight Shared Federated Learning": {
      "paper_pk": "f78e8d4e-8b59-40c6-acc4-5c9d9eac17bb",
      "title": "SuperFed: Weight Shared Federated Learning",
      "abstract": "Federated Learning (FL) is a well-established technique for privacy preserving\ndistributed training. Much attention has been given to various aspects of FL training. A growing number of applications that consume FL-trained models, however, increasingly operate under dynamically and unpredictably variable conditions, rendering a single model insufficient. We argue for training a global \u201cfamily of models\u201d cost efficiently in a federated fashion. Training them independently for different tradeoff points incurs \u2248 O(k) cost for any k architectures of interest, however.\nStraightforward applications of FL techniques to recent weight-shared training\napproaches is either infeasible or prohibitively expensive. We propose SuperFed \u2014 an architectural framework that incurs O(1) cost to co-train a large family ofmodels in a federated fashion by leveraging weight-shared learning. We achieve an order of magnitude cost savings on both communication and computation by proposing two novel training mechanisms: (a) distribution of weight-shared models to federated clients, (b) central aggregation of arbitrarily overlapping weight-shared model parameters. The combination of these mechanisms is shown to reach an order of magnitude (9.43x) reduction in computation and communication cost for training a 5*10^18-sized family of models, compared to independently training as few as k = 9 DNNs without any accuracy loss.",
      "authors": [
        "Alind Khare",
        "Animesh Agrawal",
        "Alexey Tumanov"
      ],
      "keywords": [
        "Weight Shared",
        "Federated Learning"
      ],
      "real_avg_scores": 4.0,
      "real_all_scores": [
        5,
        5,
        3,
        3
      ],
      "real_confidences": [
        4,
        4,
        5,
        4
      ],
      "real_contents": [],
      "real_rank": 15,
      "real_decision": "Reject",
      "sim_avg_scores": 3.6666666666666665,
      "sim_all_scores": [
        9,
        1,
        1
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: \"A Game-Changer for Federated Learning\"**\n\nThe authors of SuperFed have made a significant breakthrough in federated learning, proposing a novel architectural framework that enables the cost-efficient training of a large family of models in a federated fashion. By leveraging weight-shared learning, SuperFed achieves an order of magnitude reduction in computation and communication cost, making it a game-changer for applications that require dynamically and unpredictably variable conditions. The proposed mechanisms of distributing weight-shared models to federated clients and centrally aggregating arbitrarily overlapping model parameters are ingenious and well-executed. This paper is a must-read for anyone working in federated learning.\n\n**Review 2: \"Efficient Federated Learning for Dynamic Applications\"**\n\nSuperFed addresses a critical gap in federated learning, where a single model is often insufficient for applications that operate under dynamic and unpredictable conditions. The authors' proposal for training a global \"family of models\" in a federated fashion is both timely and innovative. By incurring O(1) cost, SuperFed makes it possible to co-train a large family of models, achieving an impressive 9.43x reduction in computation and communication cost. This paper has significant implications for real-world applications, such as edge computing and IoT, where adaptability and efficiency are paramount.\n\n**Review 3: \"A Novel Approach to Weight-Shared Federated Learning\"**\n\nThe authors of SuperFed have taken a fresh look at weight-shared training approaches and federated learning, proposing a novel framework that combines the benefits of both. By distributing weight-shared models to federated clients and aggregating overlapping model parameters, SuperFed achieves remarkable cost savings without sacrificing accuracy. This paper is a valuable contribution to the field, offering a new perspective on how to efficiently train a large family of models in a federated fashion. The experimental results are impressive, and the proposed mechanisms have the potential to be widely adopted.\n\n**Review 4: \"Federated Learning for the Edge: A New Frontier\"**\n\nSuperFed represents a significant step forward in federated learning, enabling the efficient training of a large family of models in a federated fashion. The authors' focus on dynamic and unpredictable conditions is particularly relevant for edge computing and IoT applications, where adaptability and efficiency are crucial. By achieving an order of magnitude reduction in computation and communication cost, SuperFed opens up new possibilities for federated learning at the edge. This paper is a must-read for anyone interested in exploring",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: \"A Game-Changer for Federated Learning\"**\n\nThe authors of SuperFed have made a significant breakthrough in federated learning, proposing a novel architectural framework that enables the cost-efficient training of a large family of models in a federated fashion. By leveraging weight-shared learning, SuperFed achieves an order of magnitude cost savings on both communication and computation, making it a game-changer for applications that require dynamically and unpredictably variable conditions. The results are impressive, with a 9.43x reduction in computation and communication cost for training a massive family of models. This paper is a must-read for anyone working in federated learning.\n\n**Review 2: \"Efficient Federated Learning for Dynamic Applications\"**\n\nSuperFed addresses a critical limitation of traditional federated learning approaches, which are often insufficient for applications that operate under dynamic and unpredictable conditions. By proposing a framework for training a global \"family of models\" in a federated fashion, the authors of SuperFed have opened up new possibilities for efficient and cost-effective model training. The paper's focus on weight-shared learning and novel training mechanisms makes it a valuable contribution to the field. The experimental results are convincing, demonstrating the potential of SuperFed to reduce computation and communication costs without sacrificing accuracy.\n\n**Review 3: \"A Scalable Solution for Federated Model Training\"**\n\nThe SuperFed framework is a scalable solution for federated model training, enabling the cost-efficient training of a large family of models in a federated fashion. By distributing weight-shared models to federated clients and centrally aggregating arbitrarily overlapping model parameters, SuperFed achieves an order of magnitude cost savings on both communication and computation. This paper is a significant step forward in federated learning, offering a practical solution for applications that require dynamic and adaptive model training. The authors' approach is well-motivated and well-executed, making SuperFed a valuable contribution to the field.\n\n**Review 4: \"Weight-Shared Learning for Efficient Federated Training\"**\n\nThe authors of SuperFed have made a compelling case for the use of weight-shared learning in federated training, demonstrating its potential to reduce computation and communication costs without sacrificing accuracy. By proposing a novel architectural framework that leverages weight-shared learning, SuperFed offers a cost-efficient solution for training a large family of models in a federated fashion. The paper's focus on efficient federated training makes it a valuable contribution to the field, and the experimental results are convincing. Overall, Super",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: \"A Game-Changer for Federated Learning\"**\n\nThe authors of SuperFed have made a significant breakthrough in the field of federated learning. By proposing a novel architectural framework that leverages weight-shared learning, they have achieved an order of magnitude cost savings on both communication and computation. This is a game-changer for applications that require training multiple models under dynamically and unpredictably variable conditions. The ability to co-train a large family of models in a federated fashion with O(1) cost is a major advancement that will have a significant impact on the field.\n\n**Review 2: \"Efficient Federated Learning for Dynamic Environments\"**\n\nIn this paper, the authors tackle the challenge of training models in federated learning settings where conditions are dynamically and unpredictably variable. They propose SuperFed, a framework that enables the training of a global \"family of models\" in a federated fashion, with a cost savings of O(1) compared to training multiple models independently. The results are impressive, with a 9.43x reduction in computation and communication cost for training a large family of models. This work has significant implications for applications that require adaptability and efficiency in dynamic environments.\n\n**Review 3: \"Weight-Shared Learning for Scalable Federated Training\"**\n\nThe authors of SuperFed have made a significant contribution to the field of federated learning by proposing a novel approach to weight-shared learning. By distributing weight-shared models to federated clients and centrally aggregating arbitrarily overlapping model parameters, they have achieved an order of magnitude cost savings on both communication and computation. This work demonstrates the potential of weight-shared learning for scalable federated training and has significant implications for applications that require training large families of models.\n\n**Review 4: \"A New Paradigm for Federated Model Training\"**\n\nSuperFed represents a new paradigm for federated model training, one that is capable of handling dynamically and unpredictably variable conditions. By proposing a framework that enables the training of a global \"family of models\" in a federated fashion, the authors have opened up new possibilities for applications that require adaptability and efficiency. The results are impressive, with a significant reduction in computation and communication cost compared to traditional approaches. This work has the potential to transform the field of federated learning and has significant implications for a wide range of applications.\n\n**Review 5: \"Scalable and Efficient Federated Learning with SuperFed\"**\n\nThe authors of SuperFed"
      ],
      "sim_rank": 7,
      "sim_decision": "None"
    },
    "Tackling Imbalanced Class in Federated Learning via Class Distribution Estimation": {
      "paper_pk": "34400e9b-c7d1-49ab-a40f-42cff4679c17",
      "title": "Tackling Imbalanced Class in Federated Learning via Class Distribution Estimation",
      "abstract": "Federated Learning (FL) has become an upsurging machine learning method due to its applicability in large-scale distributed system and its privacy-preserving property. However, in real-world applications, the presence of class imbalance issue, especially the mismatch between local and global class distribution, greatly degrades the performance of FL. Moreover, due to the privacy constrain, the class distribution information of clients can not be accessed directly. To tackle class imbalance issue under FL setting, a novel algorithm, FedRE, is proposed in this paper. We propose a new class distribution estimation method for the FedRE algorithm, which requires no extra client data information and thus has no privacy concern. Both experimental results and theoretical analysis are provided to support the validity of our distribution estimation method. The proposed algorithm is verified with several experiment, including different datasets with the presence of class imbalance and local-global distribution mismatch. The experimental results show that FedRE is effective and it outperforms other related methods in terms of both overall and minority class classification accuracy.",
      "authors": [
        "You-Ru Lu",
        "Xiaoqian Wang",
        "Dengfeng Sun"
      ],
      "keywords": [
        "Federated Learning",
        "class imbalance",
        "class distribution estimation"
      ],
      "real_avg_scores": 5.75,
      "real_all_scores": [
        6,
        5,
        6,
        6
      ],
      "real_confidences": [
        4,
        4,
        4,
        5
      ],
      "real_contents": [],
      "real_rank": 9,
      "real_decision": "Reject",
      "sim_avg_scores": 1.0,
      "sim_all_scores": [
        1,
        1,
        1
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a crucial issue in Federated Learning (FL) - class imbalance - which has been largely overlooked in the literature. The proposed algorithm, FedRE, is a significant contribution to the field as it addresses the mismatch between local and global class distributions without compromising client privacy. The authors' novel class distribution estimation method is both theoretically sound and experimentally validated, demonstrating improved performance over existing methods. A must-read for anyone working on FL or imbalanced classification problems.\"\n\n**Review 2:**\n\"Federated Learning has shown great promise in large-scale distributed systems, but its performance is often hindered by class imbalance issues. This paper presents a timely solution to this problem with the introduction of FedRE, an algorithm that effectively estimates class distributions without requiring additional client data. The experimental results are impressive, showcasing the superiority of FedRE over other methods in terms of overall and minority class classification accuracy. This work has important implications for real-world FL applications.\"\n\n**Review 3:**\n\"The authors of this paper have made a valuable contribution to the field of Federated Learning by proposing a novel algorithm, FedRE, that tackles the challenging problem of class imbalance. The key innovation lies in the class distribution estimation method, which is both privacy-preserving and effective. The theoretical analysis and experimental results provide strong evidence for the validity of the proposed approach. This work is a significant step forward in addressing the limitations of FL and has the potential to impact a wide range of applications.\"\n\n**Review 4:**\n\"Class imbalance is a pervasive problem in machine learning, and its impact is exacerbated in Federated Learning settings. This paper presents a thoughtful solution to this problem with the development of FedRE, an algorithm that leverages class distribution estimation to improve classification performance. The authors' approach is well-motivated, and the experimental results demonstrate the effectiveness of FedRE in various scenarios. This work is a valuable addition to the FL literature and has important implications for practitioners and researchers alike.\"\n\n**Review 5:**\n\"FedRE, the algorithm proposed in this paper, is a game-changer for Federated Learning applications plagued by class imbalance issues. The authors' innovative class distribution estimation method is a key differentiator, enabling the algorithm to outperform existing methods without compromising client privacy. The paper is well-written, and the experimental results are convincing. This work has the potential to significantly impact the field of FL and is a must-read for anyone interested in this area.\"",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a crucial issue in Federated Learning (FL) - class imbalance - which has been largely overlooked in the literature. The proposed algorithm, FedRE, is a significant contribution to the field as it addresses the mismatch between local and global class distributions without compromising client privacy. The authors' novel class distribution estimation method is both theoretically sound and experimentally validated, demonstrating improved performance over existing methods. A must-read for anyone working on FL or imbalanced classification problems.\"\n\n**Review 2:**\n\"Federated Learning has shown great promise in large-scale distributed systems, but its performance is often hindered by class imbalance issues. This paper presents a timely solution to this problem with the introduction of FedRE, an algorithm that effectively estimates class distributions without requiring additional client data. The experimental results are impressive, showcasing the superiority of FedRE over other methods in terms of overall and minority class classification accuracy. This work has important implications for real-world FL applications.\"\n\n**Review 3:**\n\"The authors of this paper have made a valuable contribution to the field of Federated Learning by proposing a novel algorithm, FedRE, that tackles the challenging problem of class imbalance. The key innovation lies in the class distribution estimation method, which is both privacy-preserving and effective. The theoretical analysis and experimental results provide strong evidence for the validity of the proposed approach. This work is a significant step forward in addressing the limitations of FL and has the potential to impact a wide range of applications.\"\n\n**Review 4:**\n\"Class imbalance is a pervasive problem in machine learning, and its impact is exacerbated in Federated Learning settings. This paper presents a thoughtful solution to this problem with the development of FedRE, an algorithm that leverages class distribution estimation to improve classification performance. The authors' approach is well-motivated, and the experimental results demonstrate the effectiveness of FedRE in various scenarios. This work is a valuable addition to the FL literature and has important implications for practitioners and researchers alike.\"\n\n**Review 5:**\n\"FedRE, the algorithm proposed in this paper, is a game-changer for Federated Learning applications plagued by class imbalance issues. The authors' innovative class distribution estimation method is a key differentiator, enabling the algorithm to outperform existing methods without compromising client privacy. The paper is well-written, and the experimental results are convincing. This work has the potential to significantly impact the field of FL and is a must-read for anyone interested in this area.\"",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\n\"This paper tackles a crucial issue in Federated Learning (FL) - class imbalance - which has been largely overlooked in the literature. The proposed algorithm, FedRE, is a significant contribution to the field as it addresses the mismatch between local and global class distributions without compromising client privacy. The authors' novel class distribution estimation method is both theoretically sound and experimentally validated, demonstrating improved performance over existing methods. A must-read for anyone working on FL or imbalanced classification problems.\"\n\n**Review 2:**\n\"Federated Learning has shown great promise in large-scale distributed systems, but its performance is often hindered by class imbalance issues. This paper presents a timely solution to this problem with the introduction of FedRE, an algorithm that effectively estimates class distributions without requiring additional client data. The experimental results are impressive, showcasing the superiority of FedRE over other methods in terms of overall and minority class classification accuracy. This work has important implications for real-world FL applications.\"\n\n**Review 3:**\n\"The authors of this paper have made a valuable contribution to the field of Federated Learning by proposing a novel algorithm, FedRE, that tackles the challenging problem of class imbalance. The key innovation lies in the class distribution estimation method, which is both privacy-preserving and effective. The theoretical analysis and experimental results provide strong evidence for the validity of the proposed approach. This work is a significant step forward in addressing the limitations of FL and has the potential to impact a wide range of applications.\"\n\n**Review 4:**\n\"Class imbalance is a pervasive problem in machine learning, and its impact is exacerbated in Federated Learning settings. This paper presents a thoughtful solution to this problem with the development of FedRE, an algorithm that leverages class distribution estimation to improve classification performance. The authors' approach is well-motivated, and the experimental results demonstrate the effectiveness of FedRE in various scenarios. This work is a valuable addition to the FL literature and has important implications for practitioners and researchers alike.\"\n\n**Review 5:**\n\"FedRE, the algorithm proposed in this paper, is a game-changer for Federated Learning applications plagued by class imbalance issues. The authors' innovative class distribution estimation method is a key differentiator, enabling the algorithm to outperform existing methods without compromising client privacy. The paper is well-written, and the experimental results are convincing. This work has the potential to significantly impact the field of FL and is a must-read for anyone interested in this area.\""
      ],
      "sim_rank": 12,
      "sim_decision": "None"
    },
    "CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning": {
      "paper_pk": "f063d064-2368-4c68-b5fc-87557678bed9",
      "title": "CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning",
      "abstract": "Federated Learning (FL) is a setting for training machine learning models in distributed environments where the clients do not share their raw data but instead send model updates to a server. However, model updates can be subject to attacks and leak private information. Differential Privacy (DP) is a leading mitigation strategy which involves adding noise to clipped model updates, trading off performance for strong theoretical privacy guarantees. Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice. In this paper, we aim to achieve a tighter measurement of the model exposure by considering a realistic threat model. We propose a novel method, CANIFE, that uses canaries - carefully crafted samples by a strong adversary to evaluate the empirical privacy of a training round. We apply this attack to vision models trained on CIFAR-10 and CelebA and to language models trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios, we demonstrate that the empirical per-round epsilon obtained with CANIFE is 4 -- 5$\\times$ lower than the theoretical bound.",
      "authors": [
        "Samuel Maddock",
        "Alexandre Sablayrolles",
        "Pierre Stock"
      ],
      "keywords": [
        "Federated Learning",
        "Differential Privacy",
        "Empirical Privacy",
        "Model Auditing",
        "Membership Inference Attack"
      ],
      "real_avg_scores": 7.5,
      "real_all_scores": [
        8,
        8,
        6,
        8
      ],
      "real_confidences": [
        2,
        4,
        4,
        4
      ],
      "real_contents": [],
      "real_rank": 1,
      "real_decision": "Accept: notable-top-25%",
      "sim_avg_scores": 6.0,
      "sim_all_scores": [
        0,
        9,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to Empirical Privacy Measurement**\n\nThe authors of CANIFE propose a novel method for measuring the empirical privacy of federated learning models, which is a crucial step towards understanding the privacy risks associated with model updates. By using carefully crafted canary samples, CANIFE provides a more realistic threat model than traditional differential privacy approaches. The results, which demonstrate a 4-5x reduction in empirical per-round epsilon compared to theoretical bounds, are impressive and have significant implications for the development of more private federated learning systems.\n\n**Review 2: A Step Towards More Realistic Privacy Guarantees**\n\nCANIFE is a timely contribution to the field of federated learning, as it addresses the limitations of traditional differential privacy approaches. By considering a more realistic threat model, the authors are able to provide tighter bounds on the empirical privacy of model updates. The results, which demonstrate the effectiveness of CANIFE on a range of vision and language models, are promising and suggest that this approach could be used to develop more private and practical federated learning systems.\n\n**Review 3: A New Era for Federated Learning Privacy**\n\nThe CANIFE method represents a significant shift in the way we approach privacy measurement in federated learning. By moving away from theoretical bounds and towards empirical measurements, the authors are able to provide a more accurate picture of the privacy risks associated with model updates. The results, which demonstrate the effectiveness of CANIFE in a range of scenarios, are compelling and suggest that this approach could be used to develop more private and trustworthy federated learning systems.\n\n**Review 4: A Promising Approach for Practical Privacy**\n\nCANIFE is a promising approach for measuring the empirical privacy of federated learning models, and its focus on practicality is a welcome change from traditional theoretical approaches. The results, which demonstrate the effectiveness of CANIFE in realistic FL scenarios, are impressive and suggest that this approach could be used to develop more private and practical federated learning systems. However, further work is needed to fully explore the limitations and potential applications of CANIFE.\n\n**Review 5: A Novel Method for Evaluating Model Exposure**\n\nThe CANIFE method is a novel approach for evaluating the model exposure in federated learning, and its use of carefully crafted canary samples is a clever idea. The results, which demonstrate the effectiveness of CANIFE in a range of scenarios, are promising and suggest that this approach could be used to develop more private and trustworthy federated learning systems. However",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to Empirical Privacy Measurement**\n\nThe authors of CANIFE propose a novel method for measuring the empirical privacy of federated learning models, which is a crucial step towards understanding the privacy risks associated with model updates. By using carefully crafted canary samples, CANIFE provides a more realistic threat model than traditional differential privacy approaches. The results, which demonstrate a 4-5x reduction in empirical per-round epsilon compared to theoretical bounds, are impressive and have significant implications for the development of more secure federated learning systems.\n\n**Review 2: A Step Towards More Realistic Privacy Guarantees**\n\nCANIFE is a timely contribution to the field of federated learning, as it addresses the limitations of traditional differential privacy approaches. By considering a more realistic threat model, the authors are able to provide tighter bounds on the empirical privacy of model updates. The results, which demonstrate the effectiveness of CANIFE on a range of vision and language models, are promising and suggest that this approach could be used to develop more secure and private federated learning systems.\n\n**Review 3: A New Era for Federated Learning Privacy**\n\nThe CANIFE method represents a significant shift in the way we approach privacy measurement in federated learning. By moving away from theoretical bounds and towards empirical measurements, the authors are able to provide a more accurate picture of the privacy risks associated with model updates. The results, which demonstrate the effectiveness of CANIFE in a range of scenarios, are compelling and suggest that this approach could be used to develop more secure and private federated learning systems.\n\n**Review 4: A Promising Approach for Real-World Federated Learning**\n\nCANIFE is a promising approach for measuring the empirical privacy of federated learning models in real-world scenarios. By using carefully crafted canary samples, the authors are able to provide a more realistic threat model than traditional differential privacy approaches. The results, which demonstrate the effectiveness of CANIFE on a range of vision and language models, are encouraging and suggest that this approach could be used to develop more secure and private federated learning systems in a variety of domains.\n\n**Review 5: A Novel Method for Evaluating Federated Learning Privacy**\n\nThe authors of CANIFE propose a novel method for evaluating the empirical privacy of federated learning models, which is a crucial step towards understanding the privacy risks associated with model updates. By using canary samples to evaluate the privacy of model updates, CANIFE provides a more realistic and accurate picture of the",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Novel Approach to Empirical Privacy Measurement**\n\nThe authors of CANIFE propose a novel method for measuring the empirical privacy of federated learning models, which is a crucial step towards understanding the privacy risks associated with model updates. By using carefully crafted canary samples, CANIFE provides a more realistic threat model than traditional differential privacy approaches. The results, which demonstrate a 4-5x reduction in empirical per-round epsilon compared to theoretical bounds, are impressive and have significant implications for the development of more secure federated learning systems.\n\n**Review 2: A Step Towards More Realistic Privacy Guarantees**\n\nCANIFE is a timely contribution to the field of federated learning, as it addresses the limitations of traditional differential privacy approaches. By considering a more realistic threat model, the authors are able to provide tighter bounds on model exposure, which is essential for building trust in federated learning systems. The experimental results, which demonstrate the effectiveness of CANIFE on various datasets, are convincing and highlight the potential of this approach for achieving more realistic privacy guarantees.\n\n**Review 3: A Promising Alternative to Differential Privacy**\n\nThe CANIFE method offers a promising alternative to traditional differential privacy approaches, which have been shown to be overly conservative and provide vacuous guarantees in practice. By using canary samples to evaluate the empirical privacy of a training round, CANIFE provides a more practical and effective way to measure model exposure. The results, which demonstrate a significant reduction in empirical per-round epsilon, are encouraging and suggest that CANIFE may be a more effective way to achieve privacy in federated learning systems.\n\n**Review 4: A New Frontier in Federated Learning Privacy**\n\nCANIFE represents a new frontier in federated learning privacy, as it moves beyond traditional differential privacy approaches and considers a more realistic threat model. The use of canary samples to evaluate the empirical privacy of a training round is a clever and effective way to measure model exposure, and the results demonstrate the potential of this approach for achieving more realistic privacy guarantees. This paper is a must-read for anyone interested in the development of more secure and private federated learning systems.\n\n**Review 5: A Significant Contribution to the Field**\n\nThe authors of CANIFE have made a significant contribution to the field of federated learning, as they have developed a novel method for measuring the empirical privacy of model updates. The use of canary samples to evaluate the empirical privacy of a training round is a clever and effective way to measure model exposure"
      ],
      "sim_rank": 6,
      "sim_decision": "None"
    },
    "Harnessing Client Drift with Decoupled Gradient Dissimilarity": {
      "paper_pk": "e54ed8d4-f584-4f0d-8ba9-adcc11949404",
      "title": "Harnessing Client Drift with Decoupled Gradient Dissimilarity",
      "abstract": "The performance of Federated learning (FL) typically suffers from client drift caused by heterogeneous data, where data distributions vary with clients. Recent studies show that the gradient dissimilarity between clients induced by the data distribution discrepancy causes the client drift. Thus, existing methods mainly focus on correcting the gradients. However, it is challenging to identify which client should (or not) be corrected. This challenge raises a series of questions: will the local training, without gradient correction, contribute to the server model's generalization of other clients' distributions? when the generalization contribution holds? how to address the challenge when it fails? To answer these questions, we analyze the generalization contribution of local training and conclude that the generalization contribution of local training is bounded by the conditional Wasserstein distance between clients' distributions. Thus, the key to promote generalization contribution is to leverage similar conditional distributions for local training. As collecting data distribution can cause privacy leakage, we propose decoupling the deep models, i.e., splitting into high-level models and low-level models, for harnessing client drift. Namely, high-level models are trained on shared feature distributions, causing promoted generalization contribution and alleviated gradient dissimilarity. Experimental results demonstrate that FL with decoupled gradient dissimilarity is robust to data heterogeneity.",
      "authors": [
        "Zhenheng TANG",
        "Yonggang Zhang",
        "Shaohuai Shi",
        "Xinmei Tian",
        "Tongliang Liu",
        "Bo Han",
        "Xiaowen Chu"
      ],
      "keywords": [
        "Federated Learning",
        "Deep Learning"
      ],
      "real_avg_scores": 2.0,
      "real_all_scores": [
        1,
        3,
        1,
        3
      ],
      "real_confidences": [
        3,
        5,
        3,
        3
      ],
      "real_contents": [],
      "real_rank": 20,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\nTitle: A Novel Approach to Mitigating Client Drift in Federated Learning\nRating: 5/5\n\nThis paper presents a groundbreaking solution to the long-standing problem of client drift in Federated Learning (FL). By decoupling deep models into high-level and low-level models, the authors successfully harness client drift and promote generalization contribution. The theoretical analysis of the generalization contribution of local training, bounded by the conditional Wasserstein distance between clients' distributions, is particularly insightful. The experimental results demonstrate the robustness of the proposed method to data heterogeneity, making it a significant contribution to the FL community.\n\n**Review 2:**\nTitle: Decoupling Gradient Dissimilarity for Robust Federated Learning\nRating: 4.5/5\n\nThis paper tackles the challenging problem of client drift in FL by proposing a novel decoupling approach. By splitting deep models into high-level and low-level models, the authors effectively alleviate gradient dissimilarity and promote generalization contribution. The theoretical analysis is thorough, and the experimental results are convincing. However, the paper could benefit from a more detailed discussion on the privacy implications of collecting data distribution information. Nevertheless, this work is a significant step forward in addressing the client drift problem in FL.\n\n**Review 3:**\nTitle: A Theoretical Framework for Understanding Client Drift in Federated Learning\nRating: 4/5\n\nThis paper provides a valuable theoretical framework for understanding the client drift problem in FL. The analysis of the generalization contribution of local training, bounded by the conditional Wasserstein distance between clients' distributions, is a significant contribution to the field. The proposed decoupling approach is also an interesting solution to the problem. However, the paper could benefit from more extensive experimental results to demonstrate the effectiveness of the proposed method. Additionally, the discussion on the privacy implications of collecting data distribution information could be more detailed.\n\n**Review 4:**\nTitle: A Promising Solution to Client Drift in Federated Learning\nRating: 4/5\n\nThis paper proposes a novel approach to addressing client drift in FL by decoupling deep models into high-level and low-level models. The idea is promising, and the theoretical analysis is sound. The experimental results demonstrate the effectiveness of the proposed method in promoting generalization contribution and alleviating gradient dissimilarity. However, the paper could benefit from a more detailed comparison with existing methods and a more extensive discussion on the",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\nTitle: A Novel Approach to Mitigating Client Drift in Federated Learning\nRating: 5/5\n\nThis paper presents a groundbreaking solution to the long-standing problem of client drift in Federated Learning (FL). By decoupling deep models into high-level and low-level models, the authors successfully harness client drift and promote generalization contribution. The theoretical analysis of the conditional Wasserstein distance between clients' distributions provides a solid foundation for the proposed method. The experimental results demonstrate the robustness of the approach to data heterogeneity, making it a significant contribution to the FL community.\n\n**Review 2:**\nTitle: Decoupling Gradient Dissimilarity for Robust Federated Learning\nRating: 4.5/5\n\nThis paper tackles the challenging problem of client drift in FL by introducing a novel decoupling approach. By splitting deep models into high-level and low-level models, the authors effectively reduce gradient dissimilarity and promote generalization contribution. The theoretical analysis is thorough, and the experimental results are promising. However, the paper could benefit from a more detailed discussion on the trade-offs between model complexity and generalization performance.\n\n**Review 3:**\nTitle: A Step Towards Understanding Client Drift in Federated Learning\nRating: 4/5\n\nThis paper provides a valuable contribution to the understanding of client drift in FL. The authors' analysis of the generalization contribution of local training and the role of conditional Wasserstein distance is insightful. The proposed decoupling approach is an interesting solution to mitigate client drift. However, the paper could benefit from more extensive experimental evaluations and comparisons with existing methods to further demonstrate the effectiveness of the approach.\n\n**Review 4:**\nTitle: Decoupling Gradient Dissimilarity: A Promising Direction for Federated Learning\nRating: 4.5/5\n\nThis paper presents a promising direction for addressing client drift in FL. The decoupling approach is well-motivated, and the theoretical analysis is sound. The experimental results demonstrate the potential of the approach. However, the paper could benefit from a more detailed discussion on the scalability and applicability of the approach to real-world FL scenarios.\n\n**Review 5:**\nTitle: Harnessing Client Drift with Decoupled Gradient Dissimilarity: A New Perspective\nRating: 5/5\n\nThis paper offers a fresh perspective on addressing client drift in FL. The authors' approach to decoupling gradient diss",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1:**\nTitle: A Novel Approach to Mitigating Client Drift in Federated Learning\nRating: 5/5\n\nThis paper presents a groundbreaking solution to the long-standing problem of client drift in Federated Learning (FL). By decoupling deep models into high-level and low-level models, the authors successfully harness client drift and promote generalization contribution. The theoretical analysis of the conditional Wasserstein distance between clients' distributions provides a solid foundation for the proposed method. The experimental results demonstrate the robustness of the approach to data heterogeneity, making it a significant contribution to the FL community.\n\n**Review 2:**\nTitle: Decoupling Gradient Dissimilarity for Robust Federated Learning\nRating: 4.5/5\n\nThis paper tackles the challenging problem of client drift in FL by introducing a novel decoupling approach. By splitting deep models into high-level and low-level models, the authors effectively reduce gradient dissimilarity and promote generalization contribution. The theoretical analysis is thorough, and the experimental results are promising. However, the paper could benefit from a more detailed discussion on the trade-offs between model complexity and generalization performance.\n\n**Review 3:**\nTitle: A Step Towards Understanding Client Drift in Federated Learning\nRating: 4/5\n\nThis paper provides a valuable contribution to the understanding of client drift in FL. The authors' analysis of the generalization contribution of local training and the role of conditional Wasserstein distance is insightful. The proposed decoupling approach is an interesting solution to mitigate client drift. However, the paper could benefit from more extensive experimental evaluations and comparisons with existing methods to further demonstrate the effectiveness of the approach.\n\n**Review 4:**\nTitle: Decoupling Gradient Dissimilarity: A Promising Direction for Federated Learning\nRating: 4.5/5\n\nThis paper presents a promising direction for addressing client drift in FL. The decoupling approach is well-motivated, and the theoretical analysis is sound. The experimental results demonstrate the potential of the approach. However, the paper could benefit from a more detailed discussion on the scalability and applicability of the approach to real-world FL scenarios.\n\n**Review 5:**\nTitle: Harnessing Client Drift with Decoupled Gradient Dissimilarity: A New Perspective\nRating: 5/5\n\nThis paper offers a fresh perspective on addressing client drift in FL. The authors' approach to decoupling gradient diss"
      ],
      "sim_rank": 20,
      "sim_decision": "None"
    },
    "QuAFL: Federated Averaging Made Asynchronous and Communication-Efficient": {
      "paper_pk": "fd9895a7-1bcb-4ba2-ad09-08cee4af42b1",
      "title": "QuAFL: Federated Averaging Made Asynchronous and Communication-Efficient",
      "abstract": "Federated Learning (FL) is an emerging paradigm to enable the large-scale distributed training of machine learning models, while still providing privacy guarantees. \n  In this work, we address two of the main practical challenges when scaling federated optimization to large node counts: the need for tight synchronization between the central authority and individual computing nodes, and the large communication cost of transmissions between the central server and clients. \n  Specifically, we present a new variant of the classic federated averaging (FedAvg) algorithm, which supports both asynchronous communication and communication compression. We provide a new analysis technique showing that, in spite of these system relaxations, our algorithm can provide similar convergence to FedAvg in some parameter regimes.\n  On the experimental side, we show that our algorithm ensures fast  convergence for standard federated tasks. ",
      "authors": [
        "Hossein Zakerinia",
        "Shayan Talaei",
        "Giorgi Nadiradze",
        "Dan Alistarh"
      ],
      "keywords": [
        "Federated Learning",
        "Distributed optimization",
        "Load Balancing"
      ],
      "real_avg_scores": 4.0,
      "real_all_scores": [
        3,
        3,
        5,
        5
      ],
      "real_confidences": [
        4,
        4,
        4,
        4
      ],
      "real_contents": [],
      "real_rank": 16,
      "real_decision": "Reject",
      "sim_avg_scores": 3.3333333333333335,
      "sim_all_scores": [
        1,
        0,
        9
      ],
      "sim_contents": [
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\nThe authors of QuAFL have made a significant contribution to the field of Federated Learning (FL) by addressing two major practical challenges: synchronization and communication cost. By introducing asynchronous communication and communication compression, QuAFL enables faster and more efficient distributed training of machine learning models while maintaining privacy guarantees. The new analysis technique provided in the paper is a game-changer, demonstrating that QuAFL can achieve similar convergence to FedAvg in certain parameter regimes. Experimental results show that QuAFL ensures fast convergence for standard federated tasks, making it a promising solution for large-scale FL applications.\n\n**Review 2: A Step Towards Scalable Federated Optimization**\n\nQuAFL is a timely and important work that tackles the scalability limitations of traditional federated optimization methods. By relaxing the need for tight synchronization and reducing communication costs, QuAFL enables the training of machine learning models on a large number of nodes. The authors' new analysis technique provides a solid theoretical foundation for the algorithm, and the experimental results demonstrate its effectiveness in practice. QuAFL has the potential to unlock the full potential of FL, enabling the training of complex models on massive datasets while preserving privacy.\n\n**Review 3: Efficient and Asynchronous Federated Averaging**\n\nQuAFL presents a significant improvement over traditional federated averaging (FedAvg) by introducing asynchronous communication and communication compression. These innovations enable faster and more efficient distributed training of machine learning models, making QuAFL an attractive solution for large-scale FL applications. The authors' analysis technique provides a thorough understanding of the algorithm's convergence properties, and the experimental results demonstrate its effectiveness in practice. QuAFL is a must-read for researchers and practitioners working on FL, as it offers a promising solution to the scalability challenges of traditional FL methods.\n\n**Review 4: A New Era for Federated Learning**\n\nQuAFL marks a significant milestone in the development of Federated Learning (FL) by addressing two major bottlenecks: synchronization and communication cost. By introducing asynchronous communication and communication compression, QuAFL enables the training of machine learning models on a large scale while maintaining privacy guarantees. The authors' new analysis technique provides a solid theoretical foundation for the algorithm, and the experimental results demonstrate its effectiveness in practice. QuAFL has the potential to revolutionize the field of FL, enabling the training of complex models on massive datasets while preserving privacy. This work is a must-read for anyone",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\nThe authors of QuAFL have made a significant contribution to the field of Federated Learning (FL) by addressing two major practical challenges: synchronization and communication cost. By introducing asynchronous communication and communication compression, QuAFL enables faster and more efficient distributed training of machine learning models while maintaining privacy guarantees. The new analysis technique provided in the paper is a game-changer, demonstrating that QuAFL can achieve similar convergence to FedAvg in certain parameter regimes. Experimental results show that QuAFL ensures fast convergence for standard federated tasks, making it a promising solution for large-scale FL applications.\n\n**Review 2: A Step Towards Scalable Federated Optimization**\n\nQuAFL is a timely and important contribution to the field of Federated Optimization. By relaxing the need for tight synchronization and reducing communication costs, QuAFL enables the scaling of federated optimization to large node counts. The authors' new analysis technique provides a rigorous understanding of the convergence properties of QuAFL, and experimental results demonstrate its effectiveness in standard federated tasks. As the field of FL continues to grow, QuAFL is an essential step towards making federated optimization more scalable and practical.\n\n**Review 3: Efficient and Asynchronous Federated Averaging**\n\nQuAFL presents a significant improvement over traditional federated averaging (FedAvg) by introducing asynchronous communication and communication compression. This allows for faster and more efficient distributed training of machine learning models, while maintaining the privacy guarantees that are essential in FL. The authors' analysis and experimental results demonstrate the effectiveness of QuAFL in achieving fast convergence for standard federated tasks. As a result, QuAFL is a promising solution for large-scale FL applications where efficiency and scalability are critical.\n\n**Review 4: A New Era in Federated Learning**\n\nQuAFL marks a significant shift in the field of Federated Learning, enabling the large-scale distributed training of machine learning models with unprecedented efficiency and scalability. By addressing the practical challenges of synchronization and communication cost, QuAFL opens up new possibilities for FL applications in areas such as healthcare, finance, and education. The authors' innovative approach and rigorous analysis make QuAFL a landmark paper in the field, and its impact is likely to be felt for years to come.\n\nLet me know if you'd like me to generate more reviews or if you have any specific requests!",
        "Here are some potential reviews based on the input and external data:\n\n**Review 1: A Breakthrough in Federated Learning**\n\nThe authors of QuAFL have made a significant contribution to the field of Federated Learning (FL) by addressing two major practical challenges: synchronization and communication cost. By introducing asynchronous communication and communication compression, QuAFL enables faster and more efficient distributed training of machine learning models while maintaining privacy guarantees. The new analysis technique provided in the paper is a game-changer, demonstrating that QuAFL can achieve similar convergence to FedAvg in certain parameter regimes. The experimental results are impressive, showcasing fast convergence for standard federated tasks. Overall, QuAFL is a breakthrough in FL that has the potential to scale up to large node counts.\n\n**Review 2: Efficient and Scalable Federated Optimization**\n\nQuAFL is a timely and important contribution to the field of Federated Optimization. The authors' proposed variant of FedAvg, which supports asynchronous communication and communication compression, is a significant improvement over existing methods. By relaxing the need for tight synchronization and reducing communication costs, QuAFL enables faster and more efficient distributed training of machine learning models. The paper's analysis technique is thorough and convincing, and the experimental results demonstrate the effectiveness of QuAFL in achieving fast convergence for standard federated tasks. Overall, QuAFL is a must-read for researchers and practitioners working on large-scale FL applications.\n\n**Review 3: A Step Towards Practical Federated Learning**\n\nQuAFL is a crucial step towards making Federated Learning (FL) practical for real-world applications. By addressing the challenges of synchronization and communication cost, the authors have developed a more efficient and scalable variant of FedAvg. The paper's results are promising, demonstrating fast convergence for standard federated tasks. The analysis technique provided in the paper is also noteworthy, offering insights into the convergence properties of QuAFL. While there is still more work to be done to fully realize the potential of FL, QuAFL is an important contribution that brings us closer to achieving this goal.\n\n**Review 4: Advancing the State-of-the-Art in Federated Averaging**\n\nQuAFL is a significant advancement in the field of Federated Averaging, building upon the classic FedAvg algorithm. By incorporating asynchronous communication and communication compression, the authors have developed a more efficient and effective variant of FedAvg. The paper's analysis technique is rigorous and thorough, providing a deeper understanding of the convergence properties of QuAFL. The experimental results are impressive,"
      ],
      "sim_rank": 8,
      "sim_decision": "None"
    }
  }
}