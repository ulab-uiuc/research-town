{
  "d1ce652e-9424-4a28-b960-2562290a3f4f": {
    "pk": "d1ce652e-9424-4a28-b960-2562290a3f4f",
    "name": "Christoph Pohl",
    "bio": " I am a researcher with a focus on developing advanced technologies for various applications, with an emphasis on IT security, robotics, and software development. In the field of IT security, I have worked on the creation of a massive-multi-sensor zero-configuration Intrusion Detection System, specifically designed to detect sophisticated attacks such as timing attacks or brute-force attacks with increased accuracy. I have also developed a tool called BREW (Breakable Web Application) for teaching IT Security, which allows students to identify and exploit vulnerabilities in a controlled environment.\n\nIn the area of robotics, I have contributed to the development of MAkEable, a versatile uni- and multi-manual mobile manipulation framework that facilitates the transfer of capabilities and knowledge across different tasks, environments, and robots. This framework represents mobile manipulation actions through affordances, or interaction possibilities of the robot with its environment, providing a unifying framework for autonomous uni- and multi-manual manipulation of known and unknown objects in various environments.\n\nIn the field of software development, I have worked on the integration of security considerations into the Scrum framework, resulting in the creation of Secure Scrum, a variation of the Scrum framework that emphasizes the development of secure software throughout the entire software development process. I have also developed AutoGPT+P, a system that combines an affordance-based scene representation with a planning system to improve generalizability in task planning and address the challenge of dynamically capturing the initial state of the task planning problem.\n\nOverall, my research is focused on creating advanced technologies that can be applied in practical settings to improve security, robotics, and software development. I am constantly seeking new and innovative ways to solve complex problems and push the boundaries of what is possible in these fields.",
    "collaborators": [
      "Rainer Kartmann",
      "J\u00falia Borr\u00e0s",
      "Fabian Peller-Konrad",
      "Tamim Asfour",
      "Fabian Reister",
      "Hans-Joachim Hof",
      "You Zhou",
      "Kathrin Schlierkamp",
      "No\u00e9mie Jaquier",
      "Jianfeng Gao"
    ],
    "institute": null
  },
  "e9d37a19-a743-4ec4-b03b-448650369f66": {
    "pk": "e9d37a19-a743-4ec4-b03b-448650369f66",
    "name": "Xiaodong He",
    "bio": " I am a researcher with a strong background in robotics and control theory, particularly in the area of nonholonomic mobile robots. I have made significant contributions to the field through my work on trajectory tracking control for nonholonomic mobile robots under arbitrary reference input. My research has shown that it is possible to convert the tracking control of one leader with a single follower into the stabilization of two relative subsystems by designing an adjoint system. I have also extended this single follower tracking controller to the consensus tracking of multiple nonholonomic mobile robots connected by a directed acyclic graph, and established a relationship between consensus control and formation control for multiple nonholonomic mobile robots.\n\nIn addition to my work on robotics, I have also explored the use of character-level encoder-decoder frameworks for question answering with structured knowledge bases. My research in this area has shown that a character-level model can be successfully applied to single-relation question answering, and that it can outperform word-level models in terms of accuracy, number of parameters, and data efficiency.\n\nI am also interested in the simultaneous position and orientation planning of nonholonomic multirobot systems. In this area, I have proposed a dynamic vector field (DVF) based on the rigid body modeling of the robot, which allows for the orientation and position constraints of the robot's final states to be modeled. I have also studied obstacle avoidance and mutual-robot-collision avoidance in the motion planning using the DVF.\n\nIn the area of reinforcement learning, I have addressed the problem of predicting popularity of comments in an online discussion forum. I have augmented the state representation to incorporate the global context represented by discussions on world events available in an external knowledge source, and introduced a two-stage Q-learning framework to search the combinatorial action space while also accounting for redundancy among sub-actions.\n\nI have also worked on the joint learning of distributed representations for images and texts, and proposed a deep multimodal similarity model (DMSM) trained via maximizing global semantic similarity between images and their captions in natural language.\n\nFurthermore, I have focused on the motion planning for mobile robots in 3D, and proposed a planning algorithm involving a novel velocity vector field (VF) over the workspace. I have also designed a composite VF",
    "collaborators": [
      "Qianqian Xia",
      "Zhongkui Liu",
      "Zhongkui Li",
      "Zhiyong Geng",
      "David Golub",
      "Ahmed El-Kishky",
      "Kexin Guo",
      "Roberto Mart\u00edn-Mart\u00edn",
      "Zhisheng Duan",
      "Mari Ostendorf",
      "Shiqi Zhang",
      "Li Guo",
      "Zhiyong Sun",
      "Ji He",
      "Po-Sen Huang",
      "Li Deng",
      "Xiuhui Peng"
    ],
    "institute": null
  },
  "1cc4f0a7-f100-4007-8fde-45f2eec8c254": {
    "pk": "1cc4f0a7-f100-4007-8fde-45f2eec8c254",
    "name": "Zhengxuan Wu",
    "bio": " I am a researcher with a focus on natural language processing, machine learning, and network analysis. I have a particular interest in understanding and disentangling complex emotional narratives, as well as uncovering the mechanisms behind political promotions in autocratic regimes.\n\nIn my work on \"Disentangling Latent Emotions of Word Embeddings on Complex Emotional Narratives,\" I trained a neural network model to predict continuous emotion valence ratings using linguistic inputs from the Stanford Emotional Narratives Dataset. I found that only a few dimensions of the word vectors contributed to expressing emotions in text, and words were clustered on the basis of their emotional polarities. I also performed a linear transformation that projected high dimensional embedded vectors into an emotion space, and showed that in the proposed emotion space, we were able to better disentangle emotions than using raw GloVe vectors alone.\n\nIn my research on \"Uncovering Political Promotion in China: A Network Analysis of Patronage Relationship in Autocracy,\" I used publicly available datasets to implement network analysis techniques and examine drivers of political promotions in the Chinese bureaucracy. I found that careers of politicians are closely associated with their genders, home origins, and positions in the patronage networks.\n\nIn addition to my work on emotional narratives and political promotions, I have also explored the nature of cross-domain transfer in pretrained language models and the factors that make transfer learning hard. I have conducted controlled transfer studies and found that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization.\n\nI am passionate about using quantitative methods to model and make inferences on complex systems, and I am committed to advancing our understanding of these systems through rigorous research and analysis.",
    "collaborators": [
      "Zhi-Xuan Tan",
      "Christopher D. Manning",
      "Piotr Winkielman",
      "Nicholas Dingwall",
      "Bruno Godefroy",
      "Tan Zhi-Xuan",
      "Varsha Suresh",
      "Liu Leqi",
      "Harold Soh",
      "Samuel R. Bowman",
      "Yueyi Jiang",
      "Jason Luo",
      "Christopher Potts",
      "Xiyu Zhang",
      "Desmond C. Ong",
      "Yunfan Jiang"
    ],
    "institute": null
  },
  "b3b77ea1-7f5c-4fd0-a39b-5b5c2ad949bd": {
    "pk": "b3b77ea1-7f5c-4fd0-a39b-5b5c2ad949bd",
    "name": "Tim Dettmers",
    "bio": " I am a researcher focused on developing efficient methods for parallelizing deep learning models, with a particular emphasis on convolutional networks and transformers. My work has shown that 8-bit approximation algorithms can provide significant speedups in data transfer for both model and data parallelism, without sacrificing predictive performance. I have also demonstrated the effectiveness of sparse learning, which involves training deep neural networks with sparse weights throughout the training process while still achieving dense performance levels. I developed sparse momentum, an algorithm that uses exponentially smoothed gradients to identify and redistribute pruned weights across layers, resulting in state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet.\n\nIn addition, I have studied the trade-off between accuracy and memory footprint in quantization methods, and developed inference scaling laws for Large Language Models (LLMs) to determine the bit-precision and model size that maximizes zero-shot performance. I have found that 4-bit precision is almost universally optimal for total model bits and zero-shot accuracy.\n\nI have also developed 8-bit optimizers that use block-wise quantization to maintain 32-bit performance levels with a small fraction of the memory footprint, and a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cuts the memory needed for inference by half while retaining full precision performance.\n\nMy most recent work includes QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. I have also introduced Convolutional 2D Knowledge Graph Embeddings, which is a multi-layer convolutional network model for link prediction that achieves state-of-the-art results for several established datasets.\n\nI am committed to making my research accessible and open-source, and have released software for Int8 matrix multiplication for transformers, 8-bit optimizers, and QLoRA. I am also currently working on a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers and a communication-efficient algorithm for embarrassingly parallel training of large language models.",
    "collaborators": [
      "Luke Zettlemoyer",
      "Denis Yarats",
      "Namit Katariya",
      "Younes Belkada",
      "Terra Blevins",
      "Yann Dauphin",
      "Luheng He",
      "Anitha Kannan",
      "Artidoro Pagnoni",
      "Kenton Lee",
      "Angela Fan",
      "Manish Chablani",
      "Xavier Amatriain",
      "Myle Ott",
      "Sam Shleifer",
      "Michael Petrochuk",
      "Mike Lewis"
    ],
    "institute": null
  },
  "9902e548-f3ad-4b8d-ae99-a9becc6efb22": {
    "pk": "9902e548-f3ad-4b8d-ae99-a9becc6efb22",
    "name": "Julie Tibshirani",
    "bio": " I am a researcher with a focus on developing and improving statistical models and methods. In my work, I aim to create models that are not only accurate and efficient but also robust to potential issues such as noisy data or outlier observations.\n\nOne area I have contributed to is robust statistical learning, specifically through the development of a robust extension of logistic regression that incorporates the possibility of mislabeling directly into the objective. This model, which I call \"Robust Logistic Regression using Shift Parameters,\" can be trained using similar methods as traditional logistic regression and retains its efficiency on high-dimensional datasets. I have demonstrated through named entity recognition experiments that this approach can provide significant improvements over the standard model when annotation errors are present.\n\nI have also worked on developing methods for non-parametric statistical estimation using random forests. In particular, I have proposed \"Generalized Random Forests,\" a method that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. This method uses an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest, rather than classical kernel weighting functions that are prone to a strong curse of dimensionality. I have developed a large sample theory for this method, showing that our estimates are consistent and asymptotically Gaussian, and have provided an estimator for their asymptotic variance that enables valid confidence intervals. I have used this approach to develop new methods for several statistical tasks, including non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables.\n\nIn addition to these contributions, I have also explored the use of random forests for non-parametric regression, specifically through the development of \"Local Linear Forests.\" This method pairs the forest kernel with a local linear regression adjustment to better capture smoothness and improve asymptotic rates of convergence for random forests with smooth signals. I have proven a central limit theorem valid under regularity conditions on the forest and smoothness constraints, and have proposed a computationally efficient construction for confidence intervals. I have also discussed the merits of local regression adjustments for heterogeneous treatment effect estimation and have given an example on a dataset exploring the effect word choice has on attitudes to the social safety net.\n\nOverall, my work as a researcher is focused on developing and improving statistical models and methods, with a particular emphasis on creating models",
    "collaborators": [
      "Guido Imbens",
      "Sanath Kumar Krishnamurthy",
      "Christopher D. Manning",
      "Zhaonan Qu",
      "Rina Friedberg",
      "Zhengxuan Wu",
      "Mohsen Bayati",
      "William Fithian",
      "Samuel R. Bowman",
      "Stefan Wager",
      "Guenther Walther",
      "Susan Athey",
      "David A. Hirshberg",
      "Christopher Potts",
      "Timothy Dozat",
      "Julie Josse"
    ],
    "institute": null
  },
  "fa0e59ea-5e3d-45c8-8114-bfdef9fcff51": {
    "pk": "fa0e59ea-5e3d-45c8-8114-bfdef9fcff51",
    "name": "Danqi Chen",
    "bio": " I am a researcher in the field of natural language processing, with a particular focus on information extraction, word sense disambiguation, and controllable text generation. I have recently been exploring the use of pipelined approaches for entity and relation extraction, and have found that building on independent encoders and fusing entity information early in the relation model can lead to significant improvements in relation F1 scores. I am also interested in addressing the challenge of data imbalance in word sense disambiguation, and have proposed a non-parametric few-shot learning approach called MetricWSD that transfers knowledge from high-frequency words to infrequent ones.\n\nIn addition to my work on information extraction and word sense disambiguation, I have also been investigating the use of language models for controllable text generation with constraints specified in natural language. I have created a challenging benchmark called Cognac that provides a topic with example text and a constraint on text to be avoided, and have proposed a solution called CognacGen that leverages a language model's own internal knowledge to guide generation. I have found that CognacGen can successfully generalize to unseen instructions and outperform competitive baselines in generating constraint-conforming text.\n\nI am also interested in understanding the mechanisms behind in-context learning in large language models, and have characterized two ways through which it leverages demonstrations: task recognition and task learning. I have designed controlled experiments to disentangle the roles of these two forces in in-context learning, and have found that models can achieve non-trivial performance with only task recognition, while task learning consistently improves with more demonstrations in context.\n\nIn my work on question answering, I have shown that it is possible to convert tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible solutions, and have developed a hard EM learning scheme that computes gradients relative to the most likely solution at each update. I have found that this approach significantly outperforms previous methods on six QA tasks, achieving the state-of-the-art on five of them.\n\nOverall, my research is focused on developing and improving methods for information extraction, word sense disambiguation, controllable text generation, and question answering, with a focus on addressing challenges such as data imbalance and the limitations of current dense retrieval models.",
    "collaborators": [
      "Luke Zettlemoyer",
      "Minjoon Seo",
      "Hannaneh Hajishirzi",
      "Victor Zhong",
      "Sourav Chatterjee",
      "Howard Chen",
      "Mengzhou Xia",
      "Adam Yala",
      "Regina Barzilay",
      "Yilun Du",
      "Ameet Deshpande",
      "Karthik Narasimhan",
      "Jacqueline He",
      "Sewon Min",
      "Tommi Jaakkola"
    ],
    "institute": null
  },
  "03abae7a-f582-4482-ae3f-c194bb0a2e79": {
    "pk": "03abae7a-f582-4482-ae3f-c194bb0a2e79",
    "name": "Rupesh Srivastava",
    "bio": " I am a researcher focused on machine learning, specifically in the areas of robust classification, distributed representations, and multimodal similarity models. I enjoy unifying seemingly unrelated concepts and developing theoretical frameworks to improve existing methods.\n\nIn my recent work, I have been exploring robust classification techniques, such as Provable Robust Classification via Learned Smoothed Densities. This research combines provable robustness with randomized smoothing, where I approximate the energy function of a noisy measurement using a neural network. I introduce empirical Bayes smoothed classifiers and demonstrate their ability to improve robustness above the margin for two-class linear classifiers. I have also shown that, by learning empirical Bayes smoothed classifiers with adversarial training, I can achieve state-of-the-art provable robust accuracies on MNIST.\n\nAnother area of interest is joint learning of distributed representations for images and texts. I have provided additional details on the deep multimodal similarity model (DMSM), which is trained to maximize global semantic similarity between images and their captions in natural language. The model captures various visual concepts and cues from the Microsoft COCO database, a large set of images and corresponding captions.\n\nI am also passionate about generating image descriptions using visual detectors, language models, and multimodal similarity models. I have developed a system that learns directly from a dataset of image captions, using multiple instance learning to train visual detectors for words that commonly occur in captions. The word detector outputs serve as conditional inputs to a maximum-entropy language model, which learns from a set of over 400,000 image descriptions to capture the statistics of word usage. I have demonstrated that this system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%.\n\nIn the future, I plan to continue working on robust classification methods, as well as exploring new applications for multimodal similarity models and distributed representations. I am particularly interested in addressing the challenges of randomized smoothing in high dimensions and improving the performance of image description systems.",
    "collaborators": [
      "Aapo Hyvarinen",
      "Rupesh Kumar Srivastava",
      "Tamim Asfour",
      "Xiaodong He",
      "Christoph Pohl",
      "Mari Ostendorf",
      "Xiaodong Liu",
      "Kevin Duh",
      "Francis Bach",
      "Zhiyong Geng",
      "Yichen Cai",
      "T. Senthil",
      "Li Deng",
      "Patrick A. Lee",
      "Zhongkui Li",
      "David Golub",
      "Ji He",
      "Jianfeng Gao",
      "Saeed Saremi"
    ],
    "institute": null
  },
  "476303db-d72a-4a50-8b47-aaec78a31638": {
    "pk": "476303db-d72a-4a50-8b47-aaec78a31638",
    "name": "Luheng He",
    "bio": " I am a researcher in the field of natural language processing, with a particular focus on developing high-performing models for tasks such as semantic role labeling, coreference resolution, and information extraction. My work involves proposing novel approaches to these tasks and demonstrating their effectiveness through experiments and comparisons to existing state-of-the-art models.\n\nIn my research on semantic role labeling, I have explored the use of end-to-end models for jointly predicting predicates, arguments, and the relationships between them. These models make independent decisions about the relationships between word spans and learn contextualized span representations that provide rich, shared input features for each decision. I have shown that this approach sets a new state of the art on the PropBank SRL task without using gold predicates.\n\nIn the area of coreference resolution, I have introduced a fully differentiable approximation to higher-order inference that uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. I have also developed an end-to-end coreference resolution model that significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector.\n\nIn addition to these tasks, I have also worked on developing a large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations and the first high-quality QA-SRL parser. I have presented neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship.\n\nI am interested in developing models that can perform lightweight numerical reasoning, and have shown that a BERT-based reading comprehension model can be augmented with a predefined set of executable 'programs' to achieve this. This approach has resulted in a 33% absolute improvement on the recent Discrete Reasoning Over Passages (DROP) dataset.\n\nI am also interested in the problem of paraphrase identification and have introduced the PAWS (Paraphrase Adversaries from Word Scrambling) dataset, which includes 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. This dataset has been shown to be",
    "collaborators": [
      "Luke Zettlemoyer",
      "Terra Blevins",
      "Julian Michael",
      "Kristina Toutanova",
      "Kenton Lee",
      "Mari Ostendorf",
      "Tim Dettmers",
      "Omer Levy",
      "Michael Petrochuk",
      "Mike Lewis",
      "Yi Luan"
    ],
    "institute": null
  },
  "591acd67-0566-4c16-8c81-e46aef78b642": {
    "pk": "591acd67-0566-4c16-8c81-e46aef78b642",
    "name": "Yichen Cai",
    "bio": " I am a researcher with a focus on developing innovative solutions to complex problems in the fields of power systems and robotics. In my recent work on power systems, I have been exploring rapid scalable distributed methods for solving the AC power flow problem. I have proposed a new variant of the ALADIN algorithm, which is specifically designed to solve this type of problem by reformulating it as a zero-residual least-squares problem with consensus constraints. This new variant is then solved using a Gauss-Newton based inexact ALADIN algorithm. I have also implemented this algorithm in an open-source platform called rapidPF+, which has shown great potential in terms of computing efficiency for power systems of various dimensions.\n\nIn the field of robotics, I have been working on developing methods for task-oriented object grasping and rearrangement. I have proposed a novel object representation called Multi-feature Implicit Model (MIMO), which encodes multiple spatial features between a point and an object in an implicit neural field. By training this model on multiple features, it is able to consistently embed object shapes in different aspects, which improves its performance in object shape reconstruction from partial observation, shape similarity measure, and modeling spatial relations between objects. Based on MIMO, I have also proposed a framework for learning task-oriented object grasping and rearrangement from single or multiple human demonstration videos. My evaluations in simulation have shown that this approach outperforms the state-of-the-art methods for multi- and single-view observations. I have also demonstrated the efficacy of this approach in real-world experiments for one- and few-shot imitation learning of manipulation tasks.\n\nIn summary, my research is focused on developing innovative solutions to complex problems in the fields of power systems and robotics. I have proposed a new method for solving the distributed AC power flow problem using a Gauss-Newton based inexact ALADIN algorithm and have implemented it in an open-source platform. In robotics, I have proposed a novel object representation and a framework for learning task-oriented object grasping and rearrangement from human demonstration videos. My evaluations have shown that these methods outperform the state-of-the-art in both simulation and real-world experiments.",
    "collaborators": [
      "Colin N. Jones",
      "Boris Houska",
      "Tillmann M\u00fchlpfordt",
      "Philipp Sauerteig",
      "Christoph Pohl",
      "Rebecca Bauer",
      "Tobias Kleinert",
      "Veit Hagenmeyer",
      "Timm Faulwasser",
      "Lutz Gr\u00f6ll",
      "Yuning Jiang",
      "Juraj Oravec",
      "Karl Worthmann",
      "Tessina H. Scholl",
      "Jiayuan Mao",
      "Xinliang Dai",
      "Jianfeng Gao"
    ],
    "institute": null
  },
  "25d55225-5b54-48f5-af2a-e1d527ce58d7": {
    "pk": "25d55225-5b54-48f5-af2a-e1d527ce58d7",
    "name": "Jianfeng Gao",
    "bio": " I am a researcher with a focus on developing intelligent systems, particularly in the field of robotics and natural language processing. My work involves creating models and frameworks that enable robots to perform complex tasks, such as task-oriented object grasping and rearrangement, by learning from human demonstrations. I have proposed the Multi-feature Implicit Model (MIMO), a novel object representation that encodes multiple spatial features between a point and an object in an implicit neural field, which has been shown to improve performance in object shape reconstruction and spatial relation modeling.\n\nIn addition to my work in robotics, I am also interested in multimodal representation learning, specifically for images and texts. I have worked on the deep multimodal similarity model (DMSM), which is trained to maximize global semantic similarity between images and their captions in natural language. The model is able to capture the combination of various visual concepts and cues.\n\nI have also explored the use of stochastic answer networks (SAN) for natural language inference, which maintains a state and iteratively refines its predictions, achieving state-of-the-art results on three benchmarks.\n\nMy research also includes conversational AI, where I have surveyed neural approaches and grouped conversational systems into three categories: question answering agents, task-oriented dialogue agents, and chatbots. I have discussed the progress and challenges faced in each category.\n\nI have also worked on data augmentation for spoken language understanding (SLU) using pretrained language models to boost the variability and accuracy of generated utterances, and investigated solutions for two semi-supervised learning scenarios of data scarcity in SLU.\n\nIn the field of NLG evaluation, I have surveyed evaluation methods and grouped them into three categories: human-centric evaluation metrics, automatic metrics that require no training, and machine-learned metrics. I have discussed the progress and challenges faced in each category, with a focus on the evaluation of recently proposed NLG tasks and neural NLG models.\n\nI have also worked on online classification problems with explicit regularization, proposing a voted dual averaging (RDA) method that employs the update rule of the RDA method only on the subsequence of training examples where a classification error is made.\n\nI am currently working on challenges in building intelligent open-domain dialog systems, specifically addressing the challenges of semantics",
    "collaborators": [
      "Rainer Kartmann",
      "J\u00falia Borr\u00e0s",
      "Fabian Peller-Konrad",
      "Yichen Cai",
      "Tamim Asfour",
      "Christoph Pohl",
      "Veit Hagenmeyer",
      "Fabian Reister",
      "Xiaodong Liu",
      "Yuning Jiang",
      "Hans-Joachim Hof",
      "Kevin Duh",
      "You Zhou",
      "Kathrin Schlierkamp",
      "No\u00e9mie Jaquier",
      "Xinliang Dai"
    ],
    "institute": null
  },
  "82be85a3-894e-4658-8a2b-6dbfc0344b65": {
    "pk": "82be85a3-894e-4658-8a2b-6dbfc0344b65",
    "name": "Timothy Dozat",
    "bio": " I am a researcher with a focus on natural language processing, particularly in the area of semantic dependency parsing and form document information extraction. I am interested in developing more accurate and efficient methods for understanding the meaning and structure of text.\n\nIn my work on semantic dependency parsing, I have extended an LSTM-based syntactic parser to train on and generate graph structures that capture between-word relationships more closely related to the meaning of a sentence. This system achieves state-of-the-art performance, beating the previous complex state-of-the-art system by a significant margin. I have also shown that adding linguistically richer input representations can further improve the performance of the system.\n\nIn the area of form document information extraction, I have proposed FormNet, a structure-aware sequence model that mitigates the challenges of correctly serializing tokens in form-like documents. FormNet uses rich attention and graph convolutions to leverage the spatial relationship between tokens and recover local syntactic information that may have been lost during serialization. This model has achieved state-of-the-art performance on several benchmarks with a more compact model size and less pre-training data.\n\nI have also introduced FormNetV2, which uses multimodal graph contrastive learning to unify self-supervised pre-training for all modalities in one loss. This approach maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. FormNetV2 has established new state-of-the-art performance on several benchmarks with a more compact model size.\n\nIn addition to developing new methods, I am also interested in evaluating the performance of natural language processing systems and have proposed methods for assessing the dialect robustness and awareness of evaluation metrics. I have also introduced a training schema, NANO, which introduces regional and language information to the pretraining process of a metric to improve dialect robustness.\n\nOverall, my research is focused on developing more accurate and efficient methods for understanding the meaning and structure of text, with a particular focus on semantic dependency parsing and form document information extraction. I am also interested in evaluating the performance of natural language processing systems and proposing methods for improving the robustness and fairness of evaluation metrics.",
    "collaborators": [
      "Chen-Yu Lee",
      "Christopher D. Manning",
      "Dan Garrette",
      "Ekaterina Shutova",
      "Zhengxuan Wu",
      "Julie Tibshirani",
      "Hannah Alpert-Abrams",
      "Rochelle Choenni",
      "Chun-Liang Li",
      "Samuel R. Bowman",
      "Christopher Potts",
      "Vincent Perot",
      "Maria Ryskina",
      "Tal Linzen"
    ],
    "institute": null
  },
  "cd7671ec-16f3-4c42-83c6-fcb4be8cd8d3": {
    "pk": "cd7671ec-16f3-4c42-83c6-fcb4be8cd8d3",
    "name": "Michael Petrochuk",
    "bio": " Hello, I am a researcher who has recently been working on the SimpleQuestions dataset, which is a popular benchmark for studying single-relation factoid questions. My latest paper presents new evidence that this benchmark can be nearly solved using standard methods.\n\nFirst, I discovered that ambiguity in the data limits the performance on this benchmark to 83.4%. This means that there are often multiple answers that cannot be distinguished from the linguistic signal alone. Despite this limitation, I was able to introduce a new baseline that sets a new state-of-the-art performance level at 78.1% accuracy. This is a significant improvement over previous methods, and it was achieved using only standard techniques.\n\nIn addition to these findings, I also conducted an empirical analysis that showed the upperbound is loose. Specifically, I found that roughly a third of the remaining errors are not resolvable from the linguistic signal. This suggests that there is still room for improvement in this area, and that further research is needed to fully solve the SimpleQuestions dataset.\n\nOverall, I am excited about the progress that has been made in this area, and I believe that my work has helped to advance our understanding of how to approach single-relation factoid questions. I am looking forward to continuing my research in this field, and to seeing what new discoveries and insights emerge in the coming years.",
    "collaborators": [
      "Luke Zettlemoyer",
      "Terra Blevins",
      "Hila Gonen",
      "Younes Belkada",
      "Luheng He",
      "Kenton Lee",
      "Tim Dettmers",
      "Haoqiang Kang",
      "Artidoro Pagnoni",
      "Omer Levy",
      "Sam Shleifer",
      "Mandar Joshi",
      "Mike Lewis"
    ],
    "institute": null
  },
  "84e53174-ed4d-4513-a44f-722d9a72e7c2": {
    "pk": "84e53174-ed4d-4513-a44f-722d9a72e7c2",
    "name": "Luke Zettlemoyer",
    "bio": " I am a researcher with a focus on natural language processing and machine learning. In my work, I strive to improve the performance of language models by incorporating morphological supervision and developing sparse learning algorithms.\n\nIn a recent project, I demonstrated the benefits of incorporating morphological supervision into character language models (CLMs) through multitasking. I found that this addition improved bits-per-character (BPC) performance across 24 languages, even when the morphology data and language modeling data were disjoint. I also showed that inflected words benefit more from explicitly modeling morphology than uninflected words, and that morphological supervision improves performance even as the amount of language modeling data grows. I then transferred morphological supervision across languages to improve language modeling performance in the low-resource setting.\n\nIn another project, I explored the capabilities of English pretrained language models and found that they often contain significant amounts of non-English text. I demonstrated that even small percentages of non-English data facilitate cross-lingual transfer for models trained on them, with target language performance strongly correlated to the amount of in-language data seen during pretraining.\n\nI am also interested in developing more efficient and effective language models. I have proposed a bi-encoder model for Word Sense Disambiguation (WSD) that independently embeds the target word with its surrounding context and the dictionary definition, or gloss, of each sense. This system outperforms previous state-of-the-art models on English all-words WSD, leading to a 31.1% error reduction on less frequent senses over prior work.\n\nIn addition, I have developed sparse learning algorithms that allow for accelerated training of deep neural networks while maintaining sparse weights throughout training. I have demonstrated state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, and have shown that these algorithms can decrease the mean error by a relative 8%, 15%, and 6% compared to other sparse algorithms.\n\nI am committed to making my research accessible and reproducible, and I release source code for my models and experiments on GitHub. I am excited to continue exploring the capabilities and potential of language models and machine learning algorithms.",
    "collaborators": [
      "Terra Blevins",
      "Hila Gonen",
      "Younes Belkada",
      "Luheng He",
      "Kristina Toutanova",
      "Kenton Lee",
      "Tim Dettmers",
      "Haoqiang Kang",
      "Artidoro Pagnoni",
      "Omer Levy",
      "Sam Shleifer",
      "Michael Petrochuk",
      "Mandar Joshi",
      "Mike Lewis"
    ],
    "institute": null
  },
  "de79af08-a677-4057-b1e7-cd95a03b33ee": {
    "pk": "de79af08-a677-4057-b1e7-cd95a03b33ee",
    "name": "Noah A. Smith",
    "bio": " I am a researcher in the field of natural language processing (NLP), a branch of artificial intelligence that deals with the interaction between computers and humans through natural language. My work involves developing methods and algorithms for computers to understand, interpret, and generate human language in a valuable way. I have contributed to various areas within NLP, including word representations, parsing, alignment, language modeling, and cross-lingual transfer.\n\nIn my research, I focus on creating contextual word representations, also known as word embeddings, which are vector representations of words that capture their meanings and the context they appear in. I have written an introduction to contextual word representations, where I explain their significance, the problems they solve, their evolution, and the open questions surrounding them. I believe that contextual word vectors are the most recent advancement in this area, offering a more nuanced understanding of words in different contexts.\n\nI have also worked on evaluating NLP models, particularly unsupervised NLP models, by proposing an adversarial evaluation framework. This framework makes explicit certain adversarial roles among researchers, encouraging earlier consideration of error analysis and characterizing model successes and failures.\n\nIn the area of parsing, I have explored the tradeoff between accuracy and speed in obtaining Stanford dependencies, a widely desired representation of natural language sentences. I have revisited this question in light of the evolving definition of Stanford dependencies and developments in statistical dependency parsing algorithms. I have also presented a hybrid parser and neural language model called PaLM, which outperforms strong baselines in language modeling.\n\nFurthermore, I have introduced new alignment methods for discriminative book summarization, specifically targeted to the challenges presented by aligning the full text of a book with a human-written summary. I have demonstrated gains on an extractive book summarization task using hidden Markov models.\n\nMy recent work involves producing multilingual contextual word representations by training a single language model on text from multiple languages. This method combines the advantages of contextual word representations with those of multilingual representation learning, providing further evidence for the benefits of polyglot learning.\n\nIn conclusion, my research interests lie in developing NLP methods and algorithms that enable computers to understand and generate human language more accurately and efficiently. I am particularly interested in contextual word representations, parsing, alignment, language modeling, and cross-lingual transfer, and I continually strive to push",
    "collaborators": [],
    "institute": null
  },
  "6db02a80-82a3-4445-b639-589875910dbe": {
    "pk": "6db02a80-82a3-4445-b639-589875910dbe",
    "name": "Philipp Koehn",
    "bio": " I am a researcher specializing in the field of natural language processing, with a particular focus on neural machine translation. I have written a comprehensive chapter on neural machine translation for a textbook, covering topics such as neural networks, computation graphs, and attentional sequence-to-sequence models. I have also identified and explored six challenges for neural machine translation, including domain mismatch, training data amount, rare words, long sentences, word alignment, and beam search.\n\nIn addition to my work on neural machine translation, I have also researched methods for compound splitting in NLP applications. I have developed and evaluated methods for learning splitting rules from monolingual and parallel corpora, and have shown that these methods can improve the performance of statistical machine translation systems.\n\nI have also studied the impact of various types of noise on neural machine translation systems, and have found that neural models are generally more susceptible to noise than statistical models. I have also shown that fine-tuning language models with a contrastive objective can help retrieve high-quality bitexts for low-resource languages.\n\nIn the area of document alignment, I have presented a method that incorporates sentence order information in both candidate generation and re-scoring, resulting in substantial reductions in error and improvements in downstream machine translation performance.\n\nI have also worked on improving the parallelizability of Stack Long Short-Term Memory (StackLSTM) for GPU training, and have conducted a comprehensive evaluation of saliency methods for neural language models.\n\nMost recently, I have investigated a contextual embedding alignment approach for zero-shot cross-lingual transfer tasks, and have shown that it significantly outperforms previous methods using multilingual embeddings. I have also explored a context-aware and dictionary-free mapping approach for building a shared semantic space using parallel corpora, and have shown that it provides a higher degree of isomorphism on the bilingual dictionary induction task. I have also revealed and addressed the natural properties of contextual embeddings, such as anisotropy and anisometry, to improve the quality of the mapping.",
    "collaborators": [
      "Weiting Tan",
      "Christopher Chu",
      "Kevin Knight",
      "Hainan Xu",
      "Marcin Junczys-Dowmunt",
      "Vasileios Hatzivassiloglou",
      "Adithya Renduchintala",
      "Kevin Duh",
      "Barret Zoph",
      "Shuoyang Ding",
      "Maria Ryskina",
      "Rebecca Knowles",
      "Huda Khayrallah"
    ],
    "institute": null
  },
  "8997eaa8-0ec8-4545-a927-1031413aafa4": {
    "pk": "8997eaa8-0ec8-4545-a927-1031413aafa4",
    "name": "Yejin Choi",
    "bio": " I am a researcher with a focus on natural language processing, particularly in extracting and inferring knowledge from unstructured text. I am interested in the challenges of relative physical knowledge acquisition, such as the fact that people rarely explicitly state trivial everyday knowledge, like \"My house is bigger than me.\" However, I have developed an approach to infer this relative physical knowledge along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. I have also explored large-scale zero-shot activity recognition by modeling the visual and linguistic attributes of action verbs, using these attributes as an internal mapping between visual and textual representations to reason about previously unseen actions.\n\nIn addition, I have introduced a new entity typing task, where given a sentence with an entity mention, the goal is to predict a set of free-form phrases that describe appropriate types for the target entity. I have shown that these ultra-fine types can be crowd-sourced and introduced new evaluation sets that are much more diverse and fine-grained than existing benchmarks. I have presented a model that can predict open types and is trained using a multitask objective that pools new head-word supervision with prior supervision from entity linking.\n\nI am also interested in efficiently adapting pretrained transformer language models as text summarizers, and I have proposed two solutions for this: source embeddings and domain-adaptive training. I have tested these solutions on three abstractive summarization datasets, achieving new state-of-the-art performance on two of them.\n\nFurthermore, I have introduced connotation frames as a representation formalism to organize rich dimensions of connotation using typed relations. I have investigated the feasibility of obtaining connotative labels through crowdsourcing experiments and presented models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations.\n\nI am also interested in the role of size in visual reasoning tasks and the impact of the information about sizes of objects in AI. I have introduced a method to automatically infer object sizes, leveraging visual and textual information from the web. I have also presented deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization.\n\nOverall, my research focuses on extracting and inferring knowledge from unstructured text",
    "collaborators": [
      "Mete Ismayilzada",
      "Tarik Arici",
      "Antoine Bosselut",
      "Ali Farhadi",
      "Xiaodong He",
      "Andrew Hoang",
      "Roozbeh Mottaghi",
      "Elizabeth Clark",
      "Yonatan Bisk",
      "Asli Celikyilmaz",
      "Ari Holtzman",
      "Jianfeng Gao",
      "Eunsol Choi",
      "Maxwell Forbes",
      "Rowan Zellers"
    ],
    "institute": null
  },
  "1048d16b-7bf1-439d-b03f-b39d4a1220cf": {
    "pk": "1048d16b-7bf1-439d-b03f-b39d4a1220cf",
    "name": "Christopher Potts",
    "bio": " I am a researcher with a strong interest in natural language processing, machine learning, and linguistic theory. In my work, I frequently explore the intersection of these fields and seek to develop new methods and techniques that can improve our ability to understand and analyze language data.\n\nOne area I have focused on recently is the use of deep learning methods, such as Probabilistic Soft Logic and the GloVe representation learning model, to better understand and analyze complex linguistic structures and relationships. For example, in one project, I extended the GloVe model to learn domain-specialized representations that can lead to faster learning and better results on a variety of tasks. In another project, I used Probabilistic Soft Logic to model drug-disease relations using data from FDA drug labels and health knowledge graphs, demonstrating the superiority of this approach over text-only and relation-only variants.\n\nIn addition to my work on deep learning, I am also interested in the design and evaluation of compositional generalization benchmarks for semantic parsing. In a recent study, I argued that the COGS benchmark, which seeks to assess whether models can accurately compute meanings for novel sentences, may be influenced by semantically irrelevant details of the chosen logical forms. I showed that by converting these logical forms to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, even baseline models were able to make progress on the task. Based on these findings, I proposed a modified version of COGS, called ReCOGS, that comes closer to assessing the target semantic capabilities while remaining very challenging.\n\nI am also interested in the development of systems for condescension detection in context, as condescending language use can be harmful and divisive. In a recent project, I presented TalkDown, a new labeled dataset of condescending linguistic acts in context, and showed that extending a language-only model with representations of the discourse can improve performance. I also used the model to estimate condescension rates in various online communities and related these differences to differing community norms.\n\nOverall, my research is driven by a desire to better understand and analyze language data using machine learning and linguistic theory. I am constantly seeking new and innovative ways to approach these problems, and I am excited to continue exploring the many possibilities that exist in this field.",
    "collaborators": [
      "Christopher D. Manning",
      "Yacine Jernite",
      "Zhengxuan Wu",
      "Julie Tibshirani",
      "Nicholas Dingwall",
      "Bruno Godefroy",
      "Yifeng Tao",
      "George E. Dahl",
      "Samuel R. Bowman",
      "Guillaume Genthial",
      "Timothy Dozat",
      "David Sontag"
    ],
    "institute": null
  },
  "a53d2ebf-0caa-4122-a681-fd5665408e3d": {
    "pk": "a53d2ebf-0caa-4122-a681-fd5665408e3d",
    "name": "Kenton Lee",
    "bio": " I am a researcher in the field of natural language processing, with a focus on developing and improving models for various tasks such as coreference resolution, parsing, question answering, and semantic role labeling.\n\nIn my work on higher-order coreference resolution, I introduced a fully differentiable approximation to higher-order inference for this task. My approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations, enabling the model to softly consider multiple hops in the predicted clusters. I also proposed a coarse-to-fine approach to alleviate the computational cost of this iterative process, incorporating a less accurate but more efficient bilinear factor to enable more aggressive pruning without hurting accuracy. This model significantly improves accuracy on the English OntoNotes benchmark while being far more computationally efficient compared to the existing state-of-the-art span-ranking approach.\n\nIn the area of parsing, I introduced the first global recursive neural parsing model with optimality guarantees during decoding. I gave up dynamic programs and searched directly in the space of all possible subtrees, which is exponentially large in the sentence length. I showed that it is possible to learn an efficient A* parser in this space. I augmented existing parsing models with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. This approach improved state-of-the-art accuracy by 0.4 F1 in CCG parsing and found the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.\n\nIn the field of question answering, I showed that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. I pre-trained the retriever with an Inverse Cloze Task and evaluated on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. However, on datasets where a user is genuinely seeking an answer, I showed that learned retrieval is crucial, outperforming BM2",
    "collaborators": [
      "Luke Zettlemoyer",
      "Uri Shaham",
      "Omri Keren",
      "Lior Vassertail",
      "Terra Blevins",
      "Michael Petrochuk",
      "Julian Michael",
      "Luheng He",
      "Kristina Toutanova",
      "Avia Efrat",
      "Yoav Goldberg",
      "Tim Dettmers",
      "Mari Ostendorf",
      "Omer Levy",
      "Mike Lewis",
      "Yi Luan"
    ],
    "institute": null
  },
  "b4fbd90d-1b89-481a-a425-46cde292de9e": {
    "pk": "b4fbd90d-1b89-481a-a425-46cde292de9e",
    "name": "Christopher D. Manning",
    "bio": " I am a researcher focused on natural language processing, with a particular interest in compositional generalization, semantic dependency parsing, and logical semantics. I have worked on developing models that can accurately compute meanings for novel sentences, while also considering the impact of incidental details in logical form predictions. My research has shown that by converting these logical forms to semantically equivalent ones, even baseline models can achieve traction.\n\nIn my work on semantic dependency parsing, I have extended LSTM-based syntactic parsers to train on and generate graph-structured representations, achieving state-of-the-art performance. I have also explored the use of recursive neural networks for sentence meaning, evaluating their ability to learn logical deduction and handling relational reasoning, recursive structures, and quantification.\n\nI am interested in the use of tree-structured neural networks for sentence meaning, but I also believe that neural sequence models like LSTMs can discover and implicitly use recursive compositional structure. I have demonstrated this possibility using an artificial data task and found that while an LSTM-based sequence model can learn to exploit the underlying tree structure, its performance consistently lags behind that of tree models.\n\nIn my research on natural logic, I have explored the use of distributed representations to support rich, diverse logical reasoning. I have found that neural network-based models can learn the basic algebra of natural logic relations from simulated data and from the WordNet noun graph.\n\nI am also interested in developing robust models that can handle annotation errors, which can significantly hurt classifier performance. I have presented a robust extension of logistic regression that incorporates the possibility of mislabeling directly into the objective.\n\nIn addition, I have worked on applying reinforcement learning to directly optimize neural mention-ranking models for coreference evaluation metrics, resulting in significant improvements over the current state-of-the-art.\n\nOverall, my research is focused on developing models that can accurately compute meanings for novel sentences, while also considering the impact of incidental details in logical form predictions. I am also interested in developing robust models that can handle annotation errors and applying reinforcement learning to directly optimize neural mention-ranking models for coreference evaluation metrics.",
    "collaborators": [
      "Yacine Jernite",
      "Zhengxuan Wu",
      "Julie Tibshirani",
      "Nicholas Dingwall",
      "Bruno Godefroy",
      "George E. Dahl",
      "Samuel R. Bowman",
      "Yueyi Jiang",
      "Christopher Potts",
      "Timothy Dozat",
      "David Sontag",
      "Desmond C. Ong",
      "Jason Luo",
      "Xiyu Zhang"
    ],
    "institute": null
  },
  "6eea8869-4853-43c5-acd3-053ba675dc59": {
    "pk": "6eea8869-4853-43c5-acd3-053ba675dc59",
    "name": "Tamim Asfour",
    "bio": " I am a researcher with a focus on robotics and artificial intelligence, specifically in the areas of Riemannian geometry, interactive and incremental learning, whole-body pose taxonomy, 6D object pose estimation, temporal task constraints, resource-aware programming, grasp planning, and compliant manipulation.\n\nIn my work on Riemannian geometry, I argue that it provides the most suitable tools for analyzing and generating well-coordinated, energy-efficient motions of robots with many degrees of freedom. I propose preliminary solutions and novel research directions for leveraging Riemannian geometry in robotics to design and combine physically-meaningful synergies.\n\nIn the area of interactive and incremental learning, I address the problem of incrementally learning geometric models of spatial relations from human demonstrations. I propose the use of cylindrical probability distribution as a generative representation of spatial relations and demonstrate how this model can be updated incrementally with each new demonstration in a sample-efficient way.\n\nI have also worked on developing a whole-body pose taxonomy for loco-manipulation tasks, which classifies the set of whole-body robot configurations that use the environment to enhance stability. This taxonomy induces a classification of motion primitives based on the pose used for support and a set of rules to store and generate new motions.\n\nIn the field of 6D object pose estimation, I have presented Kitchen, a novel benchmark designed specifically for estimating the 6D poses of objects located in diverse positions within kitchen settings. The benchmark includes a comprehensive dataset of around 205k real-world RGBD images for 111 kitchen objects captured in two distinct kitchens, utilizing one humanoid robot with its egocentric perspectives.\n\nI have also proposed a model-driven approach for the combined learning of symbolic and subsymbolic temporal task constraints from multiple bimanual human demonstrations. This approach describes temporal nexuses of actions in the task based on distributions of temporal differences between semantic action keypoints, and uses fuzzy logic to derive symbolic temporal task constraints from this representation.\n\nIn the area of resource-aware programming, I have analyzed a resource-aware computing methodology named Invasive Computing, and its benefits and limitations for addressing the challenges of utilizing the full power of a chip with huge amounts of resources in humanoid robots.",
    "collaborators": [
      "Leonel Rozo",
      "Rainer Kartmann",
      "J\u00falia Borr\u00e0s",
      "Yichen Cai",
      "Christoph Pohl",
      "Sylvain Calinon",
      "Xiaodong Liu",
      "You Zhou",
      "Kevin Duh",
      "David Ginsbourger",
      "Mathias B\u00fcrger",
      "No\u00e9mie Jaquier",
      "Robert Haschke",
      "Jianfeng Gao"
    ],
    "institute": null
  },
  "31639f2e-1507-496b-ae89-24b6c10c3444": {
    "pk": "31639f2e-1507-496b-ae89-24b6c10c3444",
    "name": "Terra Blevins",
    "bio": " I am a researcher in the field of natural language processing, with a particular focus on language models and word sense disambiguation. My work has shown that incorporating morphological supervision into character language models via multitasking can improve their performance across multiple languages, even when the morphology data and language modeling data are disjoint. I have also found that inflected words benefit more from explicitly modeling morphology than uninflected words, and that morphological supervision improves performance even as the amount of language modeling data grows.\n\nIn addition to this, I have investigated the phenomenon of language contamination in English pretrained language models, which are often used as the backbone of many modern NLP systems. I have found that even small amounts of non-English text in these models' training data can facilitate cross-lingual transfer, with target language performance strongly correlated to the amount of in-language data seen during pretraining. This has led me to argue that no model is truly monolingual when pretrained at scale, and that this should be considered when evaluating cross-lingual transfer.\n\nI have also proposed a bi-encoder model for word sense disambiguation (WSD) that independently embeds the target word with its surrounding context and the dictionary definition, or gloss, of each sense. This model outperforms previous state-of-the-art models on English all-words WSD, with the majority of the gains coming from improved performance on rare senses.\n\nFurthermore, I have demonstrated that deep recurrent neural networks (RNNs) learn internal representations that capture soft hierarchical notions of syntax from highly varied supervision. I have also introduced FEWS (Few-shot Examples of Word Senses), a new low-shot WSD dataset automatically extracted from example sentences in Wiktionary, which has high sense coverage across different natural language domains and provides a large training set that covers many more senses than previous datasets.\n\nI have also investigated the dynamics of the multilingual pretraining process in multilingual language models, and found that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. The point in pretraining when the model learns to transfer cross-lingually differs across language pairs, and the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network.",
    "collaborators": [
      "Luke Zettlemoyer",
      "Hila Gonen",
      "Michael Petrochuk",
      "Luheng He",
      "Yoav Goldberg",
      "Haoqiang Kang",
      "Tim Dettmers",
      "Kenton Lee",
      "Xiao-Yang Liu",
      "Kellie Webster",
      "Shauli Ravfogel",
      "Yanai Elazar",
      "Juntong Ni",
      "Huaxiu Yao",
      "Omer Levy",
      "Mandar Joshi",
      "Yova Kementchedjhieva"
    ],
    "institute": null
  },
  "ff554afa-cf43-4c8e-af36-49d4ea5031ca": {
    "pk": "ff554afa-cf43-4c8e-af36-49d4ea5031ca",
    "name": "Samuel R. Bowman",
    "bio": " I am a researcher in the field of natural language processing (NLP), with a particular interest in recursive neural networks and their ability to capture linguistic meaning and logical reasoning. In my work, I have trained recursive models on a new corpus of constructed examples of logical reasoning in short sentences, and found that these models can learn representations that generalize well to new types of reasoning patterns. This suggests that learned representation models have the potential to capture logical reasoning accurately.\n\nIn addition to my technical work, I am also interested in the way that research results in NLP are framed and discussed. I have argued that researchers in NLP often underclaim the successes of the field, which can lead to misleading or false claims about the limits of our technology. This can have serious consequences, such as harming our credibility and limiting our ability to prepare for the impacts of future advances. I urge researchers to be careful about these claims and to consider alternative communication strategies.\n\nI have also surveyed the evidence for eight potentially surprising points about large language models (LLMs), including the fact that LLMs often appear to learn and use representations of the outside world, and that there are no reliable techniques for steering the behavior of LLMs. I believe that it is important for advocates, policymakers, and scholars to be aware of these considerations as they engage with LLMs.\n\nIn my work on evaluation for natural language understanding (NLU) tasks, I have argued that unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers to demonstrate improvements. I have laid out four criteria that I believe NLU benchmarks should meet, and argued that adversarial data collection does not meaningfully address the causes of the failures of current benchmarks.\n\nOverall, my research is focused on understanding the capabilities and limitations of NLP systems, and on finding ways to improve the evaluation and communication of research results in the field.",
    "collaborators": [
      "Christopher D. Manning",
      "Yacine Jernite",
      "Ryan Prescott Adams",
      "Navdeep Jaitly",
      "Zhengxuan Wu",
      "Iain Murray",
      "Julie Tibshirani",
      "Mohammad Norouzi",
      "Nicholas Dingwall",
      "Bruno Godefroy",
      "George E. Dahl",
      "Christopher Potts",
      "David Sontag",
      "Timothy Dozat",
      "Ruslan Salakhutdinov"
    ],
    "institute": null
  }
}
