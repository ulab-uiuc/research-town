{
  "rank_consistency": 0.0,
  "papers": {
    "ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching": {
      "paper_pk": "9a1b4ebd-c979-4864-bbad-211d5313dc5b",
      "title": "ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching",
      "abstract": "Self-attention is an essential component of GPT-style models and a significant cause of LLM inference latency for long sequences. In multi-tenant LLM inference servers, the compute and memory operation cost of self-attention can be amortized by making use of the probability that sequences from users may share long prompt prefixes. This paper introduces ChunkAttention, a unique self-attention kernel built on chunking, sharing the KV cache, and batching the attention computation. ChunkAttention recognizes matching prompt prefixes across several sequences and shares their KV cache in memory by chunking the KV cache and structuring it into the auxiliary prefix tree. To significantly improve the memory reuse of KV cache and consequently the speed of self-attention for long shared prompts, we design an efficient computation kernel on this new storage structure, where two-phased partitioning is implemented to reduce memory operations on shared KV cache during self-attention. Experiments show that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, with lengths ranging from 1024 to 8192.",
      "authors": [],
      "keywords": [
        "large language model",
        "model inference",
        "self attention"
      ],
      "real_avg_scores": 4.5,
      "real_all_scores": [
        5,
        5,
        5,
        3
      ],
      "real_contents": [],
      "real_rank": 9,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" presents an innovative solution to address the significant cause of latency in GPT-style models' inference, which is self-attention. The authors propose ChunkAttention, a unique self-attention kernel that utilizes chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across multiple sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prompts. The experimental results indicate that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, making it a promising approach for improving the efficiency of multi-tenant LLM inference servers.\n\nReview 2:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" offers a novel solution to the problem of high compute and memory operation cost of self-attention in GPT-style models. The authors propose ChunkAttention, a self-attention kernel that uses chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across several sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prompts. The two-phased partitioning implemented in the computation kernel further reduces memory operations on shared KV cache during self-attention. The experimental results indicate that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, making it an effective approach for improving the efficiency of multi-tenant LLM inference servers.\n\nReview 3:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" presents a unique solution to the problem of high latency in GPT-style models' inference caused by self-attention. The authors propose ChunkAttention, a self-attention kernel that uses chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across multiple sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prom",
        " Review 1:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" presents an innovative solution to address the significant cause of latency in GPT-style models' inference, which is self-attention. The authors propose ChunkAttention, a unique self-attention kernel that utilizes chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across multiple sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prompts. The experimental results indicate that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, making it a promising approach for improving the efficiency of multi-tenant LLM inference servers.\n\nReview 2:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" offers a novel solution to the problem of high compute and memory operation cost of self-attention in GPT-style models. The authors propose ChunkAttention, a self-attention kernel that uses chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across several sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prompts. The two-phased partitioning implemented in the computation kernel further reduces memory operations on shared KV cache during self-attention. The experimental results indicate that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, making it an effective approach for improving the efficiency of multi-tenant LLM inference servers.\n\nReview 3:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" presents a unique solution to the problem of high latency in GPT-style models' inference caused by self-attention. The authors propose ChunkAttention, a self-attention kernel that uses chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across multiple sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prom",
        " Review 1:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" presents an innovative solution to address the significant cause of latency in GPT-style models' inference, which is self-attention. The authors propose ChunkAttention, a unique self-attention kernel that utilizes chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across multiple sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prompts. The experimental results indicate that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, making it a promising solution for multi-tenant LLM inference servers.\n\nReview 2:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" offers a novel approach to improve the efficiency of self-attention in GPT-style models, which is a critical cause of latency in LLM inference servers. The authors propose ChunkAttention, a unique self-attention kernel that recognizes matching prompt prefixes across several sequences and shares their KV cache in memory. The KV cache is chunked and structured into an auxiliary prefix tree, which significantly improves memory reuse and reduces memory operations during self-attention. The experimental results indicate that ChunkAttention can speed up self-attention of long shared prompts 1.6-3 times, making it an effective solution for multi-tenant LLM inference servers.\n\nReview 3:\nThe paper \"ChunkAttention: Efficient Attention on KV Cache with Chunking Sharing and Batching\" presents an innovative solution to address the latency issue in GPT-style models' inference caused by self-attention. The authors propose ChunkAttention, a unique self-attention kernel that utilizes chunking, sharing the KV cache, and batching attention computation. The approach recognizes matching prompt prefixes across multiple sequences and shares their KV cache in memory, thereby reducing memory operations and improving the speed of self-attention for long shared prompts. The experimental results indicate that ChunkAttention can speed up self-attention of long shared prompts 1.6-"
      ],
      "sim_rank": 9,
      "sim_decision": null
    },
    "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs": {
      "paper_pk": "582c312c-8ae4-45fd-ab45-a89f378f0855",
      "title": "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",
      "abstract": "Modular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, which is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to `understand' the output of an image encoder. With the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by leveraging a pretrained multilingual LLM. To this end, we re-align an image encoder previously tuned to an English LLM to a new, multilingual LLM -- for this, we leverage multilingual data from a mix of vision-and-language tasks, which we obtain by machine-translating high-quality English data to 95 languages. On the IGLUE benchmark, mBLIP yields results competitive with state-of-the-art models. Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to these very large multilingual vision-language models trained from scratch, we obtain mBLIP by training orders of magnitude fewer parameters on magnitudes less data. We release our model and code at ANONYMIZED.",
      "authors": [],
      "keywords": [
        "image",
        "text",
        "language",
        "vision",
        "llm",
        "multilingual"
      ],
      "real_avg_scores": 5.5,
      "real_all_scores": [
        5,
        6,
        5,
        6
      ],
      "real_contents": [],
      "real_rank": 8,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents an innovative approach to building multilingual vision-language models (Vision-LLMs) in a computationally efficient manner. By re-aligning a pretrained image encoder with a multilingual LLM, the authors demonstrate that it is possible to create a multilingual Vision-LLM (mBLIP) that is competitive with state-of-the-art models on the IGLUE benchmark. The use of machine-translated data from 95 languages to re-align the image encoder with the multilingual LLM is a novel and cost-effective approach. The results show that mBLIP outperforms PaLI-X, a model with 55B parameters, in image captioning on XM3600, despite training orders of magnitude fewer parameters on magnitudes less data. This paper is a significant contribution to the field of multilingual vision-language models and provides a promising direction for future research.\n\nReview 2:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents a new method for building multilingual Vision-LLMs that is computationally efficient and effective. The authors demonstrate that it is possible to create a multilingual Vision-LLM (mBLIP) that is competitive with state-of-the-art models on the IGLUE benchmark by re-aligning a pretrained image encoder with a multilingual LLM. The use of machine-translated data from 95 languages to re-align the image encoder with the multilingual LLM is a novel and cost-effective approach. The results show that mBLIP outperforms PaLI-X, a model with 55B parameters, in image captioning on XM3600, despite training orders of magnitude fewer parameters on magnitudes less data. This paper is a valuable contribution to the field of multilingual vision-language models and provides a promising direction for future research.\n\nReview 3:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents a new method for building multilingual Vision-LLMs that is computationally efficient and effective. The authors demonstrate that it is possible to create a multilingual Vision",
        " Review 1:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents an innovative approach to building multilingual vision-language models (Vision-LLMs) in a computationally efficient manner. By re-aligning a pretrained image encoder with a multilingual LLM, the authors demonstrate that it is possible to create a multilingual Vision-LLM (mBLIP) that is competitive with state-of-the-art models on the IGLUE benchmark. The use of machine-translated data from 95 languages to re-align the image encoder with the multilingual LLM is a novel and cost-effective approach. The results show that mBLIP outperforms PaLI-X, a model with 55B parameters, in image captioning on XM3600, despite training orders of magnitude fewer parameters on magnitudes less data. This paper is a significant contribution to the field of multilingual vision-language models and provides a promising direction for future research.\n\nReview 2:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents a new method for building multilingual vision-language models (Vision-LLMs) that is computationally efficient and effective. The authors demonstrate that it is possible to create a multilingual Vision-LLM (mBLIP) that is competitive with state-of-the-art models on the IGLUE benchmark. The use of a pretrained multilingual LLM and machine-translated data from 95 languages to re-align the image encoder is a novel and cost-effective approach. The results show that mBLIP outperforms PaLI-X, a model with 55B parameters, in image captioning on XM3600, despite training fewer parameters on less data. This paper is a valuable contribution to the field of multilingual vision-language models and provides a promising direction for future research.\n\nReview 3:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents a new method for building multilingual vision-language models (Vision-LLMs) that is computationally efficient and effective. The authors demonstrate that it is possible to create a multilingual Vision-LLM (mBLIP",
        " Review 1:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents an innovative approach to building multilingual vision-language models (Vision-LLMs) in a computationally efficient manner. By re-aligning a pretrained image encoder with a multilingual LLM, the authors demonstrate that it is possible to create a multilingual Vision-LLM (mBLIP) that is competitive with state-of-the-art models on the IGLUE benchmark. The use of machine-translated data from 95 languages to re-align the image encoder with the multilingual LLM is a novel and cost-effective approach. The results show that mBLIP outperforms PaLI-X, a model with 55B parameters, in image captioning on XM3600, despite training orders of magnitude fewer parameters on magnitudes less data. This paper is a significant contribution to the field of multilingual vision-language models and demonstrates the potential for building high-quality models using cost-effective methods.\n\nReview 2:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs\" presents a solution to the problem of expensive end-to-end pretraining of multilingual vision-language models. The authors propose a method for building multilingual Vision-LLMs by re-aligning a pretrained image encoder with a multilingual LLM. This approach is computationally efficient and allows for the creation of a multilingual Vision-LLM (mBLIP) that is competitive with state-of-the-art models on the IGLUE benchmark. The use of machine-translated data from 95 languages to re-align the image encoder with the multilingual LLM is a novel and cost-effective approach. The results show that mBLIP outperforms PaLI-X, a model with 55B parameters, in image captioning on XM3600, despite training orders of magnitude fewer parameters on magnitudes less data. This paper is a valuable contribution to the field of multilingual vision-language models and demonstrates the potential for building high-quality models using cost-effective methods.\n\nReview 3:\nThe paper \"mBLIP: Efficient Bootstrapping of Multilingual Vision-LL"
      ],
      "sim_rank": 8,
      "sim_decision": null
    },
    "SqueezeLLM: Dense and Sparse Quantization": {
      "paper_pk": "b82fdc09-0934-4060-9f37-09e57e615de5",
      "title": "SqueezeLLM: Dense and Sparse Quantization",
      "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1\u00d7 as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3\u00d7 speedup compared to the baseline.",
      "authors": [],
      "keywords": [
        "Quantization",
        "model compression",
        "efficient LLM",
        "efficient inference"
      ],
      "real_avg_scores": 5.75,
      "real_all_scores": [
        5,
        5,
        5,
        8
      ],
      "real_contents": [],
      "real_rank": 7,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" presents an effective solution to the challenge of deploying large language models (LLMs) for inference, which have been limited by their high resource requirements. The authors demonstrate that memory bandwidth is the main bottleneck for generative inference with LLMs, and propose a post-training quantization framework called SqueezeLLM. This framework utilizes two novel ideas: sensitivity-based non-uniform quantization and the Dense-and-Sparse decomposition. The results show that SqueezeLLM significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x and achieves up to 2.3x speedup on an A6000 GPU compared to the baseline. This work has the potential to significantly improve the deployment of LLMs for inference.\n\nReview 2:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" addresses the challenge of deploying generative large language models (LLMs) for inference, which have been limited by their high resource requirements. The authors demonstrate that memory bandwidth is the main bottleneck for generative inference with LLMs and propose a post-training quantization framework called SqueezeLLM. This framework utilizes two novel ideas: sensitivity-based non-uniform quantization and the Dense-and-Sparse decomposition. The results show that SqueezeLLM significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x and achieves up to 2.3x speedup on an A6000 GPU compared to the baseline. This work is a significant contribution to the field of LLMs and has the potential to improve the deployment of these models for inference.\n\nReview 3:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" presents a promising solution to the challenge of deploying generative large language models (LLMs) for inference, which have been limited by their high resource requirements. The authors demonstrate that memory bandwidth is the main bottleneck for generative inference with LLMs and propose a post-training quantization framework called SqueezeLLM. This framework utilizes two novel ideas: sensitivity-based non-uniform quantization and the Dense-and-Sparse decomposition.",
        " Review 1:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" presents an effective solution to the challenge of deploying large language models (LLMs) for inference, which have been limited by their high resource requirements. The authors demonstrate that memory bandwidth is the main bottleneck for generative inference with LLMs, and propose SqueezeLLM, a post-training quantization framework that enables lossless compression to ultra-low precisions of up to 3-bit. The framework incorporates two novel ideas: sensitivity-based non-uniform quantization and the Dense-and-Sparse decomposition. These techniques allow for higher quantization performance under the same memory constraint, significantly reducing the perplexity gap from the FP16 baseline and achieving up to 2.3x speedup on an A6000 GPU.\n\nReview 2:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" offers a promising approach to addressing the challenges of deploying generative LLMs for inference. The authors identify memory bandwidth as the primary bottleneck for single batch inference and propose a post-training quantization framework, SqueezeLLM, that enables lossless compression to ultra-low precisions of up to 3-bit. The framework's novel techniques, sensitivity-based non-uniform quantization and Dense-and-Sparse decomposition, result in a significant reduction in the perplexity gap from the FP16 baseline and up to 2.3x speedup on an A6000 GPU. These results suggest that SqueezeLLM is a valuable contribution to the field of LLM deployment and could have significant implications for real-world applications.\n\nReview 3:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" presents a solution to the challenge of deploying large language models (LLMs) for inference, which have been limited by their high resource requirements. The authors demonstrate that memory bandwidth is the main bottleneck for generative inference with LLMs and propose a post-training quantization framework, SqueezeLLM, that enables lossless compression to ultra-low precisions of up to 3-bit. The framework's novel techniques, sensitivity-based non-uniform quantization and Dense-and-Sparse decomposition, result in a significant reduction",
        " Review 1:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" presents an effective solution to the challenge of deploying large language models (LLMs) for inference, which have been limited by their high resource requirements. The authors demonstrate that memory bandwidth is the main bottleneck for generative inference with LLMs, and propose SqueezeLLM, a post-training quantization framework that enables lossless compression to ultra-low precisions of up to 3-bit. The framework incorporates two novel ideas: sensitivity-based non-uniform quantization and the Dense-and-Sparse decomposition. These techniques allow for higher quantization performance under the same memory constraint, significantly reducing the perplexity gap from the FP16 baseline and achieving up to 2.3x speedup on an A6000 GPU.\n\nReview 2:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" offers a promising approach to addressing the challenges of deploying generative LLMs for inference. The authors identify memory bandwidth as the primary bottleneck for single batch inference and propose a post-training quantization framework, SqueezeLLM, that enables lossless compression to ultra-low precisions of up to 3-bit. The framework's novel techniques, sensitivity-based non-uniform quantization and Dense-and-Sparse decomposition, result in a significant reduction in the perplexity gap from the FP16 baseline and up to 2.3x speedup on an A6000 GPU. These results suggest that SqueezeLLM is a valuable contribution to the field of LLM deployment and could have significant implications for the development of more efficient and cost-effective inference pipelines.\n\nReview 3:\nThe paper \"SqueezeLLM: Dense and Sparse Quantization\" presents a solution to the challenge of deploying large language models (LLMs) for inference, which have been limited by their high resource requirements. The authors demonstrate that memory bandwidth is the main bottleneck for generative inference with LLMs and propose a post-training quantization framework, SqueezeLLM, that enables lossless compression to ultra-low precisions of up to 3-bit. The framework's novel techniques, sensitivity-based non-uniform quantization and Dense-and-"
      ],
      "sim_rank": 7,
      "sim_decision": null
    },
    "KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization": {
      "paper_pk": "fc1e29ef-a359-41af-ac12-48798fddaa7c",
      "title": "KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization",
      "abstract": "Large language models(LLMs) exhibit capabilities beyond expectations in various NLP tasks. Since the inference of LLM consumes huge resources, optimizing the inference process of LLM is of great significance to promote the application of LLM. In the text generation process, caching the key-value embeddings (KV cache) for subsequent generation process is a basic optimization method. However, huge size of the KV cache limits the inference batch size. Compressing the space occupied by the cached key-value embeddings can enlarge the batch size of LLM inference to improve throughput. Besides, based on the analysis of the usage mode of the KV cache, we find compressing the KV cache to ternary digits can not only compress the space occupied by the KV cache, but also greatly reduce the required multiplication operation in the attention block. Combined with the numerical features of the KV cache, we propose KVTQ, a method which compresses the KV cache to hardware efficient ternary digits. We validate our KVTQ method on different series of LLMs and get the conclusion that the KVTQ method which compresses the KV cache to ultra-low bits can still preserve the model quality.",
      "authors": [],
      "keywords": [
        "compression",
        "dynamic quantization",
        "ternary digits",
        "KV cache"
      ],
      "real_avg_scores": 4.4,
      "real_all_scores": [
        3,
        5,
        3,
        6,
        5
      ],
      "real_contents": [],
      "real_rank": 10,
      "real_decision": "Reject",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization\" presents an innovative solution to optimize the inference process of large language models (LLMs) by compressing the key-value cache (KV cache) using ternary digits. The authors provide a comprehensive analysis of the KV cache usage pattern and demonstrate that compressing it to ternary digits not only reduces the space required but also significantly decreases the multiplication operations in the attention block. The experimental results on various LLMs validate the effectiveness of the proposed method, showing that it can preserve model quality even when compressing the KV cache to ultra-low bits. This research could significantly improve the throughput of LLMs and promote their practical applications.\n\nReview 2:\nIn the paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization,\" the authors address the challenge of optimizing the inference process of large language models (LLMs) by focusing on the key-value cache (KV cache) compression. They propose a novel method, KVTQ, that compresses the KV cache to hardware-efficient ternary digits using fine-grained dynamic quantization. The authors' analysis of KV cache usage patterns reveals that compressing it to ternary digits can save space and reduce multiplication operations in the attention block. The experimental results on different LLMs validate the KVTQ method's effectiveness, demonstrating that it can maintain model quality even when compressing the KV cache to ultra-low bits. This research has the potential to enhance the efficiency of LLMs and expand their practical use.\n\nReview 3:\nThe paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization\" offers a valuable contribution to the optimization of large language models (LLMs) inference process. The authors propose the KVTQ method, which compresses the key-value cache (KV cache) to hardware-efficient ternary digits using fine-grained dynamic quantization. By analyzing the KV cache usage pattern, the authors show that compressing it to ternary digits can save space and decrease multiplication operations in the",
        " Review 1:\nThe paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization\" presents an innovative solution to optimize the inference process of large language models (LLMs) by compressing the key-value cache (KV cache) using ternary digits. The authors provide a comprehensive analysis of the KV cache usage pattern and demonstrate that compressing it to ternary digits not only reduces the space required but also significantly decreases the multiplication operations in the attention block. The experimental results on various LLMs validate the effectiveness of the proposed method, showing that it can preserve model quality even when compressing the KV cache to ultra-low bits. This research could significantly improve the throughput of LLMs and promote their practical applications.\n\nReview 2:\nIn the paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization,\" the authors address the challenge of optimizing the inference process of large language models (LLMs) by focusing on the key-value cache (KV cache) compression. They propose a novel method, KVTQ, that compresses the KV cache to hardware-efficient ternary digits using fine-grained dynamic quantization. The authors' analysis of KV cache usage patterns reveals that compressing it to ternary digits can save space and reduce multiplication operations in the attention block. The experimental results on different LLMs validate the KVTQ method's effectiveness, demonstrating that it can maintain model quality even when compressing the KV cache to ultra-low bits. This research has the potential to enhance the efficiency of LLMs and expand their practical use.\n\nReview 3:\nThe paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization\" offers a valuable contribution to the optimization of large language models (LLMs) inference process. The authors propose the KVTQ method, which compresses the key-value cache (KV cache) to hardware-efficient ternary digits using fine-grained dynamic quantization. By analyzing the KV cache usage pattern, the authors show that compressing it to ternary digits can save space and decrease multiplication operations in the",
        " Review 1:\nThe paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization\" presents an innovative solution to optimize the inference process of large language models (LLMs) by compressing the key-value cache (KV cache) using ternary digits. The authors provide a comprehensive analysis of the KV cache usage pattern and demonstrate that compressing it to ternary digits not only reduces the space required but also significantly decreases the multiplication operations in the attention block. The experimental results on various LLMs validate the effectiveness of the proposed method, showing that it can preserve model quality even when compressing the KV cache to ultra-low bits. This research could significantly improve the throughput of LLMs and promote their practical applications.\n\nReview 2:\nIn the paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization,\" the authors address the challenge of optimizing the inference process of large language models (LLMs) by focusing on the key-value cache (KV cache) compression. They propose a novel method, KVTQ, that compresses the KV cache to hardware-efficient ternary digits using fine-grained dynamic quantization. The authors' analysis of KV cache usage patterns reveals that compressing it to ternary digits can save space and reduce multiplication operations in the attention block. The experimental results on different LLMs validate the KVTQ method's effectiveness, demonstrating that it can maintain model quality even when compressing the KV cache to ultra-low bits. This research has the potential to enhance the efficiency of LLMs and expand their practical use.\n\nReview 3:\nThe paper \"KVTQ: Compressing the KV Cache to Hardware Efficient Ternary Digits by Fine-Grained Dynamic Quantization\" offers a valuable contribution to the optimization of large language models (LLMs) inference process. The authors propose the KVTQ method, which compresses the key-value cache (KV cache) to hardware-efficient ternary digits using fine-grained dynamic quantization. By analyzing the KV cache usage pattern, the authors show that compressing it to ternary digits can save space and decrease multiplication operations in the"
      ],
      "sim_rank": 10,
      "sim_decision": null
    },
    "Efficient Streaming Language Models with Attention Sinks": {
      "paper_pk": "211ce599-acb9-428b-881b-fba7ba41e6b6",
      "title": "Efficient Streaming Language Models with Attention Sinks",
      "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a sink even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2 speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.",
      "authors": [],
      "keywords": [
        "Large Language Models",
        "Length Extrapolation",
        "Efficiency"
      ],
      "real_avg_scores": 7.5,
      "real_all_scores": [
        8,
        8,
        6,
        8
      ],
      "real_contents": [],
      "real_rank": 2,
      "real_decision": "Accept(poster)",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"Efficient Streaming Language Models with Attention Sinks\" presents an innovative solution to the challenges of deploying Large Language Models (LLMs) in streaming applications. The authors identify two major issues: memory consumption due to caching previous tokens' Key and Value states, and the inability of popular LLMs to generalize to longer texts than the training sequence length. The paper's key contribution is the introduction of StreamingLLM, a framework that enables LLMs to generalize to infinite sequence length without any fine-tuning. The authors demonstrate that StreamingLLM can improve the efficiency and stability of language modeling with up to 4 million tokens and more. The code and datasets are publicly available, providing a valuable resource for researchers in the field.\n\nReview 2:\nThe paper addresses the challenges of deploying LLMs in streaming applications such as multi-round dialogue. The authors propose StreamingLLM, a framework that enables LLMs to generalize to infinite sequence length without any fine-tuning. The paper's strength lies in its thorough analysis of the attention mechanism in LLMs and the discovery of the attention sink phenomenon. The authors show that keeping the Key and Value of initial tokens will largely recover the performance of window attention. Additionally, the paper demonstrates that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. The results show that StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2 speedup.\n\nReview 3:\nThe paper \"Efficient Streaming Language Models with Attention Sinks\" presents a novel approach to deploying LLMs in streaming applications. The authors identify two major challenges: memory consumption due to caching previous tokens' Key and Value states and the inability of popular LLMs to generalize to longer texts than the training sequence length. The paper's key contribution is the introduction of StreamingLLM, a framework that addresses these challenges without any fine-tuning. The authors demonstrate that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. The paper's thorough analysis of the attention mechanism and the discovery of the attention sink phenomenon are noteworthy. The results show that StreamingLLM outperforms the sliding window recomputation baseline",
        " Review 1:\nThe paper \"Efficient Streaming Language Models with Attention Sinks\" presents an innovative solution to the challenges of deploying Large Language Models (LLMs) in streaming applications. The authors identify two major issues: memory consumption due to caching previous tokens' Key and Value states, and the inability of popular LLMs to generalize to longer texts than the training sequence length. The paper's key contribution is the introduction of StreamingLLM, a framework that enables LLMs to generalize to infinite sequence length without any fine-tuning. The authors demonstrate that StreamingLLM can improve the efficiency and stability of language modeling with up to 4 million tokens and more. The code and datasets are publicly available, providing a valuable resource for researchers in this field.\n\nReview 2:\nThe paper addresses the challenges of deploying LLMs in streaming applications such as multi-round dialogue. The authors propose StreamingLLM, a framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. The paper's strength lies in its thorough analysis of the attention sink phenomenon, where keeping the KV of initial tokens largely recovers the performance of window attention. The authors also demonstrate that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. The results show that StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2 speedup, making it a promising approach for efficient language modeling in streaming applications.\n\nReview 3:\nThe paper \"Efficient Streaming Language Models with Attention Sinks\" presents a novel solution to the challenges of deploying LLMs in streaming applications. The authors propose StreamingLLM, a framework that enables LLMs to generalize to infinite sequence length without any fine-tuning. The paper's contribution is the discovery of the attention sink phenomenon, where keeping the KV of initial tokens largely recovers the performance of window attention. The authors also demonstrate that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. The results show that StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2 speedup, making it a valuable contribution to the field of language modeling in streaming applications.\n\nOverall, the paper \"Efficient Streaming Language Models with",
        " Review 1:\nThe paper \"Efficient Streaming Language Models with Attention Sinks\" presents an innovative solution to the challenges of deploying Large Language Models (LLMs) in streaming applications. The authors identify two major issues: memory consumption due to caching previous tokens' Key and Value states, and the inability of popular LLMs to generalize to longer texts than the training sequence length. The paper's key contribution is the introduction of StreamingLLM, a framework that enables LLMs to generalize to infinite sequence length without any fine-tuning. The authors demonstrate that StreamingLLM can improve the efficiency and stability of language modeling with up to 4 million tokens and more. The code and datasets are publicly available, providing a valuable resource for researchers in this field.\n\nReview 2:\nThe paper addresses the challenges of deploying LLMs in streaming applications such as multi-round dialogue. The authors propose StreamingLLM, a framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. The paper's strength lies in its thorough analysis of the attention sink phenomenon, where keeping the KV of initial tokens largely recovers the performance of window attention. The authors also demonstrate that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. The results show that StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2 speedup, making it a promising approach for efficient language modeling in streaming applications.\n\nReview 3:\nThe paper \"Efficient Streaming Language Models with Attention Sinks\" presents a novel solution to the challenges of deploying LLMs in streaming applications. The authors propose StreamingLLM, a framework that enables LLMs to generalize to infinite sequence length without any fine-tuning. The paper's contribution is the discovery of the attention sink phenomenon, where keeping the KV of initial tokens largely recovers the performance of window attention. The authors also demonstrate that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. The results show that StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2 speedup, making it a valuable contribution to the field of language modeling in streaming applications.\n\nOverall, the paper \"Efficient Streaming Language Models with"
      ],
      "sim_rank": 2,
      "sim_decision": null
    },
    "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression": {
      "paper_pk": "884b4f94-8a6a-4df7-bddb-28b3d0eb39f2",
      "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
      "abstract": "Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. Quantizing models to 3-4 bits per parameter can lead to moderate to high accuracy losses, especially for smaller models (1-10B parameters), which are suitable for edge deployment. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique that enables for the first time \\emph{near-lossless} compression of LLMs across model scales while reaching similar compression levels to previous methods. SpQR works by identifying and isolating \\emph{outlier weights}, which cause particularly large quantization errors, and storing them in higher precision while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run a 33B parameter LLM on a single 24 GB consumer GPU without performance degradation at 15% speedup, thus making powerful LLMs available to consumers without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR, which yields faster inference than 16-bit baselines at similar accuracy while enabling memory compression gains of more than 4x.",
      "authors": [],
      "keywords": [
        "quantization",
        "sparsity",
        "large language models"
      ],
      "real_avg_scores": 6.5,
      "real_all_scores": [
        6,
        6,
        8,
        6
      ],
      "real_contents": [],
      "real_rank": 4,
      "real_decision": "Accept(poster)",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" presents an innovative solution to the problem of compressing large language models (LLMs) while minimizing accuracy loss. By identifying and isolating outlier weights that cause significant quantization errors, the authors propose a new compressed format and quantization technique called SpQR. This method enables near-lossless compression of LLMs across various model scales, reaching similar compression levels to previous methods. The results show that SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs, making it possible to run a 33B parameter LLM on a single 24 GB consumer GPU without performance degradation at 15% speedup.\n\nReview 2:\nThe authors of \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" address the challenge of compressing large language models (LLMs) to fit into memory-limited devices such as laptops and mobile phones. They propose a novel sparse-quantized representation technique that quantizes LLMs to 3-4 bits per parameter, while minimizing accuracy loss. The paper introduces an efficient GPU inference algorithm for SpQR, which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x. This makes powerful LLMs available to consumers without any downsides, making it a significant contribution to the field of LLM compression.\n\nReview 3:\nThe paper \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" presents a promising solution to the problem of compressing LLMs while minimizing accuracy loss. The authors propose a new compressed format and quantization technique called SpQR, which identifies and isolates outlier weights that cause significant quantization errors. By storing these outlier weights in higher precision and compressing all other weights to 3-4 bits, SpQR achieves near-lossless compression of LLMs across various model scales. The results show that SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs,",
        " Review 1:\nThe paper \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" presents an innovative solution to the problem of compressing large language models (LLMs) while minimizing accuracy loss. By identifying and isolating outlier weights that cause significant quantization errors, the authors propose a new compressed format and quantization technique called SpQR. This method enables near-lossless compression of LLMs across various model scales, reaching similar compression levels to previous methods. The results show that SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs, making it possible to run a 33B parameter LLM on a single 24 GB consumer GPU without performance degradation at 15% speedup.\n\nReview 2:\nThe authors of \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" address the challenge of compressing large language models (LLMs) to fit into memory-limited devices such as laptops and mobile phones. They propose a novel sparse-quantized representation technique that quantizes LLMs to 3-4 bits per parameter, while minimizing accuracy loss. The paper introduces an efficient GPU inference algorithm for SpQR, which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x. This makes powerful LLMs available to consumers without any downsides, making it a significant contribution to the field of LLM compression.\n\nReview 3:\nThe paper \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" presents a promising solution to the problem of compressing LLMs while minimizing accuracy loss. The authors propose a new compressed format and quantization technique called SpQR, which identifies and isolates outlier weights that cause significant quantization errors. By storing these outlier weights in higher precision and compressing all other weights to 3-4 bits, SpQR achieves near-lossless compression of LLMs across various model scales. The results show that SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs,",
        " Review 1:\nThe paper \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" presents an innovative solution to the problem of compressing large language models (LLMs) while minimizing accuracy loss. By identifying and isolating outlier weights that cause significant quantization errors, the authors propose a new compressed format and quantization technique called SpQR. This method enables near-lossless compression of LLMs across various model scales, reaching similar compression levels to previous methods. The results show that SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs, making it possible to run a 33B parameter LLM on a single 24 GB consumer GPU without performance degradation at 15% speedup.\n\nReview 2:\nThe authors of \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" address the challenge of compressing large language models (LLMs) to fit into memory-limited devices such as laptops and mobile phones. They propose a novel sparse-quantized representation technique that quantizes LLMs to 3-4 bits per parameter, while minimizing accuracy loss. The paper introduces an efficient GPU inference algorithm for SpQR, which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x. This makes powerful LLMs available to consumers without any downsides, making it a significant contribution to the field of LLM compression.\n\nReview 3:\nThe paper \"SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\" presents a promising solution to the problem of compressing LLMs while minimizing accuracy loss. The authors propose a new compressed format and quantization technique called SpQR, which identifies and isolates outlier weights that cause significant quantization errors. By storing these outlier weights in higher precision and compressing all other weights to 3-4 bits, SpQR achieves near-lossless compression of LLMs across various model scales. The results show that SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs,"
      ],
      "sim_rank": 4,
      "sim_decision": null
    },
    "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models": {
      "paper_pk": "3df785e8-f433-43ed-8256-19f872e70d3a",
      "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models",
      "abstract": "Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. ",
      "authors": [],
      "keywords": [
        "Large Language Model Compression",
        "Differentiable Quantization"
      ],
      "real_avg_scores": 6.5,
      "real_all_scores": [
        6,
        6,
        6,
        8
      ],
      "real_contents": [],
      "real_rank": 5,
      "real_decision": "Accept(spoltight)",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" presents an innovative solution to the problem of large memory and computation requirements for deploying large language models (LLMs). The authors propose a new technique called Omnidirectionally calibrated Quantization, which optimizes various quantization parameters to achieve good performance in diverse quantization settings. The technique comprises two components, Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), which modulate extreme values of weights and tackle activation outliers, respectively. The results show that OmniQuant can process the LLaMA-2 model family size 7-70B on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16.\n\nReview 2:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" introduces a new post-training quantization (PTQ) method for LLMs that efficiently optimizes various quantization parameters. The authors propose two innovative components, Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), which modulate extreme values of weights and tackle activation outliers, respectively. The results show that OmniQuant can process the LLaMA-2 model family size 7-70B on a single A100-40G GPU within 1-16 hours using 128 samples. The technique is effective in reducing memory footprint and improving the computational efficiency of LLMs, and it demonstrates superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16.\n\nReview 3:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" proposes a new PTQ method for LLMs that hand-crafts quantization parameters, leading to low performance, especially in extremely low-bit quantization. The authors introduce an Omnidirectionally calibrated Quantization technique",
        " Review 1:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" presents an innovative solution to the problem of large memory and computation requirements for deploying large language models (LLMs). The authors propose a new technique called Omnidirectionally calibrated Quantization, which optimizes various quantization parameters to achieve good performance in diverse quantization settings. The technique comprises two components, Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), which modulate extreme values of weights and tackle activation outliers, respectively. The results show that OmniQuant can process the LLaMA-2 model family size 7-70B on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16.\n\nReview 2:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" introduces a new post-training quantization (PTQ) method for LLMs that efficiently optimizes various quantization parameters. The authors propose two innovative components, Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), which modulate extreme values of weights and tackle activation outliers, respectively. The results show that OmniQuant can process the LLaMA-2 model family size 7-70B on a single A100-40G GPU within 1-16 hours using 128 samples. The technique is effective in reducing memory footprint and improving the computational efficiency of LLMs, and it demonstrates superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16.\n\nReview 3:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" proposes a new PTQ method for LLMs that hand-crafts quantization parameters, leading to low performance, especially in extremely low-bit quantization. The authors introduce an Omnidirectionally calibrated Quantization technique",
        " Review 1:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" presents an innovative solution to the problem of large memory and computation requirements for deploying large language models (LLMs). The authors propose a new technique called Omnidirectionally calibrated Quantization, which optimizes various quantization parameters to achieve good performance in diverse quantization settings. The technique comprises two components, Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), which modulate extreme values of weights and tackle activation outliers, respectively. The results show that OmniQuant can process the LLaMA-2 model family size 7-70B on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16.\n\nReview 2:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" introduces a new post-training quantization (PTQ) method for LLMs that efficiently optimizes various quantization parameters. The authors propose two innovative components, Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), which modulate extreme values of weights and tackle activation outliers, respectively. The results show that OmniQuant can process the LLaMA-2 model family size 7-70B on a single A100-40G GPU within 1-16 hours using 128 samples. The technique is effective in reducing memory footprint and improving the computational efficiency of LLMs, and it demonstrates superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16.\n\nReview 3:\nThe paper \"OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models\" proposes a new PTQ method for LLMs that hand-crafts quantization parameters, leading to low performance, especially in extremely low-bit quantization. The authors introduce an Omnidirectionally calibrated Quantization technique"
      ],
      "sim_rank": 5,
      "sim_decision": null
    },
    "EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models": {
      "paper_pk": "16866ef1-a172-45a3-9299-e491d4c9cdfc",
      "title": "EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models",
      "abstract": "Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for low-latency real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and accelerate diffusion models, where post-training quantization (PTQ) and quantization-aware training (QAT) are two main approaches, each bearing its own properties. While PTQ exhibits efficiency in terms of both time and data usage, it may lead to diminished performance in low bit-width settings. On the other hand, QAT can help alleviate performance degradation but comes with substantial demands on computational and data resources. To capitalize on the advantages while avoiding their respective drawbacks, we introduce a data-free, quantization-aware and parameter-efficient fine-tuning framework for low-bit diffusion models, dubbed EfficientDM, to achieve QAT-level performance with PTQ-like efficiency. Specifically, we propose a quantization-aware variant of the low-rank adapter (QALoRA) that can be merged with model weights and jointly quantized to low bit-width. The fine-tuning process distills the denoising capabilities of the full-precision model into its quantized counterpart, eliminating the requirement for training data. To further enhance performance, we introduce scale-aware optimization to address ineffective learning of QALoRA due to variations in weight quantization scales across different layers. We also employ temporal learned step-size quantization to handle notable variations in activation distributions across denoising steps. Extensive experimental results demonstrate that our method significantly outperforms previous PTQ-based diffusion models while maintaining similar time and data efficiency. Specifically, there is only a marginals. FID increase when quantizing both weights and activations of LDM-4 to 4-bit on ImageNet . Compared to QAT-based methods, our EfficientDM also boasts a faster quantization speed with comparable generation quality, rendering it a compelling choice for practical applications.",
      "authors": [],
      "keywords": [
        "Diffusion Models",
        "Model Quantization",
        "Model Compression",
        "Efficient Models"
      ],
      "real_avg_scores": 6.5,
      "real_all_scores": [
        6,
        6,
        6,
        8
      ],
      "real_contents": [],
      "real_rank": 6,
      "real_decision": "Accept(spoltight)",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nEfficientDM presents an innovative solution to the problem of impractical computational costs and latency issues in diffusion models, which have demonstrated remarkable capabilities in image synthesis and related generative tasks. The paper proposes a data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, achieving QAT-level performance with PTQ-like efficiency. The introduction of a quantization-aware variant of the low-rank adapter (QALoRA) and the fine-tuning process that distills the denoising capabilities of the full-precision model into its quantized counterpart are noteworthy contributions. The results show significant improvements over previous PTQ-based diffusion models while maintaining similar time and data efficiency. EfficientDM's faster quantization speed and comparable generation quality compared to QAT-based methods make it a compelling choice for practical applications.\n\nReview 2:\nThis paper addresses the challenge of deploying diffusion models in real-world applications, where substantial computational costs and latency issues hinder their practicality. Quantization is a common approach to compress and accelerate diffusion models, with post-training quantization (PTQ) and quantization-aware training (QAT) being the two main methods. EfficientDM introduces a novel data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, aiming to combine the advantages of PTQ and QAT while avoiding their respective drawbacks. The proposed QALoRA and the fine-tuning process demonstrate impressive results, significantly outperforming previous PTQ-based diffusion models and maintaining similar time and data efficiency. The comparison with QAT-based methods shows that EfficientDM boasts a faster quantization speed with comparable generation quality, making it an attractive option for practical applications.\n\nReview 3:\nThe authors of EfficientDM tackle the challenge of deploying diffusion models in real-world applications by addressing their substantial computational costs and latency issues. They propose a data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, combining the advantages of PTQ and QAT while avoiding their respective drawbacks. The paper's main contributions include the introduction of QALoRA and the fine-tuning process, which significantly outperforms previous PTQ-based diffusion models while maintaining",
        " Review 1:\nEfficientDM presents an innovative solution to the problem of impractical computational costs and latency issues in diffusion models, which have demonstrated remarkable capabilities in image synthesis and related generative tasks. The paper proposes a data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, achieving QAT-level performance with PTQ-like efficiency. The introduction of a quantization-aware variant of the low-rank adapter (QALoRA) and the fine-tuning process that distills the denoising capabilities of the full-precision model into its quantized counterpart are noteworthy contributions. The results show significant improvements over previous PTQ-based diffusion models while maintaining similar time and data efficiency. EfficientDM's faster quantization speed and comparable generation quality compared to QAT-based methods make it a compelling choice for practical applications.\n\nReview 2:\nThis paper addresses the challenge of deploying diffusion models in real-world applications, where substantial computational costs and latency issues hinder their practicality. Quantization is a common approach to compress and accelerate diffusion models, with post-training quantization (PTQ) and quantization-aware training (QAT) being the two main methods. EfficientDM introduces a novel data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, aiming to combine the advantages of PTQ and QAT while avoiding their respective drawbacks. The proposed QALoRA and the fine-tuning process demonstrate impressive results, significantly outperforming previous PTQ-based diffusion models and maintaining similar time and data efficiency. The comparison with QAT-based methods shows that EfficientDM boasts a faster quantization speed with comparable generation quality, making it an attractive option for practical applications.\n\nReview 3:\nThe authors of EfficientDM tackle the challenge of deploying diffusion models in real-world applications by addressing their substantial computational costs and latency issues. They propose a data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, combining the advantages of PTQ and QAT while avoiding their respective drawbacks. The paper's main contributions include the introduction of QALoRA and the fine-tuning process, which significantly outperforms previous PTQ-based diffusion models while maintaining",
        " Review 1:\nEfficientDM presents an innovative solution to the problem of impractical computational costs and latency issues in diffusion models, which have demonstrated remarkable capabilities in image synthesis and related generative tasks. The paper proposes a data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, achieving QAT-level performance with PTQ-like efficiency. The introduction of a quantization-aware variant of the low-rank adapter (QALoRA) and the fine-tuning process that distills the denoising capabilities of the full-precision model into its quantized counterpart are noteworthy contributions. The results show significant improvements over previous PTQ-based diffusion models while maintaining similar time and data efficiency. EfficientDM's faster quantization speed and comparable generation quality compared to QAT-based methods make it a compelling choice for practical applications.\n\nReview 2:\nThis paper addresses the challenge of deploying diffusion models in real-world applications, where substantial computational costs and latency issues hinder their practicality. Quantization is a common approach to compress and accelerate diffusion models, with post-training quantization (PTQ) and quantization-aware training (QAT) being the two main methods. EfficientDM introduces a novel data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, aiming to combine the advantages of PTQ and QAT while avoiding their respective drawbacks. The proposed QALoRA and the fine-tuning process demonstrate impressive results, significantly outperforming previous PTQ-based diffusion models and maintaining similar time and data efficiency. The comparison with QAT-based methods shows that EfficientDM boasts a faster quantization speed with comparable generation quality, making it an attractive option for practical applications.\n\nReview 3:\nThe authors of EfficientDM tackle the challenge of deploying diffusion models in real-world applications by addressing their substantial computational costs and latency issues. They propose a data-free, quantization-aware, and parameter-efficient fine-tuning framework for low-bit diffusion models, combining the advantages of PTQ and QAT while avoiding their respective drawbacks. The paper's main contributions include the introduction of QALoRA and the fine-tuning process, which significantly outperforms previous PTQ-based diffusion models while maintaining"
      ],
      "sim_rank": 6,
      "sim_decision": null
    },
    "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models": {
      "paper_pk": "eabd7060-3544-4a9d-aa8e-e62e174dca6d",
      "title": "LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models",
      "abstract": "Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrep- ancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural lan- guage understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and out- performs existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code.",
      "authors": [],
      "keywords": [
        "quantization",
        "compression",
        "large language models",
        "NLP",
        "machine learning",
        "low rank"
      ],
      "real_avg_scores": 7.333333333333333,
      "real_all_scores": [
        6,
        8,
        8
      ],
      "real_contents": [],
      "real_rank": 3,
      "real_decision": "Accept(oral)",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" presents a novel quantization framework, LoftQ, that addresses the performance gap between full fine-tuning and quantization plus LoRA fine-tuning approaches in large language models (LLMs). The authors identify the discrepancy between quantized and full-precision models as a key challenge and propose a low-rank initialization method for LoRA fine-tuning that significantly improves the generalization in downstream tasks. The proposed method is evaluated on natural language understanding, question answering, summarization, and natural language generation tasks, and the results show that LoftQ outperforms existing quantization methods, especially in the 2-bit and 2/4-bit mixed precision regimes.\n\nReview 2:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" focuses on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained LLM. The authors identify a consistent gap in performance on downstream tasks between full fine-tuning and the quantization plus LoRA fine-tuning approach. To address this issue, they propose LoftQ, a quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. The low-rank initialization helps alleviate the discrepancy between the quantized and full-precision models, resulting in improved generalization in downstream tasks. The method is evaluated on various natural language processing tasks, and the results demonstrate the effectiveness of LoftQ, especially in the challenging 2-bit and 2/4-bit mixed precision regimes.\n\nReview 3:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" presents a new quantization framework for large language models (LLMs) that combines quantization and LoRA fine-tuning. The authors identify a performance gap between full fine-tuning and quantization plus LoRA fine-tuning approaches and propose LoftQ to address this issue. LoftQ finds a proper low-rank initialization for LoRA fine-tuning that helps alleviate the discrepancy between quantized and full-precision models. The proposed method is evaluated on natural",
        " Review 1:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" presents a novel quantization framework, LoftQ, that addresses the performance gap between full fine-tuning and quantization plus LoRA fine-tuning approaches in large language models (LLMs). The authors identify the discrepancy between quantized and full-precision models as a key challenge and propose a low-rank initialization method for LoRA fine-tuning that significantly improves the generalization in downstream tasks. The proposed method is evaluated on natural language understanding, question answering, summarization, and natural language generation tasks, and the results show that LoftQ outperforms existing quantization methods, especially in the 2-bit and 2/4-bit mixed precision regimes.\n\nReview 2:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" focuses on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained LLM. The authors identify a consistent gap in performance on downstream tasks between full fine-tuning and the quantization plus LoRA fine-tuning approach. To address this issue, they propose LoftQ, a quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. The low-rank initialization helps alleviate the discrepancy between the quantized and full-precision models, resulting in improved generalization in downstream tasks. The method is evaluated on various natural language processing tasks, and the results demonstrate the effectiveness of LoftQ, especially in the challenging 2-bit and 2/4-bit mixed precision regimes.\n\nReview 3:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" presents a new quantization framework for large language models (LLMs) that combines quantization and LoRA fine-tuning. The authors identify a performance gap between full fine-tuning and quantization plus LoRA fine-tuning approaches and propose LoftQ to address this issue. LoftQ quantizes the LLM and finds a proper low-rank initialization for LoRA fine-tuning, which significantly improves the generalization in downstream tasks. The paper evaluates the proposed method",
        " Review 1:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" presents a novel quantization framework, LoftQ, that addresses the performance gap between full fine-tuning and quantization plus LoRA fine-tuning approaches in large language models (LLMs). The authors identify the discrepancy between quantized and full-precision models as a key challenge and propose a low-rank initialization method for LoRA fine-tuning that significantly improves the generalization in downstream tasks. The proposed method is evaluated on natural language understanding, question answering, summarization, and natural language generation tasks, and the results show that LoftQ outperforms existing quantization methods, especially in the 2-bit and 2/4-bit mixed precision regimes.\n\nReview 2:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" focuses on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained LLM. The authors identify a consistent gap in performance on downstream tasks between full fine-tuning and the quantization plus LoRA fine-tuning approach. To address this issue, they propose LoftQ, a quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. The low-rank initialization helps alleviate the discrepancy between the quantized and full-precision models, resulting in improved generalization in downstream tasks. The method is evaluated on various natural language processing tasks, and the results demonstrate the effectiveness of LoftQ, especially in the challenging 2-bit and 2/4-bit mixed precision regimes.\n\nReview 3:\nThe paper \"LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models\" presents a new quantization framework that aims to improve the performance of LLMs when quantization and LoRA fine-tuning are applied together. The authors identify a performance gap in downstream tasks between full fine-tuning and the quantization plus LoRA fine-tuning approach. To address this issue, they propose LoftQ, a quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. The low-rank initialization helps reduce"
      ],
      "sim_rank": 3,
      "sim_decision": null
    },
    "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs": {
      "paper_pk": "c19158bf-09ed-46fc-a979-4474a74adf09",
      "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
      "abstract": "In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",
      "authors": [],
      "keywords": [
        "Large Language Model",
        "Efficient Inference",
        "Generative Inference",
        "Key-Value Cache"
      ],
      "real_avg_scores": 8.0,
      "real_all_scores": [
        8,
        8,
        8,
        8,
        8
      ],
      "real_contents": [],
      "real_rank": 1,
      "real_decision": "Accept(oral)",
      "sim_avg_scores": 0.0,
      "sim_all_scores": [
        0,
        0,
        0
      ],
      "sim_contents": [
        " Review 1:\nThe paper \"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\" presents an innovative approach to reduce the memory footprint of generative inference for Large Language Models (LLMs). The proposed adaptive KV cache compression method intelligently discerns the intrinsic structure of attention modules and constructs the KV cache in an adaptive manner, leading to substantial reduction in GPU memory consumption without compromising generation quality. The use of lightweight attention profiling to guide the construction of the adaptive KV cache eliminates the need for resource-intensive fine-tuning or re-training. The authors' experimental results demonstrate the effectiveness of their approach, making it a promising solution for memory-constrained environments.\n\nReview 2:\nThis paper introduces an adaptive KV cache compression method for Large Language Models (LLMs) that significantly reduces memory consumption during generative inference. By profiling the intrinsic structure of attention modules, the method intelligently discards irrelevant long-range contexts, non-special tokens, and focuses on attention heads that broadly attend to all tokens. This approach results in a more efficient KV cache construction, leading to substantial memory savings without compromising generation quality. The authors' use of lightweight attention profiling to guide the construction of the adaptive KV cache eliminates the need for resource-intensive fine-tuning or re-training, making it a practical solution for real-world applications.\n\nReview 3:\nThe paper \"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\" offers a novel approach to reducing the memory footprint of generative inference for Large Language Models (LLMs). The proposed adaptive KV cache compression method intelligently discerns the intrinsic structure of attention modules and constructs the KV cache in an adaptive manner, leading to substantial reduction in GPU memory consumption. The use of lightweight attention profiling to guide the construction of the adaptive KV cache eliminates the need for resource-intensive fine-tuning or re-training, making it a practical and efficient solution. The authors' experimental results demonstrate the effectiveness of their approach, making it a promising solution for memory-constrained environments.\n\nOverall, the paper presents a novel and practical solution to reduce the memory footprint of generative inference for Large Language",
        " Review 1:\nThe paper \"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\" presents an innovative approach to reduce the memory footprint of generative inference for Large Language Models (LLMs). The proposed adaptive KV cache compression method intelligently discerns the intrinsic structure of attention modules and constructs the KV cache in an adaptive manner, leading to substantial reduction in GPU memory consumption without compromising generation quality. The use of lightweight attention profiling to guide the construction of the adaptive KV cache eliminates the need for resource-intensive fine-tuning or re-training. The authors' experimental results demonstrate the effectiveness of their approach, making it a promising solution for memory-constrained environments.\n\nReview 2:\nThis paper introduces an adaptive KV cache compression method for Large Language Models (LLMs) that significantly reduces memory consumption during generative inference. By profiling the intrinsic structure of attention modules, the method intelligently discards irrelevant contexts and tokens, leading to a more efficient KV cache construction. The authors' approach eliminates the need for fine-tuning or re-training, making it a practical solution for real-world applications. The experimental results demonstrate the method's effectiveness, with substantial memory savings and negligible generation quality loss. The authors' commitment to releasing their code and compatible CUDA kernel will further promote research in this area.\n\nReview 3:\nThe paper \"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\" proposes an adaptive KV cache compression method for Large Language Models (LLMs) that reduces memory consumption during generative inference. The method's targeted profiling of attention modules and adaptive KV cache construction lead to significant memory savings without compromising generation quality. The use of lightweight attention profiling to guide the construction of the adaptive KV cache is a practical solution for memory-constrained environments. The authors' experimental results demonstrate the effectiveness of their approach, making it a promising solution for real-world applications. The release of their code and compatible CUDA kernel will further promote research in this area.\n\nRelated Papers:\n\n* \"Efficient Attention with Structured State Spaces\" by Rewon Child et al.\n* \"BigBird: Transformers for Longer Sequ",
        " Review 1:\nThe paper \"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\" presents an innovative approach to reduce the memory footprint of generative inference for Large Language Models (LLMs). The proposed adaptive KV cache compression method intelligently discerns the intrinsic structure of attention modules and constructs the KV cache in an adaptive manner, leading to substantial reduction in GPU memory consumption without compromising generation quality. The use of lightweight attention profiling to guide the construction of the adaptive KV cache eliminates the need for resource-intensive fine-tuning or re-training. The authors' experimental results demonstrate the effectiveness of their approach, making it a promising solution for memory-constrained environments.\n\nReview 2:\nThis paper introduces an adaptive KV cache compression method for Large Language Models (LLMs) that significantly reduces memory consumption during generative inference. By profiling the intrinsic structure of attention modules, the method intelligently discards irrelevant long-range contexts, non-special tokens, and focuses on attention heads that broadly attend to all tokens. This approach results in a more efficient KV cache construction, leading to substantial memory savings without compromising generation quality. The authors' use of lightweight attention profiling to guide the construction of the adaptive KV cache eliminates the need for resource-intensive fine-tuning or re-training, making it a practical solution for real-world applications.\n\nReview 3:\nThe paper \"Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\" offers a novel approach to reducing the memory footprint of generative inference for Large Language Models (LLMs). The proposed adaptive KV cache compression method intelligently discerns the intrinsic structure of attention modules and constructs the KV cache in an adaptive manner, leading to substantial reduction in GPU memory consumption. The use of lightweight attention profiling to guide the construction of the adaptive KV cache eliminates the need for resource-intensive fine-tuning or re-training, making it a practical and efficient solution. The authors' experimental results demonstrate the effectiveness of their approach, making it a promising solution for memory-constrained environments.\n\nOverall, the paper presents a novel and practical solution to reduce the memory footprint of generative inference for Large Language"
      ],
      "sim_rank": 1,
      "sim_decision": null
    }
  }
}