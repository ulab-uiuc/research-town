{
  "440a1646-7462-4e22-a9f0-e588fa290e8e": {
    "pk": "440a1646-7462-4e22-a9f0-e588fa290e8e",
    "title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights",
    "abstract": "Severe data imbalance naturally exists among web-scale vision-language datasets. Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations. With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training. This isolates the bias from dominant classes and implicitly balances the learning signal. Furthermore, the robustness and discriminability of CLIP improve with more descriptive language supervision, larger data scale, and broader open-world concepts, which are inaccessible to supervised learning. Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community. The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks. Code will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.",
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Xiaojuan Qi"
    ],
    "url": "http://arxiv.org/abs/2405.21070v1",
    "timestamp": 1717178244,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c88a8634-66f9-4ecc-b2cf-658b29f351c3": {
    "pk": "c88a8634-66f9-4ecc-b2cf-658b29f351c3",
    "title": "Recurrent neural networks: vanishing and exploding gradients are not the end of the story",
    "abstract": "Recurrent neural networks (RNNs) notoriously struggle to learn long-term memories, primarily due to vanishing and exploding gradients. The recent success of state-space models (SSMs), a subclass of RNNs, to overcome such difficulties challenges our theoretical understanding. In this paper, we delve into the optimization challenges of RNNs and discover that, as the memory of a network increases, changes in its parameters result in increasingly large output variations, making gradient-based learning highly sensitive, even without exploding gradients. Our analysis further reveals the importance of the element-wise recurrence design pattern combined with careful parametrizations in mitigating this effect. This feature is present in SSMs, as well as in other architectures, such as LSTMs. Overall, our insights provide a new explanation for some of the difficulties in gradient-based learning of RNNs and why some architectures perform better than others.",
    "authors": [
      "Nicolas Zucchet",
      "Antonio Orvieto"
    ],
    "url": "http://arxiv.org/abs/2405.21064v1",
    "timestamp": 1717177980,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9ea02ca3-820e-487e-8607-c4384c58e6c5": {
    "pk": "9ea02ca3-820e-487e-8607-c4384c58e6c5",
    "title": "Neural Network Verification with Branch-and-Bound for General Nonlinearities",
    "abstract": "Branch-and-bound (BaB) is among the most effective methods for neural network (NN) verification. However, existing works on BaB have mostly focused on NNs with piecewise linear activations, especially ReLU networks. In this paper, we develop a general framework, named GenBaB, to conduct BaB for general nonlinearities in general computational graphs based on linear bound propagation. To decide which neuron to branch, we design a new branching heuristic which leverages linear bounds as shortcuts to efficiently estimate the potential improvement after branching. To decide nontrivial branching points for general nonlinear functions, we propose to optimize branching points offline, which can be efficiently leveraged during verification with a lookup table. We demonstrate the effectiveness of our GenBaB on verifying a wide range of NNs, including networks with activation functions such as Sigmoid, Tanh, Sine and GeLU, as well as networks involving multi-dimensional nonlinear operations such as multiplications in LSTMs and Vision Transformers. Our framework also allows the verification of general nonlinear computation graphs and enables verification applications beyond simple neural networks, particularly for AC Optimal Power Flow (ACOPF). GenBaB is part of the latest $\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of Neural Networks Competition (VNN-COMP 2023).",
    "authors": [
      "Zhouxing Shi",
      "Qirui Jin",
      "Zico Kolter",
      "Suman Jana",
      "Cho-Jui Hsieh",
      "Huan Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.21063v1",
    "timestamp": 1717177867,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "38d90784-066f-4525-80ea-96d205a1adc9": {
    "pk": "38d90784-066f-4525-80ea-96d205a1adc9",
    "title": "Graph External Attention Enhanced Transformer",
    "abstract": "The Transformer architecture has recently gained considerable attention in the field of graph representation learning, as it naturally overcomes several limitations of Graph Neural Networks (GNNs) with customized attention mechanisms or positional and structural encodings. Despite making some progress, existing works tend to overlook external information of graphs, specifically the correlation between graphs. Intuitively, graphs with similar structures should have similar representations. Therefore, we propose Graph External Attention (GEA) -- a novel attention mechanism that leverages multiple external node/edge key-value units to capture inter-graph correlations implicitly. On this basis, we design an effective architecture called Graph External Attention Enhanced Transformer (GEAET), which integrates local structure and global interaction information for more comprehensive graph representations. Extensive experiments on benchmark datasets demonstrate that GEAET achieves state-of-the-art empirical performance. The source code is available for reproducibility at: https://github.com/icm1018/GEAET.",
    "authors": [
      "Jianqing Liang",
      "Min Chen",
      "Jiye Liang"
    ],
    "url": "http://arxiv.org/abs/2405.21061v1",
    "timestamp": 1717177827,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "790af916-9587-4a24-8e6f-2764d3eb336d": {
    "pk": "790af916-9587-4a24-8e6f-2764d3eb336d",
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
    "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "authors": [
      "Tri Dao",
      "Albert Gu"
    ],
    "url": "http://arxiv.org/abs/2405.21060v1",
    "timestamp": 1717177801,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0ecaf354-9f35-461d-afe4-187c53c6d83e": {
    "pk": "0ecaf354-9f35-461d-afe4-187c53c6d83e",
    "title": "An Organic Weed Control Prototype using Directed Energy and Deep Learning",
    "abstract": "Organic weed control is a vital to improve crop yield with a sustainable approach. In this work, a directed energy weed control robot prototype specifically designed for organic farms is proposed. The robot uses a novel distributed array robot (DAR) unit for weed treatment. Soybean and corn databases are built to train deep learning neural nets to perform weed recognition. The initial deep learning neural nets show a high performance in classifying crops. The robot uses a patented directed energy plant eradication recipe that is completely organic and UV-C free, with no chemical damage or physical disturbance to the soil. The deep learning can classify 8 common weed species in a soybean field under natural environment with up to 98% accuracy.",
    "authors": [
      "Deng Cao",
      "Hongbo Zhang",
      "Rajveer Dhillon"
    ],
    "url": "http://arxiv.org/abs/2405.21056v1",
    "timestamp": 1717177642,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c1449bd4-39d1-42f7-bfc7-de13761653d2": {
    "pk": "c1449bd4-39d1-42f7-bfc7-de13761653d2",
    "title": "Factors Influencing Performance of Students in Software Automated Test Tools Course",
    "abstract": "Formal software testing education is important for building efficient QA professionals. Various aspects of quality assurance approaches are usually covered in courses for training software testing students. Automated Test Tools is one of the core courses in the software testing post-graduate curriculum due to the high demand for automated testers in the workforce. It is important to understand which factors are affecting student performance in the automated testing course to be able to assist the students early on based on their needs. Various metrics that are considered for predicting student performance in this testing course are student engagement, grades on individual deliverables, and prerequisite courses. This study identifies the impact of assessing students based on individual vs. group activities, theoretical vs. practical components, and the effect of having taken prerequisite courses in their final grade. To carry out this research, student data was collected from the automated test tools course of a community college-based postgraduate certificate program in software testing. The dataset contained student records from the years 2021 to 2022 and consisted of information from five different semesters. Various machine learning algorithms were applied to develop an effective model for predicting students performance in the automated software testing tools course, and finally, important features affecting the students performance were identified. The predictive performance model of the automated test tools course that was developed by applying the logistic regression technique, showed the best performance, with an accuracy score of 90%.",
    "authors": [
      "Susmita Haldar",
      "Mary Pierce",
      "Luiz Fernando Capretz"
    ],
    "url": "http://arxiv.org/abs/2405.21055v1",
    "timestamp": 1717177617,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fdd4f2ef-17e6-4d5b-9a58-ce306cca86db": {
    "pk": "fdd4f2ef-17e6-4d5b-9a58-ce306cca86db",
    "title": "RydbergGPT",
    "abstract": "We introduce a generative pretained transformer (GPT) designed to learn the measurement outcomes of a neutral atom array quantum computer. Based on a vanilla transformer, our encoder-decoder architecture takes as input the interacting Hamiltonian, and outputs an autoregressive sequence of qubit measurement probabilities. Its performance is studied in the vicinity of a quantum phase transition in Rydberg atoms in a square lattice array. We explore the ability of the architecture to generalize, by producing groundstate measurements for Hamiltonian parameters not seen in the training set. We focus on examples of physical observables obtained from inference on three different models, trained in fixed compute time on a single NVIDIA A100 GPU. These can act as benchmarks for the scaling of larger RydbergGPT models in the future. Finally, we provide RydbergGPT open source, to aid in the development of foundation models based off of a wide variety of quantum computer interactions and data sets in the future.",
    "authors": [
      "David Fitzek",
      "Yi Hong Teoh",
      "Hin Pok Fung",
      "Gebremedhin A. Dagnew",
      "Ejaaz Merali",
      "M. Schuyler Moss",
      "Benjamin MacLellan",
      "Roger G. Melko"
    ],
    "url": "http://arxiv.org/abs/2405.21052v1",
    "timestamp": 1717177476,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "21d8b503-af4e-4cf2-a2b6-ac933bd6ffc8": {
    "pk": "21d8b503-af4e-4cf2-a2b6-ac933bd6ffc8",
    "title": "Spectrum-Aware Parameter Efficient Fine-Tuning for Diffusion Models",
    "abstract": "Adapting large-scale pre-trained generative models in a parameter-efficient manner is gaining traction. Traditional methods like low rank adaptation achieve parameter efficiency by imposing constraints but may not be optimal for tasks requiring high representation capacity. We propose a novel spectrum-aware adaptation framework for generative models. Our method adjusts both singular values and their basis vectors of pretrained weights. Using the Kronecker product and efficient Stiefel optimizers, we achieve parameter-efficient adaptation of orthogonal matrices. We introduce Spectral Orthogonal Decomposition Adaptation (SODA), which balances computational efficiency and representation capacity. Extensive evaluations on text-to-image diffusion models demonstrate SODA's effectiveness, offering a spectrum-aware alternative to existing fine-tuning methods.",
    "authors": [
      "Xinxi Zhang",
      "Song Wen",
      "Ligong Han",
      "Felix Juefei-Xu",
      "Akash Srivastava",
      "Junzhou Huang",
      "Hao Wang",
      "Molei Tao",
      "Dimitris N. Metaxas"
    ],
    "url": "http://arxiv.org/abs/2405.21050v1",
    "timestamp": 1717177415,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "232ef756-fda9-4eb4-b5b3-c8e3b1342323": {
    "pk": "232ef756-fda9-4eb4-b5b3-c8e3b1342323",
    "title": "Grammar-Aligned Decoding",
    "abstract": "Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.",
    "authors": [
      "Kanghee Park",
      "Jiayu Wang",
      "Taylor Berg-Kirkpatrick",
      "Nadia Polikarpova",
      "Loris D'Antoni"
    ],
    "url": "http://arxiv.org/abs/2405.21047v1",
    "timestamp": 1717177155,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}