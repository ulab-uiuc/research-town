{
  "143dbd7b-4c4e-4165-b04b-7b9132e69220": {
    "pk": "143dbd7b-4c4e-4165-b04b-7b9132e69220",
    "title": "X-VILA: Cross-Modality Alignment for Large Language Model",
    "abstract": "We introduce X-VILA, an omni-modality model designed to extend the capabilities of large language models (LLMs) by incorporating image, video, and audio modalities. By aligning modality-specific encoders with LLM inputs and diffusion decoders with LLM outputs, X-VILA achieves cross-modality understanding, reasoning, and generation. To facilitate this cross-modality alignment, we curate an effective interleaved any-to-any modality instruction-following dataset. Furthermore, we identify a significant problem with the current cross-modality alignment method, which results in visual information loss. To address the issue, we propose a visual alignment mechanism with a visual embedding highway module. We then introduce a resource-efficient recipe for training X-VILA, that exhibits proficiency in any-to-any modality conversation, surpassing previous approaches by large margins. X-VILA also showcases emergent properties across modalities even in the absence of similar training data. The project will be made open-source.",
    "authors": [
      "Hanrong Ye",
      "De-An Huang",
      "Yao Lu",
      "Zhiding Yu",
      "Wei Ping",
      "Andrew Tao",
      "Jan Kautz",
      "Song Han",
      "Dan Xu",
      "Pavlo Molchanov",
      "Hongxu Yin"
    ],
    "url": "http://arxiv.org/abs/2405.19335v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "af10f77d-9be1-4a40-b3ec-995aa061cc3d": {
    "pk": "af10f77d-9be1-4a40-b3ec-995aa061cc3d",
    "title": "LLMs Meet Multimodal Generation and Editing: A Survey",
    "abstract": "With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on understanding. This survey elaborates on multimodal generation across different domains, including image, video, 3D, and audio, where we highlight the notable advancements with milestone works in these fields. Specifically, we exhaustively investigate the key technical components behind methods and multimodal datasets utilized in these studies. Moreover, we dig into tool-augmented multimodal agents that can use existing generative models for human-computer interaction. Lastly, we also comprehensively discuss the advancement in AI safety and investigate emerging applications as well as future prospects. Our work provides a systematic and insightful overview of multimodal generation, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation",
    "authors": [
      "Yingqing He",
      "Zhaoyang Liu",
      "Jingye Chen",
      "Zeyue Tian",
      "Hongyu Liu",
      "Xiaowei Chi",
      "Runtao Liu",
      "Ruibin Yuan",
      "Yazhou Xing",
      "Wenhai Wang",
      "Jifeng Dai",
      "Yong Zhang",
      "Wei Xue",
      "Qifeng Liu",
      "Yike Guo",
      "Qifeng Chen"
    ],
    "url": "http://arxiv.org/abs/2405.19334v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b168305e-041d-4bae-8fe6-641dbd5a5d3d": {
    "pk": "b168305e-041d-4bae-8fe6-641dbd5a5d3d",
    "title": "Multi-Modal Generative Embedding Model",
    "abstract": "Most multi-modal tasks can be formulated into problems of either generation or embedding. Existing models usually tackle these two types of problems by decoupling language modules into a text decoder for generation, and a text encoder for embedding. To explore the minimalism of multi-modal paradigms, we attempt to achieve only one model per modality in this work. We propose a Multi-Modal Generative Embedding Model (MM-GEM), whereby the generative and embedding objectives are encapsulated in one Large Language Model. We also propose a PoolAggregator to boost efficiency and enable the ability of fine-grained embedding and generation. A surprising finding is that these two objectives do not significantly conflict with each other. For example, MM-GEM instantiated from ViT-Large and TinyLlama shows competitive performance on benchmarks for multimodal embedding models such as cross-modal retrieval and zero-shot classification, while has good ability of image captioning. Additionally, MM-GEM can seamlessly execute region-level image caption generation and retrieval tasks. Besides, the advanced text model in MM-GEM brings over 5% improvement in Recall@1 for long text and image retrieval.",
    "authors": [
      "Feipeng Ma",
      "Hongwei Xue",
      "Guangting Wang",
      "Yizhou Zhou",
      "Fengyun Rao",
      "Shilin Yan",
      "Yueyi Zhang",
      "Siying Wu",
      "Mike Zheng Shou",
      "Xiaoyan Sun"
    ],
    "url": "http://arxiv.org/abs/2405.19333v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "48bc6c31-b3cd-4ae6-a363-5b84eb322fa4": {
    "pk": "48bc6c31-b3cd-4ae6-a363-5b84eb322fa4",
    "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
    "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when finetuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings. Our code and models are available at https://github.com/shenao-zhang/SELM.",
    "authors": [
      "Shenao Zhang",
      "Donghan Yu",
      "Hiteshi Sharma",
      "Ziyi Yang",
      "Shuohang Wang",
      "Hany Hassan",
      "Zhaoran Wang"
    ],
    "url": "http://arxiv.org/abs/2405.19332v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7de0bb7c-3d5b-4d08-94ed-fda4ee771814": {
    "pk": "7de0bb7c-3d5b-4d08-94ed-fda4ee771814",
    "title": "NPGA: Neural Parametric Gaussian Avatars",
    "abstract": "The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our method around 3D Gaussian Splatting for its highly efficient rendering and to inherit the topological flexibility of point clouds. In contrast to previous work, we condition our avatars' dynamics on the rich expression space of neural parametric head models (NPHM), instead of mesh-based 3DMMs. To this end, we distill the backward deformation field of our underlying NPHM into forward deformations which are compatible with rasterization-based rendering. All remaining fine-scale, expression-dependent details are learned from the multi-view videos. To increase the representational capacity of our avatars, we augment the canonical Gaussian point cloud using per-primitive latent features which govern its dynamic behavior. To regularize this increased dynamic expressivity, we propose Laplacian terms on the latent features and predicted dynamics. We evaluate our method on the public NeRSemble dataset, demonstrating that NPGA significantly outperforms the previous state-of-the-art avatars on the self-reenactment task by 2.6 PSNR. Furthermore, we demonstrate accurate animation capabilities from real-world monocular videos.",
    "authors": [
      "Simon Giebenhain",
      "Tobias Kirschstein",
      "Martin R\u00fcnz",
      "Lourdes Agapito",
      "Matthias Nie\u00dfner"
    ],
    "url": "http://arxiv.org/abs/2405.19331v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "18907068-d8c2-4807-ba47-79b501b868f1": {
    "pk": "18907068-d8c2-4807-ba47-79b501b868f1",
    "title": "Barium stars as tracers of s-process nucleosynthesis in AGB stars III. Systematic deviations from the AGB models",
    "abstract": "Barium (Ba) stars help to verify asymptotic giant branch (AGB) star nucleosynthesis models since they experienced pollution from an AGB binary companion and thus their spectra carry the signatures of the slow neutron capture process (s process). For 180 Ba stars, we searched for AGB stellar models that match the observed abundance patterns. We employed three machine learning algorithms as classifiers: a Random Forest method, developed for this work, and the two classifiers used in our previous study. We studied the statistical behaviour of the s-process elements in the observational sample to investigate if the AGB models systematically under- or overpredict the abundances observed in the Ba stars and show the results in the form of violin plots of the residuals between spectroscopic abundances and model predictions. We find a significant trend in the residuals that implies an underproduction of the elements Nb, Mo, and Ru in the models relative to the observations. This may originate from a process (e.g. the intermediate neutron-capture process, i process) at the metallicity of the Ba stars not yet included in the AGB models. Correlations are found between the residuals of these elements, suggesting a common origin for the deviations. In addition, there is a weak metallicity dependence of their residuals. The s-process temperatures derived with the [Zr/Fe] - [Nb/Fe] thermometer have an unrealistic value for the majority of our stars. The most likely explanation is that at least a fraction of these elements are not produced in a steady-state s process, and instead may be due to processes not included in the AGB models. The mass distribution of the identified models confirms that our sample of Ba stars was polluted by low-mass AGB stars. Most of the matching AGB models require low accreted mass, but a few systems with high accreted mass are needed to explain the observations. (abridged)",
    "authors": [
      "B. Vil\u00e1gos",
      "B. Cseh",
      "A. Yag\u00fce L\u00f3pez",
      "M. Joyce",
      "A. Karakas",
      "G. Tagliente",
      "M. Lugaro"
    ],
    "url": "http://arxiv.org/abs/2405.19330v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.SR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2d3660e3-dfa6-43cd-b0a0-a4d633c1df4f": {
    "pk": "2d3660e3-dfa6-43cd-b0a0-a4d633c1df4f",
    "title": "Normative Modules: A Generative Agent Architecture for Learning Norms that Supports Multi-Agent Cooperation",
    "abstract": "Generative agents, which implement behaviors using a large language model (LLM) to interpret and evaluate an environment, has demonstrated the capacity to solve complex tasks across many social and technological domains. However, when these agents interact with other agents and humans in presence of social structures such as existing norms, fostering cooperation between them is a fundamental challenge. In this paper, we develop the framework of a 'Normative Module': an architecture designed to enhance cooperation by enabling agents to recognize and adapt to the normative infrastructure of a given environment. We focus on the equilibrium selection aspect of the cooperation problem and inform our agent design based on the existence of classification institutions that implement correlated equilibrium to provide effective resolution of the equilibrium selection problem. Specifically, the normative module enables agents to learn through peer interactions which of multiple candidate institutions in the environment, does a group treat as authoritative. By enabling normative competence in this sense, agents gain ability to coordinate their sanctioning behaviour; coordinated sanctioning behaviour in turn shapes primary behaviour within a social environment, leading to higher average welfare. We design a new environment that supports institutions and evaluate the proposed framework based on two key criteria derived from agent interactions with peers and institutions: (i) the agent's ability to disregard non-authoritative institutions and (ii) the agent's ability to identify authoritative institutions among several options. We show that these capabilities allow the agent to achieve more stable cooperative outcomes compared to baseline agents without the normative module, paving the way for research in a new avenue of designing environments and agents that account for normative infrastructure.",
    "authors": [
      "Atrisha Sarkar",
      "Andrei Ioan Muresanu",
      "Carter Blair",
      "Aaryam Sharma",
      "Rakshit S Trivedi",
      "Gillian K Hadfield"
    ],
    "url": "http://arxiv.org/abs/2405.19328v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.MA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "47426371-54e9-43c1-ae59-7ce699701dab": {
    "pk": "47426371-54e9-43c1-ae59-7ce699701dab",
    "title": "MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series",
    "abstract": "Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.",
    "authors": [
      "Ge Zhang",
      "Scott Qu",
      "Jiaheng Liu",
      "Chenchen Zhang",
      "Chenghua Lin",
      "Chou Leuang Yu",
      "Danny Pan",
      "Esther Cheng",
      "Jie Liu",
      "Qunshu Lin",
      "Raven Yuan",
      "Tuney Zheng",
      "Wei Pang",
      "Xinrun Du",
      "Yiming Liang",
      "Yinghao Ma",
      "Yizhi Li",
      "Ziyang Ma",
      "Bill Lin",
      "Emmanouil Benetos",
      "Huan Yang",
      "Junting Zhou",
      "Kaijing Ma",
      "Minghao Liu",
      "Morry Niu",
      "Noah Wang",
      "Quehry Que",
      "Ruibo Liu",
      "Sine Liu",
      "Shawn Guo",
      "Soren Gao",
      "Wangchunshu Zhou",
      "Xinyue Zhang",
      "Yizhi Zhou",
      "Yubo Wang",
      "Yuelin Bai",
      "Yuhan Zhang",
      "Yuxiang Zhang",
      "Zenith Wang",
      "Zhenzhu Yang",
      "Zijian Zhao",
      "Jiajun Zhang",
      "Wanli Ouyang",
      "Wenhao Huang",
      "Wenhu Chen"
    ],
    "url": "http://arxiv.org/abs/2405.19327v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0c9fba8c-035b-4654-872b-6f6de28fa12e": {
    "pk": "0c9fba8c-035b-4654-872b-6f6de28fa12e",
    "title": "Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language Models",
    "abstract": "In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation for parts searching and localization for objects, which is a new paradigm to 3D segmentation that transcends limitations for previous category-specific 3D semantic segmentation, 3D instance segmentation, and open-vocabulary 3D segmentation. We design a simple baseline method, Reasoning3D, with the capability to understand and execute complex commands for (fine-grained) segmenting specific parts for 3D meshes with contextual awareness and reasoned answers for interactive segmentation. Specifically, Reasoning3D leverages an off-the-shelf pre-trained 2D segmentation network, powered by Large Language Models (LLMs), to interpret user input queries in a zero-shot manner. Previous research have shown that extensive pre-training endows foundation models with prior world knowledge, enabling them to comprehend complex commands, a capability we can harness to \"segment anything\" in 3D with limited 3D datasets (source efficient). Experimentation reveals that our approach is generalizable and can effectively localize and highlight parts of 3D objects (in 3D mesh) based on implicit textual queries, including these articulated 3d objects and real-world scanned data. Our method can also generate natural language explanations corresponding to these 3D models and the decomposition. Moreover, our training-free approach allows rapid deployment and serves as a viable universal baseline for future research of part-level 3d (semantic) object understanding in various fields including robotics, object manipulation, part assembly, autonomous driving applications, augment reality and virtual reality (AR/VR), and medical applications. The code, the model weight, the deployment guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/",
    "authors": [
      "Tianrun Chen",
      "Chunan Yu",
      "Jing Li",
      "Jianqi Zhang",
      "Lanyun Zhu",
      "Deyi Ji",
      "Yong Zhang",
      "Ying Zang",
      "Zejian Li",
      "Lingyun Sun"
    ],
    "url": "http://arxiv.org/abs/2405.19326v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "da80cf78-98fb-4083-88fb-0fffd37e47e9": {
    "pk": "da80cf78-98fb-4083-88fb-0fffd37e47e9",
    "title": "Nearest Neighbor Speculative Decoding for LLM Generation and Attribution",
    "abstract": "Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B.",
    "authors": [
      "Minghan Li",
      "Xilun Chen",
      "Ari Holtzman",
      "Beidi Chen",
      "Jimmy Lin",
      "Wen-tau Yih",
      "Xi Victoria Lin"
    ],
    "url": "http://arxiv.org/abs/2405.19325v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c0aaac83-5c29-41b2-9e47-7c868aefe9af": {
    "pk": "c0aaac83-5c29-41b2-9e47-7c868aefe9af",
    "title": "Are Large Language Models Chameleons?",
    "abstract": "Do large language models (LLMs) have their own worldviews and personality tendencies? Simulations in which an LLM was asked to answer subjective questions were conducted more than 1 million times. Comparison of the responses from different LLMs with real data from the European Social Survey (ESS) suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. Methods for measuring the difference between LLMs and survey data are discussed, such as calculating weighted means and a new proposed measure inspired by Jaccard similarity. We conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior, as their imitation abilities are approximate at best.",
    "authors": [
      "Mingmeng Geng",
      "Sihong He",
      "Roberto Trotta"
    ],
    "url": "http://arxiv.org/abs/2405.19323v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "dff68004-0909-4934-96df-9b04dada0532": {
    "pk": "dff68004-0909-4934-96df-9b04dada0532",
    "title": "Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF",
    "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.   In this paper, we introduce a unified approach to online and offline RLHF -- value-incentivized preference optimization (VPO) -- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a $\\textit{sign}$ to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization and dialog verify the practicality and effectiveness of VPO.",
    "authors": [
      "Shicong Cen",
      "Jincheng Mei",
      "Katayoon Goshvadi",
      "Hanjun Dai",
      "Tong Yang",
      "Sherry Yang",
      "Dale Schuurmans",
      "Yuejie Chi",
      "Bo Dai"
    ],
    "url": "http://arxiv.org/abs/2405.19320v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1f0123bf-498d-41c3-89e4-91eb45878524": {
    "pk": "1f0123bf-498d-41c3-89e4-91eb45878524",
    "title": "ACE: A general-purpose non-Markovian open quantum systems simulation toolkit based on process tensors",
    "abstract": "We describe a general-purpose computational toolkit for simulating open quantum systems, which provides numerically exact solutions for composites of zero-dimensional quantum systems that may be strongly coupled to multiple, quite general non-Markovian environments. It is based on process tensor matrix product operators (PT-MPOs), which efficiently encapsulate environment influences. The code features implementations of several PT-MPO algorithms, in particular, Automated Compression of Environments (ACE) for general environments comprised of independent modes as well as schemes for generalized spin boson models. The latter includes a divide-and-conquer scheme for periodic PT-MPOs, which enable million time step simulations for realistic models. PT-MPOs can be precalculated and reused for efficiently probing different time-dependent system Hamiltonians. They can also be stacked together and combined to provide numerically complete solutions of small networks of open quantum systems. The code is written in C++ and is fully controllable by configuration files, for which we have developed a versatile and compact human-readable format.",
    "authors": [
      "Moritz Cygorek",
      "Erik M. Gauger"
    ],
    "url": "http://arxiv.org/abs/2405.19319v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9499d052-7bbc-4452-94c3-f6e4d1760b64": {
    "pk": "9499d052-7bbc-4452-94c3-f6e4d1760b64",
    "title": "Robust Preference Optimization through Reward Model Distillation",
    "abstract": "Language model (LM) post-training (or alignment) involves maximizing a reward function that is derived from preference annotations. Direct Preference Optimization (DPO) is a popular offline alignment method that trains a policy directly on preference data without the need to train a reward model or apply reinforcement learning. However, typical preference datasets have only a single, or at most a few, annotation per preference pair, which causes DPO to overconfidently assign rewards that trend towards infinite magnitude. This frequently leads to degenerate policies, sometimes causing even the probabilities of the preferred generations to go to zero. In this work, we analyze this phenomenon and propose distillation to get a better proxy for the true preference distribution over generation pairs: we train the LM to produce probabilities that match the distribution induced by a reward model trained on the preference data. Moreover, to account for uncertainty in the reward model we are distilling from, we optimize against a family of reward models that, as a whole, is likely to include at least one reasonable proxy for the preference distribution. Our results show that distilling from such a family of reward models leads to improved robustness to distribution shift in preference annotations, while preserving the simple supervised nature of DPO.",
    "authors": [
      "Adam Fisch",
      "Jacob Eisenstein",
      "Vicky Zayats",
      "Alekh Agarwal",
      "Ahmad Beirami",
      "Chirag Nagpal",
      "Pete Shaw",
      "Jonathan Berant"
    ],
    "url": "http://arxiv.org/abs/2405.19316v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f44240bd-e8c0-486d-988f-e62920500520": {
    "pk": "f44240bd-e8c0-486d-988f-e62920500520",
    "title": "Matryoshka Query Transformer for Large Vision-Language Models",
    "abstract": "Large Vision-Language Models (LVLMs) typically encode an image into a fixed number of visual tokens (e.g., 576) and process these tokens with a language model. Despite their strong performance, LVLMs face challenges in adapting to varying computational constraints. This raises the question: can we achieve flexibility in the number of visual tokens to suit different tasks and computational resources? We answer this with an emphatic yes. Inspired by Matryoshka Representation Learning, we introduce the Matryoshka Query Transformer (MQT), capable of encoding an image into m visual tokens during inference, where m can be any number up to a predefined maximum. This is achieved by employing a query transformer with M latent query tokens to compress the visual embeddings. During each training step, we randomly select m <= M latent query tokens and train the model using only these first m tokens, discarding the rest. Combining MQT with LLaVA, we train a single model once, and flexibly and drastically reduce the number of inference-time visual tokens while maintaining similar or better performance compared to training independent models for each number of tokens. Our model, MQT-LLAVA, matches LLaVA-1.5 performance across 11 benchmarks using a maximum of 256 tokens instead of LLaVA's fixed 576. Reducing to 16 tokens (8x less TFLOPs) only sacrifices the performance by 2.4 points on MMBench. On certain tasks such as ScienceQA and MMMU, we can even go down to only 2 visual tokens with performance drops of just 3% and 6% each. Our exploration of the trade-off between the accuracy and computational cost brought about by the number of visual tokens facilitates future research to achieve the best of both worlds.",
    "authors": [
      "Wenbo Hu",
      "Zi-Yi Dou",
      "Liunian Harold Li",
      "Amita Kamath",
      "Nanyun Peng",
      "Kai-Wei Chang"
    ],
    "url": "http://arxiv.org/abs/2405.19315v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "770062b6-1b0f-458c-9159-67e6f4f05c6e": {
    "pk": "770062b6-1b0f-458c-9159-67e6f4f05c6e",
    "title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice",
    "abstract": "The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.",
    "authors": [
      "Jian-Qiao Zhu",
      "Haijiang Yan",
      "Thomas L. Griffiths"
    ],
    "url": "http://arxiv.org/abs/2405.19313v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b4d6418d-f4c2-4aa2-b8d6-16f283e86773": {
    "pk": "b4d6418d-f4c2-4aa2-b8d6-16f283e86773",
    "title": "Causal Inference for Balanced Incomplete Block Designs",
    "abstract": "Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multi-site trials. However, if the number of treatments under consideration is large it might not be practical or even feasible to assign all treatments within each block. We develop novel inference results under the finite-population design-based framework for a natural alternative to the complete block design that does not require reducing the number of treatment arms, the balanced incomplete block design (BIBD). This includes deriving the properties of two estimators for BIBDs and proposing conservative variance estimators. To assist practitioners in understanding the trade-offs of using BIBDs over other designs, the precisions of resulting estimators are compared to standard estimators for the complete block design, the cluster-randomized design, and the completely randomized design. Simulations and a data illustration demonstrate the strengths and weaknesses of using BIBDs. This work highlights BIBDs as practical and currently underutilized designs.",
    "authors": [
      "Taehyeon Koo",
      "Nicole E. Pashley"
    ],
    "url": "http://arxiv.org/abs/2405.19312v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ME",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ec589011-1702-4f29-abb2-2c7c9944872a": {
    "pk": "ec589011-1702-4f29-abb2-2c7c9944872a",
    "title": "Network Connectivity--Information Freshness Tradeoff in Information Dissemination Over Networks",
    "abstract": "We consider a gossip network consisting of a source generating updates and $n$ nodes connected according to a given graph structure. The source keeps updates of a process, that might be generated or observed, and shares them with the gossiping network. The nodes in the network communicate with their neighbors and disseminate these version updates using a push-style gossip strategy. We use the version age metric to quantify the timeliness of information at the nodes. We first find an upper bound for the average version age for a set of nodes in a general network. Using this, we find the average version age scaling of a node in several network graph structures, such as two-dimensional grids, generalized rings and hyper-cubes. Prior to our work, it was known that when $n$ nodes are connected on a ring the version age scales as $O(n^{\\frac{1}{2}})$, and when they are connected on a fully-connected graph the version age scales as $O(\\log n)$. Ours is the first work to show an age scaling result for a connectivity structure other than the ring and the fully-connected network, which constitute the two extremes of network connectivity. Our work helps fill the gap between these two extremes by analyzing a large variety of graphs with intermediate connectivity, thus providing insight into the relationship between the connectivity structure of the network and the version age, and uncovering a network connectivity--information freshness tradeoff.",
    "authors": [
      "Arunabh Srivastava",
      "Sennur Ulukus"
    ],
    "url": "http://arxiv.org/abs/2405.19310v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IT",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "939017f8-9587-4410-8d0c-ecc5a53ab42c": {
    "pk": "939017f8-9587-4410-8d0c-ecc5a53ab42c",
    "title": "SDPRLayers: Certifiable Backpropagation Through Polynomial Optimization Problems in Robotics",
    "abstract": "Differentiable optimization is a powerful new paradigm capable of reconciling model-based and learning-based approaches in robotics. However, the majority of robotics optimization problems are non-convex and current differentiable optimization techniques are therefore prone to convergence to local minima. When this occurs, the gradients provided by these existing solvers can be wildly inaccurate and will ultimately corrupt the training process. On the other hand, any non-convex robotics problems can be framed as polynomial optimization problems and, in turn, admit convex relaxations that can be used to recover a global solution via so-called certifiably correct methods. We present SDPRLayers, an approach that leverages these methods as well as state-of-the-art convex implicit differentiation techniques to provide certifiably correct gradients throughout the training process. We introduce this approach and showcase theoretical results that provide conditions under which correctness of the gradients is guaranteed. We demonstrate our approach on two simple-but-demonstrative simulated examples, which expose the potential pitfalls of existing, state-of-the-art, differentiable optimization methods. We apply our method in a real-world application: we train a deep neural network to detect image keypoints for robot localization in challenging lighting conditions. An open-source, PyTorch implementation of SDPRLayers will be made available upon paper acceptance.",
    "authors": [
      "Connor Holmes",
      "Frederike D\u00fcmbgen",
      "Timothy D. Barfoot"
    ],
    "url": "http://arxiv.org/abs/2405.19309v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8d39c4af-e70e-417a-adce-9983e6492cc6": {
    "pk": "8d39c4af-e70e-417a-adce-9983e6492cc6",
    "title": "Uniform-in-time estimates on the size of chaos for interacting Brownian particles",
    "abstract": "We consider a system of classical Brownian particles interacting via a smooth long-range potential in the mean-field regime, and we analyze the propagation of chaos in form of sharp, uniform-in-time estimates on many-particle correlation functions. Our results cover both the kinetic Langevin setting and the corresponding overdamped Brownian dynamics. The approach is mainly based on so-called Lions expansions, which we combine with new diagrammatic tools to capture many-particle cancellations, as well as with fine ergodic estimates on the linearized mean-field equation, and with discrete stochastic calculus with respect to initial data. In the process, we derive some new ergodic estimates for the linearized Vlasov-Fokker-Planck kinetic equation that are of independent interest. Our analysis also leads to uniform-in-time concentration estimates and to a uniform-in-time quantitative central limit theorem for the empirical measure associated with the particle dynamics.",
    "authors": [
      "Armand Bernou",
      "Mitia Duerinckx"
    ],
    "url": "http://arxiv.org/abs/2405.19306v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.AP",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f5cf1d82-476b-4824-9bd7-90c16a39b418": {
    "pk": "f5cf1d82-476b-4824-9bd7-90c16a39b418",
    "title": "Safe and Efficient Estimation for Robotics through the Optimal Use of Resources",
    "abstract": "In order to operate in and interact with the physical world, robots need to have estimates of the current and future state of the environment. We thus equip robots with sensors and build models and algorithms that, given some measurements, produce estimates of the current or future states. Environments can be unpredictable and sensors are not perfect. Therefore, it is important to both use all information available, and to do so optimally: making sure that we get the best possible answer from the amount of information we have. However, in prevalent research, uncommon sensors, such as sound or radio-frequency signals, are commonly ignored for state estimation; and the most popular solvers employed to produce state estimates are only of local nature, meaning they may produce suboptimal estimates for the typically non-convex estimation problems. My research aims to use resources more optimally, by building on 1) multi-modality: using ubiquitous RF transceivers and microphones to support state estimation, 2) building certifiably optimal solvers and 3) learning and improving adequate models from data.",
    "authors": [
      "Frederike D\u00fcmbgen"
    ],
    "url": "http://arxiv.org/abs/2405.19301v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "77793fb3-4abc-471c-abb5-cf789361edd6": {
    "pk": "77793fb3-4abc-471c-abb5-cf789361edd6",
    "title": "Expert-Guided Extinction of Toxic Tokens for Debiased Generation",
    "abstract": "Large language models (LLMs) can elicit social bias during generations, especially when inference with toxic prompts. Controlling the sensitive attributes in generation encounters challenges in data distribution, generalizability, and efficiency. Specifically, fine-tuning and retrieval demand extensive unbiased corpus, while direct prompting requires meticulously curated instructions for correcting the output in multiple rounds of thoughts but poses challenges on memory and inference latency. In this work, we propose the Expert-Guided Extinction of Toxic Tokens for Debiased Generation (EXPOSED) to eliminate the undesired harmful outputs for LLMs without the aforementioned requirements. EXPOSED constructs a debiasing expert based on the abundant toxic corpus to expose and elicit the potentially dangerous tokens. It then processes the output to the LLMs and constructs a fair distribution by suppressing and attenuating the toxic tokens. EXPOSED is evaluated on fairness benchmarks over three LLM families. Extensive experiments demonstrate that compared with other baselines, the proposed EXPOSED significantly reduces the potential social bias while balancing fairness and generation performance.",
    "authors": [
      "Xueyao Sun",
      "Kaize Shi",
      "Haoran Tang",
      "Guandong Xu",
      "Qing Li"
    ],
    "url": "http://arxiv.org/abs/2405.19299v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b109b6af-dc08-473f-9e55-6e96eeacb922": {
    "pk": "b109b6af-dc08-473f-9e55-6e96eeacb922",
    "title": "Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare",
    "abstract": "While recent advancements in large multimodal models (LMMs) have significantly improved their abilities in image quality assessment (IQA) relying on absolute quality rating, how to transfer reliable relative quality comparison outputs to continuous perceptual quality scores remains largely unexplored. To address this gap, we introduce Compare2Score-an all-around LMM-based no-reference IQA (NR-IQA) model, which is capable of producing qualitatively comparative responses and effectively translating these discrete comparative levels into a continuous quality score. Specifically, during training, we present to generate scaled-up comparative instructions by comparing images from the same IQA dataset, allowing for more flexible integration of diverse IQA datasets. Utilizing the established large-scale training corpus, we develop a human-like visual quality comparator. During inference, moving beyond binary choices, we propose a soft comparison method that calculates the likelihood of the test image being preferred over multiple predefined anchor images. The quality score is further optimized by maximum a posteriori estimation with the resulting probability matrix. Extensive experiments on nine IQA datasets validate that the Compare2Score effectively bridges text-defined comparative levels during training with converted single image quality score for inference, surpassing state-of-the-art IQA models across diverse scenarios. Moreover, we verify that the probability-matrix-based inference conversion not only improves the rating accuracy of Compare2Score but also zero-shot general-purpose LMMs, suggesting its intrinsic effectiveness.",
    "authors": [
      "Hanwei Zhu",
      "Haoning Wu",
      "Yixuan Li",
      "Zicheng Zhang",
      "Baoliang Chen",
      "Lingyu Zhu",
      "Yuming Fang",
      "Guangtao Zhai",
      "Weisi Lin",
      "Shiqi Wang"
    ],
    "url": "http://arxiv.org/abs/2405.19298v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b8e8441a-69f4-4072-b4bd-f1b0e8a9a083": {
    "pk": "b8e8441a-69f4-4072-b4bd-f1b0e8a9a083",
    "title": "Act Natural! Projecting Autonomous System Trajectories Into Naturalistic Behavior Sets",
    "abstract": "Autonomous agents operating around human actors must consider how their behaviors might affect those humans, even when not directly interacting with them. To this end, it is often beneficial to be predictable and appear naturalistic. Existing methods to address this problem use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior or require significant amounts of data. In contrast, we propose a technique for modeling naturalistic behavior as a set of convex hulls computed over a relatively small dataset of human behavior. Given this set, we design an optimization-based filter which projects arbitrary trajectories into it to make them more naturalistic for autonomous agents to execute while also satisfying dynamics constraints. We demonstrate our methods on real-world human driving data from the inD intersection dataset (Bock et al., 2020).",
    "authors": [
      "Hamzah I. Khan",
      "Adam J. Thorpe",
      "David Fridovich-Keil"
    ],
    "url": "http://arxiv.org/abs/2405.19292v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.MA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2f24047c-e835-46cc-b26a-e7c1884e5e9c": {
    "pk": "2f24047c-e835-46cc-b26a-e7c1884e5e9c",
    "title": "Grasp as You Say: Language-guided Dexterous Grasp Generation",
    "abstract": "This paper explores a novel task \"\"Dexterous Grasp as You Say\"\" (DexGYS), enabling robots to perform dexterous grasping based on human commands expressed in natural language. However, the development of this field is hindered by the lack of datasets with natural human guidance; thus, we propose a language-guided dexterous grasp dataset, named DexGYSNet, offering high-quality dexterous grasp annotations along with flexible and fine-grained human language guidance. Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system. Equipped with this dataset, we introduce the DexGYSGrasp framework for generating dexterous grasps based on human language instructions, with the capability of producing grasps that are intent-aligned, high quality and diversity. To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives and introduce two components to realize them. The first component learns the grasp distribution focusing on intention alignment and generation diversity. And the second component refines the grasp quality while maintaining intention consistency. Extensive experiments are conducted on DexGYSNet and real world environment for validation.",
    "authors": [
      "Yi-Lin Wei",
      "Jian-Jian Jiang",
      "Chengyi Xing",
      "Xiantuo Tan",
      "Xiao-Ming Wu",
      "Hao Li",
      "Mark Cutkosky",
      "Wei-Shi Zheng"
    ],
    "url": "http://arxiv.org/abs/2405.19291v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3366fea4-9b3c-4178-a7e8-4410d9317996": {
    "pk": "3366fea4-9b3c-4178-a7e8-4410d9317996",
    "title": "Integrating Multi-scale Contextualized Information for Byte-based Neural Machine Translation",
    "abstract": "Subword tokenization is a common method for vocabulary building in Neural Machine Translation (NMT) models. However, increasingly complex tasks have revealed its disadvantages. First, a vocabulary cannot be modified once it is learned, making it hard to adapt to new words. Second, in multilingual translation, the imbalance in data volumes across different languages spreads to the vocabulary, exacerbating translations involving low-resource languages. While byte-based tokenization addresses these issues, byte-based models struggle with the low information density inherent in UTF-8 byte sequences. Previous works enhance token semantics through local contextualization but fail to select an appropriate contextualizing scope based on the input. Consequently, we propose the Multi-Scale Contextualization (MSC) method, which learns contextualized information of varying scales across different hidden state dimensions. It then leverages the attention module to dynamically integrate the multi-scale contextualized information. Experiments show that MSC significantly outperforms subword-based and other byte-based methods in both multilingual and out-of-domain scenarios. Code can be found in https://github.com/ictnlp/Multiscale-Contextualization.",
    "authors": [
      "Langlin Huang",
      "Yang Feng"
    ],
    "url": "http://arxiv.org/abs/2405.19290v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "4edff845-7dd0-47fe-987d-536db2b79835": {
    "pk": "4edff845-7dd0-47fe-987d-536db2b79835",
    "title": "Archetype-Based Redshift Estimation for the Dark Energy Spectroscopic Instrument Survey",
    "abstract": "We present a computationally efficient galaxy archetype-based redshift estimation and spectral classification method for the Dark Energy Survey Instrument (DESI) survey. The DESI survey currently relies on a redshift fitter and spectral classifier using a linear combination of PCA-derived templates, which is very efficient in processing large volumes of DESI spectra within a short time frame. However, this method occasionally yields unphysical model fits for galaxies and fails to adequately absorb calibration errors that may still be occasionally visible in the reduced spectra. Our proposed approach improves upon this existing method by refitting the spectra with carefully generated physical galaxy archetypes combined with additional terms designed to absorb data reduction defects and provide more physical models to the DESI spectra. We test our method on an extensive dataset derived from the survey validation (SV) and Year 1 (Y1) data of DESI. Our findings indicate that the new method delivers marginally better redshift success for SV tiles while reducing catastrophic redshift failure by $10-30\\%$. At the same time, results from millions of targets from the main survey show that our model has relatively higher redshift success and purity rates ($0.5-0.8\\%$ higher) for galaxy targets while having similar success for QSOs. These improvements also demonstrate that the main DESI redshift pipeline is generally robust. Additionally, it reduces the false positive redshift estimation by $5-40\\%$ for sky fibers. We also discuss the generic nature of our method and how it can be extended to other large spectroscopic surveys, along with possible future improvements.",
    "authors": [
      "Abhijeet Anand",
      "Julien Guy",
      "Stephen Bailey",
      "John Moustakas",
      "J. Aguilar",
      "S. Ahlen",
      "A. Bolton",
      "A. Brodzeller",
      "D. Brooks",
      "T. Claybaugh",
      "S. Cole",
      "B. Dey",
      "K. Fanning",
      "J. Forero-Romero",
      "E. Gazta\u00f1aga",
      "S. Gontcho A Gontcho",
      "L. Le Guillou",
      "G. Gutierrez",
      "K. Honscheid",
      "C. Howlett",
      "S. Juneau",
      "D. Kirkby",
      "T. Kisner",
      "A. Kremin",
      "A. Lambert",
      "M. Landriau",
      "A. de la Macorra",
      "M. Manera",
      "A. Meisner",
      "R. Miquel",
      "E. Mueller",
      "G. Niz",
      "N. Palanque-Delabrouille",
      "W. Percival",
      "C. Poppett",
      "F. Prada",
      "A. Raichoor",
      "M. Rezaie",
      "G. Rossi",
      "E. Sanchez",
      "E. Schlafly",
      "D. Schlegel",
      "M. Schubnell",
      "D. Sprayberry",
      "G. Tarl\u00e9",
      "C. Warner",
      "B. A. Weaver",
      "R. Zhou",
      "H. Zou"
    ],
    "url": "http://arxiv.org/abs/2405.19288v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.CO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c81846d1-e7ff-4bf8-970c-9c1abd13a85e": {
    "pk": "c81846d1-e7ff-4bf8-970c-9c1abd13a85e",
    "title": "EDGE: A new model for Nuclear Star Cluster formation in dwarf galaxies",
    "abstract": "Nuclear Star Clusters (NSCs) are amongst the densest stellar systems in the Universe and are found at the centres of many bright spiral and elliptical galaxies, and up to ${\\sim}$40% of dwarf galaxies. However, their formation mechanisms, and possible links to globular clusters (GCs), remain debated. This paper uses the EDGE simulations - a collection of zoom-in, cosmological simulations of isolated dwarf galaxies -- to present a new formation mechanism for NSCs. We find that, at a gas spatial and mass resolution of ${\\sim}3\\,$pc and ${\\sim}161$ M$_\\odot$, respectively, NSCs naturally emerge in a subset of our EDGE dwarfs with redshift-zero halo masses of $\\rm{M}_{\\rm{r}200\\rm{c}} \\sim 5 \\times 10^9$ M$_\\odot$. These dwarfs are quenched by reionisation, but retain a significant reservoir of gas that is unable to cool and form stars. Sometime after reionisation, the dwarfs then undergo a major (${\\sim}$1:1) merger that excites rapid gas cooling, leading to a significant starburst. An NSC forms in this starburst that then quenches star formation thereafter. The result is a nucleated dwarf that has two stellar populations with distinct age: one pre-reionisation and one post-reionisation. Our mechanism is unique for two key reasons. Firstly, the low mass of the host dwarf means that NSCs, formed in this way, can accrete onto galaxies of almost all masses, potentially seeding the formation of NSCs everywhere. Secondly, our model predicts that NSCs should have at least two stellar populations with a large ($\\gtrsim$1 billion year) age separation. This yields a predicted colour magnitude diagram for our nucleated dwarfs that has two distinct main sequence turnoffs. Several GCs orbiting the Milky Way, including Omega Centauri and M54, show exactly this behaviour, suggesting that they may, in fact, be accreted NSCs.",
    "authors": [
      "Emily I. Gray",
      "Justin I. Read",
      "Ethan Taylor",
      "Matthew D. A. Orkney",
      "Martin P. Rey",
      "Robert M. Yates",
      "Stacy Y. Kim",
      "Noelia E. D. No\u00ebl",
      "Oscar Agertz",
      "Eric Andersson",
      "Andrew Pontzen"
    ],
    "url": "http://arxiv.org/abs/2405.19286v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.GA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3756892b-7bf0-46fa-b1f4-0c32a7113a83": {
    "pk": "3756892b-7bf0-46fa-b1f4-0c32a7113a83",
    "title": "MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection",
    "abstract": "Abstract Meaning Representation (AMR) is a semantic formalism that captures the core meaning of an utterance. There has been substantial work developing AMR corpora in English and more recently across languages, though the limited size of existing datasets and the cost of collecting more annotations are prohibitive. With both engineering and scientific questions in mind, we introduce MASSIVE-AMR, a dataset with more than 84,000 text-to-graph annotations, currently the largest and most diverse of its kind: AMR graphs for 1,685 information-seeking utterances mapped to 50+ typologically diverse languages. We describe how we built our resource and its unique features before reporting on experiments using large language models for multilingual AMR and SPARQL parsing as well as applying AMRs for hallucination detection in the context of knowledge base question answering, with results shedding light on persistent issues using LLMs for structured parsing.",
    "authors": [
      "Michael Regan",
      "Shira Wein",
      "George Baker",
      "Emilio Monti"
    ],
    "url": "http://arxiv.org/abs/2405.19285v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "11f337b8-94be-4dbd-bb29-fde49edc30e3": {
    "pk": "11f337b8-94be-4dbd-bb29-fde49edc30e3",
    "title": "Optimizing Foundation Model Inference on a Many-tiny-core Open-source RISC-V Platform",
    "abstract": "Transformer-based foundation models have become crucial for various domains, most notably natural language processing (NLP) or computer vision (CV). These models are predominantly deployed on high-performance GPUs or hardwired accelerators with highly customized, proprietary instruction sets. Until now, limited attention has been given to RISC-V-based general-purpose platforms. In our work, we present the first end-to-end inference results of transformer models on an open-source many-tiny-core RISC-V platform implementing distributed Softmax primitives and leveraging ISA extensions for SIMD floating-point operand streaming and instruction repetition, as well as specialized DMA engines to minimize costly main memory accesses and to tolerate their latency. We focus on two foundational transformer topologies, encoder-only and decoder-only models. For encoder-only models, we demonstrate a speedup of up to 12.8x between the most optimized implementation and the baseline version. We reach over 79% FPU utilization and 294 GFLOPS/W, outperforming State-of-the-Art (SoA) accelerators by more than 2x utilizing the HW platform while achieving comparable throughput per computational unit. For decoder-only topologies, we achieve 16.1x speedup in the Non-Autoregressive (NAR) mode and up to 35.6x speedup in the Autoregressive (AR) mode compared to the baseline implementation. Compared to the best SoA dedicated accelerator, we achieve 2.04x higher FPU utilization.",
    "authors": [
      "Viviane Potocnik",
      "Luca Colagrande",
      "Tim Fischer",
      "Luca Bertaccini",
      "Daniele Jahier Pagliari",
      "Alessio Burrello",
      "Luca Benini"
    ],
    "url": "http://arxiv.org/abs/2405.19284v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "83b48085-1547-40c8-930c-f174efe5ba33": {
    "pk": "83b48085-1547-40c8-930c-f174efe5ba33",
    "title": "Programmable Motion Generation for Open-Set Motion Control Tasks",
    "abstract": "Character animation in real-world scenarios necessitates a variety of constraints, such as trajectories, key-frames, interactions, etc. Existing methodologies typically treat single or a finite set of these constraint(s) as separate control tasks. They are often specialized, and the tasks they address are rarely extendable or customizable. We categorize these as solutions to the close-set motion control problem. In response to the complexity of practical motion control, we propose and attempt to solve the open-set motion control problem. This problem is characterized by an open and fully customizable set of motion control tasks. To address this, we introduce a new paradigm, programmable motion generation. In this paradigm, any given motion control task is broken down into a combination of atomic constraints. These constraints are then programmed into an error function that quantifies the degree to which a motion sequence adheres to them. We utilize a pre-trained motion generation model and optimize its latent code to minimize the error function of the generated motion. Consequently, the generated motion not only inherits the prior of the generative model but also satisfies the required constraints. Experiments show that we can generate high-quality motions when addressing a wide range of unseen tasks. These tasks encompass motion control by motion dynamics, geometric constraints, physical laws, interactions with scenes, objects or the character own body parts, etc. All of these are achieved in a unified approach, without the need for ad-hoc paired training data collection or specialized network designs. During the programming of novel tasks, we observed the emergence of new skills beyond those of the prior model. With the assistance of large language models, we also achieved automatic programming. We hope that this work will pave the way for the motion control of general AI agents.",
    "authors": [
      "Hanchao Liu",
      "Xiaohang Zhan",
      "Shaoli Huang",
      "Tai-Jiang Mu",
      "Ying Shan"
    ],
    "url": "http://arxiv.org/abs/2405.19283v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a4aecc0e-75b3-4a88-a76e-dac97ee50547": {
    "pk": "a4aecc0e-75b3-4a88-a76e-dac97ee50547",
    "title": "On the theory of relativistic Brownian motion",
    "abstract": "The approach to the theory of a relativistic random process is considered by the path integral method as Brownian motion taking into account the boundedness of speed. An attempt was made to build a relativistic analogue of the Wiener measure as a weak limit of finite-difference approximations. A formula has been proposed for calculating the probability particle transition during relativistic Brownian motion. Calculations were carried out by three different methods with identical results. Along the way, exact and asymptotic formulas for the volume of some parts and sections of an N-1-dimensional unit cube were obtained. They can have independent value.",
    "authors": [
      "E. A. Kurianovich",
      "A. I. Mikhailov",
      "I. V. Volovich"
    ],
    "url": "http://arxiv.org/abs/2405.19282v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "gr-qc",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c23d78de-b746-4b77-95c4-e3c8f77ce2b2": {
    "pk": "c23d78de-b746-4b77-95c4-e3c8f77ce2b2",
    "title": "Detecting the Stochastic Gravitational Wave Background from Primordial Black Holes in Slow-reheating Scenarios",
    "abstract": "After primordial inflation, the universe may have experienced a prolonged reheating epoch, potentially leading to a phase of matter domination supported by the oscillating inflaton field. During such an epoch, perturbations in the inflaton virialize upon reentering the cosmological horizon, forming inflaton structures. If the primordial overdensities are sufficiently large, these structures collapse to form primordial black holes (PBHs). To occur at a significant rate, this process requires an enhanced primordial power spectrum (PPS) at small scales. The enhancement of the PPS, as well as the formation and tidal interaction of the primordial structures, will in turn source a stochastic gravitational wave background(SGWB) that could be detected by current and/or future gravitational wave detectors. In this paper, we study the SGWB arising from these different sources during slow-reheating, focusing on a PPS that satisfies the requirements necessary for the formation of PBHs with a mass of $M_{\\rm PBH}\\simeq 10^{21}$ and that constitute the entirety of dark matter in the universe.",
    "authors": [
      "Luis E. Padilla",
      "Juan Carlos Hidalgo",
      "Karim A. Malik",
      "David Mulryne"
    ],
    "url": "http://arxiv.org/abs/2405.19271v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.CO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "291efac9-ba9c-4da6-a86a-426a3cf18d33": {
    "pk": "291efac9-ba9c-4da6-a86a-426a3cf18d33",
    "title": "Rich-Observation Reinforcement Learning with Continuous Latent Dynamics",
    "abstract": "Sample-efficiency and reliability remain major bottlenecks toward wide adoption of reinforcement learning algorithms in continuous settings with high-dimensional perceptual inputs. Toward addressing these challenges, we introduce a new theoretical framework, RichCLD (Rich-Observation RL with Continuous Latent Dynamics), in which the agent performs control based on high-dimensional observations, but the environment is governed by low-dimensional latent states and Lipschitz continuous dynamics. Our main contribution is a new algorithm for this setting that is provably statistically and computationally efficient. The core of our algorithm is a new representation learning objective; we show that prior representation learning schemes tailored to discrete dynamics do not naturally extend to the continuous setting. Our new objective is amenable to practical implementation, and empirically, we find that it compares favorably to prior schemes in a standard evaluation protocol. We further provide several insights into the statistical complexity of the RichCLD framework, in particular proving that certain notions of Lipschitzness that admit sample-efficient learning in the absence of rich observations are insufficient in the rich-observation setting.",
    "authors": [
      "Yuda Song",
      "Lili Wu",
      "Dylan J. Foster",
      "Akshay Krishnamurthy"
    ],
    "url": "http://arxiv.org/abs/2405.19269v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bb359331-8263-4bdd-8411-1f3da841bbaa": {
    "pk": "bb359331-8263-4bdd-8411-1f3da841bbaa",
    "title": "Photonic bilayer Chern insulator with corner states",
    "abstract": "Photonic Chern insulators can be implemented in gyromagnetic photonic crystals with broken time-reversal (TR) symmetry. They exhibit gapless chiral edge states (CESs), enabling unidirectional propagation and demonstrating exceptional resilience to localization even in the presence of defects or disorders. However, when two Chern insulators with opposite Chern numbers are stacked together, this one-way nature can be nullified, causing the originally gapless CESs to become gapped. Recent theoretical works have proposed achieving such a topological phase transition in condensed matter systems using antiferromagnetic thin films such as MnBi2Te4 or by coupling two quantum spin/anomalous Hall insulators, but these approaches have yet to be realized experimentally. In a bilayer gyromagnetic photonic crystal arranged in an antiferromagnetic layer configuration, our experimental observations reveal that interlayer coupling initiates a transition from a Chern insulating phase to a higher-order topological phase. This transition results in the gapping of CESs and triggers the emergence of corner states within the bandgap. The corner mode energy within the gap can be attributed to CESs interaction, forming a Jackiw-Rebbi topological domain wall mode at the corner. These states exhibit heightened resilience against defects, setting them apart from their time-reversal symmetric counterparts.",
    "authors": [
      "Subhaskar Mandal",
      "Ziyao Wang",
      "Rimi Banerjee",
      "Hau Tian Teo",
      "Peiheng Zhou",
      "Xiang Xi",
      "Zhen Gao",
      "Gui-Geng Liu",
      "Baile Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.19267v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.mes-hall",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "074e65d9-8344-41f5-aebd-6d2f72f70fa7": {
    "pk": "074e65d9-8344-41f5-aebd-6d2f72f70fa7",
    "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications",
    "abstract": "Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures. To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline. In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase, a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct doctor downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. Our model and dataset will be open-source for community development.",
    "authors": [
      "Dingkang Yang",
      "Jinjie Wei",
      "Dongling Xiao",
      "Shunli Wang",
      "Tong Wu",
      "Gang Li",
      "Mingcheng Li",
      "Shuaibing Wang",
      "Jiawei Chen",
      "Yue Jiang",
      "Qingyao Xu",
      "Ke Li",
      "Peng Zhai",
      "Lihua Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.19266v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "09d56dd3-9865-45e7-ba9e-9ec929bfa748": {
    "pk": "09d56dd3-9865-45e7-ba9e-9ec929bfa748",
    "title": "AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data",
    "abstract": "Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence.",
    "authors": [
      "Zifan Song",
      "Yudong Wang",
      "Wenwei Zhang",
      "Kuikun Liu",
      "Chengqi Lyu",
      "Demin Song",
      "Qipeng Guo",
      "Hang Yan",
      "Dahua Lin",
      "Kai Chen",
      "Cairong Zhao"
    ],
    "url": "http://arxiv.org/abs/2405.19265v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7999b8a1-33fd-4055-86ff-010de9a91017": {
    "pk": "7999b8a1-33fd-4055-86ff-010de9a91017",
    "title": "Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models",
    "abstract": "Large language models are usually fine-tuned to align with human preferences. However, fine-tuning a large language model can be challenging. In this work, we introduce $\\textit{weak-to-strong search}$, framing the alignment of a large language model as a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model. This method serves both as (i) a compute-efficient model up-scaling strategy that avoids directly tuning the large model and as (ii) an instance of weak-to-strong generalization that enhances a strong model with weak test-time guidance. Empirically, we demonstrate the flexibility of weak-to-strong search across different tasks. In controlled-sentiment generation and summarization, we use tuned and untuned $\\texttt{gpt2}$s to effectively improve the alignment of large models without additional training. Crucially, in a more difficult instruction-following benchmark, AlpacaEval 2.0, we show that reusing off-the-shelf small model pairs (e.g., $\\texttt{zephyr-7b-beta}$ and its untuned version) can significantly improve the length-controlled win rates of both white-box and black-box large models against $\\texttt{gpt-4-turbo}$ (e.g., $34.4 \\rightarrow 37.9$ for $\\texttt{Llama-3-70B-Instruct}$ and $16.0 \\rightarrow 20.1$ for $\\texttt{gpt-3.5-turbo-instruct}$), despite the small models' low win rates $\\approx 10.0$.",
    "authors": [
      "Zhanhui Zhou",
      "Zhixuan Liu",
      "Jie Liu",
      "Zhichen Dong",
      "Chao Yang",
      "Yu Qiao"
    ],
    "url": "http://arxiv.org/abs/2405.19262v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8f19ab12-da05-4598-8637-285f77cf786b": {
    "pk": "8f19ab12-da05-4598-8637-285f77cf786b",
    "title": "Faster Cascades via Speculative Decoding",
    "abstract": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency. Both approaches involve interleaving models of different sizes, but via fundamentally distinct mechanisms: cascades employ a deferral rule that invokes the larger model only for \"hard\" inputs, while speculative decoding uses speculative execution to primarily invoke the larger model in parallel verification mode. These mechanisms offer different benefits: empirically, cascades are often capable of yielding better quality than even the larger model, while theoretically, speculative decoding offers a guarantee of quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule. Through experiments with T5 models on benchmark language tasks, we show that the proposed approach yields better cost-quality trade-offs than cascading and speculative decoding baselines.",
    "authors": [
      "Harikrishna Narasimhan",
      "Wittawat Jitkrittum",
      "Ankit Singh Rawat",
      "Seungyeon Kim",
      "Neha Gupta",
      "Aditya Krishna Menon",
      "Sanjiv Kumar"
    ],
    "url": "http://arxiv.org/abs/2405.19261v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d6f05e8e-9b7f-4d4f-a3fa-5cb1305db1e5": {
    "pk": "d6f05e8e-9b7f-4d4f-a3fa-5cb1305db1e5",
    "title": "Hilbert Space Diffusion in Systems with Approximate Symmetries",
    "abstract": "Random matrix theory (RMT) universality is the defining property of quantum mechanical chaotic systems, and can be probed by observables like the spectral form factor (SFF). In this paper, we describe systematic deviations from RMT behaviour at intermediate time scales in systems with approximate symmetries. At early times, the symmetries allow us to organize the Hilbert space into approximately decoupled sectors, each of which contributes independently to the SFF. At late times, the SFF transitions into the final ramp of the fully mixed chaotic Hamiltonian. For approximate continuous symmetries, the transitional behaviour is governed by a universal process that we call Hilbert space diffusion. The diffusion constant corresponding to this process is related to the relaxation rate of the associated nearly conserved charge. By implementing a chaotic sigma model for Hilbert-space diffusion, we formulate an analytic theory of this process which agrees quantitatively with our numerical results for different examples.",
    "authors": [
      "Rahel L. Baumgartner",
      "Luca V. Delacr\u00e9taz",
      "Pranjal Nayak",
      "Julian Sonner"
    ],
    "url": "http://arxiv.org/abs/2405.19260v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.stat-mech",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "35d4f5b1-7f45-41a8-8847-4e82979ce5b2": {
    "pk": "35d4f5b1-7f45-41a8-8847-4e82979ce5b2",
    "title": "Weak Generative Sampler to Efficiently Sample Invariant Distribution of Stochastic Differential Equation",
    "abstract": "Sampling invariant distributions from an Ito diffusion process presents a significant challenge in stochastic simulation. Traditional numerical solvers for stochastic differential equations require both a fine step size and a lengthy simulation period, resulting in both biased and correlated samples. Current deep learning-based method solves the stationary Fokker--Planck equation to determine the invariant probability density function in form of deep neural networks, but they generally do not directly address the problem of sampling from the computed density function. In this work, we introduce a framework that employs a weak generative sampler (WGS) to directly generate independent and identically distributed (iid) samples induced by a transformation map derived from the stationary Fokker--Planck equation. Our proposed loss function is based on the weak form of the Fokker--Planck equation, integrating normalizing flows to characterize the invariant distribution and facilitate sample generation from the base distribution. Our randomized test function circumvents the need for mini-max optimization in the traditional weak formulation. Distinct from conventional generative models, our method neither necessitates the computationally intensive calculation of the Jacobian determinant nor the invertibility of the transformation map. A crucial component of our framework is the adaptively chosen family of test functions in the form of Gaussian kernel functions with centres selected from the generated data samples. Experimental results on several benchmark examples demonstrate the effectiveness of our method, which offers both low computational costs and excellent capability in exploring multiple metastable states.",
    "authors": [
      "Zhiqiang Cai",
      "Yu Cao",
      "Yuanfei Huang",
      "Xiang Zhou"
    ],
    "url": "http://arxiv.org/abs/2405.19256v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "01c5528a-9507-4843-9e81-f29f4cc9db54": {
    "pk": "01c5528a-9507-4843-9e81-f29f4cc9db54",
    "title": "Towards Next-Generation Urban Decision Support Systems through AI-Powered Generation of Scientific Ontology using Large Language Models -- A Case in Optimizing Intermodal Freight Transportation",
    "abstract": "The incorporation of Artificial Intelligence (AI) models into various optimization systems is on the rise. Yet, addressing complex urban and environmental management problems normally requires in-depth domain science and informatics expertise. This expertise is essential for deriving data and simulation-driven for informed decision support. In this context, we investigate the potential of leveraging the pre-trained Large Language Models (LLMs). By adopting ChatGPT API as the reasoning core, we outline an integrated workflow that encompasses natural language processing, methontology-based prompt tuning, and transformers. This workflow automates the creation of scenario-based ontology using existing research articles and technical manuals of urban datasets and simulations. The outcomes of our methodology are knowledge graphs in widely adopted ontology languages (e.g., OWL, RDF, SPARQL). These facilitate the development of urban decision support systems by enhancing the data and metadata modeling, the integration of complex datasets, the coupling of multi-domain simulation models, and the formulation of decision-making metrics and workflow. The feasibility of our methodology is evaluated through a comparative analysis that juxtaposes our AI-generated ontology with the well-known Pizza Ontology employed in tutorials for popular ontology software (e.g., prot\\'eg\\'e). We close with a real-world case study of optimizing the complex urban system of multi-modal freight transportation by generating anthologies of various domain data and simulations to support informed decision-making.",
    "authors": [
      "Jose Tupayachi",
      "Haowen Xu",
      "Olufemi A. Omitaomu",
      "Mustafa Can Camur",
      "Aliza Sharmin",
      "Xueping Li"
    ],
    "url": "http://arxiv.org/abs/2405.19255v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a99d6a80-30f3-4368-8fd1-5e2b7f0d2570": {
    "pk": "a99d6a80-30f3-4368-8fd1-5e2b7f0d2570",
    "title": "Dark matter admixed neutron stars with a realistic nuclear equation of state from chiral nuclear interactions",
    "abstract": "We study the effects of dark matter on the structural properties of neutron stars. In particular we investigate how the presence of a dark matter component influences the mass-radius relation, the value of the maximum mass of a neutron star and others stellar properties. To model ordinary matter we use a state-of-the-art equation of state of $\\beta$-stable nuclear matter obtained using the Brueckner-Hartree-Fock quantum many-body approach starting from two-body and three-body nuclear interactions derived from chiral effective field theory. The dark matter component of the star is modeled as a non-self-annihilating system of spin $1/2$ fermions and its equation of state as an ideal relativistic Fermi gas. The equilibrium configurations of these dark matter admixed neutron stars (DANS) are calculated by solving a generalization of the Tolman-Oppenheimer-Volkoff equations to the case where the system consists of two perfect fluids interacting solely through gravity. We find that, depending on the dark matter particle mass $m_\\chi$, one can have somehow opposite effects on the stellar properties. In the case $m_\\chi = 1\\, \\mathrm{GeV}$, the stellar gravitational maximum mass $M_{max}$ decreases, whereas in the case $m_\\chi = 0.1\\, \\mathrm{GeV}$, $M_{max}$ increases with respect to the maximum mass of ordinary neutron stars. We also show that the presence of dark matter has indirect sizeable effect on the proton fraction in the ordinary matter fluid and, in the case $m_\\chi = 1\\, \\mathrm{GeV}$, results in a decrease of the threshold gravitational mass $M_{tot}^{durca}$ for having direct URCA processes and fast stellar cooling. Finally we study the stability of dark matter admixed neutron stars with respect to radial perturbations.",
    "authors": [
      "Domenico Scordino",
      "Ignazio Bombaci"
    ],
    "url": "http://arxiv.org/abs/2405.19251v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.HE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a8f7a7a0-e2ba-4876-be6b-0dea5a7adbb0": {
    "pk": "a8f7a7a0-e2ba-4876-be6b-0dea5a7adbb0",
    "title": "Kotlin ML Pack: Technical Report",
    "abstract": "In this technical report, we present three novel datasets of Kotlin code: KStack, KStack-clean, and KExercises. We also describe the results of fine-tuning CodeLlama and DeepSeek models on this data. Additionally, we present a version of the HumanEval benchmark rewritten by human experts into Kotlin - both the solutions and the tests. Our results demonstrate that small, high-quality datasets (KStack-clean and KExercises) can significantly improve model performance on code generation tasks, achieving up to a 16-point increase in pass rate on the HumanEval benchmark. Lastly, we discuss potential future work in the field of improving language modeling for Kotlin, including the use of static analysis tools in the learning process and the introduction of more intricate and realistic benchmarks.",
    "authors": [
      "Sergey Titov",
      "Mikhail Evtikhiev",
      "Anton Shapkin",
      "Oleg Smirnov",
      "Sergei Boytsov",
      "Sergei Boytsov",
      "Dariia Karaeva",
      "Maksim Sheptyakov",
      "Mikhail Arkhipov",
      "Timofey Bryksin",
      "Egor Bogomolov"
    ],
    "url": "http://arxiv.org/abs/2405.19250v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SE",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a40cfced-0216-4006-8a06-d45239dd3134": {
    "pk": "a40cfced-0216-4006-8a06-d45239dd3134",
    "title": "Comparative Study of Neighbor-based Methods for Local Outlier Detection",
    "abstract": "The neighbor-based method has become a powerful tool to handle the outlier detection problem, which aims to infer the abnormal degree of the sample based on the compactness of the sample and its neighbors. However, the existing methods commonly focus on designing different processes to locate outliers in the dataset, while the contributions of different types neighbors to outlier detection has not been well discussed. To this end, this paper studies the neighbor in the existing outlier detection algorithms and a taxonomy is introduced, which uses the three-level components of information, neighbor and methodology to define hybrid methods. This taxonomy can serve as a paradigm where a novel neighbor-based outlier detection method can be proposed by combining different components in this taxonomy. A large number of comparative experiments were conducted on synthetic and real-world datasets in terms of performance comparison and case study, and the results show that reverse K-nearest neighbor based methods achieve promising performance and dynamic selection method is suitable for working in high-dimensional space. Notably, it is verified that rationally selecting components from this taxonomy may create an algorithms superior to existing methods.",
    "authors": [
      "Zhuang Qi",
      "Junlin Zhang",
      "Xiaming Chen",
      "Xin Qi"
    ],
    "url": "http://arxiv.org/abs/2405.19247v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5c0747ba-e4e7-4070-bdbe-d7bbf77fabbd": {
    "pk": "5c0747ba-e4e7-4070-bdbe-d7bbf77fabbd",
    "title": "A numerical algorithm with linear complexity for Multi-marginal Optimal Transport with $L^1$ Cost",
    "abstract": "Numerically solving multi-marginal optimal transport (MMOT) problems is computationally prohibitive, even for moderate-scale instances involving $l\\ge4$ marginals with support sizes of $N\\ge1000$. The cost in MMOT is represented as a tensor with $N^l$ elements. Even accessing each element once incurs a significant computational burden. In fact, many algorithms require direct computation of tensor-vector products, leading to a computational complexity of $O(N^l)$ or beyond. In this paper, inspired by our previous work [$Comm. \\ Math. \\ Sci.$, 20 (2022), pp. 2053 - 2057], we observe that the costly tensor-vector products in the Sinkhorn Algorithm can be computed with a recursive process by separating summations and dynamic programming. Based on this idea, we propose a fast tensor-vector product algorithm to solve the MMOT problem with $L^1$ cost, achieving a miraculous reduction in the computational cost of the entropy regularized solution to $O(N)$. Numerical experiment results confirm such high performance of this novel method which can be several orders of magnitude faster than the original Sinkhorn algorithm.",
    "authors": [
      "Chunhui Chen",
      "Jing Chen",
      "Baojia Luo",
      "Shi Jin",
      "Hao Wu"
    ],
    "url": "http://arxiv.org/abs/2405.19246v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.NA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "255b42a7-f5fd-4409-af50-e743373c28a2": {
    "pk": "255b42a7-f5fd-4409-af50-e743373c28a2",
    "title": "DiPPeST: Diffusion-based Path Planner for Synthesizing Trajectories Applied on Quadruped Robots",
    "abstract": "We present DiPPeST, a novel image and goal conditioned diffusion-based trajectory generator for quadrupedal robot path planning. DiPPeST is a zero-shot adaptation of our previously introduced diffusion-based 2D global trajectory generator (DiPPeR). The introduced system incorporates a novel strategy for local real-time path refinements, that is reactive to camera input, without requiring any further training, image processing, or environment interpretation techniques. DiPPeST achieves 92% success rate in obstacle avoidance for nominal environments and an average of 88% success rate when tested in environments that are up to 3.5 times more complex in pixel variation than DiPPeR. A visual-servoing framework is developed to allow for real-world execution, tested on the quadruped robot, achieving 80% success rate in different environments and showcasing improved behavior than complex state-of-the-art local planners, in narrow environments.",
    "authors": [
      "Maria Stamatopoulou",
      "Jianwei Liu",
      "Dimitrios Kanoulas"
    ],
    "url": "http://arxiv.org/abs/2405.19232v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.RO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "35bc0585-e387-44f6-85c9-c56855c91234": {
    "pk": "35bc0585-e387-44f6-85c9-c56855c91234",
    "title": "Motor Imagery Task Alters Dynamics of Human Body Posture",
    "abstract": "Motor Imagery (MI) is gaining traction in both rehabilitation and sports settings, but its immediate influence on human postural control is not yet clearly understood. The focus of this study is to examine the effects of MI on the dynamics of the Center of Pressure (COP), a crucial metric for evaluating postural stability. In the experiment, thirty healthy young adults participated in four different scenarios: normal standing with both open and closed eyes, and kinesthetic motor imagery focused on mediolateral (ML) and anteroposterior (AP) sway movements. A mathematical model was developed to characterize the nonlinear dynamics of the COP and to assess the impact of MI on these dynamics. Our results show a statistically significant increase (p-value<0.05) in variables such as COP path length and Long-Range Correlation (LRC) during MI compared to the closed-eye and normal standing conditions. These observations align well with psycho-neuromuscular theory, which suggests that imagining a specific movement activates neural pathways, consequently affecting postural control. This study presents compelling evidence that motor imagery not only has a quantifiable impact on COP dynamics but also that changes in the Center of Pressure (COP) are directionally consistent with the imagined movements. This finding holds significant implications for the field of rehabilitation science, suggesting that motor imagery could be strategically utilized to induce targeted postural adjustments. Nonetheless, additional research is required to fully understand the complex mechanisms that underlie this relationship and to corroborate these results across a more diverse set of populations.",
    "authors": [
      "Fatemeh Delavari",
      "Seyyed Mohammad Reza Hashemi Golpayegani",
      "Mohammad Ali Ahmadi-Pajouh"
    ],
    "url": "http://arxiv.org/abs/2405.19228v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "q-bio.NC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "edbe4e1a-1e0b-4403-accc-5915792f7c3f": {
    "pk": "edbe4e1a-1e0b-4403-accc-5915792f7c3f",
    "title": "Metallicity Dependence of Pressure-Regulated Feedback-Modulated Star Formation in the TIGRESS-NCR Simulation Suite",
    "abstract": "We present a new suite of numerical simulations of the star-forming interstellar medium (ISM) using the TIGRESS-NCR framework, covering a wide range of galactic conditions including metallicity. The TIGRESS-NCR framework is a model of the ISM in galactic disks that solves ideal MHD equations with self-gravity in a local shearing-box, including explicit treatment of cooling and heating processes coupled with ray-tracing UV radiation transfer and resolved supernova feedback. The TIGRESS-NCR suite presented in this paper covers metallicity variation $Z'\\equiv Z/Z_\\odot\\sim 0.1-3$, gas surface density $\\Sigma_{\\rm gas}\\sim5-150{\\,M_{\\odot}{\\rm pc^{-2}}}$, and stellar surface density $\\Sigma_{\\rm star}\\sim1-50{M_{\\odot}{\\rm pc^{-2}}}$, leading to emergent SFR surface density $\\Sigma_{\\rm SFR}\\sim 10^{-4}-0.5{M_{\\odot}{\\rm kpc^{-2}yr^{-1}}}$ and ISM total midplane pressure $P_{\\rm tot}/k_B=10^3-10^6 {\\rm cm^{-3}K}$, with $P_{\\rm tot}$ equal to the ISM weight $W$. In our simulation suite, $\\Sigma_{\\rm SFR} \\propto {Z'}^{0.3}$, which can be understood based on feedback physics. We present a new calibration for the components of feedback yield $\\Upsilon$, defined as ratios between pressure (thermal, turbulent, and magnetic) and $\\Sigma_{\\rm SFR}$. We find that the thermal feedback yield varies sensitively as $\\Upsilon_{\\rm th}\\propto W^{-0.46}Z'^{-0.53}$, while the combined turbulent and magnetic feedback yield shows weaker dependence $\\Upsilon_{\\rm turb+mag}\\propto W^{-0.22}Z'^{-0.18}$. The reduced $\\Sigma_{\\rm SFR}$ at low metallicity is due mainly to enhanced thermal feedback yield resulting from reduced attenuation of UV radiation. Combining vertical dynamical equilibrium, feedback yield, and effective equation of state, we provide a new metallicity-dependent subgrid star formation prescription that can be used in cosmological simulations where the ISM is unresolved.",
    "authors": [
      "Chang-Goo Kim",
      "Eve C. Ostriker",
      "Jeong-Gyu Kim",
      "Munan Gong",
      "Greg L. Bryan",
      "Drummond B. Fielding",
      "Sultan Hassan",
      "Matthew Ho",
      "Sarah M. R. Jeffreson",
      "Rachel S. Somerville",
      "Ulrich P. Steinwandel"
    ],
    "url": "http://arxiv.org/abs/2405.19227v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.GA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "12a83982-365e-4aa7-873f-8f3fbf33e494": {
    "pk": "12a83982-365e-4aa7-873f-8f3fbf33e494",
    "title": "A study on the adequacy of common IQA measures for medical images",
    "abstract": "Image quality assessment (IQA) is standard practice in the development stage of novel machine learning algorithms that operate on images. The most commonly used IQA measures have been developed and tested for natural images, but not in the medical setting. Reported inconsistencies arising in medical images are not surprising, as they have different properties than natural images. In this study, we test the applicability of common IQA measures for medical image data by comparing their assessment to manually rated chest X-ray (5 experts) and photoacoustic image data (1 expert). Moreover, we include supplementary studies on grayscale natural images and accelerated brain MRI data. The results of all experiments show a similar outcome in line with previous findings for medical imaging: PSNR and SSIM in the default setting are in the lower range of the result list and HaarPSI outperforms the other tested measures in the overall performance. Also among the top performers in our medical experiments are the full reference measures DISTS, FSIM, LPIPS and MS-SSIM. Generally, the results on natural images yield considerably higher correlations, suggesting that the additional employment of tailored IQA measures for medical imaging algorithms is needed.",
    "authors": [
      "Anna Breger",
      "Clemens Karner",
      "Ian Selby",
      "Janek Gr\u00f6hl",
      "S\u00f6ren Dittmer",
      "Edward Lilley",
      "Judith Babar",
      "Jake Beckford",
      "Timothy J Sadler",
      "Shahab Shahipasand",
      "Arthikkaa Thavakumar",
      "Michael Roberts",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "url": "http://arxiv.org/abs/2405.19224v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fb9989ac-c6d8-45c7-af30-94f6b3ca0180": {
    "pk": "fb9989ac-c6d8-45c7-af30-94f6b3ca0180",
    "title": "Lower Bounds on the Expressivity of Recurrent Neural Language Models",
    "abstract": "The recent successes and spread of large neural language models (LMs) call for a thorough understanding of their computational ability. Describing their computational abilities through LMs' \\emph{representational capacity} is a lively area of research. However, investigation into the representational capacity of neural LMs has predominantly focused on their ability to \\emph{recognize} formal languages. For example, recurrent neural networks (RNNs) with Heaviside activations are tightly linked to regular languages, i.e., languages defined by finite-state automata (FSAs). Such results, however, fall short of describing the capabilities of RNN \\emph{language models} (LMs), which are definitionally \\emph{distributions} over strings. We take a fresh look at the representational capacity of RNN LMs by connecting them to \\emph{probabilistic} FSAs and demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs.",
    "authors": [
      "Anej Svete",
      "Franz Nowak",
      "Anisha Mohamed Sahabdeen",
      "Ryan Cotterell"
    ],
    "url": "http://arxiv.org/abs/2405.19222v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a2542cb7-1cce-4c1f-ba81-545169542711": {
    "pk": "a2542cb7-1cce-4c1f-ba81-545169542711",
    "title": "WRDScore: New Metric for Evaluation of Natural Language Generation Models",
    "abstract": "The problem of natural language generation, and, more specifically, method name prediction, faces significant difficulties when proposed models need to be evaluated on test data. Such a metric would need to consider the versatility with which a single method can be named, with respect to both semantics and syntax. Measuring the direct overlap between the predicted and reference (true) sequences will not be able to capture these subtleties. Other existing embedding based metrics either do not measure precision and recall or impose strict unrealistic assumptions on both sequences. To address these issues, we propose a new metric that, on the one hand, is very simple and lightweight, and, on the other hand, is able to calculate precision and recall without resorting to any assumptions while obtaining good performance with respect to the human judgement.",
    "authors": [
      "Ravil Mussabayev"
    ],
    "url": "http://arxiv.org/abs/2405.19220v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f052c22b-d8e9-47d4-942d-36194a36ff0b": {
    "pk": "f052c22b-d8e9-47d4-942d-36194a36ff0b",
    "title": "The $SL_2(\\mathbb{R})$ duality and the non-invertible $U(1)$ symmetry of Maxwell theory",
    "abstract": "Recent proposals for the Symmetry Topological Field Theory (SymTFT) of Maxwell theory admit a 0-form symmetry compatible with the classical $SL_2(\\mathbb{R})$ duality of electromagnetism. We describe how to realize these automorphisms of the SymTFT in terms of its operators and we detail their effects on the dynamical theory and its global variants. In the process, we show that the classical $U(1)$ symmetry, corresponding to the stabilizer of $SL_2(\\mathbb{R})$, can be restored as a non-invertible one, by means of an infinite series of discrete gauging. This provides an example of the reemergence of a classical symmetry in the quantum regime, which was not broken by anomalies, but rather by the quantization of electromagnetic fluxes. However, this procedure comes at the price of introducing \"continuous\" condensates that trivialize all line operators.",
    "authors": [
      "Azeem Hasan",
      "Shani Meynet",
      "Daniele Migliorati"
    ],
    "url": "http://arxiv.org/abs/2405.19218v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "hep-th",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d5a975ba-06d6-4cb9-a560-6e1997cc5fa1": {
    "pk": "d5a975ba-06d6-4cb9-a560-6e1997cc5fa1",
    "title": "Two dimensional potential theory with a view towards vortex motion: Energy, capacity and Green functions",
    "abstract": "The paper reviews some parts of classical potential theory with applications to two dimensional fluid dynamics, in particular vortex motion. Energy and forces within a system of point vortices are similar to those for point charges when the vortices are kept fixed, but the dynamics is different in the case of free vortices. Starting from Bernoulli's equation we derive these laws. Letting the number of vortices tend to infinity leads in the limit to considerations of capacity, harmonic measure and many other notions in potential theory. In particular various kinds of Green functions have a central role in the paper, where we make a difference between electrostatic and hydrodynamic Green function.   We also consider the corresponding concepts in the case of closed Riemann surfaces provided with a metric. From a canonically defined monopole Green function we rederive much of the classical theory of harmonic and analytic forms. In the final section of the paper we return to the planar case, then reappearing in form of a symmetric Riemann surface, the Schottky double. Bergman kernels, electrostatic and hydrodynamic, come up naturally, and associated to the Green function the is a certain Robin function which is important for vortex motion and which also relates to capacity functions in classical potential theory.",
    "authors": [
      "Bj\u00f6rn Gustafsson"
    ],
    "url": "http://arxiv.org/abs/2405.19215v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "44084caf-9250-4267-b295-7518c75f5d78": {
    "pk": "44084caf-9250-4267-b295-7518c75f5d78",
    "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos",
    "abstract": "Video-language understanding tasks have focused on short video clips, often struggling with long-form video understanding tasks. Recently, many long video-language understanding approaches have leveraged the reasoning capabilities of Large Language Models (LLMs) to perform long video QA, transforming videos into densely sampled frame captions, and asking LLMs to respond to text queries over captions. However, the frames used for captioning are often redundant and contain irrelevant information, making dense sampling inefficient, and ignoring the fact that video QA requires varying levels of granularity, with some video segments being highly relevant to the question (needing more fine-grained detail) while others being less relevant. Thus, these LLM-based approaches are prone to missing information and operate on large numbers of irrelevant captions, lowering both performance and efficiency. To address these issues, we introduce VideoTree, a query-adaptive and hierarchical framework for long-video understanding with LLMs. VideoTree dynamically extracts query-related information from a video and builds a tree-based representation for LLM reasoning. First, VideoTree adaptively selects frames for captioning by iteratively clustering frames based on their visual features and scoring clusters using their relevance to the query. Second, it organizes visual clusters into a query-adaptive and hierarchical tree structure; the tree encodes varying levels of granularity, with higher resolution on relevant segments. Finally, VideoTree produces an answer by traversing the tree's keyframes and passing their captions to an LLM answerer. Our method improves both reasoning accuracy and efficiency compared to existing methods: VideoTree achieves a 7.0%, 2.2%, and 2.7% accuracy gain over baselines on the EgoSchema, NExT-QA, and IntentQA benchmarks, respectively, while reducing inference time by 40%.",
    "authors": [
      "Ziyang Wang",
      "Shoubin Yu",
      "Elias Stengel-Eskin",
      "Jaehong Yoon",
      "Feng Cheng",
      "Gedas Bertasius",
      "Mohit Bansal"
    ],
    "url": "http://arxiv.org/abs/2405.19209v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5428cad1-9641-4c2b-b9b5-63510bb0a17d": {
    "pk": "5428cad1-9641-4c2b-b9b5-63510bb0a17d",
    "title": "A Multi-Source Retrieval Question Answering Framework Based on RAG",
    "abstract": "With the rapid development of large-scale language models, Retrieval-Augmented Generation (RAG) has been widely adopted. However, existing RAG paradigms are inevitably influenced by erroneous retrieval information, thereby reducing the reliability and correctness of generated results. Therefore, to improve the relevance of retrieval information, this study proposes a method that replaces traditional retrievers with GPT-3.5, leveraging its vast corpus knowledge to generate retrieval information. We also propose a web retrieval based method to implement fine-grained knowledge retrieval, Utilizing the powerful reasoning capability of GPT-3.5 to realize semantic partitioning of problem.In order to mitigate the illusion of GPT retrieval and reduce noise in Web retrieval,we proposes a multi-source retrieval framework, named MSRAG, which combines GPT retrieval with web retrieval. Experiments on multiple knowledge-intensive QA datasets demonstrate that the proposed framework in this study performs better than existing RAG framework in enhancing the overall efficiency and accuracy of QA systems.",
    "authors": [
      "Ridong Wu",
      "Shuhong Chen",
      "Xiangbiao Su",
      "Yuankai Zhu",
      "Yifei Liao",
      "Jianming Wu"
    ],
    "url": "http://arxiv.org/abs/2405.19207v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "fd5e571b-4601-46da-a6b2-51c15fee8eb2": {
    "pk": "fd5e571b-4601-46da-a6b2-51c15fee8eb2",
    "title": "Matrix Manifold Neural Networks++",
    "abstract": "Deep neural networks (DNNs) on Riemannian manifolds have garnered increasing interest in various applied areas. For instance, DNNs on spherical and hyperbolic manifolds have been designed to solve a wide range of computer vision and nature language processing tasks. One of the key factors that contribute to the success of these networks is that spherical and hyperbolic manifolds have the rich algebraic structures of gyrogroups and gyrovector spaces. This enables principled and effective generalizations of the most successful DNNs to these manifolds. Recently, some works have shown that many concepts in the theory of gyrogroups and gyrovector spaces can also be generalized to matrix manifolds such as Symmetric Positive Definite (SPD) and Grassmann manifolds. As a result, some building blocks for SPD and Grassmann neural networks, e.g., isometric models and multinomial logistic regression (MLR) can be derived in a way that is fully analogous to their spherical and hyperbolic counterparts. Building upon these works, we design fully-connected (FC) and convolutional layers for SPD neural networks. We also develop MLR on Symmetric Positive Semi-definite (SPSD) manifolds, and propose a method for performing backpropagation with the Grassmann logarithmic map in the projector perspective. We demonstrate the effectiveness of the proposed approach in the human action recognition and node classification tasks.",
    "authors": [
      "Xuan Son Nguyen",
      "Shuo Yang",
      "Aymeric Histace"
    ],
    "url": "http://arxiv.org/abs/2405.19206v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ML",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0e2bac9e-f349-44e7-ad5e-22fc555972b1": {
    "pk": "0e2bac9e-f349-44e7-ad5e-22fc555972b1",
    "title": "Detection of entanglement by harnessing extracted work in an opto-magno-mechanics",
    "abstract": "The connections between thermodynamics and quantum information processing are of paramount importance. Here, we address a bipartite entanglement via extracted work in a cavity magnomechanical system contained inside an yttrium iron garnet (YIG) sphere. The photons and magnons interact through an interaction between magnetic dipoles. A magnetostrictive interaction, analogous to radiation pressure, couple's phonons and magnons. The extracted work was obtained through a device similar to the Szil\\'ard engine. This engine operates by manipulating the photon-magnon as a bipartite quantum state. We employ logarithmic negativity to measure the amount of entanglement between photon and magnon modes in steady and dynamical states. We explore the extracted work, separable work, and maximum work for squeezed thermal states. We investigate the amount of work extracted from a bipartite quantum state, which can potentially determine the degree of entanglement present in that state. Numerical studies show that entanglement, as detected by the extracted work and quantified by logarithmic negativity, is in good agreement. We show the reduction of extracted work by a second measurement compared to a single measurement. Also, the efficiency of the Szilard engine in steady and dynamical states is investigated. We hope this work is of paramount importance in quantum information processing.",
    "authors": [
      "M'bark Amghar",
      "Mohamed Amazioug"
    ],
    "url": "http://arxiv.org/abs/2405.19205v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "d736334c-7785-4710-b89b-088b694aa98e": {
    "pk": "d736334c-7785-4710-b89b-088b694aa98e",
    "title": "Contrastive-Adversarial and Diffusion: Exploring pre-training and fine-tuning strategies for sulcal identification",
    "abstract": "In the last decade, computer vision has witnessed the establishment of various training and learning approaches. Techniques like adversarial learning, contrastive learning, diffusion denoising learning, and ordinary reconstruction learning have become standard, representing state-of-the-art methods extensively employed for fully training or pre-training networks across various vision tasks. The exploration of fine-tuning approaches has emerged as a current focal point, addressing the need for efficient model tuning with reduced GPU memory usage and time costs while enhancing overall performance, as exemplified by methodologies like low-rank adaptation (LoRA). Key questions arise: which pre-training technique yields optimal results - adversarial, contrastive, reconstruction, or diffusion denoising? How does the performance of these approaches vary as the complexity of fine-tuning is adjusted? This study aims to elucidate the advantages of pre-training techniques and fine-tuning strategies to enhance the learning process of neural networks in independent identical distribution (IID) cohorts. We underscore the significance of fine-tuning by examining various cases, including full tuning, decoder tuning, top-level tuning, and fine-tuning of linear parameters using LoRA. Systematic summaries of model performance and efficiency are presented, leveraging metrics such as accuracy, time cost, and memory efficiency. To empirically demonstrate our findings, we focus on a multi-task segmentation-classification challenge involving the paracingulate sulcus (PCS) using different 3D Convolutional Neural Network (CNN) architectures by using the TOP-OSLO cohort comprising 596 subjects.",
    "authors": [
      "Michail Mamalakis",
      "H\u00e9lo\u00efse de Vareilles",
      "Shun-Chin Jim Wu",
      "Ingrid Agartz",
      "Lynn Egeland M\u00f8rch-Johnsen",
      "Jane Garrison",
      "Jon Simons",
      "Pietro Lio",
      "John Suckling",
      "Graham Murray"
    ],
    "url": "http://arxiv.org/abs/2405.19204v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bf585b82-b9fb-4982-ba04-087bbc0f9647": {
    "pk": "bf585b82-b9fb-4982-ba04-087bbc0f9647",
    "title": "$E^{3}$Gen: Efficient, Expressive and Editable Avatars Generation",
    "abstract": "This paper aims to introduce 3D Gaussian for efficient, expressive, and editable digital avatar generation. This task faces two major challenges: (1) The unstructured nature of 3D Gaussian makes it incompatible with current generation pipelines; (2) the expressive animation of 3D Gaussian in a generative setting that involves training with multiple subjects remains unexplored. In this paper, we propose a novel avatar generation method named $E^3$Gen, to effectively address these challenges. First, we propose a novel generative UV features plane representation that encodes unstructured 3D Gaussian onto a structured 2D UV space defined by the SMPL-X parametric model. This novel representation not only preserves the representation ability of the original 3D Gaussian but also introduces a shared structure among subjects to enable generative learning of the diffusion model. To tackle the second challenge, we propose a part-aware deformation module to achieve robust and accurate full-body expressive pose control. Extensive experiments demonstrate that our method achieves superior performance in avatar generation and enables expressive full-body pose control and editing.",
    "authors": [
      "Weitian Zhang",
      "Yichao Yan",
      "Yunhui Liu",
      "Xingdong Sheng",
      "Xiaokang Yang"
    ],
    "url": "http://arxiv.org/abs/2405.19203v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6fb9b291-c91c-4fcd-ac9e-362f3c54ca5b": {
    "pk": "6fb9b291-c91c-4fcd-ac9e-362f3c54ca5b",
    "title": "Vulnerable Road User Detection and Safety Enhancement: A Comprehensive Survey",
    "abstract": "Traffic incidents involving vulnerable road users (VRUs) constitute a significant proportion of global road accidents. Advances in traffic communication ecosystems, coupled with sophisticated signal processing and machine learning techniques, have facilitated the utilization of data from diverse sensors. Despite these advancements and the availability of extensive datasets, substantial progress is required to mitigate traffic casualties. This paper provides a comprehensive survey of state-of-the-art technologies and methodologies to enhance the safety of VRUs. The study delves into the communication networks between vehicles and VRUs, emphasizing the integration of advanced sensors and the availability of relevant datasets. It explores preprocessing techniques and data fusion methods to enhance sensor data quality. Furthermore, our study assesses critical simulation environments essential for developing and testing VRU safety systems. Our research also highlights recent advances in VRU detection and classification algorithms, addressing challenges such as variable environmental conditions. Additionally, we cover cutting-edge research in predicting VRU intentions and behaviors, which is crucial for proactive collision avoidance strategies. Through this survey, we aim to provide a comprehensive understanding of the current landscape of VRU safety technologies, identifying areas of progress and areas needing further research and development.",
    "authors": [
      "Renato M. Silva",
      "Greg\u00f3rio F. Azevedo",
      "Matheus V. V. Berto",
      "Jean R. Rocha",
      "Eduardo C. Fidelis",
      "Matheus V. Nogueira",
      "Pedro H. Lisboa",
      "Tiago A. Almeida"
    ],
    "url": "http://arxiv.org/abs/2405.19202v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1cf8fb6b-7dea-4fbf-8888-629f9476b7a8": {
    "pk": "1cf8fb6b-7dea-4fbf-8888-629f9476b7a8",
    "title": "LOGO: Video Text Spotting with Language Collaboration and Glyph Perception Model",
    "abstract": "Video text spotting aims to simultaneously localize, recognize and track text instances in videos. To address the limited recognition capability of end-to-end methods, tracking the zero-shot results of state-of-the-art image text spotters directly can achieve impressive performance. However, owing to the domain gap between different datasets, these methods usually obtain limited tracking trajectories on extreme dataset. Fine-tuning transformer-based text spotters on specific datasets could yield performance enhancements, albeit at the expense of considerable training resources. In this paper, we propose a Language Collaboration and Glyph Perception Model, termed LOGO to enhance the performance of conventional text spotters through the integration of a synergy module. To achieve this goal, a language synergy classifier (LSC) is designed to explicitly discern text instances from background noise in the recognition stage. Specially, the language synergy classifier can output text content or background code based on the legibility of text regions, thus computing language scores. Subsequently, fusion scores are computed by taking the average of detection scores and language scores, and are utilized to re-score the detection results before tracking. By the re-scoring mechanism, the proposed LSC facilitates the detection of low-resolution text instances while filtering out text-like regions. Besides, the glyph supervision and visual position mixture module are proposed to enhance the recognition accuracy of noisy text regions, and acquire more discriminative tracking features, respectively. Extensive experiments on public benchmarks validate the effectiveness of the proposed method.",
    "authors": [
      "Hongen Liu",
      "Yi Liu",
      "Di Sun",
      "Jiahao Wang",
      "Gang Pan"
    ],
    "url": "http://arxiv.org/abs/2405.19194v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3e985d97-efc0-4caa-89a8-186728ee419f": {
    "pk": "3e985d97-efc0-4caa-89a8-186728ee419f",
    "title": "Sparse High Dimensional Expanders via Local Lifts",
    "abstract": "High dimensional expanders (HDXs) are a hypergraph generalization of expander graphs. They are extensively studied in the math and TCS communities due to their many applications. Like expander graphs, HDXs are especially interesting for applications when they are bounded degree, namely, if the number of edges adjacent to every vertex is bounded. However, only a handful of constructions are known to have this property, all of which rely on non-trivial algebraic techniques. In particular, no random or combinatorial construction of bounded degree HDXs is known. As a result, our understanding of these objects is limited.   The degree of an $i$-face in an HDX is the number of $(i+1)$-faces containing it. In this work we construct HDXs whose higher dimensional faces have bounded degree. This is done by giving an elementary and deterministic algorithm that takes as input a regular $k$-dimensional HDX $X$ and outputs another $k$-dimensional HDX $\\widehat{X}$ with twice as many vertices. While the degree of vertices in $\\widehat{X}$ grows, the degree of the $(k-1)$-faces in $\\widehat{X}$ stays the same. As a result, we obtain a new `algebra-free' construction of HDXs whose $(k-1)$-face degree is bounded.   Our algorithm is based on a simple and natural generalization of the construction by Bilu and Linial (Combinatorica, 2006), which build expanders using lifts coming from edge signings. Our construction is based on local lifts of HDXs, where a local lift is a complex whose top-level links are lifts of links in the original complex. We demonstrate that a local lift of an HDX is an HDX in many cases.   In addition, combining local lifts with existing bounded degree constructions creates new families of bounded degree HDXs with significantly different links than before. We use this technique to construct bounded degree high dimensional expanders with links that have arbitrarily large diameters.",
    "authors": [
      "Inbar Ben Yaacov",
      "Yotam Dikstein",
      "Gal Maor"
    ],
    "url": "http://arxiv.org/abs/2405.19191v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DM",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "01710883-8e5b-4fe7-834a-e497a1a68173": {
    "pk": "01710883-8e5b-4fe7-834a-e497a1a68173",
    "title": "Reconstructing the G2HDM Charged Higgs Boson at the LHC",
    "abstract": "We study the discovery prospects for a charged Higgs boson via the $b g\\to c H^- \\to c \\bar t b$ process at the Large Hadron Collier (LHC). Focusing on the general Two Higgs Doublet Model (G2HDM) that possesses extra Yukawa couplings, the process is controlled by extra top couplings $\\rho_{tc}$ and $\\rho_{tt}$, which can drive electroweak baryogenesis (EWBG) to account for the baryon asymmetry of the Universe (BAU). We propose benchmark points (BPs) and demonstrate that evidence could emerge at the LHC with 14 TeV collision energy and luminosity of 300 fb$^{-1}$, with discovery potential at 600 fb$^{-1}$.",
    "authors": [
      "Wei-Shu Hou",
      "Mohamed Krab"
    ],
    "url": "http://arxiv.org/abs/2405.19190v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "hep-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "58c9d49b-a96a-4412-a5f8-6749da500d9f": {
    "pk": "58c9d49b-a96a-4412-a5f8-6749da500d9f",
    "title": "Personalized Interiors at Scale: Leveraging AI for Efficient and Customizable Design Solutions",
    "abstract": "In this paper, we introduce an innovative application of artificial intelligence in the realm of interior design through the integration of Stable Diffusion and Dreambooth models. This paper explores the potential of these advanced generative models to streamline and democratize the process of room interior generation, offering a significant departure from conventional, labor-intensive techniques. Our approach leverages the capabilities of Stable Diffusion for generating high-quality images and Dreambooth for rapid customization with minimal training data, addressing the need for efficiency and personalization in the design industry. We detail a comprehensive methodology that combines these models, providing a robust framework for the creation of tailored room interiors that reflect individual tastes and functional requirements. We presents an extensive evaluation of our method, supported by experimental results that demonstrate its effectiveness and a series of case studies that illustrate its practical application in interior design projects. Our study contributes to the ongoing discourse on the role of AI in creative fields, highlighting the benefits of leveraging generative models to enhance creativity and reshape the future of interior design.",
    "authors": [
      "Kaiwen Zhou",
      "Tianyu Wang"
    ],
    "url": "http://arxiv.org/abs/2405.19188v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.HC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "58710a71-1a62-4a75-be4f-28efa961181f": {
    "pk": "58710a71-1a62-4a75-be4f-28efa961181f",
    "title": "Algorithmic Transparency and Participation through the Handoff Lens: Lessons Learned from the U.S. Census Bureau's Adoption of Differential Privacy",
    "abstract": "Emerging discussions on the responsible government use of algorithmic technologies propose transparency and public participation as key mechanisms for preserving accountability and trust. But in practice, the adoption and use of any technology shifts the social, organizational, and political context in which it is embedded. Therefore translating transparency and participation efforts into meaningful, effective accountability must take into account these shifts. We adopt two theoretical frames, Mulligan and Nissenbaum's handoff model and Star and Griesemer's boundary objects, to reveal such shifts during the U.S. Census Bureau's adoption of differential privacy (DP) in its updated disclosure avoidance system (DAS) for the 2020 census. This update preserved (and arguably strengthened) the confidentiality protections that the Bureau is mandated to uphold, and the Bureau engaged in a range of activities to facilitate public understanding of and participation in the system design process. Using publicly available documents concerning the Census' implementation of DP, this case study seeks to expand our understanding of how technical shifts implicate values, how such shifts can afford (or fail to afford) greater transparency and participation in system design, and the importance of localized expertise throughout. We present three lessons from this case study toward grounding understandings of algorithmic transparency and participation: (1) efforts towards transparency and participation in algorithmic governance must center values and policy decisions, not just technical design decisions; (2) the handoff model is a useful tool for revealing how such values may be cloaked beneath technical decisions; and (3) boundary objects alone cannot bridge distant communities without trusted experts traveling alongside to broker their adoption.",
    "authors": [
      "Amina A. Abdu",
      "Lauren M. Chambers",
      "Deirdre K. Mulligan",
      "Abigail Z. Jacobs"
    ],
    "url": "http://arxiv.org/abs/2405.19187v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CY",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "50f479bb-5da7-4bbf-b7cd-478b7d43e194": {
    "pk": "50f479bb-5da7-4bbf-b7cd-478b7d43e194",
    "title": "MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification",
    "abstract": "Large Vision Language Models (LVLMs) have shown remarkable capabilities in multimodal tasks like visual question answering or image captioning. However, inconsistencies between the visual information and the generated text, a phenomenon referred to as hallucinations, remain an unsolved problem with regard to the trustworthiness of LVLMs. To address this problem, recent works proposed to incorporate computationally costly Large (Vision) Language Models in order to detect hallucinations on a sentence- or subsentence-level. In this work, we introduce MetaToken, a lightweight binary classifier to detect hallucinations on the token-level at negligible cost. Based on a statistical analysis, we reveal key factors of hallucinations in LVLMs which have been overseen in previous works. MetaToken can be applied to any open-source LVLM without any knowledge about ground truth data providing a reliable detection of hallucinations. We evaluate our method on four state-of-the-art LVLMs demonstrating the effectiveness of our approach.",
    "authors": [
      "Laura Fieback",
      "Jakob Spiegelberg",
      "Hanno Gottschalk"
    ],
    "url": "http://arxiv.org/abs/2405.19186v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "174e7e3e-3a03-4bfb-9e08-9b433e60ec9a": {
    "pk": "174e7e3e-3a03-4bfb-9e08-9b433e60ec9a",
    "title": "Benford's law in atomic spectra and opacity databases",
    "abstract": "The intriguing law of anomalous numbers, also named Benford's law, states that the significant digits of data follow a logarithmic distribution favoring the smallest values. In this work, we test the compliance with this law of the atomic databases developed at the National Institute of Standards and Technology (NIST) focusing on line energies, oscillator strengths, Einstein coefficients and radiative opacities. The considered databases are the Atomic Spectra Database (ASD) and the NIST-LANL (Los Alamos National Laboratory) Lanthanide/Actinide Opacity Database. The present study is not limited to the first digit and the case of multipole lines is also considered. The fact that the law is verified with a high accuracy means that the occurrence of digits reflects the constraints induced, in a given angular-momentum coupling, by the selection rules for atomic processes. As a consequence, Benford's law may be of great interest to detect inconsistencies in atomic databases.",
    "authors": [
      "Jean-Christophe Pain",
      "Yuri Ralchenko"
    ],
    "url": "http://arxiv.org/abs/2405.19185v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "physics.atom-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e59f1c1f-bac1-404e-86d4-6ecb03553e87": {
    "pk": "e59f1c1f-bac1-404e-86d4-6ecb03553e87",
    "title": "Delay-Doppler Domain Pulse Design for OTFS-NOMA",
    "abstract": "We address the challenge of developing an orthogonal time-frequency space (OTFS)-based non-orthogonal multiple access (NOMA) system where each user is modulated using orthogonal pulses in the delay Doppler domain. Building upon the concept of the sufficient (bi)orthogonality train-pulse [1], we extend this idea by introducing Hermite functions, known for their orthogonality properties. Simulation results demonstrate that our proposed Hermite functions outperform the traditional OTFS-NOMA schemes, including power-domain (PDM) NOMA and code-domain (CDM) NOMA, in terms of bit error rate (BER) over a high-mobility channel. The algorithm's complexity is minimal, primarily involving the demodulation of OTFS. The spectrum efficiency of Hermite-based OTFS-NOMA is K times that of OTFS-CDM-NOMA scheme, where K is the spreading length of the NOMA waveform.",
    "authors": [
      "Michel Kulhandjian",
      "Hovannes Kulhandjian",
      "Gunes Karabulut Kurt",
      "Halim Yanikomeroglu"
    ],
    "url": "http://arxiv.org/abs/2405.19182v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SP",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "a97f2449-c27f-499f-8a94-60eb7f4cc7f9": {
    "pk": "a97f2449-c27f-499f-8a94-60eb7f4cc7f9",
    "title": "Model Agnostic Defense against Adversarial Patch Attacks on Object Detection in Unmanned Aerial Vehicles",
    "abstract": "Object detection forms a key component in Unmanned Aerial Vehicles (UAVs) for completing high-level tasks that depend on the awareness of objects on the ground from an aerial perspective. In that scenario, adversarial patch attacks on an onboard object detector can severely impair the performance of upstream tasks. This paper proposes a novel model-agnostic defense mechanism against the threat of adversarial patch attacks in the context of UAV-based object detection. We formulate adversarial patch defense as an occlusion removal task. The proposed defense method can neutralize adversarial patches located on objects of interest, without exposure to adversarial patches during training. Our lightweight single-stage defense approach allows us to maintain a model-agnostic nature, that once deployed does not require to be updated in response to changes in the object detection pipeline. The evaluations in digital and physical domains show the feasibility of our method for deployment in UAV object detection pipelines, by significantly decreasing the Attack Success Ratio without incurring significant processing costs. As a result, the proposed defense solution can improve the reliability of object detection for UAVs.",
    "authors": [
      "Saurabh Pathak",
      "Samridha Shrestha",
      "Abdelrahman AlMahmoud"
    ],
    "url": "http://arxiv.org/abs/2405.19179v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7426fc7e-803a-4585-8026-ef46966c2977": {
    "pk": "7426fc7e-803a-4585-8026-ef46966c2977",
    "title": "Model-independent cosmological inference post DESI DR1 BAO measurements",
    "abstract": "In this work, we implement Gaussian process regression to reconstruct the expansion history of the universe in a model-agnostic manner, using the Pantheon-Plus SN-Ia compilation in combination with two different BAO measurements (SDSS-IV and DESI DR1). In both the reconstructions, the $\\Lambda$CDM model is always included in the 95\\% confidence intervals. We find evidence that the DESI LRG data at $z_{\\text{eff}} = 0.51$ is not an outlier within our model-independent framework. We study the $\\mathcal{O}m$-diagnostics and the evolution of the total equation of state (EoS) of our universe, which hint towards the possibility of a quintessence-like dark energy scenario with a very slowly varying EoS, and a phantom-crossing in higher $z$. The entire exercise is later complemented by considering two more SN-Ia compilations - DES-5YR and Union3 - in combination with DESI BAO. Reconstruction with the DESI BAO + DES-5YR SN data sets predicts that the $\\Lambda$CDM model lies outside the 3$\\sigma$ confidence levels, whereas with DESI BAO + Union3 data, the $\\Lambda$CDM model is always included within 1$\\sigma$. We also report constraints on $H_0 r_d$ from our model-agnostic analysis, independent of the pre-recombination physics. Our results point towards an $\\approx$ 2$\\sigma$ discrepancy between the DESI + Pantheon-Plus and DESI + DES-5YR data sets, which calls for further investigation.",
    "authors": [
      "Purba Mukherjee",
      "Anjan Ananda Sen"
    ],
    "url": "http://arxiv.org/abs/2405.19178v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.CO",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "3362da48-dab2-469a-84b6-c54ae0950479": {
    "pk": "3362da48-dab2-469a-84b6-c54ae0950479",
    "title": "Exploring AI-based Anonymization of Industrial Image and Video Data in the Context of Feature Preservation",
    "abstract": "With rising technologies, the protection of privacy-sensitive information is becoming increasingly important. In industry and production facilities, image or video recordings are beneficial for documentation, tracing production errors or coordinating workflows. Individuals in images or videos need to be anonymized. However, the anonymized data should be reusable for further applications. In this work, we apply the Deep Learning-based full-body anonymization framework DeepPrivacy2, which generates artificial identities, to industrial image and video data. We compare its performance with conventional anonymization techniques. Therefore, we consider the quality of identity generation, temporal consistency, and the applicability of pose estimation and action recognition.",
    "authors": [
      "Sabrina Cynthia Triess",
      "Timo Leitritz",
      "Christian Jauch"
    ],
    "url": "http://arxiv.org/abs/2405.19173v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "ac8619c4-1b0d-4d95-a838-459bcfccb535": {
    "pk": "ac8619c4-1b0d-4d95-a838-459bcfccb535",
    "title": "Dedekind-MacNeille and related completions: subfitness, regularity, and Booleanness",
    "abstract": "Completions play an important r\\^ole for studying structure by supplying elements that in some sense ``ought to be.\" Among these, the Dedekind-MacNeille completion is of particular importance. In 1968 Janowitz provided necessary and sufficient conditions for it to be subfit or Boolean. Another natural separation axiom situated between the two is regularity. We explore similar characterizations of when closely related completions are subfit, regular, or Boolean. We are mainly interested in the Bruns-Lakser, ideal, and canonical completions, which are useful in pointfree topology since (unlike the Dedekind-MacNeille completion) they satisfy stronger forms of distributivity.",
    "authors": [
      "G. Bezhanishvili",
      "F. Dashiell Jr",
      "A. Moshier",
      "J. Walters-Wayland"
    ],
    "url": "http://arxiv.org/abs/2405.19171v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.GN",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0214892f-22bd-4624-9a58-45e2d7f07d90": {
    "pk": "0214892f-22bd-4624-9a58-45e2d7f07d90",
    "title": "Measuring differential particle correlations in relativistic nuclear collisions",
    "abstract": "This study explores the transverse momentum ($p_T$) dependencies of Symmetric and Asymmetric Correlations (SC and ASC) with one and two particles of interest in Au+Au collisions at 200 GeV. Leveraging the AMPT model, the investigation delves into the sensitivity of these correlations to the final state effects, providing valuable insights into their potential for constraining the final state effects' $p_T$ dependencies. The HIJING model is employed as a benchmark for non-flow correlations, shedding light on their impact on interpreting SC and ASC data. Moreover, the study points out that differential SC and ASC with one and two particles of interest (POIs) typically incorporate contributions from event-plane angle fluctuations. Consequently, this work highlights the significance of SC and ASC with one and two POIs as valuable tools for investigating the $p_T$ nature of the final state effects and advocates for comprehensive experimental measurements across various beam energies and system sizes to enhance our understanding and provide additional constraints for theoretical models.",
    "authors": [
      "Niseem Magdy"
    ],
    "url": "http://arxiv.org/abs/2405.19169v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "hep-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "297cec4e-5580-43f8-8024-64c611784511": {
    "pk": "297cec4e-5580-43f8-8024-64c611784511",
    "title": "Measuring the Higgs spin and parity in $H\\to \\ell^-\\ell^+ Z$",
    "abstract": "We develop an effective and methodical algorithm for the construction of general covariant four-point $H\\ell\\ell Z$ vertices, accommodating leptons $\\ell=e, \\mu$, and designed to handle a Higgs boson $H$ of any integer spin, not merely confined to spins up to 2. While our numerical analysis assumes the Higgs boson mass to be $m_H=125\\,{\\rm GeV}$, the analytical framework we propose is versatile, enabling the examination of various mass scenarios. These meticulously devised general covariant four-point $H\\ell\\ell Z$ vertices are pivotal in the systematic determination of the spin and parity of the Standard Model Higgs boson, especially via one of its primary decay channels, the three-body decay process $H\\to \\ell^-\\ell^+ Z$, observable at the Large Hadron Collider. Our innovative strategy extends beyond the limitations of previous investigations on the Higgs spin and parity determinations. It not only encompasses the analysis of the two-step cascade decay $H\\to Z^{\\star} Z \\to \\ell^-\\ell^+ Z$, featuring an intermediate virtual $Z^{\\star}$ exchange but also integrates consideration for the direct 4-point contact $H\\ell\\ell Z$ interaction. Our significantly expanded scheme provides a definitive and unequivocal platform for determining and confirming the spinless nature and even parity of the SM Higgs boson by leveraging threshold effects and angular correlations, even though achieving such conclusive results in practical and exhaustive analyses necessitates high event rates.",
    "authors": [
      "Seong Youl Choi",
      "Jaehoon Jeong",
      "Dong Woo Kang"
    ],
    "url": "http://arxiv.org/abs/2405.19167v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "hep-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "eed58c19-0057-49b1-a77d-4f590499f2a8": {
    "pk": "eed58c19-0057-49b1-a77d-4f590499f2a8",
    "title": "Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery",
    "abstract": "Electronic Discovery (eDiscovery) involves identifying relevant documents from a vast collection based on legal production requests. The integration of artificial intelligence (AI) and natural language processing (NLP) has transformed this process, helping document review and enhance efficiency and cost-effectiveness. Although traditional approaches like BM25 or fine-tuned pre-trained models are common in eDiscovery, they face performance, computational, and interpretability challenges. In contrast, Large Language Model (LLM)-based methods prioritize interpretability but sacrifice performance and throughput. This paper introduces DISCOvery Graph (DISCOG), a hybrid approach that combines the strengths of two worlds: a heterogeneous graph-based method for accurate document relevance prediction and subsequent LLM-driven approach for reasoning. Graph representational learning generates embeddings and predicts links, ranking the corpus for a given request, and the LLMs provide reasoning for document relevance. Our approach handles datasets with balanced and imbalanced distributions, outperforming baselines in F1-score, precision, and recall by an average of 12%, 3%, and 16%, respectively. In an enterprise context, our approach drastically reduces document review costs by 99.9% compared to manual processes and by 95% compared to LLM-based classification methods",
    "authors": [
      "Sounak Lahiri",
      "Sumit Pai",
      "Tim Weninger",
      "Sanmitra Bhattacharya"
    ],
    "url": "http://arxiv.org/abs/2405.19164v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "96fcfa88-73eb-4cdc-89d5-9474d688f13c": {
    "pk": "96fcfa88-73eb-4cdc-89d5-9474d688f13c",
    "title": "Does learning the right latent variables necessarily improve in-context learning?",
    "abstract": "Large autoregressive models like Transformers can solve tasks through in-context learning (ICL) without learning new weights, suggesting avenues for efficiently solving new tasks. For many tasks, e.g., linear regression, the data factorizes: examples are independent given a task latent that generates the data, e.g., linear coefficients. While an optimal predictor leverages this factorization by inferring task latents, it is unclear if Transformers implicitly do so or if they instead exploit heuristics and statistical shortcuts enabled by attention layers. Both scenarios have inspired active ongoing work. In this paper, we systematically investigate the effect of explicitly inferring task latents. We minimally modify the Transformer architecture with a bottleneck designed to prevent shortcuts in favor of more structured solutions, and then compare performance against standard Transformers across various ICL tasks. Contrary to intuition and some recent works, we find little discernible difference between the two; biasing towards task-relevant latent variables does not lead to better out-of-distribution performance, in general. Curiously, we find that while the bottleneck effectively learns to extract latent task variables from context, downstream processing struggles to utilize them for robust prediction. Our study highlights the intrinsic limitations of Transformers in achieving structured ICL solutions that generalize, and shows that while inferring the right latents aids interpretability, it is not sufficient to alleviate this problem.",
    "authors": [
      "Sarthak Mittal",
      "Eric Elmoznino",
      "Leo Gagnon",
      "Sangnie Bhardwaj",
      "Dhanya Sridhar",
      "Guillaume Lajoie"
    ],
    "url": "http://arxiv.org/abs/2405.19162v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "79141ed3-4ebd-4a62-bdf1-946cd51476ae": {
    "pk": "79141ed3-4ebd-4a62-bdf1-946cd51476ae",
    "title": "Search and analysis of giant radio galaxies with associated nuclei (SAGAN) IV. Interplay with the Supercluster environment",
    "abstract": "We investigate the Giant Radio Galaxies' (GRGs)-some of the largest structures powered by supermassive black holes-prevalence within supercluster environments and the influence of such environments on their properties. Utilising two large catalogues of superclusters (401) and GRGs (1446), we establish the existence of 77 GRGs (5.3%) residing in 64 superclusters (16%) within $\\rm 0.05 \\leq z \\leq 0.42$. Among the 77 GRGs found in superclusters, we identify $\\sim$70% as residing within galaxy clusters. Within the subset of GRGs not located in superclusters, which constitutes 94.7% of the sample, a mere 21% are associated with galaxy clusters, while the remaining majority are situated in sparser environments. We examine the influence of differing environments -such as cluster versus non-cluster and supercluster versus non-supercluster regions -on the size of GRGs while also exploring the driving factors behind their overall growth. Our findings show that the largest GRGs ($\\gtrsim$3 Mpc) grow in underdense environments beyond the confines of dense environments. Moreover, we show that $\\sim$ 24% of 1446 GRGs reside in galaxy clusters. We conclude that GRGs preferentially grow in sparser regions of the cosmic web and have a significantly larger median size. Finally, we demonstrate the potential of GRGs as astrophysical probes with specific cases where GRGs, exhibiting polarised emissions and located behind superclusters (acting as natural Faraday screens), were used to estimate magnetic field strengths of the supercluster environment at sub-micro Gauss levels.",
    "authors": [
      "Shishir Sankhyayan",
      "Pratik Dabhade"
    ],
    "url": "http://arxiv.org/abs/2405.19154v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.GA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1779e55e-69f0-42e1-87c0-ab9857575f43": {
    "pk": "1779e55e-69f0-42e1-87c0-ab9857575f43",
    "title": "First high peak and average power single-pass THz FEL based on high brightness photoinjector",
    "abstract": "Advanced experiments using THz pump and X-ray probe pulses at modern free-electron lasers (FELs) like the European X-ray FEL require a frequency-tunable, high-power, narrow-band THz source maintaining the repetition rate and pulse structure of the X-ray pulses. This paper reports the first results from a THz source, that is based on a single-pass high-gain THz FEL operating with a central wavelength of 100 micrometers. The THz FEL prototype is currently in operation at the Photo Injector Test facility at DESY in Zeuthen (PITZ) and uses the same type of electron source as the European XFEL photo injector. A self-amplified spontaneous emission (SASE) FEL was envisioned as the main mechanism for generating the THz pulses. Although the THz FEL at PITZ is supposed to use the same mechanism as at X-ray facilities, it cannot be considered as a simple scaling of the radiation wavelength because there is a large difference in the number of electrons per radiation wavelength, which is five orders of magnitude higher for the THz case. The bunching factor arising from the electron beam current profile contributes strongly to the initial spontaneous emission starting the FEL process. Proof-of-principle experiments were done at PITZ using an LCLS-I undulator to generate the first high-power, high-repetition-rate single-pass THz FEL radiation. Electron bunches with a beam energy of ~17 MeV and a bunch charge of up to several nC are used to generate THz pulses with a pulse energy of several tens of microjoules. For example, for an electron beam with a charge of ~2.4 nC, more than 100 microjoules were generated at a central wavelength of 100 micrometers. The narrowband spectrum was also demonstrated by spectral measurements. These proof-of-principle experiments pave the way for a tunable, high-repetition-rate THz source providing pulses with energies in the millijoule range.",
    "authors": [
      "M. Krasilnikov",
      "Z. Aboulbanine",
      "G. Adhikari",
      "N. Aftab",
      "A. Asoyan",
      "P. Boonpornprasert",
      "H. Davtyan",
      "G. Georgiev",
      "J. Good",
      "A. Grebinyk",
      "M. Gross",
      "A. Hoffmann",
      "E. Kongmon",
      "X. -K. Li",
      "A. Lueangaramwong",
      "D. Melkumyan",
      "S. Mohanty",
      "R. Niemczyk",
      "A. Oppelt",
      "H. Qian",
      "C. Richard",
      "F. Stephan",
      "G. Vashchenko",
      "T. Weilbach",
      "X. Zhang",
      "M. Tischer",
      "E. Schneidmiller",
      "P. Vagin",
      "M. Yurkov",
      "E. Zapolnova",
      "W. Hillert",
      "J. Rossbach A. Brachmann",
      "N. Holtkamp",
      "H. -D. Nuhn"
    ],
    "url": "http://arxiv.org/abs/2405.19152v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "physics.acc-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "42366329-0018-4909-b355-e31ab04da7b6": {
    "pk": "42366329-0018-4909-b355-e31ab04da7b6",
    "title": "I Bet You Did Not Mean That: Testing Semantic Importance via Betting",
    "abstract": "Recent works have extended notions of feature importance to \\emph{semantic concepts} that are inherently interpretable to the users interacting with a black-box predictive model. Yet, precise statistical guarantees, such as false positive rate control, are needed to communicate findings transparently and to avoid unintended consequences in real-world scenarios. In this paper, we formalize the global (i.e., over a population) and local (i.e., for a sample) statistical importance of semantic concepts for the predictions of opaque models, by means of conditional independence, which allows for rigorous testing. We use recent ideas of sequential kernelized testing (SKIT) to induce a rank of importance across concepts, and showcase the effectiveness and flexibility of our framework on synthetic datasets as well as on image classification tasks using vision-language models such as CLIP.",
    "authors": [
      "Jacopo Teneggi",
      "Jeremias Sulam"
    ],
    "url": "http://arxiv.org/abs/2405.19146v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "stat.ML",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "66dfd6c5-595f-48b1-9ed5-c899d5dc027a": {
    "pk": "66dfd6c5-595f-48b1-9ed5-c899d5dc027a",
    "title": "Lagrangian metric geometry with Riemannian bounds",
    "abstract": "We study collections of exact Lagrangian submanifolds respecting some uniform Riemannian bounds, which we equip with a metric naturally arising in symplectic topology (e.g. the Lagrangian Hofer metric or the spectral metric). We exhibit many metric and symplectic properties of these spaces, such that they have compact completions and that they contain only finitely many Hamiltonian isotopy classes. We then use this to exclude many unusual phenomena from happening in these bounded spaces. Taking limits in the bounds, we also conclude that there are at most countably many Hamiltonian isotopy classes of exact Lagrangian submanifolds in a Liouville manifold. Under some mild topological assumptions, we get analogous results for monotone Lagrangian submanifolds with a fixed monotonicity constant. Finally, in the process of showing these results, we get new results on the Riemannian geometry of cotangent bundles and surfaces which might be of independent interest.",
    "authors": [
      "Jean-Philippe Chass\u00e9"
    ],
    "url": "http://arxiv.org/abs/2405.19144v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "math.SG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "9ebedaad-f822-45aa-aade-af5d897bda21": {
    "pk": "9ebedaad-f822-45aa-aade-af5d897bda21",
    "title": "DGRC: An Effective Fine-tuning Framework for Distractor Generation in Chinese Multi-choice Reading Comprehension",
    "abstract": "When evaluating a learner's knowledge proficiency, the multiple-choice question is an efficient and widely used format in standardized tests. Nevertheless, generating these questions, particularly plausible distractors (incorrect options), poses a considerable challenge. Generally, the distractor generation can be classified into cloze-style distractor generation (CDG) and natural questions distractor generation (NQDG). In contrast to the CDG, utilizing pre-trained language models (PLMs) for NQDG presents three primary challenges: (1) PLMs are typically trained to generate ``correct'' content, like answers, while rarely trained to generate ``plausible\" content, like distractors; (2) PLMs often struggle to produce content that aligns well with specific knowledge and the style of exams; (3) NQDG necessitates the model to produce longer, context-sensitive, and question-relevant distractors. In this study, we introduce a fine-tuning framework named DGRC for NQDG in Chinese multi-choice reading comprehension from authentic examinations. DGRC comprises three major components: hard chain-of-thought, multi-task learning, and generation mask patterns. The experiment results demonstrate that DGRC significantly enhances generation performance, achieving a more than 2.5-fold improvement in BLEU scores.",
    "authors": [
      "Runfeng Lin",
      "Dacheng Xu",
      "Huijiang Wang",
      "Zebiao Chen",
      "Yating Wang",
      "Shouqiang Liu"
    ],
    "url": "http://arxiv.org/abs/2405.19139v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "cfaf5f7c-04c0-4d4d-bdc6-dd61f57b8b89": {
    "pk": "cfaf5f7c-04c0-4d4d-bdc6-dd61f57b8b89",
    "title": "Multi-Channel Multi-Step Spectrum Prediction Using Transformer and Stacked Bi-LSTM",
    "abstract": "Spectrum prediction is considered as a key technology to assist spectrum decision. Despite the great efforts that have been put on the construction of spectrum prediction, achieving accurate spectrum prediction emphasizes the need for more advanced solutions. In this paper, we propose a new multichannel multi-step spectrum prediction method using Transformer and stacked bidirectional LSTM (Bi- LSTM), named TSB. Specifically, we use multi-head attention and stacked Bi-LSTM to build a new Transformer based on encoder-decoder architecture. The self-attention mechanism composed of multiple layers of multi-head attention can continuously attend to all positions of the multichannel spectrum sequences. The stacked Bi-LSTM can learn these focused coding features by multi-head attention layer by layer. The advantage of this fusion mode is that it can deeply capture the long-term dependence of multichannel spectrum data. We have conducted extensive experiments on a dataset generated by a real simulation platform. The results show that the proposed algorithm performs better than the baselines.",
    "authors": [
      "Guangliang Pan",
      "Jie Li",
      "Minglei Li"
    ],
    "url": "http://arxiv.org/abs/2405.19138v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SP",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "103c41c0-0f72-4f5b-ad64-df5b40afa57b": {
    "pk": "103c41c0-0f72-4f5b-ad64-df5b40afa57b",
    "title": "Towards the understanding of heavy quarks hadronization: from leptonic to heavy-ion collisions",
    "abstract": "The formation of hadrons is a fundamental process in nature that can be investigated at particle colliders. As several recent findings demonstrate, with \\ensuremath{\\mathrm{e^+e^-}}\\xspace collisions as a \"vacuum-like\" reference at one extreme, and central AA as a dense, extended-size system characterized by flow and local equilibrium at the opposite extreme, different collision systems offer a lever arm that can be exploited to probe with a range of heavy-flavour hadron species the onset of various hadronization processes. In this review, we present an overview of the theoretical and experimental developments. The focus is on open-heavy-flavour measurements. The comparison with model predictions and connections among the results in electron-positron, proton--proton, proton--nucleus, nucleus--nucleus collisions are discussed. After reviewing the current state, we suggest some prospects and future developments.",
    "authors": [
      "J. Altmann",
      "A. Dubla",
      "V. Greco",
      "A. Rossi",
      "P. Skands"
    ],
    "url": "http://arxiv.org/abs/2405.19137v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "hep-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "b33d475a-6628-4869-a39a-7396431133a6": {
    "pk": "b33d475a-6628-4869-a39a-7396431133a6",
    "title": "Analyzing Chat Protocols of Novice Programmers Solving Introductory Programming Tasks with ChatGPT",
    "abstract": "Large Language Models (LLMs) have taken the world by storm, and students are assumed to use related tools at a great scale. In this research paper we aim to gain an understanding of how introductory programming students chat with LLMs and related tools, e.g., ChatGPT-3.5. To address this goal, computing students at a large German university were motivated to solve programming exercises with the assistance of ChatGPT as part of their weekly introductory course exercises. Then students (n=213) submitted their chat protocols (with 2335 prompts in sum) as data basis for this analysis. The data was analyzed w.r.t. the prompts, frequencies, the chats' progress, contents, and other use pattern, which revealed a great variety of interactions, both potentially supportive and concerning. Learning about students' interactions with ChatGPT will help inform and align teaching practices and instructions for future introductory programming courses in higher education.",
    "authors": [
      "Andreas Scholl",
      "Daniel Schiffner",
      "Natalie Kiesler"
    ],
    "url": "http://arxiv.org/abs/2405.19132v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.AI",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2ba5bbfb-1f47-4fe6-8cf6-2e500f28b92c": {
    "pk": "2ba5bbfb-1f47-4fe6-8cf6-2e500f28b92c",
    "title": "Learning Interpretable Scheduling Algorithms for Data Processing Clusters",
    "abstract": "Workloads in data processing clusters are often represented in the form of DAG (Directed Acyclic Graph) jobs. Scheduling DAG jobs is challenging. Simple heuristic scheduling algorithms are often adopted in practice in production data centres. There is much room for scheduling performance optimisation for cost saving. Recently, reinforcement learning approaches (like decima) have been attempted to optimise DAG job scheduling and demonstrate clear performance gain in comparison to traditional algorithms. However, reinforcement learning (RL) approaches face their own problems in real-world deployment. In particular, their black-box decision making processes and generalizability in unseen workloads may add a non-trivial burden to the cluster administrators. Moreover, adapting RL models on unseen workloads often requires significant amount of training data, which leaves edge cases run in a sub-optimal mode. To fill the gap, we propose a new method to distill a simple scheduling policy based on observations of the behaviours of a complex deep learning model. The simple model not only provides interpretability of scheduling decisions, but also adaptive to edge cases easily through tuning. We show that our method achieves high fidelity to the decisions made by deep learning models and outperforms these models when additional heuristics are taken into account.",
    "authors": [
      "Zhibo Hu",
      "Chen Wang",
      "Helen",
      "Paik",
      "Yanfeng Shu",
      "Liming Zhu"
    ],
    "url": "http://arxiv.org/abs/2405.19131v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "6bc1a821-cb09-499d-8a88-a179c8419e22": {
    "pk": "6bc1a821-cb09-499d-8a88-a179c8419e22",
    "title": "Transport model study of transverse momentum distributions of Pion, kaon, and (anti-)proton production in U+U collisions at $\\sqrt{s_{NN}}$ = 193 GeV",
    "abstract": "The transverse momentum spectra of $\\pi ^{\\pm }$, $k ^{\\pm }$ and $p(\\bar{p})$ in midrapidity ($\\left | y \\right | < 0.1$) for nine centrality classes : $0-5\\%$, $5-10\\%$, $10-20\\%$, $20-30\\%$, $30-40\\%$, $40-50\\%$, $50-60\\%$, $60-70\\%$ and $70-80\\%$ in $^{238} U$+$^{238} U$ collisions at $\\sqrt{s_{NN}}$=193 GeV are studied within the framework of the cascade and soft momentum dependent equation of state (SM-EoS) mode of the UrQMD model. Other extracted observables from $p_{T}$ spectra such as average transverse momentum ($\\left \\langle p_{T} \\right \\rangle$), particle yields ($dN/dy$) and particle ratios are also shown as functions of collision centrality. It is found that the U+U collision process is segmented. Before the collision centrality is $50-60\\%$, the experimental data are described well using cascade mode when the $p_{T} < 1.2 GeV/c$. The results are in good agreement with the experimental data using the SM-EoS mode at $p_{T} > 1.2 GeV/c$. For the case of $60-80\\%$ centrality, the SM-EoS mode describes the data better. Anti-particle to particle yield ratios indicating pair production is the dominant mechanism of particle production at RHIC energy.",
    "authors": [
      "Ying Yuan"
    ],
    "url": "http://arxiv.org/abs/2405.19130v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "hep-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e45004ba-e90d-461e-b481-e2a0f3e42d56": {
    "pk": "e45004ba-e90d-461e-b481-e2a0f3e42d56",
    "title": "Can Graph Learning Improve Task Planning?",
    "abstract": "Task planning is emerging as an important research topic alongside the development of large language models (LLMs). It aims to break down complex user requests into solvable sub-tasks, thereby fulfilling the original requests. In this context, the sub-tasks can be naturally viewed as a graph, where the nodes represent the sub-tasks, and the edges denote the dependencies among them. Consequently, task planning is a decision-making problem that involves selecting a connected path or subgraph within the corresponding graph and invoking it. In this paper, we explore graph learning-based methods for task planning, a direction that is orthogonal to the prevalent focus on prompt design. Our interest in graph learning stems from a theoretical discovery: the biases of attention and auto-regressive loss impede LLMs' ability to effectively navigate decision-making on graphs, which is adeptly addressed by graph neural networks (GNNs). This theoretical insight led us to integrate GNNs with LLMs to enhance overall performance. Extensive experiments demonstrate that GNN-based methods surpass existing solutions even without training, and minimal training can further enhance their performance. Additionally, our approach complements prompt engineering and fine-tuning techniques, with performance further enhanced by improved prompts or a fine-tuned model.",
    "authors": [
      "Xixi Wu",
      "Yifei Shen",
      "Caihua Shan",
      "Kaitao Song",
      "Siwei Wang",
      "Bohang Zhang",
      "Jiarui Feng",
      "Hong Cheng",
      "Wei Chen",
      "Yun Xiong",
      "Dongsheng Li"
    ],
    "url": "http://arxiv.org/abs/2405.19119v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "0be3424d-a448-4aa9-abf0-bc6231f3b235": {
    "pk": "0be3424d-a448-4aa9-abf0-bc6231f3b235",
    "title": "ChartFormer: A Large Vision Language Model for Converting Chart Images into Tactile Accessible SVGs",
    "abstract": "Visualizations, such as charts, are crucial for interpreting complex data. However, they are often provided as raster images, which are not compatible with assistive technologies for people with blindness and visual impairments, such as embossed papers or tactile displays. At the same time, creating accessible vector graphics requires a skilled sighted person and is time-intensive. In this work, we leverage advancements in the field of chart analysis to generate tactile charts in an end-to-end manner. Our three key contributions are as follows: (1) introducing the ChartFormer model trained to convert raster chart images into tactile-accessible SVGs, (2) training this model on the Chart2Tactile dataset, a synthetic chart dataset we created following accessibility standards, and (3) evaluating the effectiveness of our SVGs through a pilot user study with an refreshable two-dimensional tactile display. Our work is publicly available at https://github.com/nsothman/ChartFormer .",
    "authors": [
      "Omar Moured",
      "Sara Alzalabny",
      "Anas Osman",
      "Thorsten Schwarz",
      "Karin Muller",
      "Rainer Stiefelhagen"
    ],
    "url": "http://arxiv.org/abs/2405.19117v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "534fd154-c986-4a3c-87c3-37fa0b2096a5": {
    "pk": "534fd154-c986-4a3c-87c3-37fa0b2096a5",
    "title": "Reconstructing Interpretable Features in Computational Super-Resolution microscopy via Regularized Latent Search",
    "abstract": "Supervised deep learning approaches can artificially increase the resolution of microscopy images by learning a mapping between two image resolutions or modalities. However, such methods often require a large set of hard-to-get low-res/high-res image pairs and produce synthetic images with a moderate increase in resolution. Conversely, recent methods based on GAN latent search offered a drastic increase in resolution without the need of paired images. However, they offer limited reconstruction of the high-resolution image interpretable features. Here, we propose a robust super-resolution method based on regularized latent search~(RLS) that offers an actionable balance between fidelity to the ground-truth and realism of the recovered image given a distribution prior. The latter allows to split the analysis of a low-resolution image into a computational super-resolution task performed by deep learning followed by a quantification task performed by a handcrafted algorithm and based on interpretable biological features. This two-step process holds potential for various applications such as diagnostics on mobile devices, where the main aim is not to recover the high-resolution details of a specific sample but rather to obtain high-resolution images that preserve explainable and quantifiable differences between conditions.",
    "authors": [
      "Marzieh Gheisari",
      "Auguste Genovesio"
    ],
    "url": "http://arxiv.org/abs/2405.19112v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "f31f4d7b-7c40-4d41-bd66-967587c6eef2": {
    "pk": "f31f4d7b-7c40-4d41-bd66-967587c6eef2",
    "title": "Alt4Blind: A User Interface to Simplify Charts Alt-Text Creation",
    "abstract": "Alternative Texts (Alt-Text) for chart images are essential for making graphics accessible to people with blindness and visual impairments. Traditionally, Alt-Text is manually written by authors but often encounters issues such as oversimplification or complication. Recent trends have seen the use of AI for Alt-Text generation. However, existing models are susceptible to producing inaccurate or misleading information. We address this challenge by retrieving high-quality alt-texts from similar chart images, serving as a reference for the user when creating alt-texts. Our three contributions are as follows: (1) we introduce a new benchmark comprising 5,000 real images with semantically labeled high-quality Alt-Texts, collected from Human Computer Interaction venues. (2) We developed a deep learning-based model to rank and retrieve similar chart images that share the same visual and textual semantics. (3) We designed a user interface (UI) to facilitate the alt-text creation process. Our preliminary interviews and investigations highlight the usability of our UI. For the dataset and further details, please refer to our project page: https://moured.github.io/alt4blind/.",
    "authors": [
      "Omar Moured",
      "Shahid Ali Farooqui",
      "Karin Muller",
      "Sharifeh Fadaeijouybari",
      "Thorsten Schwarz",
      "Mohammed Javed",
      "Rainer Stiefelhagen"
    ],
    "url": "http://arxiv.org/abs/2405.19111v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "84a6b3f4-9cb7-479a-8a5d-66fa22323417": {
    "pk": "84a6b3f4-9cb7-479a-8a5d-66fa22323417",
    "title": "PathReasoner: Modeling Reasoning Path with Equivalent Extension for Logical Question Answering",
    "abstract": "Logical reasoning task has attracted great interest since it was proposed. Faced with such a task, current competitive models, even large language models (e.g., ChatGPT and PaLM 2), still perform badly. Previous promising LMs struggle in logical consistency modeling and logical structure perception. To this end, we model the logical reasoning task by transforming each logical sample into reasoning paths and propose an architecture \\textbf{PathReasoner}. It addresses the task from the views of both data and model. To expand the diversity of the logical samples, we propose an atom extension strategy supported by equivalent logical formulas, to form new reasoning paths. From the model perspective, we design a stack of transformer-style blocks. In particular, we propose a path-attention module to joint model in-atom and cross-atom relations with the high-order diffusion strategy. Experiments show that PathReasoner achieves competitive performances on two logical reasoning benchmarks and great generalization abilities.",
    "authors": [
      "Fangzhi Xu",
      "Qika Lin",
      "Tianzhe Zhao",
      "Jiawei Han",
      "Jun Liu"
    ],
    "url": "http://arxiv.org/abs/2405.19109v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "804b6534-e95a-42a0-8161-6e3a03aa40e0": {
    "pk": "804b6534-e95a-42a0-8161-6e3a03aa40e0",
    "title": "Efficient and operational quantifier of divisibility in terms of channel discrimination",
    "abstract": "The understanding of open quantum systems is crucial for the development of quantum technologies. Of particular relevance is the characterisation of divisible quantum dynamics, seen as a generalisation of Markovian processes to the quantum setting. Here, we propose a way to detect divisibility and quantify how non-divisible a quantum channel is through the concept of channel discrimination. We ask how well we can distinguish generic dynamics from divisible dynamics. We show that this question can be answered efficiently through semidefinite programming, which provides us with an operational and efficient way to quantify divisibility.",
    "authors": [
      "Ranieri Nery",
      "Nadja K. Bernardes",
      "Daniel Cavalcanti",
      "Rafael Chaves",
      "Cristhiano Duarte"
    ],
    "url": "http://arxiv.org/abs/2405.19108v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "50684bb3-79cc-4250-b22e-ed263146a94a": {
    "pk": "50684bb3-79cc-4250-b22e-ed263146a94a",
    "title": "Offline Regularised Reinforcement Learning for Large Language Models Alignment",
    "abstract": "The dominant framework for alignment of large language models (LLM), whether through reinforcement learning from human feedback or direct preference optimisation, is to learn from preference data. This involves building datasets where each element is a quadruplet composed of a prompt, two independent responses (completions of the prompt) and a human preference between the two independent responses, yielding a preferred and a dis-preferred response. Such data is typically scarce and expensive to collect. On the other hand, \\emph{single-trajectory} datasets where each element is a triplet composed of a prompt, a response and a human feedback is naturally more abundant. The canonical element of such datasets is for instance an LLM's response to a user's prompt followed by a user's feedback such as a thumbs-up/down. Consequently, in this work, we propose DRO, or \\emph{Direct Reward Optimisation}, as a framework and associated algorithms that do not require pairwise preferences. DRO uses a simple mean-squared objective that can be implemented in various ways. We validate our findings empirically, using T5 encoder-decoder language models, and show DRO's performance over selected baselines such as Kahneman-Tversky Optimization (KTO). Thus, we confirm that DRO is a simple and empirically compelling method for single-trajectory policy optimisation.",
    "authors": [
      "Pierre Harvey Richemond",
      "Yunhao Tang",
      "Daniel Guo",
      "Daniele Calandriello",
      "Mohammad Gheshlaghi Azar",
      "Rafael Rafailov",
      "Bernardo Avila Pires",
      "Eugene Tarassov",
      "Lucas Spangher",
      "Will Ellsworth",
      "Aliaksei Severyn",
      "Jonathan Mallinson",
      "Lior Shani",
      "Gil Shamir",
      "Rishabh Joshi",
      "Tianqi Liu",
      "Remi Munos",
      "Bilal Piot"
    ],
    "url": "http://arxiv.org/abs/2405.19107v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "2d244baf-7c48-4935-863d-211f3a6476e8": {
    "pk": "2d244baf-7c48-4935-863d-211f3a6476e8",
    "title": "Voice Jailbreak Attacks Against GPT-4o",
    "abstract": "Recently, the concept of artificial assistants has evolved from science fiction into real-world applications. GPT-4o, the newest multimodal large language model (MLLM) across audio, vision, and text, has further blurred the line between fiction and reality by enabling more natural human-computer interactions. However, the advent of GPT-4o's voice mode may also introduce a new attack surface. In this paper, we present the first systematic measurement of jailbreak attacks against the voice mode of GPT-4o. We show that GPT-4o demonstrates good resistance to forbidden questions and text jailbreak prompts when directly transferring them to voice mode. This resistance is primarily due to GPT-4o's internal safeguards and the difficulty of adapting text jailbreak prompts to voice mode. Inspired by GPT-4o's human-like behaviors, we propose VoiceJailbreak, a novel voice jailbreak attack that humanizes GPT-4o and attempts to persuade it through fictional storytelling (setting, character, and plot). VoiceJailbreak is capable of generating simple, audible, yet effective jailbreak prompts, which significantly increases the average attack success rate (ASR) from 0.033 to 0.778 in six forbidden scenarios. We also conduct extensive experiments to explore the impacts of interaction steps, key elements of fictional writing, and different languages on VoiceJailbreak's effectiveness and further enhance the attack performance with advanced fictional writing techniques. We hope our study can assist the research community in building more secure and well-regulated MLLMs.",
    "authors": [
      "Xinyue Shen",
      "Yixin Wu",
      "Michael Backes",
      "Yang Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.19103v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "53f5c9e5-9f68-4653-ab7e-c5d3df5ea98f": {
    "pk": "53f5c9e5-9f68-4653-ab7e-c5d3df5ea98f",
    "title": "Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge Transfer",
    "abstract": "Current facial expression recognition (FER) models are often designed in a supervised learning manner thus are constrained by the lack of large-scale facial expression images with high-quality annotations. Consequently, these models often fail to generalize well, performing poorly on unseen images in training. Vision-language-based zero-shot models demonstrate a promising potential for addressing such challenges. However, these models lack task-specific knowledge therefore are not optimized for the nuances of recognizing facial expressions. To bridge this gap, this work proposes a novel method, Exp-CLIP, to enhance zero-shot FER by transferring the task knowledge from large language models (LLMs). Specifically, based on the pre-trained vision-language encoders, we incorporate a projection head designed to map the initial joint vision-language space into a space that captures representations of facial actions. To train this projection head for subsequent zero-shot predictions, we propose to align the projected visual representations with task-specific semantic meanings derived from the LLM encoder, and the text instruction-based strategy is employed to customize the LLM knowledge. Given unlabelled facial data and efficient training of the projection head, Exp-CLIP achieves superior zero-shot results to the CLIP models and several other large vision-language models (LVLMs) on seven in-the-wild FER datasets. The code and pre-trained models are available at \\url{https://github.com/zengqunzhao/Exp-CLIP}.",
    "authors": [
      "Zengqun Zhao",
      "Yu Cao",
      "Shaogang Gong",
      "Ioannis Patras"
    ],
    "url": "http://arxiv.org/abs/2405.19100v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "34bc0d93-8267-4b17-ba6a-46451e918167": {
    "pk": "34bc0d93-8267-4b17-ba6a-46451e918167",
    "title": "DataSafe: Copyright Protection with PUF Watermarking and Blockchain Tracking",
    "abstract": "Digital watermarking methods are commonly used to safeguard digital media copyrights by confirming ownership and deterring unauthorized use. However, without reliable third-party oversight, these methods risk security vulnerabilities during watermark extraction. Furthermore, digital media lacks tangible ownership attributes, posing challenges for secure copyright transfer and tracing. This study introduces DataSafe, a copyright protection scheme that combines physical unclonable functions (PUFs) and blockchain technology. PUF devices use their unique fingerprints for blockchain registration. Subsequently, these devices incorporate invisible watermarking techniques to embed digital watermarks into media for copyright protection. The watermark verification process is confined within the devices, preserving confidentiality during extraction, validating identities during copyright exchanges, and facilitating blockchain-based traceability of copyright transfers. The implementation of a prototype system on the LPC55S69-EVK development board is detailed, illustrating the practicality and effectiveness of the proposed solution.",
    "authors": [
      "Xiaolong Xue",
      "Guangyong Shang",
      "Zhen Ma",
      "Minghui Xu",
      "Hechuan Guo",
      "Kun Li",
      "Xiuzhen Cheng"
    ],
    "url": "http://arxiv.org/abs/2405.19099v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CR",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7d0237dd-33f6-41e7-8366-ce2cdb95f78a": {
    "pk": "7d0237dd-33f6-41e7-8366-ce2cdb95f78a",
    "title": "Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior",
    "abstract": "This paper studies the challenging black-box adversarial attack that aims to generate adversarial examples against a black-box model by only using output feedback of the model to input queries. Some previous methods improve the query efficiency by incorporating the gradient of a surrogate white-box model into query-based attacks due to the adversarial transferability. However, the localized gradient is not informative enough, making these methods still query-intensive. In this paper, we propose a Prior-guided Bayesian Optimization (P-BO) algorithm that leverages the surrogate model as a global function prior in black-box adversarial attacks. As the surrogate model contains rich prior information of the black-box one, P-BO models the attack objective with a Gaussian process whose mean function is initialized as the surrogate model's loss. Our theoretical analysis on the regret bound indicates that the performance of P-BO may be affected by a bad prior. Therefore, we further propose an adaptive integration strategy to automatically adjust a coefficient on the function prior by minimizing the regret bound. Extensive experiments on image classifiers and large vision-language models demonstrate the superiority of the proposed algorithm in reducing queries and improving attack success rates compared with the state-of-the-art black-box attacks. Code is available at https://github.com/yibo-miao/PBO-Attack.",
    "authors": [
      "Shuyu Cheng",
      "Yibo Miao",
      "Yinpeng Dong",
      "Xiao Yang",
      "Xiao-Shan Gao",
      "Jun Zhu"
    ],
    "url": "http://arxiv.org/abs/2405.19098v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "5cdf325b-eb9c-49d9-a770-eef5c0f0c9ce": {
    "pk": "5cdf325b-eb9c-49d9-a770-eef5c0f0c9ce",
    "title": "A study of why we need to reassess full reference image quality assessment with medical images",
    "abstract": "Image quality assessment (IQA) is not just indispensable in clinical practice to ensure high standards, but also in the development stage of novel algorithms that operate on medical images with reference data. This paper provides a structured and comprehensive collection of examples where the two most common full reference (FR) image quality measures prove to be unsuitable for the assessment of novel algorithms using different kinds of medical images, including real-world MRI, CT, OCT, X-Ray, digital pathology and photoacoustic imaging data. In particular, the FR-IQA measures PSNR and SSIM are known and tested for working successfully in many natural imaging tasks, but discrepancies in medical scenarios have been noted in the literature. Inconsistencies arising in medical images are not surprising, as they have very different properties than natural images which have not been targeted nor tested in the development of the mentioned measures, and therefore might imply wrong judgement of novel methods for medical images. Therefore, improvement is urgently needed in particular in this era of AI to increase explainability, reproducibility and generalizability in machine learning for medical imaging and beyond. On top of the pitfalls we will provide ideas for future research as well as suggesting guidelines for the usage of FR-IQA measures applied to medical images.",
    "authors": [
      "Anna Breger",
      "Ander Biguri",
      "Malena Sabat\u00e9 Landman",
      "Ian Selby",
      "Nicole Amberg",
      "Elisabeth Brunner",
      "Janek Gr\u00f6hl",
      "Sepideh Hatamikia",
      "Clemens Karner",
      "Lipeng Ning",
      "S\u00f6ren Dittmer",
      "Michael Roberts",
      "AIX-COVNET Collaboration",
      "Carola-Bibiane Sch\u00f6nlieb"
    ],
    "url": "http://arxiv.org/abs/2405.19097v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.IV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "1fa6e787-c1ca-4ae9-9710-5472b9f44bfd": {
    "pk": "1fa6e787-c1ca-4ae9-9710-5472b9f44bfd",
    "title": "Faithful Chart Summarization with ChaTS-Pi",
    "abstract": "Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people. Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors. In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence. We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences. We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets.",
    "authors": [
      "Syrine Krichene",
      "Francesco Piccinno",
      "Fangyu Liu",
      "Julian Martin Eisenschlos"
    ],
    "url": "http://arxiv.org/abs/2405.19094v1",
    "timestamp": 1716912000,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}
