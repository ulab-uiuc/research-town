{
  "af053436-11d7-4c51-bca8-a7036dc8f9ea": {
    "pk": "af053436-11d7-4c51-bca8-a7036dc8f9ea",
    "title": "The Velocity Dispersion Function for Quiescent Galaxies in Massive Clusters from IllustrisTNG",
    "abstract": "We derive the central stellar velocity dispersion function for quiescent galaxies in 280 massive clusters with $\\log (M_{200} / M_{\\odot}) > 14$ in IllustrisTNG300. The velocity dispersion function is an independent tracer of the dark matter mass distribution of subhalos in galaxy clusters. Based on the IllustrisTNG cluster catalog, we select quiescent member subhalos with a specific star formation rate $< 2 \\times 10^{-11}$ yr${^-1}$ and stellar mass $\\log (M_{*} / M_{\\odot}) > 9$. We then simulate fiber spectroscopy to measure the stellar velocity dispersion of the simulated galaxies; we compute the line-of-sight velocity dispersions of star particles within a cylindrical volume that penetrates the core of each subhalo. We construct the velocity dispersion functions for quiescent subhalos within $R_{200}$. The simulated cluster velocity dispersion function exceeds the simulated field velocity dispersion function for $\\log \\sigma_{*} > 2.2$, indicating the preferential formation of large velocity dispersion galaxies in dense environments. The excess is similar in simulations and in the observations. We also compare the simulated velocity dispersion function for the three most massive clusters with $\\log (M_{200} / M_{\\odot}) > 15$ with the observed velocity dispersion function for the two most massive clusters in the local universe, Coma and A2029. Intriguingly, the simulated velocity dispersion functions are significantly lower for $\\log \\sigma_{*} > 2.0$. This discrepancy results from 1) a smaller number of subhalos with $\\log (M_{*} / M_{\\odot}) > 10$ in TNG300 compared to the observed clusters, and 2) a significant offset between the observed and simulated $M_{*} - \\sigma_{*}$ relations. The velocity dispersion function offers a unique window on galaxy and structure formation in simulations.",
    "authors": [
      "Jubee Sohn",
      "Margaret J. Geller",
      "Josh Borrow",
      "Mark Vogelsberger"
    ],
    "url": "http://arxiv.org/abs/2405.21076v1",
    "timestamp": 1717178397,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "astro-ph.GA",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "55687e02-e6e0-4814-92bc-8d49af483843": {
    "pk": "55687e02-e6e0-4814-92bc-8d49af483843",
    "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
    "abstract": "In the quest for artificial general intelligence, Multi-modal Large Language Models (MLLMs) have emerged as a focal point in recent advancements. However, the predominant focus remains on developing their capabilities in static image understanding. The potential of MLLMs in processing sequential visual data is still insufficiently explored, highlighting the absence of a comprehensive, high-quality assessment of their performance. In this paper, we introduce Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of MLLMs in Video analysis. Our work distinguishes from existing benchmarks through four key features: 1) Diversity in video types, spanning 6 primary visual domains with 30 subfields to ensure broad scenario generalizability; 2) Duration in temporal dimension, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour, for robust contextual dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides video frames, including subtitles and audios, to unveil the all-round capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. 900 videos with a total of 256 hours are manually selected and annotated by repeatedly viewing all the video content, resulting in 2,700 question-answer pairs. With Video-MME, we extensively evaluate various state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as open-source image models like InternVL-Chat-V1.5 and video models like LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the best-performing commercial model, significantly outperforming the open-source models. Our dataset along with these findings underscores the need for further improvements in handling longer sequences and multi-modal data. Project Page: https://video-mme.github.io",
    "authors": [
      "Chaoyou Fu",
      "Yuhan Dai",
      "Yondong Luo",
      "Lei Li",
      "Shuhuai Ren",
      "Renrui Zhang",
      "Zihan Wang",
      "Chenyu Zhou",
      "Yunhang Shen",
      "Mengdan Zhang",
      "Peixian Chen",
      "Yanwei Li",
      "Shaohui Lin",
      "Sirui Zhao",
      "Ke Li",
      "Tong Xu",
      "Xiawu Zheng",
      "Enhong Chen",
      "Rongrong Ji",
      "Xing Sun"
    ],
    "url": "http://arxiv.org/abs/2405.21075v1",
    "timestamp": 1717178387,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "7c14a3fb-56f2-4441-b49a-8d500296819c": {
    "pk": "7c14a3fb-56f2-4441-b49a-8d500296819c",
    "title": "Latent Intrinsics Emerge from Training to Relight",
    "abstract": "Image relighting is the task of showing what a scene from a source image would look like if illuminated differently. Inverse graphics schemes recover an explicit representation of geometry and a set of chosen intrinsics, then relight with some form of renderer. However error control for inverse graphics is difficult, and inverse graphics methods can represent only the effects of the chosen intrinsics. This paper describes a relighting method that is entirely data-driven, where intrinsics and lighting are each represented as latent variables. Our approach produces SOTA relightings of real scenes, as measured by standard metrics. We show that albedo can be recovered from our latent intrinsics without using any example albedos, and that the albedos recovered are competitive with SOTA methods.",
    "authors": [
      "Xiao Zhang",
      "William Gao",
      "Seemandhar Jain",
      "Michael Maire",
      "David. A. Forsyth",
      "Anand Bhattad"
    ],
    "url": "http://arxiv.org/abs/2405.21074v1",
    "timestamp": 1717178352,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "c2f9f860-8828-438a-b88c-5bb306d5c982": {
    "pk": "c2f9f860-8828-438a-b88c-5bb306d5c982",
    "title": "Fast inspirals and the treatment of orbital resonances",
    "abstract": "Extreme mass ratio inspirals (EMRIs), where a compact object orbits a massive black hole, are a key source of gravitational waves for the future Laser Interferometer Space Antenna (LISA). Due to their small mass ratio, ($\\epsilon \\sim 10^{-4}$--$10^{-7}$), the binary evolves slowly and EMRI signals will be in-band for years. Additionally, astrophysical EMRIs are expected to have complex dynamics featuring both spin-precession and eccentricity. A standard approach to modelling these inspirals is via the method of osculating geodesics (OG) which we employ along with a toy model for the gravitational self-force. Using this method requires resolving tens of thousands radial and polar orbital librations over the long duration of the signal which makes the inspiral trajectory expensive to compute. In this work we accelerate these calculations by employing Near-Identity (averaging) Transformations. However, this averaging technique breaks down at orbital resonances where the radial and polar frequencies are an integer ratio of each other. Thus, we switch to a partial averaging transformation in the vicinity of the resonance where the dynamics are characterised by the slow evolution of the so-called \"resonant phase\". Additionally, we develop an optimal switching criterion to minimise the computation time while maximising accuracy. We find the error in the waveform phase is improved from $\\mathcal{O}(\\epsilon^{-1/2})$ in the fully averaged scheme to $\\mathcal{O}(\\epsilon^{4/7})$ in the switching scheme. At the same time, this scheme improves the scaling of the computation time from being inversely proportional to $\\epsilon$ using OG, to a very weak scaling with $\\epsilon$. This results in a speed-up of at least two orders of magnitude for LISA EMRIs with room for further optimisation.",
    "authors": [
      "Philip Lynch",
      "Vojt\u011bch Witzany",
      "Maarten van de Meent",
      "Niels Warburton"
    ],
    "url": "http://arxiv.org/abs/2405.21072v1",
    "timestamp": 1717178286,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "gr-qc",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "bb4c48ab-9386-4ca6-b5e8-b625718e3719": {
    "pk": "bb4c48ab-9386-4ca6-b5e8-b625718e3719",
    "title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights",
    "abstract": "Severe data imbalance naturally exists among web-scale vision-language datasets. Despite this, we find CLIP pre-trained thereupon exhibits notable robustness to the data imbalance compared to supervised learning, and demonstrates significant effectiveness in learning generalizable representations. With an aim to investigate the reasons behind this finding, we conduct controlled experiments to study various underlying factors, and reveal that CLIP's pretext task forms a dynamic classification problem wherein only a subset of classes is present in training. This isolates the bias from dominant classes and implicitly balances the learning signal. Furthermore, the robustness and discriminability of CLIP improve with more descriptive language supervision, larger data scale, and broader open-world concepts, which are inaccessible to supervised learning. Our study not only uncovers the mechanisms behind CLIP's generalizability beyond data imbalance but also provides transferable insights for the research community. The findings are validated in both supervised and self-supervised learning, enabling models trained on imbalanced data to achieve CLIP-level performance on diverse recognition tasks. Code will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.",
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Xiaojuan Qi"
    ],
    "url": "http://arxiv.org/abs/2405.21070v1",
    "timestamp": 1717178244,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "31ababc2-03f0-4950-9271-f84b4ffea7aa": {
    "pk": "31ababc2-03f0-4950-9271-f84b4ffea7aa",
    "title": "Code Pretraining Improves Entity Tracking Abilities of Language Models",
    "abstract": "Recent work has provided indirect evidence that pretraining language models on code improves the ability of models to track state changes of discourse entities expressed in natural language. In this work, we systematically test this claim by comparing pairs of language models on their entity tracking performance. Critically, the pairs consist of base models and models trained on top of these base models with additional code data. We extend this analysis to additionally examine the effect of math training, another highly structured data type, and alignment tuning, an important step for enhancing the usability of models. We find clear evidence that models additionally trained on large amounts of code outperform the base models. On the other hand, we find no consistent benefit of additional math training or alignment tuning across various model families.",
    "authors": [
      "Najoung Kim",
      "Sebastian Schuster",
      "Shubham Toshniwal"
    ],
    "url": "http://arxiv.org/abs/2405.21068v1",
    "timestamp": 1717178193,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CL",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "48e2c6a8-f643-4f49-a4ff-df6d163dfe62": {
    "pk": "48e2c6a8-f643-4f49-a4ff-df6d163dfe62",
    "title": "Mixed Diffusion for 3D Indoor Scene Synthesis",
    "abstract": "Realistic conditional 3D scene synthesis significantly enhances and accelerates the creation of virtual environments, which can also provide extensive training data for computer vision and robotics research among other applications. Diffusion models have shown great performance in related applications, e.g., making precise arrangements of unordered sets. However, these models have not been fully explored in floor-conditioned scene synthesis problems. We present MiDiffusion, a novel mixed discrete-continuous diffusion model architecture, designed to synthesize plausible 3D indoor scenes from given room types, floor plans, and potentially pre-existing objects. We represent a scene layout by a 2D floor plan and a set of objects, each defined by its category, location, size, and orientation. Our approach uniquely implements structured corruption across the mixed discrete semantic and continuous geometric domains, resulting in a better conditioned problem for the reverse denoising step. We evaluate our approach on the 3D-FRONT dataset. Our experimental results demonstrate that MiDiffusion substantially outperforms state-of-the-art autoregressive and diffusion models in floor-conditioned 3D scene synthesis. In addition, our models can handle partial object constraints via a corruption-and-masking strategy without task specific training. We show MiDiffusion maintains clear advantages over existing approaches in scene completion and furniture arrangement experiments.",
    "authors": [
      "Siyi Hu",
      "Diego Martin Arroyo",
      "Stephanie Debats",
      "Fabian Manhardt",
      "Luca Carlone",
      "Federico Tombari"
    ],
    "url": "http://arxiv.org/abs/2405.21066v1",
    "timestamp": 1717178092,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "e5cd9938-ee47-460b-95f3-254323aa600a": {
    "pk": "e5cd9938-ee47-460b-95f3-254323aa600a",
    "title": "Single-beam grating-chip 3D and 1D optical lattices",
    "abstract": "Ultracold atoms are crucial for unlocking truly precise and accurate quantum metrology, and provide an essential platform for quantum computing, communication and memories. One of the largest ongoing challenges is the miniaturization of these quantum devices. Here, we show that the typically macroscopic optical lattice architecture at the heart of many ultra-precise quantum technologies can be realized with a single input laser beam on the same diffractive chip already used to create the ultracold atoms. Moreover, this inherently ultra-stable platform enables access to a plethora of new lattice dimensionalities and geometries, ideally suited for the design of high-accuracy, portable quantum devices.",
    "authors": [
      "Alan Bregazzi",
      "James P. McGilligan",
      "Paul F. Griffin",
      "Erling Riis",
      "Aidan S. Arnold"
    ],
    "url": "http://arxiv.org/abs/2405.21065v1",
    "timestamp": 1717178088,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.quant-gas",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "8e0fca2e-d19a-4f49-8d8f-49487c2abbcb": {
    "pk": "8e0fca2e-d19a-4f49-8d8f-49487c2abbcb",
    "title": "Neural Network Verification with Branch-and-Bound for General Nonlinearities",
    "abstract": "Branch-and-bound (BaB) is among the most effective methods for neural network (NN) verification. However, existing works on BaB have mostly focused on NNs with piecewise linear activations, especially ReLU networks. In this paper, we develop a general framework, named GenBaB, to conduct BaB for general nonlinearities in general computational graphs based on linear bound propagation. To decide which neuron to branch, we design a new branching heuristic which leverages linear bounds as shortcuts to efficiently estimate the potential improvement after branching. To decide nontrivial branching points for general nonlinear functions, we propose to optimize branching points offline, which can be efficiently leveraged during verification with a lookup table. We demonstrate the effectiveness of our GenBaB on verifying a wide range of NNs, including networks with activation functions such as Sigmoid, Tanh, Sine and GeLU, as well as networks involving multi-dimensional nonlinear operations such as multiplications in LSTMs and Vision Transformers. Our framework also allows the verification of general nonlinear computation graphs and enables verification applications beyond simple neural networks, particularly for AC Optimal Power Flow (ACOPF). GenBaB is part of the latest $\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of Neural Networks Competition (VNN-COMP 2023).",
    "authors": [
      "Zhouxing Shi",
      "Qirui Jin",
      "Zico Kolter",
      "Suman Jana",
      "Cho-Jui Hsieh",
      "Huan Zhang"
    ],
    "url": "http://arxiv.org/abs/2405.21063v1",
    "timestamp": 1717177867,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null
  },
  "230959c2-525c-4068-a3e8-bbf67045857f": {
    "pk": "230959c2-525c-4068-a3e8-bbf67045857f",
    "title": "Unified Directly Denoising for Both Variance Preserving and Variance Exploding Diffusion Models",
    "abstract": "Previous work has demonstrated that, in the Variance Preserving (VP) scenario, the nascent Directly Denoising Diffusion Models (DDDM) can generate high-quality images in one step while achieving even better performance in multistep sampling. However, the Pseudo-LPIPS loss used in DDDM leads to concerns about the bias in assessment. Here, we propose a unified DDDM (uDDDM) framework that generates images in one-step/multiple steps for both Variance Preserving (VP) and Variance Exploding (VE) cases. We provide theoretical proofs of the existence and uniqueness of the model's solution paths, as well as the non-intersecting property of the sampling paths. Additionally, we propose an adaptive Pseudo-Huber loss function to balance the convergence to the true solution and the stability of convergence process.Through a comprehensive evaluation, we demonstrate that uDDDMs achieve FID scores comparable to the best-performing methods available for CIFAR-10 in both VP and VE. Specifically, uDDDM achieves one-step generation on CIFAR10 with FID of 2.63 and 2.53 for VE and VP respectively. By extending the sampling to 1000 steps, we further reduce FID score to 1.71 and 1.65 for VE and VP respectively, setting state-of-the-art performance in both cases.",
    "authors": [
      "Jingjing Wang",
      "Dan Zhang",
      "Feng Luo"
    ],
    "url": "http://arxiv.org/abs/2405.21059v1",
    "timestamp": 1717177791,
    "section_contents": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null
  }
}
