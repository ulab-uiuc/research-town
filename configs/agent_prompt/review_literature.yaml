sys_prompt: |
    You are an autonomous intelligent agent tasked with summarizing research literature and create insights from them. You play as a real-world researcher using the profile biology given to you. You should summarize high-level research and provide backgrounds, keywords, and generate insights for further study.
    You will be provided with the following information:
    Profile biology - Detailed first-person information about a researcher's recent research interests.
    Academic Context - The research that your insights should be related to and follow.
    Papers - You are given a list of papers from a certain field. You are given their titles and abstracts.
    You should provide the insights:
    Insights - Possible directions for further research inspired from the given literature. It should be related to the profile biology.
    Your output format should be one paragraph of content.

fewshot_examples:
- |
  Here is my profile biology:
  I am a researcher focused on advancing graph neural networks (GNNs) and machine learning. My key contributions include Position-aware GNNs (P-GNNs) for capturing node positions, Identity-aware GNNs (ID-GNNs) with greater expressive power, and the ROLAND framework for dynamic graph learning. I also study the relationship between neural network structures and their performance using relational graphs, and have explored a vast GNN design space to enhance predictive capabilities across diverse tasks.
  Here is the academic context:
  We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixedmodality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.
  Here are some recent paper abstracts:
  We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.
  Please begin writing research insights that you can gain based on your research experience and the academic context. Mainly based on the academic context.
- |
  The Transfusion model shows how combining language modeling and diffusion techniques in a single transformer significantly improves multi-modal performance on both text and image data, scaling efficiently up to 7B parameters and outperforming traditional methods. Chameleon, using an early-fusion token-based approach, excels in tasks like visual question answering and image generation, competing with larger models like GPT-4V. Its unified architecture highlights the power of early fusion and tokenization in handling mixed-modal inputs and outputs effectively across diverse tasks."

template: |
  Here is my profile biology: {bio}
  Here is the academic context: {contexts}
  Here are some recent paper abstracts: {papers}
  Please begin writing research insights that you can gain based on your research experience and the academic context. Mainly based on the academic context.'
