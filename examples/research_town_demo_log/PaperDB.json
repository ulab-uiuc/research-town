{
  "e8df3f4d-a074-4cc7-a88d-8e2bede97e86": {
    "pk": "e8df3f4d-a074-4cc7-a88d-8e2bede97e86",
    "project_name": null,
    "authors": [
      "Yiming Zhou",
      "Callen MacPhee",
      "Tingyi Zhou",
      "Bahram Jalali"
    ],
    "title": "Nonlinear Schr\u00f6dinger Network",
    "abstract": "Deep neural networks (DNNs) have achieved exceptional performance across various fields by learning complex nonlinear mappings from large-scale datasets. However, they encounter challenges such as high computational costs and limited interpretability. To address these issues, hybrid approaches that integrate physics with AI are gaining interest. This paper introduces a novel physics-based AI model called the \"Nonlinear Schr\\\"odinger Network\", which treats the Nonlinear Schr\\\"odinger Equation (NLSE) as a general-purpose trainable model for learning complex patterns including nonlinear mappings and memory effects from data. Existing physics-informed machine learning methods use neural networks to approximate the solutions of partial differential equations (PDEs). In contrast, our approach directly treats the PDE as a trainable model to obtain general nonlinear mappings that would otherwise require neural networks. As a physics-inspired approach, it offers a more interpretable and parameter-efficient alternative to traditional black-box neural networks, achieving comparable or better accuracy in time series classification tasks while significantly reducing the number of required parameters. Notably, the trained Nonlinear Schr\\\"odinger Network is interpretable, with all parameters having physical meanings as properties of a virtual physical system that transforms the data to a more separable space. This interpretability allows for insight into the underlying dynamics of the data transformation process. Applications to time series forecasting have also been explored. While our current implementation utilizes the NLSE, the proposed method of using physics equations as trainable models to learn nonlinear mappings from data is not limited to the NLSE and may be extended to other master equations of physics.",
    "url": "http://arxiv.org/abs/2407.14504v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "4483c488-95e2-46b1-9893-17b89e5cbc58": {
    "pk": "4483c488-95e2-46b1-9893-17b89e5cbc58",
    "project_name": null,
    "authors": [
      "Prasenjit Karmakar",
      "Swadhin Pradhan",
      "Sandip Chakraborty"
    ],
    "title": "Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities",
    "abstract": "In recent years, indoor air pollution has posed a significant threat to our society, claiming over 3.2 million lives annually. Developing nations, such as India, are most affected since lack of knowledge, inadequate regulation, and outdoor air pollution lead to severe daily exposure to pollutants. However, only a limited number of studies have attempted to understand how indoor air pollution affects developing countries like India. To address this gap, we present spatiotemporal measurements of air quality from 30 indoor sites over six months during summer and winter seasons. The sites are geographically located across four regions of type: rural, suburban, and urban, covering the typical low to middle-income population in India. The dataset contains various types of indoor environments (e.g., studio apartments, classrooms, research laboratories, food canteens, and residential households), and can provide the basis for data-driven learning model research aimed at coping with unique pollution patterns in developing countries. This unique dataset demands advanced data cleaning and imputation techniques for handling missing data due to power failure or network outages during data collection. Furthermore, through a simple speech-to-text application, we provide real-time indoor activity labels annotated by occupants. Therefore, environmentalists and ML enthusiasts can utilize this dataset to understand the complex patterns of the pollutants under different indoor activities, identify recurring sources of pollution, forecast exposure, improve floor plans and room structures of modern indoor designs, develop pollution-aware recommender systems, etc.",
    "url": "http://arxiv.org/abs/2407.14501v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "4f1d3108-d587-415f-afe7-e31ba0a20c9e": {
    "pk": "4f1d3108-d587-415f-afe7-e31ba0a20c9e",
    "project_name": null,
    "authors": [
      "Sukrut Rao",
      "Sweta Mahajan",
      "Moritz B\u00f6hle",
      "Bernt Schiele"
    ],
    "title": "Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery",
    "abstract": "Concept Bottleneck Models (CBMs) have recently been proposed to address the 'black-box' problem of deep neural networks, by first mapping images to a human-understandable concept space and then linearly combining concepts for classification. Such models typically require first coming up with a set of concepts relevant to the task and then aligning the representations of a feature extractor to map to these concepts. However, even with powerful foundational feature extractors like CLIP, there are no guarantees that the specified concepts are detectable. In this work, we leverage recent advances in mechanistic interpretability and propose a novel CBM approach -- called Discover-then-Name-CBM (DN-CBM) -- that inverts the typical paradigm: instead of pre-selecting concepts based on the downstream classification task, we use sparse autoencoders to first discover concepts learnt by the model, and then name them and train linear probes for classification. Our concept extraction strategy is efficient, since it is agnostic to the downstream task, and uses concepts already known to the model. We perform a comprehensive evaluation across multiple datasets and CLIP architectures and show that our method yields semantically meaningful concepts, assigns appropriate names to them that make them easy to interpret, and yields performant and interpretable CBMs. Code available at https://github.com/neuroexplicit-saar/discover-then-name.",
    "url": "http://arxiv.org/abs/2407.14499v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "5e247060-1050-419f-88f1-ea91c154fa9a": {
    "pk": "5e247060-1050-419f-88f1-ea91c154fa9a",
    "project_name": null,
    "authors": [
      "Rohan Gupta",
      "Iv\u00e1n Arcuschin",
      "Thomas Kwa",
      "Adri\u00e0 Garriga-Alonso"
    ],
    "title": "InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques",
    "abstract": "Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train these neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.",
    "url": "http://arxiv.org/abs/2407.14494v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "747a73d2-7104-4f78-a1ca-4ca9aa48daf2": {
    "pk": "747a73d2-7104-4f78-a1ca-4ca9aa48daf2",
    "project_name": null,
    "authors": [
      "Yajie Bao",
      "Javad Mohammadpour Velni"
    ],
    "title": "Adaptive Uncertainty Quantification for Scenario-based Control Using Meta-learning of Bayesian Neural Networks",
    "abstract": "Scenario-based optimization and control has proven to be an efficient approach to account for system uncertainty. In particular, the performance of scenario-based model predictive control (MPC) schemes depends on the accuracy of uncertainty quantification. However, current learning- and scenario-based MPC (sMPC) approaches employ a single timeinvariant probabilistic model (learned offline), which may not accurately describe time-varying uncertainties. Instead, this paper presents a model-agnostic meta-learning (MAML) of Bayesian neural networks (BNN) for adaptive uncertainty quantification that would be subsequently used for adaptive-scenario-tree model predictive control design of nonlinear systems with unknown dynamics to enhance control performance. In particular, the proposed approach learns both a global BNN model and an updating law to refine the BNN model. At each time step, the updating law transforms the global BNN model into more precise local BNN models in real time. The adapted local model is then used to generate scenarios for sMPC design at each time step. A probabilistic safety certificate is incorporated in the scenario generation to ensure that the trajectories of the generated scenarios contain the real trajectory of the system and that all the scenarios adhere to the constraints with a high probability. Experiments using closed-loop simulations of a numerical example demonstrate that the proposed approach can improve the performance of scenario-based MPC compared to using only one BNN model learned offline for all time steps.",
    "url": "http://arxiv.org/abs/2407.14492v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "eess.SY",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "4bfbcef7-ea8e-4de3-b393-a24784db9f5b": {
    "pk": "4bfbcef7-ea8e-4de3-b393-a24784db9f5b",
    "project_name": null,
    "authors": [
      "Meng Wang",
      "Bo Fang",
      "Ang Li",
      "Prashant Nair"
    ],
    "title": "Red-QAOA: Efficient Variational Optimization through Circuit Reduction",
    "abstract": "The Quantum Approximate Optimization Algorithm (QAOA) addresses combinatorial optimization challenges by converting inputs to graphs. However, the optimal parameter searching process of QAOA is greatly affected by noise. Larger problems yield bigger graphs, requiring more qubits and making their outcomes highly noise-sensitive. This paper introduces Red-QAOA, leveraging energy landscape concentration via a simulated annealing-based graph reduction.   Red-QAOA creates a smaller (distilled) graph with nearly identical parameters to the original graph. The distilled graph produces a smaller quantum circuit and thus reduces noise impact. At the end of the optimization, Red-QAOA employs the parameters from the distilled graph on the original graph and continues the parameter search on the original graph. Red-QAOA outperforms state-of-the-art Graph Neural Network (GNN)-based pooling techniques on 3200 real-world problems. Red-QAOA reduced node and edge counts by 28% and 37%, respectively, with a mean square error of only 2%.",
    "url": "http://arxiv.org/abs/2407.14490v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "quant-ph",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "e6f3389e-9806-4be2-a893-984b12694c17": {
    "pk": "e6f3389e-9806-4be2-a893-984b12694c17",
    "project_name": null,
    "authors": [
      "Alejandra de la Rica Escudero",
      "Eduardo C. Garrido-Merchan",
      "Maria Coronado-Vaca"
    ],
    "title": "Explainable Post hoc Portfolio Management Financial Policy of a Deep Reinforcement Learning agent",
    "abstract": "Financial portfolio management investment policies computed quantitatively by modern portfolio theory techniques like the Markowitz model rely on a set on assumptions that are not supported by data in high volatility markets. Hence, quantitative researchers are looking for alternative models to tackle this problem. Concretely, portfolio management is a problem that has been successfully addressed recently by Deep Reinforcement Learning (DRL) approaches. In particular, DRL algorithms train an agent by estimating the distribution of the expected reward of every action performed by an agent given any financial state in a simulator. However, these methods rely on Deep Neural Networks model to represent such a distribution, that although they are universal approximator models, they cannot explain its behaviour, given by a set of parameters that are not interpretable. Critically, financial investors policies require predictions to be interpretable, so DRL agents are not suited to follow a particular policy or explain their actions. In this work, we developed a novel Explainable Deep Reinforcement Learning (XDRL) approach for portfolio management, integrating the Proximal Policy Optimization (PPO) with the model agnostic explainable techniques of feature importance, SHAP and LIME to enhance transparency in prediction time. By executing our methodology, we can interpret in prediction time the actions of the agent to assess whether they follow the requisites of an investment policy or to assess the risk of following the agent suggestions. To the best of our knowledge, our proposed approach is the first explainable post hoc portfolio management financial policy of a DRL agent. We empirically illustrate our methodology by successfully identifying key features influencing investment decisions, which demonstrate the ability to explain the agent actions in prediction time.",
    "url": "http://arxiv.org/abs/2407.14486v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CE",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "6b9f89f4-6b3e-4d2a-a5e8-45d884ef8edc": {
    "pk": "6b9f89f4-6b3e-4d2a-a5e8-45d884ef8edc",
    "project_name": null,
    "authors": [
      "Xiaotong Sun",
      "Peijie Qiu",
      "Shengfan Zhang"
    ],
    "title": "SurvReLU: Inherently Interpretable Survival Analysis via Deep ReLU Networks",
    "abstract": "Survival analysis models time-to-event distributions with censorship. Recently, deep survival models using neural networks have dominated due to their representational power and state-of-the-art performance. However, their \"black-box\" nature hinders interpretability, which is crucial in real-world applications. In contrast, \"white-box\" tree-based survival models offer better interpretability but struggle to converge to global optima due to greedy expansion. In this paper, we bridge the gap between previous deep survival models and traditional tree-based survival models through deep rectified linear unit (ReLU) networks. We show that a deliberately constructed deep ReLU network (SurvReLU) can harness the interpretability of tree-based structures with the representational power of deep survival models. Empirical studies on both simulated and real survival benchmark datasets show the effectiveness of the proposed SurvReLU in terms of performance and interoperability. The code is available at \\href{https://github.com/xs018/SurvReLU}{\\color{magenta}{ https://github.com/xs018/SurvReLU}}.",
    "url": "http://arxiv.org/abs/2407.14463v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "470ea968-4098-44b3-bf7b-98b09f05b523": {
    "pk": "470ea968-4098-44b3-bf7b-98b09f05b523",
    "project_name": null,
    "authors": [
      "Jiahong Ma",
      "Mingguo He",
      "Zhewei Wei"
    ],
    "title": "PolyFormer: Scalable Node-wise Filters via Polynomial Graph Transformer",
    "abstract": "Spectral Graph Neural Networks have demonstrated superior performance in graph representation learning. However, many current methods focus on employing shared polynomial coefficients for all nodes, i.e., learning node-unified filters, which limits the filters' flexibility for node-level tasks. The recent DSF attempts to overcome this limitation by learning node-wise coefficients based on positional encoding. However, the initialization and updating process of the positional encoding are burdensome, hindering scalability on large-scale graphs. In this work, we propose a scalable node-wise filter, PolyAttn. Leveraging the attention mechanism, PolyAttn can directly learn node-wise filters in an efficient manner, offering powerful representation capabilities. Building on PolyAttn, we introduce the whole model, named PolyFormer. In the lens of Graph Transformer models, PolyFormer, which calculates attention scores within nodes, shows great scalability. Moreover, the model captures spectral information, enhancing expressiveness while maintaining efficiency. With these advantages, PolyFormer offers a desirable balance between scalability and expressiveness for node-level tasks. Extensive experiments demonstrate that our proposed methods excel at learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs containing up to 100 million nodes. The code is available at https://github.com/air029/PolyFormer.",
    "url": "http://arxiv.org/abs/2407.14459v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "88e69774-baaf-490f-9661-b01383abc03b": {
    "pk": "88e69774-baaf-490f-9661-b01383abc03b",
    "project_name": null,
    "authors": [
      "Yasushi Yoneta"
    ],
    "title": "Thermal pure states for systems with antiunitary symmetries and their tensor network representations",
    "abstract": "Thermal pure state algorithms, which employ pure quantum states representing thermal equilibrium states instead of statistical ensembles, are useful both for numerical simulations and for theoretical analysis of thermal states. However, their inherently large entanglement makes it difficult to represent efficiently and limits their use in analyzing large systems. Here, we propose a new algorithm for constructing thermal pure states for systems with certain antiunitary symmetries, such as time-reversal or complex conjugation symmetry. Our method utilizes thermal pure states that, while exhibiting volume-law entanglement, can be mapped to tensor network states through simple transformations. Furthermore, our approach does not rely on random sampling and thus avoids statistical uncertainty. Moreover, we can compute not only thermal expectation values of local observables but also thermodynamic functions. We demonstrate the validity and utility of our method by applying it to the one-dimensional XY model and the two-dimensional Ising model on a triangular lattice. Our results suggest a new class of variational wave functions for volume-law states that are not limited to thermal equilibrium states.",
    "url": "http://arxiv.org/abs/2407.14454v1",
    "timestamp": 1721361600,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cond-mat.stat-mech",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "4031b485-9f0f-4c84-a4a5-050e655a73cc": {
    "pk": "4031b485-9f0f-4c84-a4a5-050e655a73cc",
    "project_name": null,
    "authors": [
      "Joshua Robinson",
      "Rishabh Ranjan",
      "Weihua Hu",
      "Kexin Huang",
      "Jiaqi Han",
      "Alejandro Dobles",
      "Matthias Fey",
      "Jan E. Lenssen",
      "Yiwen Yuan",
      "Zecheng Zhang",
      "Xinwei He",
      "Jure Leskovec"
    ],
    "title": "RelBench: A Benchmark for Deep Learning on Relational Databases",
    "abstract": "We present RelBench, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RelBench provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RelBench to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RelBench.",
    "url": "http://arxiv.org/abs/2407.20060v1",
    "timestamp": 1722264373,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nRelational databases are the most widely used database management system, underpinning much of the digital economy. Their popularity stems from their table storage structure, making maintenance relatively easy, and data simple to access using powerful query languages such as SQL. Because of their popularity, AI systems across a wide variety of domains are built using data stored in relational databases, including e-commerce, social media, banking systems, healthcare, manufacturing, and open-source scientific repositories\u00a0(Johnson et\u00a0al., 2016; PubMed, 1996).\n\n\nDespite the importance of relational databases, the rich relational information is typically foregone, as no model architecture is capable of handling varied database structures. Instead, data is \u201cflattened\u201d into a simpler format such as a single table, often by manual feature engineering, on which standard tabular models can be used\u00a0(Kaggle, 2022). This results in a significant loss in predictive signal, and creates a need for data extraction pipelines that frequently cause bugs and add to software complexity.\n\n\nFigure 1: RelBench enables training and evaluation of deep learning models on relational databases. RelBench supports framework agnostic data loading, task specification, standardized data splitting, standardized evaluation metrics, and a leaderboard for tracking progress. RelBench also includes a pilot implementation of the relational deep learning blueprint of Fey et\u00a0al. (2024).\n\n\nTo fully exploit the predictive signal encoded in the relations between entities, a new proposal is to re-cast relational data as an exact graph representation, with a node for each entity in the database, edges indicating primary-foreign key links, and node features extracted using deep tabular models, an approach termed Relational Deep Learning (RDL)\u00a0(Fey et\u00a0al., 2024). The graph representation allows Graph Neural Networks (GNNs)\u00a0(Gilmer et\u00a0al., 2017; Hamilton et\u00a0al., 2017) to be used as predictive models. RDL is the first approach for an end-to-end learnable neural network model with access to all possible predictive signal in a relational databases, and has the potential to unlock new levels of predictive power.\nHowever, the development of relational deep learning is limited by a complete lack of infrastructure to support research, including: (i) standardized benchmark databases and tasks to compare methods, (ii) initial implementation of RDL, including converting data to graph form and GNN training, and (iii) a pilot study of the effectiveness of relational deep learning.\n\n\nHere we present RelBench, the first benchmark for relational deep learning. RelBench is intended to be the foundational infrastructure for future research into relational deep learning, providing a comprehensive set of databases across a variety of domains, including e-commerce, Q&A platforms, medical, and sports databases. RelBench databases span orders of magnitude in size, from 74K entities to 41M entities, and have very different time spans, between 2 weeks and 55 years of training data. They also vary significantly in their relational structure, with the total number of tables varying between 3 and 15, and total number of columns varying from 15 to 140.\nEach database comes with multiple predictive tasks, 30 in total, including entity classification/regression and recommendation tasks, each chosen for their real-world significance.\n\n\nIn addition to databases and tasks, we release open-source software designed to make relational deep learning widely available. This includes (i) the RelBench Python package for easy database and task loading, (ii) the first open-source implementation of relational deep learning, designed to be easily modified by researchers, and (iii) a public leaderboard for tracking progress. We comprehensively benchmark our initial RDL implementation on all RelBench tasks, comparing to various baselines.\n\n\nThe most important baseline we compare to is a strong \u201cdata scientist\u201d approach, for which we recruited an experienced individual to solve each task by manually engineering features and feeding them into tabular models. This approach is the current gold-standard for building predictive models on relational databases. The study, which we open source for reproducibility, finds that RDL models match or outperform the data scientist\u2019s models in accuracy, whilst reducing human hours worked by 96%percent9696\\%96 %, and lines of code by 94%percent9494\\%94 % on average. This cons14titutes the first empirical demonstration of the central promise of RDL, and points to\na long-awaited end-to-end deep learning solution for relational data.\n\n\nOur website111https://relbench.stanford.edu. is a comprehensive entry point to RDL, describing RelBench databases and tasks, access to code on GitHub, the full relational deep learning blueprint, and tutorials for adding new databases and tasks to RelBench to allow researchers to experiment with their problems of interest.\n\n",
      "2 Overview and Design": "\n\n2 Overview and Design\n\nRelBench provides a collection of diverse real-world relational databases along with a set of realistic predictive tasks associated with each database. Concretely, we provide:\n\n\n\n\n\u2022\n\nRelational databases, consisting of a set of tables connected via primary-foreign key relationships. Each table has columns storing diverse information about each entity. Some tables also come with time columns, indicating the time at which the entity is created (e.g., transaction date).\n\n\n\n\u2022\n\nPredictive tasks over a relational database, which are defined by a training table\u00a0(Fey et\u00a0al., 2024) with columns for Entity ID, seed time, and target labels.The seed time indicates at which time the target is to be predicted, filtering future data.\n\n\n\n\n\nNext we outline key design principles of RelBench with an emphasis on data curation, data splits, research flexibility, and open-source implementation.\n\n\nData Curation. Relational databases are widespread, so there are many candidate predictive tasks.\nFor the purpose of benchmarking we carefully curate a collection of relational databases and tasks chosen for their rich relational structure and column features. We also adopt the following principles:\n\n\n\n\n\u2022\n\nDiverse domains: To ensure algorithms developed on RelBench will be useful across a wide range of application domains, we select real-world relational databases from diverse domains.\n\n\n\n\u2022\n\nDiverse task types: Tasks cover a wide range of real-world use-cases, including three representative task types: entity classification, entity regression, and recommendation.\n\n\n\n\n\nTable 1: Statistics of RelBench datasets. Datasets vary significantly in the number of tables, total number of rows, and number of columns. In this table, we only count rows available for test inference, i.e., rows upto the test time cutoff.\n\n\n\nName\nDomain\n#Tasks\nTables\nTimestamp (year-mon-day)\n\n\n#Tables\n#Rows\n#Cols\nStart\nVal\nTest\n\n\nrel-amazon\nE-commerce\n7\n3\n15,000,713\n15\n2008-01-01\n2015-10-01\n2016-01-01\n\n\nrel-avito\nE-commerce\n4\n8\n20,679,117\n42\n2015-04-25\n2015-05-08\n2015-05-14\n\n\nrel-event\nSocial\n3\n5\n41,328,337\n128\n1912-01-01\n2012-11-21\n2012-11-29\n\n\nrel-f1\nSports\n3\n9\n74,063\n67\n1950-05-13\n2005-01-01\n2010-01-01\n\n\nrel-hm\nE-commerce\n3\n3\n16,664,809\n37\n2019-09-07\n2020-09-07\n2020-09-14\n\n\nrel-stack\nSocial\n5\n7\n4,247,264\n52\n2009-02-02\n2020-10-01\n2021-01-01\n\n\nrel-trial\nMedical\n5\n15\n5,434,924\n140\n2000-01-01\n2020-01-01\n2021-01-01\n\n\nTotal\n30\n51\n103,466,370\n489\n/\n/\n/\n\n\n\n\nRelBench databases are summarized in Table\u00a01, covering E-commerce, social, medical, and sports domains. The databases vary significantly in the numbers of rows (i.e., data scale) the number of columns and tables, as well as the time ranges of the databases. Tasks are summarized in Table\u00a02, each corresponding to a predictive problem of practical interest such as predicting customer churn, predicting the number of adverse events in a clinical trial, and recommending posts to users.\n\n\nData Splits. Data is split temporally, with models trained on rows up to val_timestamp, validated on the rows between val_timestamp and test_timestamp, and tested on the rows after test_timestamp.\nOur implementation carefully hides data after test_timestamp during inference to systematically avoid test time data leakage\u00a0(Kapoor and Narayanan, 2023), and uses an elegant solution proposed by Fey et\u00a0al. (2024) to avoid time leakage during training and validation through temporal neighbor sampling. In general, it is the designers responsibility to avoid time leakage. We recommend using our carefully tested implementation where possible.\n\n\nResearch Flexibility. RelBench is designed to allow significant freedom in future research directions. For example, RelBench tasks share the same (val_timestamp and test_timestamp) splits across tasks within the same relational database. This opens up exciting opportunities for multi-task learning and pre-training to simultaneously improve different predictive tasks within the same relational database. We also expose the logic for converting databases into graphs. This allows future work to consider modified graph constructions, or creative uses of the raw data.\n\n\nOpen-source RDL Implementation.\nAs well as datasets and tasks, we provide the first open-source implementation of relational deep learning. See Figure 2 of Fey et\u00a0al. (2024) for a high-level overview. A neural network is learned over a heterogeneous temporal graph that exactly represents the database in order to make prediction over nodes (for entity classification and regression) and links (for recommendation). Our implementation is built on top of PyTorch Frame\u00a0(Hu et\u00a0al., 2024) for extracting initial node embeddings from raw table features, and PyTorch Geometric\u00a0(Fey and Lenssen, 2019) for GNN modeling. See Section 3 for further details.\n\n\nThe rest of the paper is organized as follows. Section\u00a04 describes the RelBench relational databases.\nSection\u00a05 introduces predictive tasks for each RelBench databases covering the three task types in Sections\u00a05.1, 5.2, and 5.3, respectively.\nSection\u00a05 also extensively benchmarks our RDL implementation against challenging baselines. Most importantly, we compare to a strong \u201cdata scientist\u201d baseline (Section 6), finding that end-to-end RDL models outperform manual feature engineering, the current gold-standard\nFinally, Section\u00a07 discusses related work and Section\u00a08 draws final conclusions.\n\n",
      "3 Relational Deep Learning Implementation": "\n\n3 Relational Deep Learning Implementation\n\nAs part of RelBench, we provide an initial implementation of relational deep learning, based on the blueprint of\nFey et\u00a0al. (2024).222Code available at: https://github.com/snap-stanford/relbench. Our implementation consists four major components: (1) heterogeneous temporal graph, (2) deep learning model, (3) temporal-aware training of the model, and (4) task-specific loss, which we briefly discuss now.\n\n\nHeterogeneous temporal graph.\nGiven a set of tables with primary-foreigh key relations between them we follow Fey et\u00a0al. (2024) to automatically construct a heterogeneous temporal graph, where each table represents a node type, each row in a table represents a node, and a primary-foreign-key relation between two table rows (nodes) represent an edge between the respective nodes. Some node types are associated with time attributes, representing the timestamp at which a node appears. The heterogeneous temporal graph is represented as a PyTorch Geometric graph object.\nEach node in the heterogeneous graph comes with a rich feature derived from diverse columns of the corresponding table. We use Tensor Frame provided by PyTorch Frame\u00a0(Hu et\u00a0al., 2024) to represent rich node features with diverse column types, e.g., numerical, categorical, timestamp, and text.\n\n\nDeep learning model.\nFirst, we use deep tabular models that encode raw row-level data into initial node embeddings using PyTorch Frame (Hu et\u00a0al., 2024) (specifically, we use the ResNet tabular model\u00a0(Gorishniy et\u00a0al., 2021)). These initial node embeddings are then fed into a GNN to iteratively update the node embeddings based on their neighbors.\nFor the GNN we use the heterogeneous version of the GraphSAGE model\u00a0(Hamilton et\u00a0al., 2017; Fey and Lenssen, 2019) with sum-based neighbor aggregation. Output node embeddings are fed into task-specific prediction heads and are learned end-to-end.\n\n\nTemporal-aware subgraph sampling.\nWe perform temporal neighbor sampling, which samples\na subgraph around each entity node at a given seed time.\nSeed time is the time in history at which the prediction is made. When collecting the information to make a prediction at a given seed time, it is important for the model to only use information from before the seed time and thus not learn from the future (post the seed time). Crucially, when sampling mini-batch subgraphs we make sure that all nodes within the sampled subgraph appear before the seed time\u00a0(Hamilton et\u00a0al., 2017; Fey et\u00a0al., 2024), which systematically avoids time leakage during training.\nThe sampled subgraph is fed as input to the GNN, and trained to predict the target label.\n\n\nTask-specific prediction head and loss.\nFor entity-level classification, we simply apply an MLP on an entity embedding computed by our GNN to make prediction. For the loss function, we use the binary cross entropy loss for entity classification and L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT loss for entity regression.\n\n\nRecommendation requires computing scores between pairs of source nodes and target nodes.\nFor this task type, we consider two representative predictive architectures: two-tower GNN\u00a0(Wang et\u00a0al., 2019) and identity-aware GNN (ID-GNN)\u00a0(You et\u00a0al., 2021). First, the two-tower GNN computes the pairwise scores via inner product between source and target node embeddings, and the standard Bayesian Personalized Ranking loss\u00a0(Rendle et\u00a0al., 2012) is used to train the two-tower model\u00a0(Wang et\u00a0al., 2019).\nSecond, the ID-GNN computes the pairwise scores by applying an MLP prediction head on target entity embeddings computed by GNN for each source entity. The ID-GNN is trained by the standard binary cross entropy loss.\n\n",
      "4 RelBench Datasets": "\n\n4 RelBench Datasets\n\nFigure 2: Example RelBench schema for rel-trial. RelBench databases have complex relational structure and rich column features. \n\n\nRelBench contains 7 datasets each with rich relational structure, providing a challenging environment for developing and comparing relational deep learning methods (see Figure 2 for an example). The datasets are carefully processed from real-world relational databases and span diverse domains and sizes.\nEach database is associated with multiple individual predictive tasks defined in Section\u00a05.\nDetailed statistics of each dataset can be found in Table\u00a01. We briefly describe each dataset.\n\n\nrel-amazon. The Amazon E-commerce database records products, users, and reviews across Amazon\u2019s E-commerce platform. It contains rich information about products and reviews. Products include the price and category of each, reviews have the overall rating, whether the user has actually bought the product, and the text of the review itself. We use the subset of book-related products.\n\n\nrel-f1. The F1 database tracks all-time Formula 1 racing data and statistics since 1950. It provides detailed information for various stakeholders including drivers, constructors, engine manufacturers, and tyre manufacturers. Highlights include data on all circuits (e.g.geographical details), and full historical data from every season. This includes overall standings, race results, and more specific data like practice sessions, qualifying positions, sprints, and pit stops.\n\n\nrel-stack. Stack Exchange is a network of question-and-answer websites on different topics, where questions, answers, and users are subject to a reputation award process. The reputation system allows the sites to be self-moderating. The database includes detailed records of activity including user biographies, posts and comments (with raw text), edit histories, voting, and related posts. In our benchmark, we use the stats-exchange site.\n\n\nTable 2: Full list of predictive tasks for each RelBench dataset (introduced in Table\u00a01). \n\n\nDataset\nTask name\nTask type\n#Rows of training table\n#Unique\n%train/test\n#Dst\n\n\nTrain\nValidation\nTest\nEntities\nEntity Overlap\nEntities\n\n\nrel-amazon\nuser-churn\nentity-cls\n4,732,555\n409,792\n351,885\n1,585,983\n88.0\n\u2014\n\n\nitem-churn\nentity-cls\n2,559,264\n177,689\n166,842\n416,352\n93.1\n\u2014\n\n\nuser-ltv\nentity-reg\n4,732,555\n409,792\n351,885\n1,585,983\n88.0\n\u2014\n\n\nitem-ltv\nentity-reg\n2,707,679\n166,978\n178,334\n427,537\n93.5\n\u2014\n\n\nuser-item-purchase\nrecommendation\n5,112,803\n351,876\n393,985\n1,632,909\n87.4\n12,562,384\n\n\nuser-item-rate\nrecommendation\n3,667,157\n257,939\n292,609\n1,481,360\n81.0\n7,665,611\n\n\n\nuser-item-review\nrecommendation\n2,324,177\n116,970\n127,021\n894,136\n74.1\n5,406,835\n\n\nrel-avito\nad-ctr\nentity-reg\n5,100\n1,766\n1,816\n4,997\n59.8\n\u2014\n\n\nuser-clicks\nentity-cls\n59,454\n21,183\n47,996\n66,449\n45.3\n\u2014\n\n\nuser-visits\nentity-cls\n86,619\n29,979\n36,129\n63,405\n64.6\n\u2014\n\n\nuser-ad-visit\nrecommendation\n86,616\n29,979\n36,129\n63,402\n64.6\n3,616,174\n\n\nrel-event\nuser-attendance\nentity-reg\n19,261\n2,014\n2,006\n9,694\n14.6\n\u2014\n\n\nuser-repeat\nentity-cls\n3,842\n268\n246\n1,514\n11.5\n\u2014\n\n\nuser-ignore\nentity-cls\n19,239\n4,185\n4,010\n9,799\n21.1\n\u2014\n\n\nrel-f1\ndriver-dnf\nentity-cls\n11,411\n566\n702\n821\n50.0\n\u2014\n\n\ndriver-top3\nentity-cls\n1,353\n588\n726\n134\n50.0\n\u2014\n\n\ndriver-position\nentity-reg\n7,453\n499\n760\n826\n44.6\n\u2014\n\n\nrel-hm\nuser-churn\nentity-cls\n3,871,410\n76,556\n74,575\n1,002,984\n89.7\n\u2014\n\n\nitem-sales\nentity-reg\n5,488,184\n105,542\n105,542\n105,542\n100.0\n\u2014\n\n\nuser-item-purchase\nrecommendation\n3,878,451\n74,575\n67,144\n1,004,046\n89.2\n13,428,473\n\n\nrel-stack\nuser-engagement\nentity-cls\n1,360,850\n85,838\n88,137\n88,137\n97.4\n\u2014\n\n\nuser-badge\nentity-cls\n3,386,276\n247,398\n255,360\n255,360\n96.9\n\u2014\n\n\npost-votes\nentity-reg\n2,453,921\n156,216\n160,903\n160,903\n97.1\n\u2014\n\n\nuser-post-comment\nrecommendation\n21,239\n825\n758\n11,453\n59.9\n44,940\n\n\npost-post-related\nrecommendation\n5,855\n226\n258\n5,924\n8.5\n7,456\n\n\nrel-trial\nstudy-outcome\nentity-cls\n11,994\n960\n825\n13,779\n0.0\n\u2014\n\n\nstudy-adverse\nentity-reg\n43,335\n3,596\n3,098\n50,029\n0.0\n\u2014\n\n\nsite-success\nentity-reg\n151,407\n19,740\n22,617\n129,542\n42.0\n\u2014\n\n\ncondition-sponsor-run\nrecommendation\n36,934\n2,081\n2,057\n3,956\n98.4\n533,624\n\n\nsite-sponsor-run\nrecommendation\n669,310\n37,003\n27,428\n445,513\n48.3\n1,565,463\n\n\n\n\nrel-trial. The clinical trial database is curated from AACT initiative, which consolidates all protocol and results data from studies registered on ClinicalTrials.gov. It offers extensive information about clinical trials, including study designs, participant demographics, intervention details, and outcomes. It is an important resource for health research, policy making, and therapeutic development.\n\n\nrel-hm. The H&M relational database hosts extensive customer and product data for online shopping experiences across its extensive network of brands and stores. This database includes detailed customer purchase histories and a rich set of metadata, encompassing everything from basic demographic information to extensive details about each product available.\n\n\nrel-event. The Event Recommendation database is obtained from user data on a mobile app called Hangtime. This app allows users to keep track of their friends\u2019 social plans. The database contains data on user actions, event metadata, and demographic information, as well as users\u2019 social relations, which captures how social relations can affect user behavior. Data is fully anonymized, with no personally identifiable information (such as names or aliases) available.\n\n\nrel-avito. Avito is a leading online advertisement platform, providing a marketplace for users to buy and sell a wide variety of products and services, including real estate, vehicles, jobs, and goods. The Avito Context Ad Clicks dataset on Kaggle is part of a competition aimed at predicting whether an ad will be clicked based on contextual information. This dataset includes user searches, ad attributes, and other related data to help build predictive models.\n\n\nData Provenance. All data is sourced from publicly available repositories with licenses permitting usage for research purposes. See Appendix D for details of data sources, licenses, and more.\n\n",
      "5 Predictive Tasks on RelBench Datasets": "\n\n5 Predictive Tasks on RelBench Datasets\n\nRelBench introduces 30 new predictive tasks defined over the databases introduced in Section\u00a04.\nA full list of tasks is given in Table\u00a02, with high-level descriptions given in Appendix A (and our website) due to space limitations.\nTasks are grouped into three task types: entity classification (Section\u00a05.1), entity regression (Section\u00a05.2), and entity link prediction (Section\u00a05.3). Tasks differ significantly in the number of train/val/test entities, number of unique entities (the same entity may appear multiple times at different timestamps), and the proportion of test entities seen during training. Note this is not data leakage, since entity predictions are timestamp dependent, and can change over time. Tasks with no overlap are pure inductive tasks, whilst other tasks are (partially) transductive.\n\n\n\n5.1 Entity Classification\n\nTable 3: Entity classification results (AUROC, higher is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nThe first task type is entity-level classification. The task is to predict binary labels of a given entity at a given seed time. We use the ROC-AUC\u00a0(Hanley and McNeil, 1983) metric for evaluation (higher is better). We compare to a LightGBM classifier baseline over the raw entity table features. Note that here only information from the single entity table is used.\n\n\n\n\nExperimental results.\n\nResults are given in Table 5.1, with RDL outperforming or matching baselines in all cases. Notably, LightGBM achieves similar performance to RDL on the study-outcome task from rel-trial. This task has extremely rich features in the target table (28 columns total), giving the LightGBM many potentially useful features even without feature engineering. It is an interesting research question how to design RDL models better able to extract these features and unify them with cross-table information in order to outperform the LightGBM model on this dataset.\n\n\n\n5.2 Entity Regression\n\nEntity-level regression tasks involve predicting numerical labels of an entity at a given seed time. We use Mean Absolute Error (MAE) as our metric (lower is better). We consider the following baselines:\n\n\n\u2022\n\nEntity mean/median calculates the mean/median label value for each entity in training data and predicts the mean/median value for the entity.\n\n\n\n\u2022\n\nGlobal mean/median calculates the global mean/median label value over the training data and predicts the same mean/median value across all entities.\n\n\n\n\u2022\n\nGlobal zero predicts zero for all entities.\n\n\n\n\u2022\n\nLightGBM learns a LightGBM\u00a0(Ke et\u00a0al., 2017) regressor over the raw entity features to predict the numerical targets. Note that only information from the single entity table is used.\n\n\n\n\n\nExperimental results. Results in Table 5.2 show our RDL implementation outperforms or matches baselines in all cases. A number of tasks, such as driver-position and study-adverse, have matching performance up to statistical significance, suggesting some room for improvement. We analyze this further in Appendix C, identifying one potential cause, suggesting an opportunity for improved performance for regression tasks.\n\n\nTable 4: Entity regression results (MAE, lower is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3 Recommendation\n\nFinally, we also introduce recommendation tasks on pairs of entities. The task is to predict a list of top K\ud835\udc3eKitalic_K target entities given a source entity at a given seed time.\nThe metric we use is Mean Average Precision (MAP) @K\ud835\udc3eKitalic_K, where K\ud835\udc3eKitalic_K is set per task (higher is better). We consider the following baselines:\n\n\n\u2022\n\nGlobal popularity computes the top K\ud835\udc3eKitalic_K most popular target entities (by count) across the entire training table and predict the K\ud835\udc3eKitalic_K globally popular target entities across all source entities.\n\n\n\n\u2022\n\nPast visit computes the top K\ud835\udc3eKitalic_K most visited target entities for each source entity within the training table and predict those past-visited target entities for each entity.\n\n\n\n\u2022\n\nLightGBM learns a LightGBM\u00a0(Ke et\u00a0al., 2017) classifier over the raw features of the source and target entities (concatenated) to predict the link. Additionally, global popularity and past visit ranks are also provided as inputs.\n\n\n\n\n\nFor recommendation, it is also important to ensure a certain density of links in the training data in order for there to be sufficient predictive signal. In Appendix A we report statistics on the average number of destination entities each source entity links to. For most tasks the density is \u22651absent1\\geq 1\u2265 1, with the exception of rel-stack which is more sparse, but is included to test in extreme sparse settings.\n\n\nExperimental results. Results are given in Table 5.3. We find that either the RDL implementation using GraphSAGE (Hamilton et\u00a0al., 2017), or ID-GNN (You et\u00a0al., 2021) as the GNN component performs best, often by a very significant margin. ID-GNN excels in cases were predictions are entity-specific (i.e., Past Visit baseline outperforms Global Popularity), whilst the plain GNN excels in the reverse case. This reflects the inductive biases of each model, with GraphSAGE being able to learn structural features, and ID-GNN able to take into account the specific node ID.\n\n\nTable 5: Recommendation results (MAP, higher is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 Expert Data Scientist User Study\n\nTo test RDL in the most challenging circumstances possible, we undertake a human trial wherein a data scientist solves each task by manually designing features and feeds them into tabular methods such at LightGBM or XGBoost\u00a0(Chen and Guestrin, 2016; Ke et\u00a0al., 2017). This represents the prior gold-standard for building predictive models on relational databases (Heaton, 2016), and the key point of comparison for RDL.\n\n\nWe structure our user study along the five main data science workflow steps:\n\n\n1.\n\nExploratory data analysis (EDA): Explore the dataset and task to understand its characteristics, including what column features there are, and if there is any missing data.\n\n\n\n2.\n\nFeature ideation: Based on EDA and intuition from prior experiences, propose a set of entity-level features that the data scientist believes may contain predictive signal for the task.\n\n\n\n3.\n\nFeature enginnering: Using query languages such as SQL to compute the proposed features, and add them as extra columns to the target table of interest.\n\n\n\n4.\n\nTabular ML: Run tabular methods such as LightGBM or XGBoost on the table with extra features to produce a predictive model, and record the test performance.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (Optional): Common tools include SHAP and LIME, which aim to explain the contribution of each input feature to the final performance.\n\n\n\n\n\nConsider for example the rel-hm dataset (schema in Appendix D) and the task of predicting customer churn. Here the customer table only contains simple biographical information such as username and joining date. To capture more predictive information, additional features, such as time since last purchase, can be computed using the other tables, and added to the customer table. We give a detailed walk-through of the data scientist\u2019s work process for solving this specific task in Appendix C. We strongly encourage the interested reader to review this, as it highlights the significant amount of task-specific effort that this workflow necessitates.\n\n\nLimitations of Manual Feature Engineering. This workflow suffers from several fundamental limitations. Most obviously, since features are hand designed they only capture part of the predictive signal in the database, useful signal is easily missed. Additionally, feature complexity is limited by human reasoning abilities, meaning that higher-order interactions between entities are often overlooked. Beyond predictive signal, the other crucial limitation of feature engineering is its extremely manual nature\u2014every time a new model is built a data scientist has to repeat this process, requiring many hours of human labor, and significant quantities of new SQL code to design features (Zheng and Casari, 2018). Our RDL models avoid these limitations (see Section 6.1).\n\n\nData Scientist. To conduct a thorough comparison to this process, we recruit a high-end data scientist with Stanford CS MSc degree, 4.0 GPA, and 5 years of experience of building machine learning models in the financial industry. This experience includes a significant amount of time building machine learning models in exactly above five steps, as well as broader data science expertise.\n\n\nUser Study Protocol.\n\n\nFigure 3: RDL vs. Data Scientist. Relational Deep Learning matches or outperforms the data scientist in 11 of 15 tasks. Left shows entity classification AUROC, right shows entity regression, reporting MAE normalized so that the RDL MAE is always 1.\n\n\n\nBecause of the open-ended nature of feature engineering and model development, we follow a specific protocol for the user study in order to standardize the amount of effort dedicated to each dataset and task. Tracking the 5 steps outlined above, we impose the following rules:\n\n\n\n\n1.\n\nEDA: The time allotted for data exploration is capped at 4 hours. This threshold was chosen to give the data scientist enough time to familiarize themselves with the schema, visualize key relationships and distributions, and take stock of any outliers in the dataset, while providing a reasonable limit to the effort applied.\n\n\n\n2.\n\nFeature ideation: Feature ideation is performed manually with pen and paper, and is limited to 1 hour. In practice, the data scientist found that 1 hour was plenty of time to enumerate all promising features at that time, especially since many ideas naturally arise during the EDA process already.\n\n\n\n3.\n\nFeature engineering: The features described during the ideation phase are then computed using SQL queries. The time taken to write SQL code to generate the features is unconstrained in order to eliminate code writing speed as a factor in the study. We do, however, record code writing time for our timing benchmarking. This stage presented the most variability in terms of time commitment, partly because it is unconstrained, but mostly because the implementation complexity of the features itself is highly variable.\n\n\n\n4.\n\nTabular ML: For tabular ML training, we provide a standardized LightGBM training script including comprehensive hyperparameter tuning. The data scientist needs only to feed the table full of engineered features into this training script, which returns test performance results. However, there is some non-trivial amount of work required to transform the output of the SQL queries from the previous section into the Python objects (arrays) required for training LightGBM. Again, the time taken for this additional pre-preocessing is recorded.\n\n\n\n5.\n\nPost-hoc analysis of feature importance: Finally, after successfully training a model, an evaluation of model predictions and feature importance is carried out. This mostly serves as a general sanity check and an interesting corollary of the data scientist\u2019s work that provides task-specific insights (see Appendix C). In practice, this took no more than a few minutes per task and this time was not counted toward the total time commitment.\n\n\n\n\n\nReproducibility. All of the data scientist\u2019s workings are released333See https://github.com/snap-stanford/relbench-user-study. to ensure reproducibility and demonstrate the significant lengths gone through to build as accurate models as possible. In Appendix C we walk through a complete example for a single dataset and task, showing the data-centric insights it yields. An important by-product is a close analysis of which features contribute to model performance, which we believe will help inspire future well-motivated RDL research directions.\n\n\nFigure 4: RDL vs. Data Scientist. Relational Deep Learning reduces the hours of human work required to solve a new task by 96% on average (from 12.3 to 0.5 hours). Left shows node-level classification, right shows node-level regression.\n\n\n\n\n6.1 Results\n\nAs well as (i) raw predictive power, we compare the data scientist to our RDL models in terms of (ii) hours of human work, and (iii) number of new lines of code required to solve each task. We measure the marginal effort, meaning that we do not include code infrastructure that is reused across tasks, including for example data loading logic and training scripts for RDL or LightGBM models.\n\n\nSummary. Figures 3, 4, and 5 show that RDL learns highly predictive models, outperforming the data scientist in 11 of 15 tasks, whilst reducing hours worked by 96%percent9696\\%96 % on average, and lines of code by 94%percent9494\\%94 % on average. On average, it took the data scientist 12.3 hours to solve each task using traditional feature engineering. By contrast it takes roughly 30 minutes to solve a task with RDL.\n\n\nThis observation is the central value proposition of relational deep learning, pointing the way to unlocking new levels of predictive power, and potentially a new economic model for solving predictive tasks on relational databases. Replacing hand-crafted solutions with end-to-end learnable models has been a key takeaway from the last 15 years of AI research. It is therefore remarkable how little impact deep learning has had on ML on relational databases, one of the most widespread applied ML use cases. To the best of our knowledge, RDL represents the first proposal for a deep learning approach for relational databases that has demonstrated efficacy compared with established data science workflows.\n\n\nWe highlight that all RelBench tasks were solved with a single set of default hyperparameters (with 2 exceptions requiring small modifications to learning rate, number of epochs, and GNN aggregation function). This demonstrates the robustness of RDL, and that the performance of RDL in Figure 3 is not due to extensive hyperparamter search. Indeed, the single set of RDL hyperparameters is compared to a carefully tuned LightGBM, which was allowed to search over 10 sets of hyperparameters.\n\n\nPredictive Power. Results shown in Figures 3. Whilst outperforming the data scientist in 11 of 15 tasks, we note that RDL best outperforms the data scientist on classification tasks, struggling more on regression. Indeed it was necessary for us to apply a \u201cboosting\u201d to the RDL model to improve performance (see Appendix C for details). Even with boosting, the data scientist model outperforms RDL in several cases. One cause we identify is that the MLP output head of the GNN is poorly suited to regression tasks (see Appendix C for our analysis). This suggests an opportunity for improved output heads for regression tasks. We stress that our RDL implementation is an initial demonstration. We believe there is significant scope for new research leading to large improvements in performance. In particular, ideas from graph ML, deep tabular ML, and time-series modeling are well suited to advance RDL.\n\n\nFigure 5: RDL vs. Data Scientist. Relational Deep Learning reduces the new lines of code needed to solve a new task by 94%. Left shows entity classification, right shows entity regression.\n\n\n\nHuman Work. Results shown in Figure 4. In our user study RDL required 96% less hours work to solve a new task, compared to the data scientist work flow. The RDL solutions always took less than an hour to write, whilst the data scientist took 12121212 hours on average, with a standard deviation of 1.61.61.61.6 hours. We emphasize that this measures marginal effort, i.e., it does not include reusable code that can be amortized over many tasks. RDL compares favorably to data scientist because a large majority of RDL code is reusable for new tasks (a GNN architecture and training loop needs only to be defined once) whereas a large portion of the data scientist\u2019s code is task specific and must be re-done afresh for every new task that needs to be solved.\n\n\nLines of Code. Results shown in Figure 5. For the RDL model, the only new addition needed to solve a new task is the code describing how to compute the training supervision for the RDL, which is stored in the training table. This requires a similar number of lines of code for each task, with 56 lines of code on average, with standard deviation 8.88.88.88.8, with the data scientist requiring with 878\u00b177plus-or-minus87877878\\pm 77878 \u00b1 77. The minimum lines of code required by RDL is 44, compared to 734 for the data scientist, and maximum is 84 compared to 1039 for the data scientist. Examples of the RDL code required to solve rel-amazon tasks can be viewed here. For the data scientist pipeline, we record the number of lines of code for EDA and SQL files, and the manipulations needed to format data to be fed into the pre-prepared LightGBM script.\n\n\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "6 Expert Data Scientist User Study": "\n\n6 Expert Data Scientist User Study\n\nTo test RDL in the most challenging circumstances possible, we undertake a human trial wherein a data scientist solves each task by manually designing features and feeds them into tabular methods such at LightGBM or XGBoost\u00a0(Chen and Guestrin, 2016; Ke et\u00a0al., 2017). This represents the prior gold-standard for building predictive models on relational databases (Heaton, 2016), and the key point of comparison for RDL.\n\n\nWe structure our user study along the five main data science workflow steps:\n\n\n1.\n\nExploratory data analysis (EDA): Explore the dataset and task to understand its characteristics, including what column features there are, and if there is any missing data.\n\n\n\n2.\n\nFeature ideation: Based on EDA and intuition from prior experiences, propose a set of entity-level features that the data scientist believes may contain predictive signal for the task.\n\n\n\n3.\n\nFeature enginnering: Using query languages such as SQL to compute the proposed features, and add them as extra columns to the target table of interest.\n\n\n\n4.\n\nTabular ML: Run tabular methods such as LightGBM or XGBoost on the table with extra features to produce a predictive model, and record the test performance.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (Optional): Common tools include SHAP and LIME, which aim to explain the contribution of each input feature to the final performance.\n\n\n\n\n\nConsider for example the rel-hm dataset (schema in Appendix D) and the task of predicting customer churn. Here the customer table only contains simple biographical information such as username and joining date. To capture more predictive information, additional features, such as time since last purchase, can be computed using the other tables, and added to the customer table. We give a detailed walk-through of the data scientist\u2019s work process for solving this specific task in Appendix C. We strongly encourage the interested reader to review this, as it highlights the significant amount of task-specific effort that this workflow necessitates.\n\n\nLimitations of Manual Feature Engineering. This workflow suffers from several fundamental limitations. Most obviously, since features are hand designed they only capture part of the predictive signal in the database, useful signal is easily missed. Additionally, feature complexity is limited by human reasoning abilities, meaning that higher-order interactions between entities are often overlooked. Beyond predictive signal, the other crucial limitation of feature engineering is its extremely manual nature\u2014every time a new model is built a data scientist has to repeat this process, requiring many hours of human labor, and significant quantities of new SQL code to design features (Zheng and Casari, 2018). Our RDL models avoid these limitations (see Section 6.1).\n\n\nData Scientist. To conduct a thorough comparison to this process, we recruit a high-end data scientist with Stanford CS MSc degree, 4.0 GPA, and 5 years of experience of building machine learning models in the financial industry. This experience includes a significant amount of time building machine learning models in exactly above five steps, as well as broader data science expertise.\n\n\nUser Study Protocol.\n\n\nFigure 3: RDL vs. Data Scientist. Relational Deep Learning matches or outperforms the data scientist in 11 of 15 tasks. Left shows entity classification AUROC, right shows entity regression, reporting MAE normalized so that the RDL MAE is always 1.\n\n\n\nBecause of the open-ended nature of feature engineering and model development, we follow a specific protocol for the user study in order to standardize the amount of effort dedicated to each dataset and task. Tracking the 5 steps outlined above, we impose the following rules:\n\n\n\n\n1.\n\nEDA: The time allotted for data exploration is capped at 4 hours. This threshold was chosen to give the data scientist enough time to familiarize themselves with the schema, visualize key relationships and distributions, and take stock of any outliers in the dataset, while providing a reasonable limit to the effort applied.\n\n\n\n2.\n\nFeature ideation: Feature ideation is performed manually with pen and paper, and is limited to 1 hour. In practice, the data scientist found that 1 hour was plenty of time to enumerate all promising features at that time, especially since many ideas naturally arise during the EDA process already.\n\n\n\n3.\n\nFeature engineering: The features described during the ideation phase are then computed using SQL queries. The time taken to write SQL code to generate the features is unconstrained in order to eliminate code writing speed as a factor in the study. We do, however, record code writing time for our timing benchmarking. This stage presented the most variability in terms of time commitment, partly because it is unconstrained, but mostly because the implementation complexity of the features itself is highly variable.\n\n\n\n4.\n\nTabular ML: For tabular ML training, we provide a standardized LightGBM training script including comprehensive hyperparameter tuning. The data scientist needs only to feed the table full of engineered features into this training script, which returns test performance results. However, there is some non-trivial amount of work required to transform the output of the SQL queries from the previous section into the Python objects (arrays) required for training LightGBM. Again, the time taken for this additional pre-preocessing is recorded.\n\n\n\n5.\n\nPost-hoc analysis of feature importance: Finally, after successfully training a model, an evaluation of model predictions and feature importance is carried out. This mostly serves as a general sanity check and an interesting corollary of the data scientist\u2019s work that provides task-specific insights (see Appendix C). In practice, this took no more than a few minutes per task and this time was not counted toward the total time commitment.\n\n\n\n\n\nReproducibility. All of the data scientist\u2019s workings are released333See https://github.com/snap-stanford/relbench-user-study. to ensure reproducibility and demonstrate the significant lengths gone through to build as accurate models as possible. In Appendix C we walk through a complete example for a single dataset and task, showing the data-centric insights it yields. An important by-product is a close analysis of which features contribute to model performance, which we believe will help inspire future well-motivated RDL research directions.\n\n\nFigure 4: RDL vs. Data Scientist. Relational Deep Learning reduces the hours of human work required to solve a new task by 96% on average (from 12.3 to 0.5 hours). Left shows node-level classification, right shows node-level regression.\n\n\n\n\n6.1 Results\n\nAs well as (i) raw predictive power, we compare the data scientist to our RDL models in terms of (ii) hours of human work, and (iii) number of new lines of code required to solve each task. We measure the marginal effort, meaning that we do not include code infrastructure that is reused across tasks, including for example data loading logic and training scripts for RDL or LightGBM models.\n\n\nSummary. Figures 3, 4, and 5 show that RDL learns highly predictive models, outperforming the data scientist in 11 of 15 tasks, whilst reducing hours worked by 96%percent9696\\%96 % on average, and lines of code by 94%percent9494\\%94 % on average. On average, it took the data scientist 12.3 hours to solve each task using traditional feature engineering. By contrast it takes roughly 30 minutes to solve a task with RDL.\n\n\nThis observation is the central value proposition of relational deep learning, pointing the way to unlocking new levels of predictive power, and potentially a new economic model for solving predictive tasks on relational databases. Replacing hand-crafted solutions with end-to-end learnable models has been a key takeaway from the last 15 years of AI research. It is therefore remarkable how little impact deep learning has had on ML on relational databases, one of the most widespread applied ML use cases. To the best of our knowledge, RDL represents the first proposal for a deep learning approach for relational databases that has demonstrated efficacy compared with established data science workflows.\n\n\nWe highlight that all RelBench tasks were solved with a single set of default hyperparameters (with 2 exceptions requiring small modifications to learning rate, number of epochs, and GNN aggregation function). This demonstrates the robustness of RDL, and that the performance of RDL in Figure 3 is not due to extensive hyperparamter search. Indeed, the single set of RDL hyperparameters is compared to a carefully tuned LightGBM, which was allowed to search over 10 sets of hyperparameters.\n\n\nPredictive Power. Results shown in Figures 3. Whilst outperforming the data scientist in 11 of 15 tasks, we note that RDL best outperforms the data scientist on classification tasks, struggling more on regression. Indeed it was necessary for us to apply a \u201cboosting\u201d to the RDL model to improve performance (see Appendix C for details). Even with boosting, the data scientist model outperforms RDL in several cases. One cause we identify is that the MLP output head of the GNN is poorly suited to regression tasks (see Appendix C for our analysis). This suggests an opportunity for improved output heads for regression tasks. We stress that our RDL implementation is an initial demonstration. We believe there is significant scope for new research leading to large improvements in performance. In particular, ideas from graph ML, deep tabular ML, and time-series modeling are well suited to advance RDL.\n\n\nFigure 5: RDL vs. Data Scientist. Relational Deep Learning reduces the new lines of code needed to solve a new task by 94%. Left shows entity classification, right shows entity regression.\n\n\n\nHuman Work. Results shown in Figure 4. In our user study RDL required 96% less hours work to solve a new task, compared to the data scientist work flow. The RDL solutions always took less than an hour to write, whilst the data scientist took 12121212 hours on average, with a standard deviation of 1.61.61.61.6 hours. We emphasize that this measures marginal effort, i.e., it does not include reusable code that can be amortized over many tasks. RDL compares favorably to data scientist because a large majority of RDL code is reusable for new tasks (a GNN architecture and training loop needs only to be defined once) whereas a large portion of the data scientist\u2019s code is task specific and must be re-done afresh for every new task that needs to be solved.\n\n\nLines of Code. Results shown in Figure 5. For the RDL model, the only new addition needed to solve a new task is the code describing how to compute the training supervision for the RDL, which is stored in the training table. This requires a similar number of lines of code for each task, with 56 lines of code on average, with standard deviation 8.88.88.88.8, with the data scientist requiring with 878\u00b177plus-or-minus87877878\\pm 77878 \u00b1 77. The minimum lines of code required by RDL is 44, compared to 734 for the data scientist, and maximum is 84 compared to 1039 for the data scientist. Examples of the RDL code required to solve rel-amazon tasks can be viewed here. For the data scientist pipeline, we record the number of lines of code for EDA and SQL files, and the manipulations needed to format data to be fed into the pre-prepared LightGBM script.\n\n\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "7 Related Work": "\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "8 Conclusion": "\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Acknowledgments and Disclosure of Funding": "\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix A Additional Task Information": "\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n",
      "Appendix B Experiment Details and Additional Results": "\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix C User Study Additional Details": "\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix D Dataset Origins and Licenes": "\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix E Additional Training Table Statistics": "\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix F Dataset Schema": "\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n",
      "Appendix G Broader Impact": "\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n"
    },
    "table_captions": null,
    "figure_captions": null,
    "bibliography": {
      "Chen and Guestrin (2016)": "\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n",
      "Cvitkovic (2019)": "\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n",
      "Deng et\u00a0al. (2009)": "\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n",
      "Devlin et\u00a0al. (2018)": "\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n",
      "Fey and Lenssen (2019)": "\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n",
      "Fey et\u00a0al. (2024)": "\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n",
      "Gilmer et\u00a0al. (2017)": "\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n",
      "Gorishniy et\u00a0al. (2021)": "\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n",
      "Hamilton et\u00a0al. (2017)": "\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Hanley and McNeil (1983)": "\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n",
      "Heaton (2016)": "\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n",
      "Hu et\u00a0al. (2020)": "\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n",
      "Hu et\u00a0al. (2024)": "\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n",
      "Huang et\u00a0al. (2024)": "\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n",
      "Johnson et\u00a0al. (2016)": "\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n",
      "Kaggle (2022)": "\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n",
      "Kapoor and Narayanan (2023)": "\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n",
      "Ke et\u00a0al. (2017)": "\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n",
      "Krizhevsky et\u00a0al. (2017)": "\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n",
      "Lundberg and Lee (2017)": "\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n",
      "Morris et\u00a0al. (2020)": "\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n",
      "Ni et\u00a0al. (2019)": "\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n",
      "Pennington et\u00a0al. (2014)": "\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n",
      "PubMed (1996)": "\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n",
      "Rendle et\u00a0al. (2012)": "\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n",
      "Schlichtkrull et\u00a0al. (2018)": "\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n",
      "\u0160\u00edr (2021)": "\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n",
      "Wang et\u00a0al. (2019)": "\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n",
      "You et\u00a0al. (2021)": "\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n",
      "Zahradn\u00edk et\u00a0al. (2023)": "\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n",
      "Zheng and Casari (2018)": "\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n"
    },
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "f7e213b5-a8bb-4ef3-b799-ad98e76ac085": {
    "pk": "f7e213b5-a8bb-4ef3-b799-ad98e76ac085",
    "project_name": null,
    "authors": [
      "Matthias Fey",
      "Weihua Hu",
      "Kexin Huang",
      "Jan Eric Lenssen",
      "Rishabh Ranjan",
      "Joshua Robinson",
      "Rex Ying",
      "Jiaxuan You",
      "Jure Leskovec"
    ],
    "title": "Relational Deep Learning: Graph Representation Learning on Relational Databases",
    "abstract": "Much of the world's most valued data is stored in relational databases and data warehouses, where the data is organized into many tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming. The core problem is that no machine learning method is capable of learning on multiple tables interconnected by primary-foreign key relations. Current methods can only learn from a single table, so the data must first be manually joined and aggregated into a single training table, the process known as feature engineering. Feature engineering is slow, error prone and leads to suboptimal models. Here we introduce an end-to-end deep representation learning approach to directly learn on data laid out across multiple tables. We name our approach Relational Deep Learning (RDL). The core idea is to view relational databases as a temporal, heterogeneous graph, with a node for each row in each table, and edges specified by primary-foreign key links. Message Passing Graph Neural Networks can then automatically learn across the graph to extract representations that leverage all input data, without any manual feature engineering. Relational Deep Learning leads to more accurate models that can be built much faster. To facilitate research in this area, we develop RelBench, a set of benchmark datasets and an implementation of Relational Deep Learning. The data covers a wide spectrum, from discussions on Stack Exchange to book reviews on the Amazon Product Catalog. Overall, we define a new research area that generalizes graph machine learning and broadens its applicability to a wide set of AI use cases.",
    "url": "http://arxiv.org/abs/2312.04615v1",
    "timestamp": 1701975101,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe information age is driven by data stored in ever-growing relational databases and data warehouses\nthat have come to underpin nearly all technology stacks. Data warehouses typically store information in multiple tables, with entities/rows in different tables connected using primary-foreign key relations, and managed using powerful query languages such as SQL (Codd, 1970; Chamberlin and Boyce, 1974). For this reason, data warehouses underpin many of today\u2019s large information systems, including e-commerce, social media, banking systems, healthcare, manufacturing, and open-source scientific knowledge repositories\u00a0(Johnson et\u00a0al., 2016; PubMed, 1996).\n\n\nMany predictive problems over relational data have significant implications for human decision making. A hospital wants to predict the risk of discharging a patient; an e-commerce company wishes to forecast future sales of each of their products; a telecommunications provider wants to predict which customers will churn; and a music streaming platform must decide which songs to recommend to a user. Behind each of these tasks is a rich relational schema of relational tables, and many machine learning models are built using this data\u00a0(Kaggle, 2022).\n\n\nHowever, existing learning paradigms, notably tabular learning, cannot be directly applied to an interlinked set relational tables.\nInstead, a manual feature engineering step is first taken, where a data scientist uses domain knowledge to manually join and aggregate tables to generate many features in a regular single table format.\nTo illustrate this, consider a simple e-commerce schema (Fig. 1) of three tables: Customers, Transactions and Products, where Customers and Products tables link into the Transactions table via primary-foreign keys, and the task is to predict if a customer is going to churn (i.e., make zero transactions in the next k\ud835\udc58kitalic_k days). In this case, the data scientist would aggregate information from the Transactions table to make new features for the Customers table such as: \u201cnumber of purchases of a given customer in the last 30 days\u201d, \u201csum of purchase amounts of a given customer in the last 30 days\u201d, \u201cnumber of purchases on a Sunday\u201d, \u201csum of purchase amounts on a Sunday\u201d, \u201cnumber of purchases on a Monday\u201d, and so on. The computed customer features are then stored in a single table, ready for tabular machine learning. Another challenge is the temporal nature of the churn predictive tasks. As new transactions appear, the customer\u2019s churn label and the customer\u2019s features may change from day to day, so features need to be recomputed for each day. Overall, the temporal nature of relational databases adds computational cost and further complexity, which often results in bugs and information leakage including the so-called \u201ctime travel\u201d (Kapoor and Narayanan, 2023).\n\n(a) Relational Database(b) Define Tasks(c) Relational Deep Learning\nFigure 1: Relational Deep Learning solves predictive tasks on relational data with end-to-end learnable models. There are three main steps. (a) A relational database with multiple tables connected by primary-foreign keys is given. (b) A predictive task is specified and added to the database by introducing an additional training table. (c) Relational data is transformed into its Relational Entity Graph, and a Graph Neural Network is trained over the graph with the supervision provided by the training table. The predictive task can be node level (as in this illustration), link level (pairs of nodes), or higher-order.\n\n\nThere are several issues with with the above approach: (1) it is a manual, slow and labor intensive process; (2) feature choices are essentially arbitrary and likely highly-suboptimal;\n(3) only a small fraction of the overall space of possible features can be manually explored; (4) by forcing data into a single table, information is aggregated into lower-granularity features, thus losing out on valuable fine-grain signal; (5) whenever the data distribution changes or drifts, current features become obsolete and new features have to be manually reinvented.\n\n\n\nMany domains have been in a similar position, including pre-deep-learning computer vision, where hand-chosen convolutional filters (e.g., Gabor) were used to extract features, followed by models such as SVMs or nearest neighbor search (Varma and Zisserman, 2005). Today, in contrast, deep neural networks skip the feature engineering and learn directly on the raw pixels, which results in large gains in model accuracy.\nMore broadly, the deep learning revolution has had a huge impact in many fields, including computer vision\u00a0(He et\u00a0al., 2016; Russakovsky et\u00a0al., 2015), natural language processing\u00a0(Vaswani et\u00a0al., 2017; Devlin et\u00a0al., 2018; Brown et\u00a0al., 2020), and speech\u00a0(Hannun et\u00a0al., 2014; Amodei et\u00a0al., 2016), and has led to super-human performance in many tasks. In all cases, the key was to move from manual feature engineering and handcrafted systems to fully data-driven, end-to-end representation learning systems.\nFor relational data, this transition has not yet occurred, as existing tabular deep learning approaches still heavily rely on manual feature engineering.\nConsequently, there remains a huge unexplored opportunity.\n\n\nHere we introduce Relational Deep Learning (RDL), a blueprint for fulfilling the need for an end-to-end deep learning paradigm for relational tables (Fig. 1).\nThrough end-to-end representation learning, Relational Deep Learning fully utilizes the rich predictive signals available in relational tables. The core of RDL is to represent relational tables as a temporal, heterogeneous Relational Entity Graph, where each row defines a node, columns define node features, and primary-foreign key links define edges. Graph Neural Networks (GNNs)\u00a0(Gilmer et\u00a0al., 2017; Hamilton et\u00a0al., 2017) can then be applied to build end-to-end data-driven predictive models.\n\n\nPredictive tasks are specified on relational data by introducing a training table that holds supervision label information, (Fig. 1b) but no input features.\nTraining tables have two critically important characteristics. First, labels can be automatically computed from historical relational data, without any need for outside annotation; second, they may contain any number of foreign keys, permitting many task types including entity level (1 key, as in Fig. 1b), link-level tasks such as recommendation (2 keys) and multi-entity tasks (>>>2 keys). Training tables permit many different types of prediction targets, including multi-class, multi-label, regression and more, ensuring high task generality.\n\n\nAll in all, RDL model pipeline has four main steps (Fig.\u00a02): Given a predictive machine learning task, (1) A training table containing supervision labels is constructed in a task-specific manner based on historic data in the relational database, (2) entity-level features are extracted and encoded from each row in each table to serve as node features, (3) node representations are learned through an inter-entity message-passing GNN that exchanges information between entities with primary-foreign key links, (4) a task-specific model head produces predictions for training data, and errors are backpropogated through the network.\n\n(a) Rel. Tables with Training Table(b) Entities Linked by Foreign Keys(c) Relational Entity Graph(d) Graph Neural Network\nFigure 2: Relational Deep Learning Pipeline. (a) Given relational tables and a predictive task, a training table, containing supervised label information, is constructed and attached to the entity table(s). (b) Relational tables contain individual entities that are linked by foreign-primary key relations. (c) Relational data can be viewed as a single Relational Entity graph, which has a node for each entity, and edges given by primary-foreign key links. (d) Initial node features are extracted from each row in each table using modality-specific neural networks. Then a message passing graph neural network computes relation-aware node embeddings, a model head produces predictions for training table entities, and errors are backpropogated.\n\n\n\nCrucially, RDL models natively integrate temporality by only allowing entities to receive messages from other entities with earlier timestamps. This ensures that learned representation is automatically updated during GNN forward pass when new data is collected, and prevents information leakage and time travel bugs. Furthermore, this also stabilizes the generalization across time since models are trained to make predictions at multiple time snapshots by dynamically passing messages between entities at different time snapshots, whilst remaining grounded in a single relational database.\n\n\n\nRelBench.\n\nTo facilitate research into Relational Deep Learning, we introduce RelBench, a benchmarking and an evaluation Python package. Data in RelBench cover rich relational databases from many different domains. RelBench has the following key modules (1) Data: data loading, specifying a predictive task, and (temporal) data splitting, (2) Model: transforming data to a heterogeneous graph, building graph neural network predictive models, (3) Evaluation: standardized evaluation protocol given a file of predictions. Importantly, data and evaluation modules are deep learning framework agnostic, enabling broad compatibility. To facilitate research, we provide our initial model implementation based on PyTorch Frame\u00a0(Hu et\u00a0al., 2023) for encoding table rows into input node embeddings, which is then processed by GNN models in PyTorch Geometric\u00a0(Fey and Lenssen, 2019) to update the embeddings via message passing over the relational entity graph.\n\n\nFor the initial release, RelBench contains two databases, each with two predictive tasks. The first database is from Stack Exchange, the question-answering website, and includes 7 tables such as posts, users, and votes. The predictive tasks are (1) to predict if a user is going to make a new contribution (post, answer etc.), and (2) to predict the popularity of a new question. The second database is a subset of the Amazon Product Catalog focusing on books. There are three tables: users, products, and reviews. The tasks are (1) to predict the lifetime value of a user, and (2) whether a user will stop using the site.\n\n\nOur objective is to establish deep learning on relational data as a new subfield of machine learning. We hope that this will be a fruitful research direction, with many opportunities for impactful ideas that make much better use of the rich predictive signal in relational data. This paper lays the ground for future work by making the following main sections:\n\n\n\u2022\n\nBlueprint. Relational Deep Learning, an end-to-end learnable approach that ultilizes the predictive signals available in relational data, and supports temporal predictions.\n\n\n\n\u2022\n\nBenchmarking Package RelBench, an open-source Python package for benchmarking and evaluating GNNs on relational data. RelBench beta release introduces two relational databases, and specifies two prediction tasks for each.\n\n\n\n\u2022\n\nResearch Opportunities. Outlining a new research program for Relational Deep Learning, including multi-task learning, new GNN architectures, multi-hop learning, and more.\n\n\n\n\n\n\nOrganization.\n\nSection 2 provides background on relational tables and predictive task specification.\nSection 3 introduces our central methodological contribution, a graph neural network approach to solving predictive tasks on relational data.\nSection 4 introduces RelBench, a new benchmark for relational tables, and standardized evaluation protocols.\nSection 5 outlines a landscape of new research opportunities for graph machine learning on relational data. Finally, Section 6 concludes by contextualizing our new framework within the tabular and graph machine learning literature.\n\n\n",
      "2 Predictive Tasks on Relational Databases": "\n\n2 Predictive Tasks on Relational Databases\n\nThis section outlines our problem scope: predictive tasks on relational tables. In the process, we define what we mean by relational tables, and how to specify predictive tasks on them. This section focuses exclusively on the structure of data and tasks, laying the groundwork for Section 3, which presents our GNN-based modelling approach.\n\n\n\n2.1 Relational Data\n\nA Brief History.\n\nRelational tables and relational databases emerged in the 1970s as a means to standardize data retrieval and management (Codd, 1970). As society digitized, relational databases came to fulfill a foundational purpose, and today are estimated to comprise 72% of the world\u2019s data\u00a0(DB-Engines, 2023). Whilst there is no single agreed upon definition of a relational database, three essential characteristics are shared in all cases (cf. Figure 1a):\n\n\n1.\n\nData is stored in multiple tables.\n\n\n\n2.\n\nEach row in each table contains an entity, which possesses a unique primary key ID, along with multiple attributes stored as columns of the table.\n\n\n\n3.\n\nOne entity may refer to another entity using a foreign key\u2014the primary key of another entity.\n\n\n\n\n\nAs well as a standardized storage system, relational databases typically come equipped with a powerful set of relational operations, which are used to manipulate and access data. Codd (1970) introduced 8 relational operations, including set operations such as taking the union of two tables, and other operations such as joining two tables based on their common attributes. Popular query languages such as SQL (Chamberlin and Boyce, 1974) provide commercial-grade implementations of a wide variety of relational operations.\nNext we formally define relational data, as suits our purposes.\n\n\n\nDefinition of Relational Databases.\n\nA relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) is comprised of a collection of tables \ud835\udcaf={T1,\u2026,Tn}\ud835\udcafsubscript\ud835\udc471\u2026subscript\ud835\udc47\ud835\udc5b\\mathcal{T}=\\{T_{1},\\ldots,T_{n}\\}caligraphic_T = { italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, and links between tables \u2112\u2286\ud835\udcaf\u00d7\ud835\udcaf\u2112\ud835\udcaf\ud835\udcaf\\mathcal{L}\\subseteq\\mathcal{T}\\times\\mathcal{T}caligraphic_L \u2286 caligraphic_T \u00d7 caligraphic_T (cf. Figure 2a). A link L=(TfkeyL=(T_{\\rm fkey}italic_L = ( italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT, Tpkey)T_{\\rm pkey})italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT ) between tables exists if a foreign key column in Tfkeysubscript\ud835\udc47fkeyT_{\\rm fkey}italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT points to a primary key column of Tpkeysubscript\ud835\udc47pkeyT_{\\rm pkey}italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT.\nEach table is a set T={v1,\u2026,vnT}\ud835\udc47subscript\ud835\udc631\u2026subscript\ud835\udc63subscript\ud835\udc5b\ud835\udc47T=\\{v_{1},...,v_{n_{T}}\\}italic_T = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_v start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT }, whose elements vi\u2208Tsubscript\ud835\udc63\ud835\udc56\ud835\udc47v_{i}\\in Titalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_T are called rows, or entities (cf. Figure 2b). Each entity v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T, has four constituent parts v=(pv,\ud835\udca6v,xv,tv)\ud835\udc63subscript\ud835\udc5d\ud835\udc63subscript\ud835\udca6\ud835\udc63subscript\ud835\udc65\ud835\udc63subscript\ud835\udc61\ud835\udc63v=(p_{v},\\mathcal{K}_{v},x_{v},t_{v})italic_v = ( italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ):\n\n\n1.\n\nPrimary key pvsubscript\ud835\udc5d\ud835\udc63p_{v}italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, that uniquely identifies the entity v\ud835\udc63vitalic_v.\n\n\n\n2.\n\nForeign keys \ud835\udca6v\u2286{pv\u2032:v\u2032\u2208T\u2032\u2062\u00a0and\u00a0\u2062(T,T\u2032)\u2208\u2112}subscript\ud835\udca6\ud835\udc63conditional-setsubscript\ud835\udc5dsuperscript\ud835\udc63\u2032superscript\ud835\udc63\u2032superscript\ud835\udc47\u2032\u00a0and\u00a0\ud835\udc47superscript\ud835\udc47\u2032\u2112\\mathcal{K}_{v}\\subseteq\\{p_{v^{\\prime}}:v^{\\prime}\\in T^{\\prime}\\text{ and }(%\nT,T^{\\prime})\\in\\mathcal{L}\\}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2286 { italic_p start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT : italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT and ( italic_T , italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) \u2208 caligraphic_L }, defining links between element v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T to elements v\u2032\u2208T\u2032superscript\ud835\udc63\u2032superscript\ud835\udc47\u2032v^{\\prime}\\in T^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, where pv\u2032subscript\ud835\udc5dsuperscript\ud835\udc63\u2032p_{v^{\\prime}}italic_p start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT is the primary key of an entity v\u2032superscript\ud835\udc63\u2032v^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT in table T\u2032superscript\ud835\udc47\u2032T^{\\prime}italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.\n\n\n\n3.\n\nAttributes xvsubscript\ud835\udc65\ud835\udc63x_{v}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, holding the informational content of the entity. \n\n\n\n4.\n\nTimestamp An optional timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, indicating the time an event occurred.\n\n\n\n\n\nFor example, the Transactions table in Figure 2a has the primary key (TransactionID), two foreign keys (ProductID and CustomerID), one attribute (Price), and timestamp column (Timestamp). Similarly, the Products table has the primary key (ProductID), no foreign keys, attributes (Description, Image and Size), and no timestamp. The connection between foreign keys and primary keys is illustrated by black connecting lines in Figure 2.\n\n\nIn general, the attributes in table T\ud835\udc47Titalic_T contain dTsubscript\ud835\udc51\ud835\udc47d_{T}italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT values: xv=(xv1,\u2026,xvdT)subscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ), each belonging to a particular column. Critically, all entities in the same table have the same columns (values may be absent). Formally, this is described by membership xv=(xv1,\u2026,xvdT)\u2208\ud835\udc9cT1\u00d7\u2026\u00d7\ud835\udc9cTdTsubscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47superscriptsubscript\ud835\udc9c\ud835\udc471\u2026superscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})\\in\\mathcal{A}_{T}^{1}\\times\\ldots\\times%\n\\mathcal{A}_{T}^{d_{T}}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) \u2208 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u00d7 \u2026 \u00d7 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, where \ud835\udc9cTisubscriptsuperscript\ud835\udc9c\ud835\udc56\ud835\udc47\\mathcal{A}^{i}_{T}caligraphic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT denotes the value space of i\ud835\udc56iitalic_i-th column of table T\ud835\udc47Titalic_T, and is shared between all entities v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T.\nFor example, the Products table from Fig.\u00a02a contains three different attributes: the product description (text type), the image of the product (image type), and the size of the product (numerical type). Each of these types has their own encoders as discussed in Sec.\u00a03.4.3.\n\n\n\nFact and Dimension Tables.\n\nTables are categorized into two types, fact or dimension, with complementary roles (Garcia-Molina et\u00a0al., 2008). Dimension tables provide contextual information, such as biographical information, macro statistics (such as number of beds in a hospital), or immutable properties, such as the size of a product (as in the Products table in Figure 2a). Dimension tables tend to have relatively few rows, as it is limited to one per real-world object. Fact tables record interactions between other entities, such as all patient admissions to hospital, or all customer transactions (as in the Transactions table in Figure 2a). Since entities can interact repeatedly, fact tables often contain the majority of rows in a relational database. Typically, features in dimension tables are static over their whole lifetime, while fact tables usually contain temporal information with a dedicated time column that denotes the time of appearance.\n\n\n\nTemporality as a First-Class Citizen.\n\nRelational data evolves over time as events occur and are recorded.\nThis is captured by the (optional) timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT attached to each entity v\ud835\udc63vitalic_v. For example, each transaction in Transactions has a time stamp.\nFurthermore, many tasks of interest involve forecasting future events. For example, how much will a customer spend in next k\ud835\udc58kitalic_k days. It is therefore essential that time is conferred a special status unlike other attributes. Our formulation, introduced in Section 3, achieves this through a temporal message passing scheme (similar to Rossi et\u00a0al. (2020)), that only permits nodes to receive messages from neighbors with earlier timestamps. This ensures that models do not leak information from the future during training, avoiding shortcut decision rules that achieve high training accuracy but fail at test time (Geirhos et\u00a0al., 2020). It also means that model-extracted features are automatically updated as new relational data is added.\n\n\n\n\n\n2.2 From Task to Training Table\n\nThere are many practically interesting machine learning tasks defined over relational databases, such as predicting the response of a patient to treatment, or the future sales of a product.\nThese tasks involve predicting the future state of the entities of interest.\nGiven a task we wish to solve, how can we create ground truth labels to supervise machine learning models training?\n\n\nOur key insight is that we can generate training labels using historical data.\nFor instance, at time t\ud835\udc61titalic_t, ground truth labels for predicting \u201chow much each customer will buy in the next 90909090 days?\u201d are computed by summing up each customer\u2019s spending within the interval t\ud835\udc61titalic_t and t+90\ud835\udc6190t+90italic_t + 90 days. Importantly, as long as t+90\ud835\udc6190t+90italic_t + 90 is less than the most recent timestamp in the database, then these ground truth labels can be computed purely from historical data without any need for external annotation. Further, by choosing different time points t\ud835\udc61titalic_t across the database time horizon, it is possible to naturally compute many ground truth training labels for each entity.\n\n\nTo hold the labels for a new predictive task, we introduce a new table known as a training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT (Fig.\u00a03).\nEach entity v=(\ud835\udca6v,tv,yv)\ud835\udc63subscript\ud835\udca6\ud835\udc63subscript\ud835\udc61\ud835\udc63subscript\ud835\udc66\ud835\udc63v=(\\mathcal{K}_{v},t_{v},y_{v})italic_v = ( caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) in the training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT has three components: (1) A (set of) foreign keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT indicating the entities the training example is associated to, (2) a timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, and (3) the ground truth label itself yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. In contrast to tabular learning settings, the training table does not contain input data xvsubscript\ud835\udc65\ud835\udc63x_{v}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. The training table is linked to the main relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) by updating: (1) the set of tables to \ud835\udcaf\u222a{Ttrain}\ud835\udcafsubscript\ud835\udc47train\\mathcal{T}\\cup\\{{T_{\\text{train}}}\\}caligraphic_T \u222a { italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT }, and (2) the links between tables to \u2112\u222a\u2112Ttrain\u2112subscript\u2112subscript\ud835\udc47train\\mathcal{L}\\cup\\mathcal{L}_{T_{\\text{train}}}caligraphic_L \u222a caligraphic_L start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT end_POSTSUBSCRIPT, where \u2112Ttrainsubscript\u2112subscript\ud835\udc47train\\mathcal{L}_{T_{\\text{train}}}caligraphic_L start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT end_POSTSUBSCRIPT specifies tables that training table keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT point to.\n\n\nAs discussed in Sec. 2.1, careful handling of what data the model sees during training is crucial in order to ensure temporal leakage does not happen. This is achieved using the training timestamp. When the model is trained to output target yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT for entity v\ud835\udc63vitalic_v with timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, temporal consistency is ensured by only permitting the model to receive input information from entities u\ud835\udc62uitalic_u with timestamp tu\u2264tvsubscript\ud835\udc61\ud835\udc62subscript\ud835\udc61\ud835\udc63t_{u}\\leq t_{v}italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT \u2264 italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (see Sec. 3.3 for details on training sampling).\n\n(a) Define Tasks(b) Training Table Generation(c) Link to Relational Tables\nFigure 3: Predictive Task Definition. A task over relational data is defined by attaching an additional training table to the existing linked tables. A training table entity specifies (a) ground truth label computed from historical information (b) the entity ID(s) the labels correspond to, and (c) a timestamp that controls what data the model can use to predict this label.\n\n\nThus, the purpose of the training table is twofold: to specify training inputs and outputs of the machine learning model. First, it provides supervision on the model output by specifying the the entities and their target training labels. Second, in the case of temporal tasks, the training table specifies the model input by specifying the timestamp at which each historical training label is generated.\n\n\nThis training table formulation can model a wide range of predictive tasks on relational databases:\n\n\n\u2022\n\nNode-level prediction tasks (e.g., multi-class classification, multi-label classification, regression): The training table has three columns (EntityID, Label, Time), indicating the foreign key, target label, and timestamp columns, respectively.\n\n\n\n\u2022\n\nLink prediction tasks: The training table has columns (SourceEntityID, TargetEntityID, Label, Time), indicating the foreign key columns for the source/target nodes, and the target label, and timestamp, respectively.\n\n\n\n\u2022\n\nTemporal and static prediction tasks: Temporal tasks make predictions about the future (and require a seed time), while non-temporal tasks impute missing values (Time is dropped).\n\n\n\n\n\nTraining Table Generation.\n\nIn practice, training tables can be computed using time-conditioned SQL queries from historic data in the database. Given a query that describes the prediction targets for all prediction entities, e.g. the sum of sells grouped by products, from time t\ud835\udc61titalic_t to time t+\u03b4\ud835\udc61\ud835\udefft+\\deltaitalic_t + italic_\u03b4 in the future, we can move t\ud835\udc61titalic_t back in time in fixed intervals to gather historical training, validation and test targets for all entities (cf. Fig.\u00a03b). We store t\ud835\udc61titalic_t as timestamp for the targets gathered in each step.\n\n\n\n",
      "3 Predictive Tasks as Graph Representation Learning Problems": "\n\n3 Predictive Tasks as Graph Representation Learning Problems\n\nHere, we formulate a generic machine learning architecture based on Graph Neural Networks, which solves predictive tasks on relational databases.\nThe following section will first introduce three important graph concepts, which are outlined in Fig.\u00a04: (a) The schema graph (cf.\u00a0Sec.\u00a03.1), table-level graph, where one table corresponds to one node. (b) The relational entity graph (cf. Sec.\u00a03.2), an entity-level graph, with a node for each entity in each table, and edges are defined via foreign-primary key connections between entities. (c) The time-consistent computation graph (cf.\u00a0Sec.\u00a03.3), which acts as an explicit training example for graph neural networks.\nWe describe generic procedures to map between graph types, and finally introduce our GNN blueprint for end-to-end learning on relational databases (cf.\u00a0Sec.\u00a03.4).\n\n\n\n\n\n(a) Schema Graph\n\n\n\n\n(b) Relational Entity Graph\n\n\n\n\n(c) Computation Graphs for different time t\ud835\udc61titalic_t\n\n\n\n\nFigure 4: Three different kinds of graphs. (a) The schema graph arises from the given relational tables. Each node denotes a table, and an edge between tables indicates that primary keys in one are foreign keys in the other. (b) The entity graph has one node for each entity in each table, and edges given by primary-foreign key links. The entity graph is heterogeneous with node and edge types defined by the schema graph. The nodes have a timestamp (illustrated by arrow-of-time), originating from the timestamp column of the table.\n(c) Using a temporal sampling strategy and a task description in form of training table containing different time s\ud835\udc60sitalic_s, we obtain time-consistent computation graphs as training examples that naturally respect temporal order and map well to parallel compute.\n\n\n\n\n3.1 Schema Graph\n\nThe first graph in our blueprint is the schema graph (cf. Fig. 4a), which describes the table-level structure of data.\nGiven a relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) as defined in Sec.\u00a02, we let \u2112\u22121={(Tpkey,Tfkey)\u2223(Tfkey,Tpkey)\u2208\u2112}superscript\u21121conditional-setsubscript\ud835\udc47pkeysubscript\ud835\udc47fkeysubscript\ud835\udc47fkeysubscript\ud835\udc47pkey\u2112\\mathcal{L}^{-1}=\\{(T_{\\rm pkey},T_{\\rm fkey})\\mid(T_{\\rm fkey},T_{\\rm pkey})%\n\\in\\mathcal{L}\\}caligraphic_L start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = { ( italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT ) \u2223 ( italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT ) \u2208 caligraphic_L } denote its inverse set of links.\nThen, the schema graph is the graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) that arises from the relational database, with node set \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T and edge set \u211b=\u2112\u222a\u2112\u22121\u211b\u2112superscript\u21121\\mathcal{R}=\\mathcal{L}\\cup\\mathcal{L}^{-1}caligraphic_R = caligraphic_L \u222a caligraphic_L start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT.\nInverse links ensure that all tables are reachable within the schema graph.\nThe schema graph nodes serve as type definitions for the heterogeneous relational entity graph, which we define next.\n\n\n\n\n3.2 Relational Entity Graph\n\nTo formulate a graph suitable for processing with GNNs, we introduce the relational entity graph, which has entity-level nodes and serves as the basis of the proposed relational learning framework.\n\n\nOur relational entity graph is a heterogeneous graph G=(\ud835\udcb1,\u2130,\u03d5,\u03c8)\ud835\udc3a\ud835\udcb1\u2130italic-\u03d5\ud835\udf13G=(\\mathcal{V},\\mathcal{E},\\phi,\\psi)italic_G = ( caligraphic_V , caligraphic_E , italic_\u03d5 , italic_\u03c8 ), with node set \ud835\udcb1\ud835\udcb1\\mathcal{V}caligraphic_V and edge set \u2130\u2286\ud835\udcb1\u00d7\ud835\udcb1\u2130\ud835\udcb1\ud835\udcb1\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}caligraphic_E \u2286 caligraphic_V \u00d7 caligraphic_V and type mapping functions \u03d5:\ud835\udcb1\u2192\ud835\udcaf:italic-\u03d5\u2192\ud835\udcb1\ud835\udcaf\\phi:\\mathcal{V}\\rightarrow\\mathcal{T}italic_\u03d5 : caligraphic_V \u2192 caligraphic_T and \u03c8:\u2130\u2192\u211b:\ud835\udf13\u2192\u2130\u211b\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}italic_\u03c8 : caligraphic_E \u2192 caligraphic_R, where each node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V belongs to a node type \u03d5\u2062(v)\u2208\ud835\udcafitalic-\u03d5\ud835\udc63\ud835\udcaf\\phi(v)\\in\\mathcal{T}italic_\u03d5 ( italic_v ) \u2208 caligraphic_T and each edge e\u2208\u2130\ud835\udc52\u2130e\\in\\mathcal{E}italic_e \u2208 caligraphic_E belongs to an edge type \u03c8\u2062(e)\u2208\u211b\ud835\udf13\ud835\udc52\u211b\\psi(e)\\in\\mathcal{R}italic_\u03c8 ( italic_e ) \u2208 caligraphic_R.\nSpecifically, the sets \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T and \u211b\u211b\\mathcal{R}caligraphic_R from the schema graph define the node and edge types of our relational entity graph.\n\n\nGiven a schema graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) with tables T={v1,\u2026,vnT}\u2208\ud835\udcaf\ud835\udc47subscript\ud835\udc631\u2026subscript\ud835\udc63subscript\ud835\udc5b\ud835\udc47\ud835\udcafT=\\{v_{1},...,v_{n_{T}}\\}\\in\\mathcal{T}italic_T = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_v start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT } \u2208 caligraphic_T as defined in Sec.\u00a02, we define the node set in our relational entity graph as the union of all entries in all tables \ud835\udcb1=\u22c3T\u2208\ud835\udcafT\ud835\udcb1subscript\ud835\udc47\ud835\udcaf\ud835\udc47\\mathcal{V}=\\bigcup_{T\\in\\mathcal{T}}Tcaligraphic_V = \u22c3 start_POSTSUBSCRIPT italic_T \u2208 caligraphic_T end_POSTSUBSCRIPT italic_T.\nIts edge set is then defined as\n\n\n\n\u2130={(v1,v2)\u2208\ud835\udcb1\u00d7\ud835\udcb1\u2223pv2\u2208\ud835\udca6v1\u2062\u00a0or\u00a0\u2062pv1\u2208\ud835\udca6vv}\u2062,\u2130conditional-setsubscript\ud835\udc631subscript\ud835\udc632\ud835\udcb1\ud835\udcb1subscript\ud835\udc5dsubscript\ud835\udc632subscript\ud835\udca6subscript\ud835\udc631\u00a0or\u00a0subscript\ud835\udc5dsubscript\ud835\udc631subscript\ud835\udca6subscript\ud835\udc63\ud835\udc63,\\mathcal{E}=\\{(v_{1},v_{2})\\in\\mathcal{V}\\times\\mathcal{V}\\mid p_{v_{2}}\\in%\n\\mathcal{K}_{v_{1}}\\text{ or }p_{v_{1}}\\in\\mathcal{K}_{v_{v}}\\}\\textnormal{,}caligraphic_E = { ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2208 caligraphic_V \u00d7 caligraphic_V \u2223 italic_p start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2208 caligraphic_K start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT or italic_p start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2208 caligraphic_K start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT } ,\n\n(1)\n\n\ni.e. the entity-level pairs that arise from the primary-foreign key relationships in the database.\nWe equip the relational entity graph with the following key information:\n\n\n\u2022\n\nType mapping functions \u03d5:\ud835\udcb1\u2192\ud835\udcaf:italic-\u03d5\u2192\ud835\udcb1\ud835\udcaf\\phi:\\mathcal{V}\\rightarrow\\mathcal{T}italic_\u03d5 : caligraphic_V \u2192 caligraphic_T and \u03c8:\u2130\u2192\u211b:\ud835\udf13\u2192\u2130\u211b\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}italic_\u03c8 : caligraphic_E \u2192 caligraphic_R, mapping nodes and edges to respective elements of the schema graph, making the graph heterogeneous. We set \u03d5\u2062(v)=Titalic-\u03d5\ud835\udc63\ud835\udc47\\phi(v)=Titalic_\u03d5 ( italic_v ) = italic_T for all v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T and \u03c8\u2062(v1,v2)=(\u03d5\u2062(v1),\u03d5\u2062(v2))\u2208\u211b\ud835\udf13subscript\ud835\udc631subscript\ud835\udc632italic-\u03d5subscript\ud835\udc631italic-\u03d5subscript\ud835\udc632\u211b\\psi(v_{1},v_{2})=(\\phi(v_{1}),\\phi(v_{2}))\\in\\mathcal{R}italic_\u03c8 ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( italic_\u03d5 ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_\u03d5 ( italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) \u2208 caligraphic_R if (v1,v2)\u2208\u2130subscript\ud835\udc631subscript\ud835\udc632\u2130(v_{1},v_{2})\\in\\mathcal{E}( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2208 caligraphic_E.\n\n\n\n\u2022\n\nTime mapping function \u03c4:\ud835\udcb1\u2192\ud835\udc9f:\ud835\udf0f\u2192\ud835\udcb1\ud835\udc9f\\tau:\\mathcal{V}\\rightarrow\\mathcal{D}italic_\u03c4 : caligraphic_V \u2192 caligraphic_D, mapping nodes to its timestamp: \u03c4:v\u21a6tv:\ud835\udf0fmaps-to\ud835\udc63subscript\ud835\udc61\ud835\udc63\\tau:v\\mapsto t_{v}italic_\u03c4 : italic_v \u21a6 italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (as defined in Sec.\u00a02.1), introducing time as a central component and establishes the temporality of the graph. The value \u03c4\u2062(v)\ud835\udf0f\ud835\udc63\\tau(v)italic_\u03c4 ( italic_v ) denotes the point in time in which the table row v\ud835\udc63vitalic_v became available or \u2212\u221e-\\infty- \u221e in case of non-temporal rows.\n\n\n\n\u2022\n\nEmbedding vectors \ud835\udc21v\u2208\u211dd\u03d5\u2062(v)subscript\ud835\udc21\ud835\udc63superscript\u211dsubscript\ud835\udc51italic-\u03d5\ud835\udc63\\mathbf{h}_{v}\\in\\mathbb{R}^{d_{\\phi(v)}}bold_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT end_POSTSUPERSCRIPT for each v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V, which contains an embedding vector for each node in the graph. Initial embeddings are obtained via multi-modal column encoders as described in Sec.\u00a03.4.3. Final embeddings are computed via GNNs outlined in Section 3.4.\n\n\n\n\n\nAn example of a relational entity graph for a given schema graph is given in Fig.\u00a03(b).\nThe graph contains a node for each row in the database tables. Two nodes are connected if the foreign key entry in one table row links to the primary key entry of another table row. Node and edge types are defined by the schema graph. Nodes resulting from temporal tables carry the timestamp from the respective row, allowing temporal message passing, which is described next.\n\n\n\n\n3.3 Time-Consistent Computational Graphs\n\nGiven a relational entity graph and a training table (cf.\u00a0Sec.\u00a02.2), we need to be able to query the graph at specific points in time which then serve as explicit training examples used as input to the model.\nIn particular, we create a subgraph from the relational entity graph induced by the set of foreign keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and its timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT of a training example in the training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT.\nThis subgraph then acts as a local and time-consistent computation graph to predict its ground-truth label yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT.\n\n\nAlgorithm 1  Time-Consistent Computation Graph\n\nRelational entity graph G=(\ud835\udcb1,\u2130)\ud835\udc3a\ud835\udcb1\u2130G=(\\mathcal{V},\\mathcal{E})italic_G = ( caligraphic_V , caligraphic_E ), number of hops L\ud835\udc3fLitalic_L, seed node v0\u2208\ud835\udcb1subscript\ud835\udc630\ud835\udcb1v_{0}\\in\\mathcal{V}italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 caligraphic_V, seed time t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R\n\nNeighborhood sizes (m1,\u2026,mL)\u2208\u2115Lsubscript\ud835\udc5a1\u2026subscript\ud835\udc5a\ud835\udc3fsuperscript\u2115\ud835\udc3f(m_{1},...,m_{L})\\in\\mathbb{N}^{L}( italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_m start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) \u2208 blackboard_N start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT\n\nComputation graph Gcomp=(\ud835\udcb1comp,\u2130comp)subscript\ud835\udc3acompsubscript\ud835\udcb1compsubscript\u2130compG_{\\rm comp}=(\\mathcal{V}_{\\rm comp},\\mathcal{E}_{\\rm comp})italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT = ( caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT , caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT )\n\n\n\ud835\udcb10\u2190{v0}\u2190subscript\ud835\udcb10subscript\ud835\udc630\\mathcal{V}_{0}\\leftarrow\\{v_{0}\\}caligraphic_V start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 { italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT }, \u2003\u21300\u2190\u2205\u2190subscript\u21300\\mathcal{E}_{0}\\leftarrow\\emptysetcaligraphic_E start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 \u2205\n\n\nfor\u00a0i\u2208{1,\u2026,L}\ud835\udc561\u2026\ud835\udc3fi\\in\\{1,...,L\\}italic_i \u2208 { 1 , \u2026 , italic_L }\u00a0do\n\n\u00a0\u00a0\u00a0\u00a0\u00a0for\u00a0v\u2208\ud835\udcb1i\u22121\ud835\udc63subscript\ud835\udcb1\ud835\udc561v\\in\\mathcal{V}_{i-1}italic_v \u2208 caligraphic_V start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT\u00a0do\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2130i\u2190SELECTmi\u2062({(w,v)\u2208\u2130\u2223\u03c4\u2062(v)\u2264t})\u2190subscript\u2130\ud835\udc56subscriptSELECTsubscript\ud835\udc5a\ud835\udc56conditional-set\ud835\udc64\ud835\udc63\u2130\ud835\udf0f\ud835\udc63\ud835\udc61\\mathcal{E}_{i}\\leftarrow\\textnormal{SELECT}_{m_{i}}(\\{(w,v)\\in\\mathcal{E}\\mid%\n\\tau(v)\\leq t\\})caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 SELECT start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( { ( italic_w , italic_v ) \u2208 caligraphic_E \u2223 italic_\u03c4 ( italic_v ) \u2264 italic_t } ) \u25b7\u25b7\\triangleright\u25b7 Select a maximum of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT filtered edges\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\ud835\udcb1i\u2190{w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130i}\u2190subscript\ud835\udcb1\ud835\udc56conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63subscript\u2130\ud835\udc56\\mathcal{V}_{i}\\leftarrow\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}_{i}\\}caligraphic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } \u25b7\u25b7\\triangleright\u25b7 Gather nodes for the sampled edges\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0end\u00a0for\n\n\nend\u00a0for\n\n\n\ud835\udcb1comp\u2190\u22c3i=1L\ud835\udcb1i\u2190subscript\ud835\udcb1compsuperscriptsubscript\ud835\udc561\ud835\udc3fsubscript\ud835\udcb1\ud835\udc56\\mathcal{V}_{\\rm comp}\\leftarrow\\bigcup_{i=1}^{L}\\mathcal{V}_{i}caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT \u2190 \u22c3 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT caligraphic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \u2003\u2130comp\u2190\u22c3i=1L\u2130i\u2190subscript\u2130compsuperscriptsubscript\ud835\udc561\ud835\udc3fsubscript\u2130\ud835\udc56\\mathcal{E}_{\\rm comp}\\leftarrow\\bigcup_{i=1}^{L}\\mathcal{E}_{i}caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT \u2190 \u22c3 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\n\n\n\n\nThe computational graphs obtained via neighbor sampling\u00a0(Hamilton et\u00a0al., 2017) allow the scalability of our proposed approach to modern large-scale relational data with billions of table rows, while ensuring the temporal constraints\u00a0(Wang et\u00a0al., 2021).\nSpecifically, given a number of hops L\ud835\udc3fLitalic_L to sample, a seed node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V, and a timestamp t\ud835\udc61titalic_t induced by a training example, the computation graph is defined as Gcomp=(\ud835\udcb1comp,\u2130comp)subscript\ud835\udc3acompsubscript\ud835\udcb1compsubscript\u2130compG_{\\rm comp}=(\\mathcal{V}_{\\rm comp},\\mathcal{E}_{\\rm comp})italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT = ( caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT , caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT ) as the output of Alg.\u00a01.\nThe algorithm traverses the graph starting from the seed node v\ud835\udc63vitalic_v for L\ud835\udc3fLitalic_L iterations. In iteration i\ud835\udc56iitalic_i, it gathers a maximum of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT neighbors available up to timestamp t\ud835\udc61titalic_t, using one of three selection strategies:\n\n\n\u2022\n\nUniform temporal sampling selects uniformly sampled random neighbors.\n\n\n\n\u2022\n\nOrdered temporal sampling takes the latest neighbors, ordered by time \u03c4\ud835\udf0f\\tauitalic_\u03c4.\n\n\n\n\u2022\n\nBiased temporal sampling selects random neighbors sampled from a multinomial probability distribution induced by \u03c4\ud835\udf0f\\tauitalic_\u03c4. For instance, sampling can be performed proportional to relative neighbor time or biased towards specific important historical moments.\n\n\n\n\n\nThe temporal neighbor sampling is performed purely on the graph structure of the relational entity graph, without requiring initial embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. The bounded size of computation graph Gcompsubscript\ud835\udc3acompG_{\\rm comp}italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT allows for efficient mini-batching on GPUs, independent of relational entity graph size.\nIn practice, we perform temporal neighbor sampling on-the-fly, which allows us to operate on a shared relational entity graph across all training examples, from which we can then restore local and historical snapshots very efficiently.\nExamples of computation graphs are shown in Fig.\u00a03(c).\n\n\n\n\n3.4 Task-Specific Temporal Graph Neural Networks\n\nGiven a time-consistent computational graph and its future label to predict, we define a generic multi-stage deep learning architecture as follows:\n\n\n1.\n\nTable-level column encoders that encode table row data into initial node embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, as described in Sec.\u00a03.4.3.\n\n\n\n2.\n\nA stack of L\ud835\udc3fLitalic_L relational-temporal message passing layers (cf.\u00a0Sec.\u00a03.4.1).\n\n\n\n3.\n\nA task-specific model head, mapping final node embeddings to a prediction (cf.\u00a0Sec.\u00a03.4.2).\n\n\n\n\nThe whole architecture, consisting of table-level encoders, message passing layers and task specific model heads can be trained end-to-end to obtain an optimal model for the given task.\n\n\n\n3.4.1 Relational-Temporal Message Passing\n\nThis section introduces a generic framework for heterogeneous message passing GNNs on relational entity graphs as defined in Sec.\u00a03.2.\nA message passing operator in the given relational framework needs to respect the heterogeneous nature as well as the temporal properties of the graph. This is ensured by filtering nodes based on types and time. Thus, we briefly introduce heterogeneous message passing before we turn to our temporal message passing.\n\n\nHeterogeneous Message Passing.\n\nMessage-Passing Graph Neural Networks (MP-GNNs)\u00a0(Gilmer et\u00a0al., 2017; Fey and Lenssen, 2019) are a generic computational framework to define deep learning architectures on graph-structered data. Given a heterogeneous graph G=(\ud835\udcb1,\u2130,\u03d5,\u03c8)\ud835\udc3a\ud835\udcb1\u2130italic-\u03d5\ud835\udf13G=(\\mathcal{V},\\mathcal{E},\\phi,\\psi)italic_G = ( caligraphic_V , caligraphic_E , italic_\u03d5 , italic_\u03c8 ) with initial node embeddings {\ud835\udc21v(0)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc210\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(0)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT, a single message passing iteration computes updated features {\ud835\udc21v(i+1)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(i+1)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT from features {\ud835\udc21v(i)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(i)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT given by the previous iteration.\nOne iteration takes the form: \n\n\n\n\ud835\udc21v(i+1)=f\u2062(\ud835\udc21v(i),{{g\u2062(\ud835\udc21w(i))\u2223w\u2208\ud835\udca9\u2062(v)}})\u2062,subscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63\ud835\udc53subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63conditional-set\ud835\udc54subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc64\ud835\udc64\ud835\udca9\ud835\udc63,\\mathbf{h}^{(i+1)}_{v}=f(\\mathbf{h}^{(i)}_{v},\\{\\{g(\\mathbf{h}^{(i)}_{w})\\mid w%\n\\in\\mathcal{N}(v)\\}\\})\\textnormal{,}bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_f ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , { { italic_g ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) \u2223 italic_w \u2208 caligraphic_N ( italic_v ) } } ) ,\n\n(2)\n\n\nwhere f\ud835\udc53fitalic_f and g\ud835\udc54gitalic_g are arbitrary differentiable functions with optimizable parameters and {{\u22c5}}\u22c5\\{\\{\\cdot\\}\\}{ { \u22c5 } } an permutation invariant set aggregator, such as mean, max, sum, or a combination. Heterogeneous message passing\u00a0(Schlichtkrull et\u00a0al., 2018; Hu et\u00a0al., 2020) is a nested version of Eq.\u00a02, adding an aggregation over all incoming edge types to learn distinct message types:\n\n\n\n\ud835\udc21v(i+1)=f\u03d5\u2062(v)\u2062(\ud835\udc21v(i),{{fR\u2062({{gR\u2062(\ud835\udc21w(i))\u2223w\u2208\ud835\udca9R\u2062(v)}})|\u2200R=(T,\u03d5\u2062(v))\u2208\u211b}})\u2062,subscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63subscript\ud835\udc53italic-\u03d5\ud835\udc63subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63conditional-setsubscript\ud835\udc53\ud835\udc45conditional-setsubscript\ud835\udc54\ud835\udc45subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc64\ud835\udc64subscript\ud835\udca9\ud835\udc45\ud835\udc63for-all\ud835\udc45\ud835\udc47italic-\u03d5\ud835\udc63\u211b,\\mathbf{h}^{(i+1)}_{v}=f_{\\phi(v)}\\Bigl{(}\\mathbf{h}^{(i)}_{v},\\Bigl{\\{}\\Bigl{%\n\\{}f_{R}(\\{\\{g_{R}(\\mathbf{h}^{(i)}_{w})\\mid w\\in\\mathcal{N}_{R}(v)\\}\\})\\,\\Big%\n{|}\\,\\forall R=(T,\\phi(v))\\in\\mathcal{R}\\Bigr{\\}}\\Bigr{\\}}\\Bigr{)}\\textnormal{,}bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , { { italic_f start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( { { italic_g start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) \u2223 italic_w \u2208 caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) } } ) | \u2200 italic_R = ( italic_T , italic_\u03d5 ( italic_v ) ) \u2208 caligraphic_R } } ) ,\n\n(3)\n\n\nwhere \ud835\udca9R\u2062(v)={w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130\u2062\u00a0and\u00a0\u2062\u03c8\u2062(w,v)=R}subscript\ud835\udca9\ud835\udc45\ud835\udc63conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63\u2130\u00a0and\u00a0\ud835\udf13\ud835\udc64\ud835\udc63\ud835\udc45\\mathcal{N}_{R}(v)=\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}\\textnormal{ and }%\n\\psi(w,v)=R\\}caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) = { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E and italic_\u03c8 ( italic_w , italic_v ) = italic_R } denotes the R-specific neighborhood of node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V.\nThis formulation supports a wide range of different graph neural network operators, which define the specific form of functions f\u03d5\u2062(v)subscript\ud835\udc53italic-\u03d5\ud835\udc63f_{\\phi(v)}italic_f start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT, fRsubscript\ud835\udc53\ud835\udc45f_{R}italic_f start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT, gRsubscript\ud835\udc54\ud835\udc45g_{R}italic_g start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT and {{\u22c5}}\u22c5\\{\\{\\cdot\\}\\}{ { \u22c5 } }\u00a0(Fey and Lenssen, 2019).\n\n\n\n\nTemporal Message Passing.\n\nGiven a relational entity graph G=(\ud835\udcb1,\u2130,\ud835\udcaf,\u211b)\ud835\udc3a\ud835\udcb1\u2130\ud835\udcaf\u211bG=(\\mathcal{V},\\mathcal{E},\\mathcal{T},\\mathcal{R})italic_G = ( caligraphic_V , caligraphic_E , caligraphic_T , caligraphic_R ) with attached mapping functions \u03c8,\u03d5,\u03c4\ud835\udf13italic-\u03d5\ud835\udf0f\\psi,\\phi,\\tauitalic_\u03c8 , italic_\u03d5 , italic_\u03c4 and initial node embeddings {\ud835\udc21v(0)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc210\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(0)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT and an example specific seed time t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R (cf. Sec.\u00a02.2) , we obtain a set of deep node embeddings {\ud835\udc21v(L)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(L)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT by L\ud835\udc3fLitalic_L consecutive applications of Eq.\u00a03, where we additionally filter R\ud835\udc45Ritalic_R-specific neighborhoods based on their timestamp, i.e. replace \ud835\udca9R\u2062(v)subscript\ud835\udca9\ud835\udc45\ud835\udc63\\mathcal{N}_{R}(v)caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) with\n\n\n\n\ud835\udca9R\u2264t\u2062(v)={w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130\u2062,\u00a0\u2062\u03c8\u2062(w,v)=R\u2062, and\u00a0\u2062\u03c4\u2062(w)\u2264t}\u2062,subscriptsuperscript\ud835\udca9absent\ud835\udc61\ud835\udc45\ud835\udc63conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63\u2130,\u00a0\ud835\udf13\ud835\udc64\ud835\udc63\ud835\udc45, and\u00a0\ud835\udf0f\ud835\udc64\ud835\udc61,\\mathcal{N}^{\\leq t}_{R}(v)=\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}%\n\\textnormal{, }\\psi(w,v)=R\\textnormal{, and }\\tau(w)\\leq t\\}\\text{,}caligraphic_N start_POSTSUPERSCRIPT \u2264 italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) = { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E , italic_\u03c8 ( italic_w , italic_v ) = italic_R , and italic_\u03c4 ( italic_w ) \u2264 italic_t } ,\n\n(4)\n\n\nrealized by the temporal sampling procedure presented in Sec.\u00a03.3. The formulation naturally respects time by only aggregating messages from nodes that were available before the given seed time s\ud835\udc60sitalic_s. The given formulation is agnostic to specific implementations of message passing and supports a wide range of different operators.\n\n\n\n\n\n3.4.2 Prediction with Model Heads\n\nThe model described so far is task-agnostic and simply propagates information through the relational entity graph to produce generic node embeddings. We obtain a task-specific model by combining our graph with a training table, leading to specific model heads and loss functions. We distinguish between (but are not limited to) two types of tasks: node-level prediction and link-level prediction.\n\n\nNode-level Model Head.\n\nGiven a batch of N\ud835\udc41Nitalic_N node level training table examples {(\ud835\udca6,t,y)i}i=1Nsuperscriptsubscriptsubscript\ud835\udca6\ud835\udc61\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\{(\\mathcal{K},t,y)_{i}\\}_{i=1}^{N}{ ( caligraphic_K , italic_t , italic_y ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT (cf. Sec.\u00a02.2), where \ud835\udca6={k}\ud835\udca6\ud835\udc58\\mathcal{K}=\\{k\\}caligraphic_K = { italic_k } contains the primary key of node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v~{}\\in\\mathcal{V}italic_v \u2208 caligraphic_V in the relational entity graph, t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R is the seed time, and y\u2208\u211dd\ud835\udc66superscript\u211d\ud835\udc51y\\in\\mathbb{R}^{d}italic_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is the target value. Then, the node-level model head is a function that maps node-level embeddings \ud835\udc21v(L)subscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63\\mathbf{h}^{(L)}_{v}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT to a prediction y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG, i.e.\n\n\n\nf:\u211ddv\u2192\u211dd\u2062,f:\ud835\udc21v(L)\u21a6y^\u2062.:\ud835\udc53\u2192superscript\u211dsubscript\ud835\udc51\ud835\udc63superscript\u211d\ud835\udc51,\ud835\udc53:maps-tosubscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63^\ud835\udc66.f:\\mathbb{R}^{d_{v}}\\rightarrow\\mathbb{R}^{d}\\textnormal{,}\\quad\\quad f:%\n\\mathbf{h}^{(L)}_{v}\\mapsto\\hat{y}\\textnormal{.}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_f : bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u21a6 over^ start_ARG italic_y end_ARG .\n\n(5)\n\n\n\n\n\nLink-level Model Head.\n\nSimilarly, we can define a link-level model head for training examples {(\ud835\udca6,t,y)i}i=1Nsuperscriptsubscriptsubscript\ud835\udca6\ud835\udc61\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\{(\\mathcal{K},t,y)_{i}\\}_{i=1}^{N}{ ( caligraphic_K , italic_t , italic_y ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT with \ud835\udca6={k1,k2}\ud835\udca6subscript\ud835\udc581subscript\ud835\udc582\\mathcal{K}=\\{k_{1},k_{2}\\}caligraphic_K = { italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT } containing primary keys of two different nodes v1,v2\u2208\ud835\udcb1subscript\ud835\udc631subscript\ud835\udc632\ud835\udcb1v_{1},v_{2}\\in\\mathcal{V}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 caligraphic_V in the relational entity graph. A function maps node embeddings \ud835\udc21v1(L)subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc631\\mathbf{h}^{(L)}_{v_{1}}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, \ud835\udc21v2(L)subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc632\\mathbf{h}^{(L)}_{v_{2}}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT to a prediction, i.e.\n\n\n\nf:\u211ddv1\u00d7\u211ddv2\u2192\u211dd\u2062,f:(\ud835\udc21v1(L),\ud835\udc21v2(L))\u21a6y^\u2062.:\ud835\udc53\u2192superscript\u211dsubscript\ud835\udc51subscript\ud835\udc631superscript\u211dsubscript\ud835\udc51subscript\ud835\udc632superscript\u211d\ud835\udc51,\ud835\udc53:maps-tosubscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc631subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc632^\ud835\udc66.f:\\mathbb{R}^{d_{v_{1}}}\\times\\mathbb{R}^{d_{v_{2}}}\\rightarrow\\mathbb{R}^{d}%\n\\textnormal{,}\\quad\\quad f:(\\mathbf{h}^{(L)}_{v_{1}},\\mathbf{h}^{(L)}_{v_{2}})%\n\\mapsto\\hat{y}\\textnormal{.}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_f : ( bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) \u21a6 over^ start_ARG italic_y end_ARG .\n\n(6)\n\n\nA task-specific loss L\u2062(y^,y)\ud835\udc3f^\ud835\udc66\ud835\udc66L(\\hat{y},y)italic_L ( over^ start_ARG italic_y end_ARG , italic_y ) provides gradient signals to all trainable parameters. The presented approach can be generalized to |\ud835\udca6|>2\ud835\udca62|\\mathcal{K}|>2| caligraphic_K | > 2 to specify subgraph-level tasks. In the first version, RelBench provides node-level tasks only.\n\n\n\n\n\n3.4.3 Multi-Modal Node Encoders\n\nThe final piece of the pipeline is to obtain the initial entity-level node embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT from the multi-modal input attributes xv=(xv1,\u2026,xvdT)\u2208\ud835\udc9cT1\u00d7\u2026\u00d7\ud835\udc9cTdTsubscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47superscriptsubscript\ud835\udc9c\ud835\udc471\u2026superscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})\\in\\mathcal{A}_{T}^{1}\\times\\ldots\\times%\n\\mathcal{A}_{T}^{d_{T}}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) \u2208 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u00d7 \u2026 \u00d7 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.\nDue to the nature of tabular data, each column element xvisuperscriptsubscript\ud835\udc65\ud835\udc63\ud835\udc56x_{v}^{i}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT lies in its own modality space \ud835\udc9cTdisuperscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc56\\mathcal{A}_{T}^{d_{i}}caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, e.g., image, text, categorical, and numerical values.\nTherefore, we use a modality-specific encoder to embed each attribute into embeddings.\nFor text and image modalities, we can naturally use pre-trained embedding models as the encoders\u00a0(Reimers and Gurevych, 2019).\nAfter all the attributes are embedded, we apply state-of-the-art tabular deep learning models\u00a0(Huang et\u00a0al., 2020; Arik and Pfister, 2021; Gorishniy et\u00a0al., 2021, 2022; Chen et\u00a0al., 2023) to fuse all the attribute embeddings into a single entity-level node embedding.\nIn practice, we rely on PyTorch Frame\u00a0(Hu et\u00a0al., 2023) that supports a variety of modality-specific encoders, such as pre-trained text embedding models, and as well as state-of-the-art deep learning models on tabular data.\n\n\n\n\n\n3.5 Discussion\n\nThe neural network architecture presented in this blueprint is end-to-end trainable on relational databases. This approach supports a wide range of tasks, such as classification, regression or link prediction in a unified way, with labels computed and stored in a training table.\nIt learns to solve tasks without requiring manual feature engineering, as typical in tabular learning. Instead, operations that are otherwise done manually, such as SQL JOIN+AGGREGATE\noperations, are learned by the GNN. More than simply replacing SQL operations, the GNN message and aggregation steps exactly match the functional form of SQL JOIN+AGGREGATE operations. In other words, the GNN is an exact neural version of SQL JOIN+AGGREGATE operations. We believe this is another important reason why message passing-based architectures are a natural learned replacement for hand-engineered features on relational tables.\n\n\n",
      "4 \u00a0RelBench: A Benchmark for Relational Deep Learning": "\n\n4 \u00a0RelBench: A Benchmark for Relational Deep Learning\n\nFigure 5: Overview of RelBench. RelBench enables training and evaluation of machine learning models on relational data. RelBench supports deep learning framework agnostic data loading, task specification, standardized data splitting, and transforming data into graph format. RelBench provides standardized evaluation metric computations, and a leaderboard for tracking progress. We additionally provide example training scripts built using PyTorch Geometric and PyTorch Frame.\n\n\nWe introduce RelBench, an open benchmark for Relational Deep Learning. The goal of RelBench is to facilitate scalable, robust, and reproducible machine learning research on relational tables. RelBench curates a diverse set of large-scale, challenging, and realistic benchmark databases and defines meaningful predictive tasks over these databases. In addition, RelBench develops a Python library for loading relational tables and tasks, constructing data graphs, and providing unified evaluation for predictive tasks. It also integrates seamlessly with existing Pytorch Geometric and PyTorch Frame functionalities. In its beta release111Website: https://relbench.stanford.edu222Package: https://github.com/snap-stanford/relbench, we announce the first two real-world relational databases, each with two curated predictive tasks.\n\n\nIn the subsequent sections (Sec.\u00a04.3 and\u00a04.4), we describe in detail the two relational databases and the predictive tasks. For each database, we show its entity relational diagrams and important statistics. For each task, we define the task formulation, entity filtering, significance of the task, and also unified evaluation metric. Finally, we demonstrate the usage of the RelBench\u2019s package in Sec.\u00a04.1.\n\n\n\n4.1 RelBench Package\n\nThe RelBench package is designed to allow easy and standardized access to Relational Deep Learning for researchers to push the state-of-the-art of this emerging field. It provides Python APIs to (1) download and process relational databases and their predictive tasks; (2) load standardized data splits and generate relevant train/validation/test tables; (3) evaluate on machine learning predictions. It also provides a flexible ecosystems of supporting tools such as automatic conversion to PyTorch Geometric graphs and integration with Pytorch Frame to produce embeddings for diverse column types. We additionally provide end-to-end scripts for training using RelBench package with GNNs and XGBoost\u00a0(Chen and Guestrin, 2016). We refer the readers to the code repository for a more detailed understanding of RelBench. Here we demonstrate the core functionality.\n\n\nTo load a relational database, simply do:\n\n\n\n\u2b07\n\n\n\nfrom\u00a0relbench.datasets\u00a0import\u00a0get_dataset\n\n\ndataset\u00a0=\u00a0get_dataset(name=\"rel-amazon\")\n\n\n\n\nIt will load the relational tables and process it into a standardized format. Next, to load the predictive task and the relevant training tables, do:\n\n\n\n\u2b07\n\n\n\ntask\u00a0=\u00a0dataset.get_task(\"rel-amazon-ltv\")\n\n\ntask.train_table,\u00a0task.val_table,\u00a0task.test_table\u00a0#\u00a0training/validation/testing\u00a0tables\n\n\n\n\nIt automatically constructs the training table for the relevant predictive task. Next, after the user trains the machine learning model, the user can use RelBench standardized evaluator:\n\n\u2b07\n\n\n\ntask.evaluate(pred)\n\n\n\n\n\n\n4.2 Temporal Splitting\n\nEvery dataset in RelBench has a validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT and a test timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. These are shared for all tasks in the dataset.\nThe test table for any task comprises of labels computed for the time window from ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT to ttest+\u03b4subscript\ud835\udc61test\ud835\udefft_{\\text{test}}+\\deltaitalic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT + italic_\u03b4, where the window size \u03b4\ud835\udeff\\deltaitalic_\u03b4 is specified for each task. Thus the model must make predictions using only information available up to time ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. Accordingly, to prevent accidental temporal leakage at test time RelBench only provides database rows with timestamps up to ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT for training and validation purposes. RelBench also provides default train and validation tables. The default validation table is constructed similar to the test table, but with the time window being tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT to tval+\u03b4subscript\ud835\udc61val\ud835\udefft_{\\text{val}}+\\deltaitalic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT + italic_\u03b4. To construct the default training table, we first sample time stamps tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT starting from tval\u2212\u03b4subscript\ud835\udc61val\ud835\udefft_{\\text{val}}-\\deltaitalic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT - italic_\u03b4 and moving backwards with a stride of \u03b4\ud835\udeff\\deltaitalic_\u03b4. This allows us to benefit from the latest available training information. Then for each tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we apply an entity filter to select the entities of interest (e.g., active users). Finally for each pair of timestamp and entity, we compute the training label based on the task definition. Users can explore other ways of constructing the training or validation table, for example by sampling timestamps with shorter strides to get more labels, as long as information after tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is not used for training.\n\n\n\n\n4.3 rel-amazon: Amazon product review e-commerce database\n\nFigure 6: rel-amazon contains two dimension tables (customers and products) and one fact table (reviews). Each review has a customer and a product foreign key. \n\n\n\nDatabase overview.\n\nThe rel-amazon relational database stores product and user purchasing behavior across Amazon\u2019s e-commerce platform. Notably, it contains rich information about each product and transaction. The product table includes price and category information; the review table includes overall rating, whether the user has actually bought the product, and the text of the review itself. We use the subset of book-related products. The entity relationships are described in Fig. 6.\n\n\n\n\nDataset statistics.\n\nrel-amazon covers 3 relational tables and contains 1.85M customers, 21.9M reviews, 506K products. This relational database spans from 1996-06-25 to 2018-09-28. The validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is set to 2014-01-21 and the testing timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is 2016-01-01. Thus, tasks can have a window size up to 2 years.\n\n\n\n\n4.3.1 rel-amazon-ltv: Predict the life time value (LTV) of a user\n\nTask definition: Predict the life time value of a user, defined as the sum of prices of the products that the user will buy and review in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that wrote review in the past two years before the timestamp.\n\n\nTask significance: By accurately forecasting LTV, the e-commerce platform can gain insights into user purchasing patterns and preferences, which is essential when making strategic decisions related to marketing, product recommendations, and inventory management. Understanding a user\u2019s future purchasing behavior helps in tailoring personalized shopping experiences and optimizing product assortments, ultimately enhancing customer satisfaction and loyalty.\n\n\nMachine learning task: Regression. The target ranges from $0-$33,858.4 in the given time window in the training table.\n\n\nEvaluation metric: Mean Absolute Error (MAE).\n\n\n\n\n4.3.2 rel-amazon-churn: Predict if the user churns\n\nTask definition: Predict if the user will not buy any product in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that wrote review in the past two years before the timestamp.\n\n\nTask significance: Predicting churn accurately allows companies to identify potential risks of customer attrition early on. By understanding which customers are at risk of disengagement, businesses can implement targeted interventions to improve customer retention. This may include personalized marketing, tailored offers, or enhanced customer service. Effective churn prediction enables businesses to maintain a stable customer base, ensuring sustained revenue streams and facilitating long-term planning and resource allocation.\n\n\n\nMachine learning task: Binary classification. The label is 1 when user churns and 0 vice versus.\n\n\nEvaluation metric: Average precision (AP).\n\n\n\n\n\n4.4 rel-stackex: Stack exchange question-and-answer website database\n\nFigure 7: Entity relational diagrams of Stack-Exchange.\n\n\nDatabase overview.\n\nStack Exchange is a network of question-and-answer websites on topics in diverse fields, each site covering a specific topic, where questions, answers, and users are subject to a reputation award process. The reputation system allows the sites to be self-moderating. In our benchmark, we use the stats-exchange site. We derive from the raw data dump from 2023-09-12. Figure\u00a07 shows its entity relational diagrams.\n\n\n\nDataset statistics.\n\nrel-stackex covers 7 relational tables and contains 333K users, 415K posts, 794K comments, 1.67M votes, 103K post links, 590K badges records, 1.49M post history records. This relational database spans from 2009-02-02 to 2023-09-03. The validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is set to be 2019-01-01 and the testing timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is set to be 2021-01-01. Thus, the maximum time window size for predictive task is 2 years.\n\n\n\n\n4.4.1 rel-stackex-engage: Predict if a user will be an active contributor to the site\n\nTask definition: Predict if the user will make any contribution, defined as vote, comment, or post, to the site in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that have made at least one comment/post/vote before the timestamp.\n\n\nTask significance: By accurately forecasting the levels of user contribution, website administrators can effectively gauge and oversee user activity. This insight allows for well-informed choices across various business aspects. For instance, it aids in preempting and mitigating user attrition, as well as in enhancing strategies to foster increased user interaction and involvement. This predictive task serves as a crucial tool in optimizing user experience and sustaining a dynamic and engaged user base.\n\n\nMachine learning task: Binary classification. The label is 1 when user contributes to the site and 0 otherwise.\n\n\nEvaluation metric: Average Precision (AP).\n\n\n\n\n4.4.2 rel-stackex-votes: Predict the number of upvotes a question will receive\n\nTask definition: Predict the popularity of a question post in the next six months. The popularity is defined as the number of upvotes the post will receive.\n\n\nEntity filtering: We filter on question posts that are posted recently in the past 2 years before the timestamp. This ensures that we do not predict on old questions that have been outdated.\n\n\nTask significance: Predicting the popularity of a question post is valuable as it empowers site managers to predict and prepare for the influx of traffic directed towards that particular post. This foresight is instrumental in making strategic business decisions, such as curating question recommendations and optimizing content visibility. Understanding which posts are likely to attract more attention helps in tailoring the user experience and managing resources effectively, ensuring that the most engaging and relevant content is highlighted to maintain and enhance user engagement.\n\n\nMachine learning task: Regression. The target ranges from 0-52 number of upvotes in the given time window in the training table.\n\n\nEvaluation metric: Mean Absolute Error (MAE).\n\n\n\n",
      "5 A New Program for Graph Representation Learning": "\n\n5 A New Program for Graph Representation Learning\n\nFigure 8: Relational Deep Learning brings new challenges at all levels of the machine learning stack.\n\n\nDeveloping Relational Deep Learning requires a new research program in graph representation learning on relational data. There are opportunities at all levels of the research stack, including (pre-)training methods, GNN architectures, multimodality, new graph formulations, and scaling to large distributed relational databases for many industrial settings. Here we discuss several promising aspects of this research program, aiming to stimulate the interest of the graph machine learning community.\n\n\n\n5.1 Scaling Relational Deep Learning\n\nRelational databases are often vast, with information distributed across many servers with constrained communication. However, relational data has a non-typical graph structure which may help scale Relational Deep Learning more efficiently.\n\n\nDistributed Training on Relational Data.\n\nExisting distributed machine learning techniques often assume that each server contains data of the same type. Relational data on the other hand, naturally partitions in to pieces, bringing new challenges depending on the partitioning technique used. Horizontal partitioning, known as sharding, is the most common approach. It creates database shards by splitting tables row-wise according to some criterion (e.g., all customer with zipcode in a given range). In this case, a table containing a customers personal record may lie on a distinct server from the table recording recent purchase activity, leading to communication bottlenecks when attempting to train models by sensing messages between purchases and customer records. Less common, but also possible, is vertical splitting. Different splitting options suggests an opportunity to (a) develop specialized distributed learning methods that exploit the vertical or horizontal partitioning, and (b) design further storage arrangements that may be more suited to Relational Deep Learning. The question of graph partitioning arises in all large-scale graph machine learning settings, however in this case we are fortunate to have non-typical graph structure (i.e., it follows the schema) which makes it easier to find favourable partitions.\n\n\n\nLocalized Learning.\n\nFor many predictive tasks it is neither feasible (due to database size) nor desirable (due to task narrowness) to propagate GNN messages across the entire graph during training. In such cases, sampling schemes that preserve locality by avoiding exponential growth in GNN receptive field are needed. This is easily addressed in cases with prior knowledge on the relevant entities. How to scale to deep models that remain biased models towards local computation in cases with no prior knowledge remains an interesting open question.\n\n\n\n\n\n5.2 Building Graphs from Relational Data\n\nAn essential ingredient of Relational Deep Learning is the use of an individual entity and relation-level graph on which to apply inter-entity message passing to learn entity embeddings based on relations to other entities. In Sec. 3.2 we introduced one such graph, the relational entity graph, a general procedure for viewing any relational database as a graph. Whilst a natural choice, we do not propose dogmatically viewing entities as nodes and relations as edges. Instead, the essential property of the relational entity graph is that it is full-resolution. That is, each entity and each primary-foreign key link in the relational database corresponds to its own graph-piece, so that the relational database is exactly encoded in graph form. It is this property that we expect potential alternative graph designs to share. Beyond this stipulation, many creative graph choices are possible, and we discuss some possibilities here.\n\n\nForeign-key Hypergraph.\n\nFact tables often contain entities with a fixed foreign-key pattern (e.g., in rel-amazon a row in a review table always refers to a customer and a product foreign key). The relational entity graph views a review as a node, with edges to a customer and product. However, another possibility is to view this as a single hyperedge between review, customer, and product. Alternative graph choices may alter (and improve) information propagation between entities (cf. Sec. 5.3).\n\n\n\nStale Link Pruning.\n\nEntities that have been active for a long time may have a lot of links to other entities. Many of these links may be stale, or uninformative, for certain tasks. For example, the purchasing patterns of a longtime customer during childhood are likely to be less relevant to their purchasing patterns in adulthood. Links that are stale for a certain task may hurt predictive power by obfuscating true predictive signals, and reduce model efficiency due to processing uninformative data. This situation calls for careful stale link and entity handling to focus on relevant information. Promising methods may include pruning or preaggregating stale links. More generally, how to deal with more gradual distribution drift over time is an open question.\n\n\n\n\n\n5.3 GNN Architectures for Relational Data\n\nViewing a relational database a graphs leads to graphs with structural properties that are consistent across databases. To properly exploit this structure new specialized GNN architectures are needed. Here we discuss several concrete directions for designing new architectures.\n\n\nExpressive GNNs for Relational Data.\n\nRelational entity graphs (cf. Sec. 3.2) obey certain structural constraints. For example, as nodes correspond to entities drawn from one of several tables, the relational entity graph is naturally n\ud835\udc5bnitalic_n-partite, where n\ud835\udc5bnitalic_n is the total number of tables. This suggests that GNNs for relational data should be designed to be capable of learning expressive decision rules over n\ud835\udc5bnitalic_n-partite graphs. Unfortunately, recent studies find that many GNN architectures fail to distinguish biconnected graphs (Zhang et\u00a0al., 2023). Further work is needed to design expressive n\ud835\udc5bnitalic_n-partite graph models.\n\n\nRelational entity graphs also have regularity in edge-connectivity. For instance, in rel-amazon entities in the review table always refer to one customer and one product.\nConsistent edge patterns are described by the structure of the schema graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) (cf. Sec. 3.1). How to integrate prior knowledge of the graph structure of (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) into GNNs that operate on an entity-level graph (the relational entity graph) remains an open question. These two examples serve only to illustrate the possibilities for architecture design based on the structure of relational entity graphs. Many other structural properties of relational data may lead to innovative new expressive GNN architectures.\n\n\n\nQuery Language Inspired Models.\n\nSQL operations are known to be extremely powerful operations for manipulating relational data. Their weakness is that they do not have differentiable parameters, making end-to-end learnability impossible. Despite this, there are close similarities between key SQL queries and the computation process of graph neural networks. For instance, a very common way to combine information across tables T1,T2subscript\ud835\udc471subscript\ud835\udc472T_{1},T_{2}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT in SQL is to (1) create a table T3subscript\ud835\udc473T_{3}italic_T start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT by applying a JOIN operation to table T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, by matching foreign keys in T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to primary keys in T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, then (2) produce a final table with the same number of rows as T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT by applying an AGGREGATE operation to rows in T3subscript\ud835\udc473T_{3}italic_T start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT with foreign keys pointing to the same entity in T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. There are many choices of AGGREGATE operation such as SUM, MEAN and COUNT. This process directly mirrors GNN computations of messages from neighboring nodes, followed by message aggregation. In other words, GNNs can be thought of as a neural version of SQL JOIN+AGGREGATE operations. This suggests that an opportunity for powerful new neural network architectures by designing differentiable computation blocks that algorithmically align (Xu et\u00a0al., 2020) to existing SQL operations that are known to be useful.\n\n\n\nNew Message Passing Schemes.\n\nBeyond expressivity, new architectures may also improve information propagation between entities. For instance, collaborative filtering methods enhance predictions by identify entities with similar behavior patterns, customers with similar purchase history. However, in the relational entity graph, the two related customers may not be directly linked. Instead they are indirectly be linked to one another through links to their respective purchases, which are linked to a particular shared product ID. This means that a standard message passing GNN will require four message passing steps to propagate the information that customer v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT purchased the same product as customer v2subscript\ud835\udc632v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (2-hops from v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to product, and 2-hops from product to v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT). New message passing schemes that do multiple hops or directly connect customers (more generally, entities) with similar behavior patterns may more effectively propagate key information through the model. As well as new message passing schemes, there is also opportunity for new message aggregation methods. One possibility is order dependent aggregation, that combines messages in a time-dependent way, as explored by Yang et\u00a0al. (2022). Another is schema dependent aggregation, that combines messages based on what part of the schema graph the messages are arriving from.\n\n\n\n\n\n\n5.4 Training Techniques for Relational Data\n\nBy its nature, relational data contains highly overlapping predictive signals and tasks. This interconnectedness of data and tasks is a big opportunity for new neural network training methods that maximally take advantage of this interconnectedness to identify useful predictive signals. This section discusses several such opportunities.\n\n\nMulti-Task Learning.\n\nMany predictive tasks on relational data are distinct but related. For example, predicting customer lifetime value, and forecasting individual product sales both involve anticipating future purchase patterns. In RelBench, this corresponds to defining multiple training tables, one for each task, and training a single model jointly on all tasks in order to benefit from shared predictive signals. How to group training tables to leverage their overlap is a promising area for further study.\n\n\n\nMulti-Modal Learning.\n\nEntities in relational databases often have attributes covering multiple modalities (e.g., products come with images, descriptions, as well as different categorical and numerical features). The Relational Deep Learning blueprint first extracting entity-level features, which are used as initial node-features for the GNN model.\nIn RelBench, this multimodal entity-level feature extraction is handled by using state-of-the-art pre-trained models using the PyTorch Frame library to pre-extract features. This maximizes convenience for graph-focused research, but is likely suboptimal because the entity-level feature extraction model is frozen. This is especially relevant in contexts with unusual data\u2014e.g., specialized medical documentation\u2014that generic pre-trained models will likely fail to extract important details.\nTo facilitate exploration of joint entity-level and graph-level modelling, RelBench also provides the option to load raw data, to allow researchers to experiment with different feature-extraction methods.\n\n\n\nFoundation Models and Data Type Encoders.\n\nIn practice, new predictive tasks on relational data are often specified on-the-fly, with fast responses required. Such situations preclude costly model training from scratch, instead requiring powerful and generic pre-trained models. Self-supervised labels for model pre-training can be mined from historical data, just as with training table construction. However, techniques for automatically deciding which labels to mine remains unexplored. Another desirable property of pre-trained models is that they are inductive, so they can be applied to entirely new relational databases out-of-the-box. This presents a challenges in how to deal with unseen column types and relations between tables. Such flexibility is needed in order to move towards foundation models for relational databases. More broadly, how to build column encoders is an important question. As well as distribution shifts as mentioned in the previous paragraph, there are also decisions on when to share column encoders (should two image columns use the same image encoder?), as well as special data types such as static time intervals (e.g., to describe the time period an employee worked at a company, or the time period in which a building project was conducted). Special data types may require specialized encoder choices, and possibly even deeper integration into the neural network computation graph. How best to aggregate of cross-modal information into a single fused embedding is another question for exploration.\n\n\n\n",
      "6 Related Work": "\n\n6 Related Work\n\nRelational Deep Learning touches on many areas of related work which we survey next.\n\n\nStatistical Relational Learning.\n\nSince the foundation of the field of AI, sought to design systems capable of reasoning about entities and their relations, often by explicitly building graph structures (Minsky, 1974). Each new era of AI research also brought its own form of relational learning. A prominent instance is statistical relational learning (De\u00a0Raedt, 2008), a common form of which seeks to describe objects and relations in terms of first-order logic, fused with graphical models to model uncertainty (Getoor et\u00a0al., 2001). These descriptions can then be used to generate new \u201cknowledge\u201d through inductive logic programming (Lavrac and Dzeroski, 1994). Markov logic networks, a prominent statistical relational approach, are defined by a collection of first-order logic formula with accompanying scalar weights (Richardson and Domingos, 2006). This information is then used to define a probability distribution over possible worlds (via Markov random fields) which enables probabilistic reasoning about the truth of new formulae. We see Relational Deep Learning as inheriting this lineage, since both approaches operate on data with rich relational structure, and both approaches integrate relational structure into the model design. Of course, there are important distinctions between the two methods too, such as the natural scalability of graph neural network-based methods, and that Relational Deep Learning does not rely on first-order logic to describe data, allowing broad applicability to relations that are hard to fit into this form.\n\n\n\nTabular Machine Learning.\n\nTree based methods, notably XGBoost (Chen and Guestrin, 2016), remain key workhorses of enterprise machine learning systems due to their scalability and reliability. In parallel, efforts to design deep learning architectures for tabular data have continued (Huang et\u00a0al., 2020; Arik and Pfister, 2021; Gorishniy et\u00a0al., 2021, 2022; Chen et\u00a0al., 2023), but have struggled to clearly establish dominance over tree-based methods (Shwartz-Ziv and Armon, 2022). The vast majority of tabular machine learning focuses on the single table setting, which we argue forgoes use of the rich interconnections between relational data. As such, it does not address the key problem, which is how to get the data from a multi-table to a single table representation. Recently, a nascent body of work has begun to consider multiple tables. For instance, Zhu et\u00a0al. (2023) pre-train tabular Transformers that generalize to new tables with unseen columns.\n\n\n\nKnowledge Graph Embedding.\n\nKnowledge graphs store relational data, and highly scalable knowledge graph embeddings methods have been developed over the last decade to embed data into spaces whose geometry reflects the relational struture\u00a0(Bordes et\u00a0al., 2013; Wang et\u00a0al., 2014, 2017). Whilst also dealing with relational data, this literature differs from this present work in the task being solved. The key task of knowledge graph embeddings is to predict missing entities (Q: Who was Yann LeCun\u2019s postdoc advisor? A: Geoffrey Hinton) or relations (Q: Did Geoffrey Hinton win a Turing Award? A: Yes). To assist in such completion tasks, knowledge graph methods learn an embedding space with the goal of exactly preserving the relation semantics between entities. This is different from Relational Deep Learning, which aims to make predictions about entities, or groups of entities. Because of this, Relational Deep Learning seeks to leverage relations to learn entity representations, but does not need to learn an embedding space that perfectly preserves all relation semantics. This gives more freedom and flexibility to our models, which may discard certain relational information it finds unhelpful. Nonetheless, adopting ideas from knowledge graph embedding may yet be fruitful.\n\n\n\nDeep Learning on Relational Data.\n\nProposals to use message passing neural networks on relational data have occasionally surfaced within the research community. In particular, Schlichtkrull et\u00a0al. (2018); Cvitkovic (2019); \u0160\u00edr (2021), and Zahradn\u00edk et\u00a0al. (2023) make the connection between relational data and graph neural networks and explore it with different network architectures, such as heterogeneous message passing. However, our aim is to move beyond the conceptual level, and clearly establish deep learning on relational data as a subfield of machine learning. Accordingly, we focus on the components needed to establish this new area and attract broader interest: (1) a clearly scoped design space for neural network architectures on relational data, (2) a carefully chosen suite of benchmark databases and predictive tasks around which the community can center its efforts, (3) standardized data loading and splitting, so that temporal leakage does not contaminate experimental results, (4) recognizing time as a first-class citizen, integrated into all sections of the experimental pipeline, including temporal data splitting, time-based forecasting tasks, and temporal-based message passing, and (5) standardized evaluation protocols to ensure comparability between reported results.\n\n\n",
      "7 Conclusion": "\n\n7 Conclusion\n\nA large proportion of the worlds data is natively stored in relational tables. Fully exploiting the rich signals in relational data therefore has the potential to rewrite what problems computing can solve. We believe that Relational Deep Learning will make it possible to achieve superior performance on various prediction problems spanning the breadth of human activity, leading to considerable improvements in automated decision making. There is currently a great scientific opportunity to develop the field of Relational Deep Learning, and further refine this vision.\n\n\nThis paper serves as a road map in this pursuit. We introduce a blueprint for a neural network architecture that directly processes relational data by casting predictive tasks as graph representation learning problems. In Sec. 5 we discuss the many new challenges and opportunities this presents for the graph machine learning community. To facilitate research, we introduce RelBench, a set of benchmark datasets, and a Python package for data loading, and model evaluation.\n\n\n\nAcknowledgments.\n\nWe thank Shirley Wu for useful discussions as we were selecting datasets to adopt. We gratefully acknowledge the support of DARPA under Nos. N660011924033 (MCS); NSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Initiative, Amazon, Genentech, GSK, Hitachi, Juniper Networks, and KDDI.\n\n\n"
    },
    "table_captions": null,
    "figure_captions": null,
    "bibliography": {
      "Amodei et\u00a0al. (2016)": "\nAmodei et\u00a0al. (2016)\n\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric\nBattenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang\nChen, et\u00a0al.\n\n\nDeep speech 2: End-to-end speech recognition in english and mandarin.\n\n\nIn International Conference on Machine Learning (ICML), 2016.\n\n\n",
      "Arik and Pfister (2021)": "\nArik and Pfister (2021)\n\nSercan\u00a0\u00d6 Arik and Tomas Pfister.\n\n\nTabnet: Attentive interpretable tabular learning.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume\u00a035, pages 6679\u20136687, 2021.\n\n\n",
      "Bordes et\u00a0al. (2013)": "\nBordes et\u00a0al. (2013)\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\nYakhnenko.\n\n\nTranslating embeddings for modeling multi-relational data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2013.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom\u00a0B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net\u00a0al.\n\n\nLanguage models are few-shot learners.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n",
      "Chamberlin and Boyce (1974)": "\nChamberlin and Boyce (1974)\n\nDonald\u00a0D Chamberlin and Raymond\u00a0F Boyce.\n\n\nSequel: A structured english query language.\n\n\nIn Proceedings of the 1974 ACM SIGFIDET (now SIGMOD) workshop\non Data description, access and control, pages 249\u2013264, 1974.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nKuan-Yu Chen, Ping-Han Chiang, Hsin-Rung Chou, Ting-Wei Chen, and Tien-Hao\nChang.\n\n\nTrompt: Towards a better deep neural network for tabular data.\n\n\nIn International Conference on Machine Learning (ICML), 2023.\n\n\n",
      "Chen and Guestrin (2016)": "\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n",
      "Codd (1970)": "\nCodd (1970)\n\nEdgar\u00a0F Codd.\n\n\nA relational model of data for large shared data banks.\n\n\nCommunications of the ACM, 13(6):377\u2013387,\n1970.\n\n\n",
      "Cvitkovic (2019)": "\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n",
      "DB-Engines (2023)": "\nDB-Engines (2023)\n\nDB-Engines.\n\n\nDBMS popularity broken down by database model, 2023.\n\n\nAvailable: https://db-engines.com/en/ranking_categories.\n\n\n",
      "De\u00a0Raedt (2008)": "\nDe\u00a0Raedt (2008)\n\nLuc De\u00a0Raedt.\n\n\nLogical and relational learning.\n\n\nSpringer Science & Business Media, 2008.\n\n\n",
      "Devlin et\u00a0al. (2018)": "\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n",
      "Fey and Lenssen (2019)": "\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n",
      "Garcia-Molina et\u00a0al. (2008)": "\nGarcia-Molina et\u00a0al. (2008)\n\nHector Garcia-Molina, Jeffrey\u00a0D. Ullman, and Jennifer Widom.\n\n\nDatabase Systems: The Complete Book.\n\n\nPrentice Hall Press, USA, 2 edition, 2008.\n\n\nISBN 9780131873254.\n\n\n",
      "Geirhos et\u00a0al. (2020)": "\nGeirhos et\u00a0al. (2020)\n\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,\nWieland Brendel, Matthias Bethge, and Felix\u00a0A Wichmann.\n\n\nShortcut learning in deep neural networks.\n\n\nNature Machine Intelligence, 2(11):665\u2013673, 2020.\n\n\n",
      "Getoor et\u00a0al. (2001)": "\nGetoor et\u00a0al. (2001)\n\nLise Getoor, Nir Friedman, Daphne Koller, and Avi Pfeffer.\n\n\nLearning probabilistic relational models.\n\n\nRelational data mining, pages 307\u2013335, 2001.\n\n\n",
      "Gilmer et\u00a0al. (2017)": "\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n",
      "Gorishniy et\u00a0al. (2021)": "\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n",
      "Gorishniy et\u00a0al. (2022)": "\nGorishniy et\u00a0al. (2022)\n\nYury Gorishniy, Ivan Rubachev, and Artem Babenko.\n\n\nOn embeddings for numerical features in tabular deep learning.\n\n\nAdvances in Neural Information Processing Systems,\n35:24991\u201325004, 2022.\n\n\n",
      "Hamilton et\u00a0al. (2017)": "\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Hannun et\u00a0al. (2014)": "\nHannun et\u00a0al. (2014)\n\nAwni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich\nElsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et\u00a0al.\n\n\nDeep speech: Scaling up end-to-end speech recognition.\n\n\narXiv preprint arXiv:1412.5567, 2014.\n\n\n",
      "He et\u00a0al. (2016)": "\nHe et\u00a0al. (2016)\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\n\nDeep residual learning for image recognition.\n\n\nIn IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 770\u2013778, 2016.\n\n\n",
      "Hu et\u00a0al. (2023)": "\nHu et\u00a0al. (2023)\n\nWeihua Hu, Matthias Fey, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao,\nand Vid Kocijan.\n\n\nPyTorch Frame: A Deep Learning Framework for Tabular Data, October\n2023.\n\n\nURL https://github.com/pyg-team/pytorch-frame.\n\n\n",
      "Hu et\u00a0al. (2020)": "\nHu et\u00a0al. (2020)\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun.\n\n\nHeterogeneous graph transformer.\n\n\nIn Proceedings of The Web Conference 2020, page 2704\u20132710,\n2020.\n\n\n",
      "Huang et\u00a0al. (2020)": "\nHuang et\u00a0al. (2020)\n\nXin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin.\n\n\nTabtransformer: Tabular data modeling using contextual embeddings.\n\n\narXiv preprint arXiv:2012.06678, 2020.\n\n\n",
      "Johnson et\u00a0al. (2016)": "\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n",
      "Kaggle (2022)": "\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n",
      "Kapoor and Narayanan (2023)": "\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n",
      "Lavrac and Dzeroski (1994)": "\nLavrac and Dzeroski (1994)\n\nNada Lavrac and Saso Dzeroski.\n\n\nInductive logic programming.\n\n\nIn WLP, pages 146\u2013160. Springer, 1994.\n\n\n",
      "Minsky (1974)": "\nMinsky (1974)\n\nMarvin Minsky.\n\n\nA framework for representing knowledge, 1974.\n\n\n",
      "PubMed (1996)": "\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n",
      "Reimers and Gurevych (2019)": "\nReimers and Gurevych (2019)\n\nNils Reimers and Iryna Gurevych.\n\n\nSentence-bert: Sentence embeddings using siamese bert-networks.\n\n\narXiv preprint arXiv:1908.10084, 2019.\n\n\n",
      "Richardson and Domingos (2006)": "\nRichardson and Domingos (2006)\n\nMatthew Richardson and Pedro Domingos.\n\n\nMarkov logic networks.\n\n\nMachine learning, 62:107\u2013136, 2006.\n\n\n",
      "Rossi et\u00a0al. (2020)": "\nRossi et\u00a0al. (2020)\n\nEmanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico\nMonti, and Michael Bronstein.\n\n\nTemporal graph networks for deep learning on dynamic graphs.\n\n\nICML Workshop on Graph Representation Learning 2020, 2020.\n\n\n",
      "Russakovsky et\u00a0al. (2015)": "\nRussakovsky et\u00a0al. (2015)\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et\u00a0al.\n\n\nImagenet large scale visual recognition challenge.\n\n\nInternational journal of computer vision, 115(3):211\u2013252, 2015.\n\n\n",
      "Schlichtkrull et\u00a0al. (2018)": "\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n",
      "Shwartz-Ziv and Armon (2022)": "\nShwartz-Ziv and Armon (2022)\n\nRavid Shwartz-Ziv and Amitai Armon.\n\n\nTabular data: Deep learning is not all you need.\n\n\nInformation Fusion, 81:84\u201390, 2022.\n\n\n",
      "\u0160\u00edr (2021)": "\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n",
      "Varma and Zisserman (2005)": "\nVarma and Zisserman (2005)\n\nManik Varma and Andrew Zisserman.\n\n\nA statistical approach to texture classification from single images.\n\n\nInternational journal of computer vision, 62:61\u201381,\n2005.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan\u00a0N Gomez, Lukasz Kaiser, and Illia Polosukhin.\n\n\nAttention is all you need.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Wang et\u00a0al. (2017)": "\nWang et\u00a0al. (2017)\n\nQuan Wang, Zhendong Mao, Bin Wang, and Li\u00a0Guo.\n\n\nKnowledge graph embedding: A survey of approaches and applications.\n\n\nIEEE Transactions on Knowledge and Data Engineering,\n29(12):2724\u20132743, 2017.\n\n\n",
      "Wang et\u00a0al. (2021)": "\nWang et\u00a0al. (2021)\n\nYiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, and Bryan\nHooi.\n\n\nTime-aware neighbor sampling for temporal graph networks.\n\n\nIn arXiv pre-print, 2021.\n\n\n",
      "Wang et\u00a0al. (2014)": "\nWang et\u00a0al. (2014)\n\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.\n\n\nKnowledge graph embedding by translating on hyperplanes.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume\u00a028, 2014.\n\n\n",
      "Xu et\u00a0al. (2020)": "\nXu et\u00a0al. (2020)\n\nKeyulu Xu, Jingling Li, Mozhi Zhang, Simon\u00a0S Du, Ken-ichi Kawarabayashi, and\nStefanie Jegelka.\n\n\nWhat can neural networks reason about?\n\n\nIn International Conference on Learning Representations\n(ICLR), 2020.\n\n\n",
      "Yang et\u00a0al. (2022)": "\nYang et\u00a0al. (2022)\n\nZhen Yang, Ming Ding, Bin Xu, Hongxia Yang, and Jie Tang.\n\n\nStam: A spatiotemporal aggregation method for graph neural\nnetwork-based recommendation.\n\n\nIn Proceedings of the ACM Web Conference 2022, pages\n3217\u20133228, 2022.\n\n\n",
      "Zahradn\u00edk et\u00a0al. (2023)": "\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n",
      "Zhang et\u00a0al. (2023)": "\nZhang et\u00a0al. (2023)\n\nBohang Zhang, Shengjie Luo, Liwei Wang, and Di\u00a0He.\n\n\nRethinking the expressive power of gnns via graph biconnectivity.\n\n\nIn International Conference on Learning Representations\n(ICLR), 2023.\n\n\n",
      "Zhu et\u00a0al. (2023)": "\nZhu et\u00a0al. (2023)\n\nBingzhao Zhu, Xingjian Shi, Nick Erickson, Mu\u00a0Li, George Karypis, and Mahsa\nShoaran.\n\n\nXtab: Cross-table pretraining for tabular transformers.\n\n\nIn International Conference on Machine Learning (ICML), 2023.\n\n\n"
    },
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "538ea671-a783-4bd8-8f94-f303583a03e7": {
    "pk": "538ea671-a783-4bd8-8f94-f303583a03e7",
    "project_name": null,
    "authors": [
      "Joshua Robinson",
      "Rishabh Ranjan",
      "Weihua Hu",
      "Kexin Huang",
      "Jiaqi Han",
      "Alejandro Dobles",
      "Matthias Fey",
      "Jan E. Lenssen",
      "Yiwen Yuan",
      "Zecheng Zhang",
      "Xinwei He",
      "Jure Leskovec"
    ],
    "title": "RelBench: A Benchmark for Deep Learning on Relational Databases",
    "abstract": "We present RelBench, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RelBench provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RelBench to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RelBench.",
    "url": "http://arxiv.org/abs/2407.20060v1",
    "timestamp": 1722264373,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nRelational databases are the most widely used database management system, underpinning much of the digital economy. Their popularity stems from their table storage structure, making maintenance relatively easy, and data simple to access using powerful query languages such as SQL. Because of their popularity, AI systems across a wide variety of domains are built using data stored in relational databases, including e-commerce, social media, banking systems, healthcare, manufacturing, and open-source scientific repositories\u00a0(Johnson et\u00a0al., 2016; PubMed, 1996).\n\n\nDespite the importance of relational databases, the rich relational information is typically foregone, as no model architecture is capable of handling varied database structures. Instead, data is \u201cflattened\u201d into a simpler format such as a single table, often by manual feature engineering, on which standard tabular models can be used\u00a0(Kaggle, 2022). This results in a significant loss in predictive signal, and creates a need for data extraction pipelines that frequently cause bugs and add to software complexity.\n\n\nFigure 1: RelBench enables training and evaluation of deep learning models on relational databases. RelBench supports framework agnostic data loading, task specification, standardized data splitting, standardized evaluation metrics, and a leaderboard for tracking progress. RelBench also includes a pilot implementation of the relational deep learning blueprint of Fey et\u00a0al. (2024).\n\n\nTo fully exploit the predictive signal encoded in the relations between entities, a new proposal is to re-cast relational data as an exact graph representation, with a node for each entity in the database, edges indicating primary-foreign key links, and node features extracted using deep tabular models, an approach termed Relational Deep Learning (RDL)\u00a0(Fey et\u00a0al., 2024). The graph representation allows Graph Neural Networks (GNNs)\u00a0(Gilmer et\u00a0al., 2017; Hamilton et\u00a0al., 2017) to be used as predictive models. RDL is the first approach for an end-to-end learnable neural network model with access to all possible predictive signal in a relational databases, and has the potential to unlock new levels of predictive power.\nHowever, the development of relational deep learning is limited by a complete lack of infrastructure to support research, including: (i) standardized benchmark databases and tasks to compare methods, (ii) initial implementation of RDL, including converting data to graph form and GNN training, and (iii) a pilot study of the effectiveness of relational deep learning.\n\n\nHere we present RelBench, the first benchmark for relational deep learning. RelBench is intended to be the foundational infrastructure for future research into relational deep learning, providing a comprehensive set of databases across a variety of domains, including e-commerce, Q&A platforms, medical, and sports databases. RelBench databases span orders of magnitude in size, from 74K entities to 41M entities, and have very different time spans, between 2 weeks and 55 years of training data. They also vary significantly in their relational structure, with the total number of tables varying between 3 and 15, and total number of columns varying from 15 to 140.\nEach database comes with multiple predictive tasks, 30 in total, including entity classification/regression and recommendation tasks, each chosen for their real-world significance.\n\n\nIn addition to databases and tasks, we release open-source software designed to make relational deep learning widely available. This includes (i) the RelBench Python package for easy database and task loading, (ii) the first open-source implementation of relational deep learning, designed to be easily modified by researchers, and (iii) a public leaderboard for tracking progress. We comprehensively benchmark our initial RDL implementation on all RelBench tasks, comparing to various baselines.\n\n\nThe most important baseline we compare to is a strong \u201cdata scientist\u201d approach, for which we recruited an experienced individual to solve each task by manually engineering features and feeding them into tabular models. This approach is the current gold-standard for building predictive models on relational databases. The study, which we open source for reproducibility, finds that RDL models match or outperform the data scientist\u2019s models in accuracy, whilst reducing human hours worked by 96%percent9696\\%96 %, and lines of code by 94%percent9494\\%94 % on average. This cons14titutes the first empirical demonstration of the central promise of RDL, and points to\na long-awaited end-to-end deep learning solution for relational data.\n\n\nOur website111https://relbench.stanford.edu. is a comprehensive entry point to RDL, describing RelBench databases and tasks, access to code on GitHub, the full relational deep learning blueprint, and tutorials for adding new databases and tasks to RelBench to allow researchers to experiment with their problems of interest.\n\n",
      "2 Overview and Design": "\n\n2 Overview and Design\n\nRelBench provides a collection of diverse real-world relational databases along with a set of realistic predictive tasks associated with each database. Concretely, we provide:\n\n\n\n\n\u2022\n\nRelational databases, consisting of a set of tables connected via primary-foreign key relationships. Each table has columns storing diverse information about each entity. Some tables also come with time columns, indicating the time at which the entity is created (e.g., transaction date).\n\n\n\n\u2022\n\nPredictive tasks over a relational database, which are defined by a training table\u00a0(Fey et\u00a0al., 2024) with columns for Entity ID, seed time, and target labels.The seed time indicates at which time the target is to be predicted, filtering future data.\n\n\n\n\n\nNext we outline key design principles of RelBench with an emphasis on data curation, data splits, research flexibility, and open-source implementation.\n\n\nData Curation. Relational databases are widespread, so there are many candidate predictive tasks.\nFor the purpose of benchmarking we carefully curate a collection of relational databases and tasks chosen for their rich relational structure and column features. We also adopt the following principles:\n\n\n\n\n\u2022\n\nDiverse domains: To ensure algorithms developed on RelBench will be useful across a wide range of application domains, we select real-world relational databases from diverse domains.\n\n\n\n\u2022\n\nDiverse task types: Tasks cover a wide range of real-world use-cases, including three representative task types: entity classification, entity regression, and recommendation.\n\n\n\n\n\nTable 1: Statistics of RelBench datasets. Datasets vary significantly in the number of tables, total number of rows, and number of columns. In this table, we only count rows available for test inference, i.e., rows upto the test time cutoff.\n\n\n\nName\nDomain\n#Tasks\nTables\nTimestamp (year-mon-day)\n\n\n#Tables\n#Rows\n#Cols\nStart\nVal\nTest\n\n\nrel-amazon\nE-commerce\n7\n3\n15,000,713\n15\n2008-01-01\n2015-10-01\n2016-01-01\n\n\nrel-avito\nE-commerce\n4\n8\n20,679,117\n42\n2015-04-25\n2015-05-08\n2015-05-14\n\n\nrel-event\nSocial\n3\n5\n41,328,337\n128\n1912-01-01\n2012-11-21\n2012-11-29\n\n\nrel-f1\nSports\n3\n9\n74,063\n67\n1950-05-13\n2005-01-01\n2010-01-01\n\n\nrel-hm\nE-commerce\n3\n3\n16,664,809\n37\n2019-09-07\n2020-09-07\n2020-09-14\n\n\nrel-stack\nSocial\n5\n7\n4,247,264\n52\n2009-02-02\n2020-10-01\n2021-01-01\n\n\nrel-trial\nMedical\n5\n15\n5,434,924\n140\n2000-01-01\n2020-01-01\n2021-01-01\n\n\nTotal\n30\n51\n103,466,370\n489\n/\n/\n/\n\n\n\n\nRelBench databases are summarized in Table\u00a01, covering E-commerce, social, medical, and sports domains. The databases vary significantly in the numbers of rows (i.e., data scale) the number of columns and tables, as well as the time ranges of the databases. Tasks are summarized in Table\u00a02, each corresponding to a predictive problem of practical interest such as predicting customer churn, predicting the number of adverse events in a clinical trial, and recommending posts to users.\n\n\nData Splits. Data is split temporally, with models trained on rows up to val_timestamp, validated on the rows between val_timestamp and test_timestamp, and tested on the rows after test_timestamp.\nOur implementation carefully hides data after test_timestamp during inference to systematically avoid test time data leakage\u00a0(Kapoor and Narayanan, 2023), and uses an elegant solution proposed by Fey et\u00a0al. (2024) to avoid time leakage during training and validation through temporal neighbor sampling. In general, it is the designers responsibility to avoid time leakage. We recommend using our carefully tested implementation where possible.\n\n\nResearch Flexibility. RelBench is designed to allow significant freedom in future research directions. For example, RelBench tasks share the same (val_timestamp and test_timestamp) splits across tasks within the same relational database. This opens up exciting opportunities for multi-task learning and pre-training to simultaneously improve different predictive tasks within the same relational database. We also expose the logic for converting databases into graphs. This allows future work to consider modified graph constructions, or creative uses of the raw data.\n\n\nOpen-source RDL Implementation.\nAs well as datasets and tasks, we provide the first open-source implementation of relational deep learning. See Figure 2 of Fey et\u00a0al. (2024) for a high-level overview. A neural network is learned over a heterogeneous temporal graph that exactly represents the database in order to make prediction over nodes (for entity classification and regression) and links (for recommendation). Our implementation is built on top of PyTorch Frame\u00a0(Hu et\u00a0al., 2024) for extracting initial node embeddings from raw table features, and PyTorch Geometric\u00a0(Fey and Lenssen, 2019) for GNN modeling. See Section 3 for further details.\n\n\nThe rest of the paper is organized as follows. Section\u00a04 describes the RelBench relational databases.\nSection\u00a05 introduces predictive tasks for each RelBench databases covering the three task types in Sections\u00a05.1, 5.2, and 5.3, respectively.\nSection\u00a05 also extensively benchmarks our RDL implementation against challenging baselines. Most importantly, we compare to a strong \u201cdata scientist\u201d baseline (Section 6), finding that end-to-end RDL models outperform manual feature engineering, the current gold-standard\nFinally, Section\u00a07 discusses related work and Section\u00a08 draws final conclusions.\n\n",
      "3 Relational Deep Learning Implementation": "\n\n3 Relational Deep Learning Implementation\n\nAs part of RelBench, we provide an initial implementation of relational deep learning, based on the blueprint of\nFey et\u00a0al. (2024).222Code available at: https://github.com/snap-stanford/relbench. Our implementation consists four major components: (1) heterogeneous temporal graph, (2) deep learning model, (3) temporal-aware training of the model, and (4) task-specific loss, which we briefly discuss now.\n\n\nHeterogeneous temporal graph.\nGiven a set of tables with primary-foreigh key relations between them we follow Fey et\u00a0al. (2024) to automatically construct a heterogeneous temporal graph, where each table represents a node type, each row in a table represents a node, and a primary-foreign-key relation between two table rows (nodes) represent an edge between the respective nodes. Some node types are associated with time attributes, representing the timestamp at which a node appears. The heterogeneous temporal graph is represented as a PyTorch Geometric graph object.\nEach node in the heterogeneous graph comes with a rich feature derived from diverse columns of the corresponding table. We use Tensor Frame provided by PyTorch Frame\u00a0(Hu et\u00a0al., 2024) to represent rich node features with diverse column types, e.g., numerical, categorical, timestamp, and text.\n\n\nDeep learning model.\nFirst, we use deep tabular models that encode raw row-level data into initial node embeddings using PyTorch Frame (Hu et\u00a0al., 2024) (specifically, we use the ResNet tabular model\u00a0(Gorishniy et\u00a0al., 2021)). These initial node embeddings are then fed into a GNN to iteratively update the node embeddings based on their neighbors.\nFor the GNN we use the heterogeneous version of the GraphSAGE model\u00a0(Hamilton et\u00a0al., 2017; Fey and Lenssen, 2019) with sum-based neighbor aggregation. Output node embeddings are fed into task-specific prediction heads and are learned end-to-end.\n\n\nTemporal-aware subgraph sampling.\nWe perform temporal neighbor sampling, which samples\na subgraph around each entity node at a given seed time.\nSeed time is the time in history at which the prediction is made. When collecting the information to make a prediction at a given seed time, it is important for the model to only use information from before the seed time and thus not learn from the future (post the seed time). Crucially, when sampling mini-batch subgraphs we make sure that all nodes within the sampled subgraph appear before the seed time\u00a0(Hamilton et\u00a0al., 2017; Fey et\u00a0al., 2024), which systematically avoids time leakage during training.\nThe sampled subgraph is fed as input to the GNN, and trained to predict the target label.\n\n\nTask-specific prediction head and loss.\nFor entity-level classification, we simply apply an MLP on an entity embedding computed by our GNN to make prediction. For the loss function, we use the binary cross entropy loss for entity classification and L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT loss for entity regression.\n\n\nRecommendation requires computing scores between pairs of source nodes and target nodes.\nFor this task type, we consider two representative predictive architectures: two-tower GNN\u00a0(Wang et\u00a0al., 2019) and identity-aware GNN (ID-GNN)\u00a0(You et\u00a0al., 2021). First, the two-tower GNN computes the pairwise scores via inner product between source and target node embeddings, and the standard Bayesian Personalized Ranking loss\u00a0(Rendle et\u00a0al., 2012) is used to train the two-tower model\u00a0(Wang et\u00a0al., 2019).\nSecond, the ID-GNN computes the pairwise scores by applying an MLP prediction head on target entity embeddings computed by GNN for each source entity. The ID-GNN is trained by the standard binary cross entropy loss.\n\n",
      "4 RelBench Datasets": "\n\n4 RelBench Datasets\n\nFigure 2: Example RelBench schema for rel-trial. RelBench databases have complex relational structure and rich column features. \n\n\nRelBench contains 7 datasets each with rich relational structure, providing a challenging environment for developing and comparing relational deep learning methods (see Figure 2 for an example). The datasets are carefully processed from real-world relational databases and span diverse domains and sizes.\nEach database is associated with multiple individual predictive tasks defined in Section\u00a05.\nDetailed statistics of each dataset can be found in Table\u00a01. We briefly describe each dataset.\n\n\nrel-amazon. The Amazon E-commerce database records products, users, and reviews across Amazon\u2019s E-commerce platform. It contains rich information about products and reviews. Products include the price and category of each, reviews have the overall rating, whether the user has actually bought the product, and the text of the review itself. We use the subset of book-related products.\n\n\nrel-f1. The F1 database tracks all-time Formula 1 racing data and statistics since 1950. It provides detailed information for various stakeholders including drivers, constructors, engine manufacturers, and tyre manufacturers. Highlights include data on all circuits (e.g.geographical details), and full historical data from every season. This includes overall standings, race results, and more specific data like practice sessions, qualifying positions, sprints, and pit stops.\n\n\nrel-stack. Stack Exchange is a network of question-and-answer websites on different topics, where questions, answers, and users are subject to a reputation award process. The reputation system allows the sites to be self-moderating. The database includes detailed records of activity including user biographies, posts and comments (with raw text), edit histories, voting, and related posts. In our benchmark, we use the stats-exchange site.\n\n\nTable 2: Full list of predictive tasks for each RelBench dataset (introduced in Table\u00a01). \n\n\nDataset\nTask name\nTask type\n#Rows of training table\n#Unique\n%train/test\n#Dst\n\n\nTrain\nValidation\nTest\nEntities\nEntity Overlap\nEntities\n\n\nrel-amazon\nuser-churn\nentity-cls\n4,732,555\n409,792\n351,885\n1,585,983\n88.0\n\u2014\n\n\nitem-churn\nentity-cls\n2,559,264\n177,689\n166,842\n416,352\n93.1\n\u2014\n\n\nuser-ltv\nentity-reg\n4,732,555\n409,792\n351,885\n1,585,983\n88.0\n\u2014\n\n\nitem-ltv\nentity-reg\n2,707,679\n166,978\n178,334\n427,537\n93.5\n\u2014\n\n\nuser-item-purchase\nrecommendation\n5,112,803\n351,876\n393,985\n1,632,909\n87.4\n12,562,384\n\n\nuser-item-rate\nrecommendation\n3,667,157\n257,939\n292,609\n1,481,360\n81.0\n7,665,611\n\n\n\nuser-item-review\nrecommendation\n2,324,177\n116,970\n127,021\n894,136\n74.1\n5,406,835\n\n\nrel-avito\nad-ctr\nentity-reg\n5,100\n1,766\n1,816\n4,997\n59.8\n\u2014\n\n\nuser-clicks\nentity-cls\n59,454\n21,183\n47,996\n66,449\n45.3\n\u2014\n\n\nuser-visits\nentity-cls\n86,619\n29,979\n36,129\n63,405\n64.6\n\u2014\n\n\nuser-ad-visit\nrecommendation\n86,616\n29,979\n36,129\n63,402\n64.6\n3,616,174\n\n\nrel-event\nuser-attendance\nentity-reg\n19,261\n2,014\n2,006\n9,694\n14.6\n\u2014\n\n\nuser-repeat\nentity-cls\n3,842\n268\n246\n1,514\n11.5\n\u2014\n\n\nuser-ignore\nentity-cls\n19,239\n4,185\n4,010\n9,799\n21.1\n\u2014\n\n\nrel-f1\ndriver-dnf\nentity-cls\n11,411\n566\n702\n821\n50.0\n\u2014\n\n\ndriver-top3\nentity-cls\n1,353\n588\n726\n134\n50.0\n\u2014\n\n\ndriver-position\nentity-reg\n7,453\n499\n760\n826\n44.6\n\u2014\n\n\nrel-hm\nuser-churn\nentity-cls\n3,871,410\n76,556\n74,575\n1,002,984\n89.7\n\u2014\n\n\nitem-sales\nentity-reg\n5,488,184\n105,542\n105,542\n105,542\n100.0\n\u2014\n\n\nuser-item-purchase\nrecommendation\n3,878,451\n74,575\n67,144\n1,004,046\n89.2\n13,428,473\n\n\nrel-stack\nuser-engagement\nentity-cls\n1,360,850\n85,838\n88,137\n88,137\n97.4\n\u2014\n\n\nuser-badge\nentity-cls\n3,386,276\n247,398\n255,360\n255,360\n96.9\n\u2014\n\n\npost-votes\nentity-reg\n2,453,921\n156,216\n160,903\n160,903\n97.1\n\u2014\n\n\nuser-post-comment\nrecommendation\n21,239\n825\n758\n11,453\n59.9\n44,940\n\n\npost-post-related\nrecommendation\n5,855\n226\n258\n5,924\n8.5\n7,456\n\n\nrel-trial\nstudy-outcome\nentity-cls\n11,994\n960\n825\n13,779\n0.0\n\u2014\n\n\nstudy-adverse\nentity-reg\n43,335\n3,596\n3,098\n50,029\n0.0\n\u2014\n\n\nsite-success\nentity-reg\n151,407\n19,740\n22,617\n129,542\n42.0\n\u2014\n\n\ncondition-sponsor-run\nrecommendation\n36,934\n2,081\n2,057\n3,956\n98.4\n533,624\n\n\nsite-sponsor-run\nrecommendation\n669,310\n37,003\n27,428\n445,513\n48.3\n1,565,463\n\n\n\n\nrel-trial. The clinical trial database is curated from AACT initiative, which consolidates all protocol and results data from studies registered on ClinicalTrials.gov. It offers extensive information about clinical trials, including study designs, participant demographics, intervention details, and outcomes. It is an important resource for health research, policy making, and therapeutic development.\n\n\nrel-hm. The H&M relational database hosts extensive customer and product data for online shopping experiences across its extensive network of brands and stores. This database includes detailed customer purchase histories and a rich set of metadata, encompassing everything from basic demographic information to extensive details about each product available.\n\n\nrel-event. The Event Recommendation database is obtained from user data on a mobile app called Hangtime. This app allows users to keep track of their friends\u2019 social plans. The database contains data on user actions, event metadata, and demographic information, as well as users\u2019 social relations, which captures how social relations can affect user behavior. Data is fully anonymized, with no personally identifiable information (such as names or aliases) available.\n\n\nrel-avito. Avito is a leading online advertisement platform, providing a marketplace for users to buy and sell a wide variety of products and services, including real estate, vehicles, jobs, and goods. The Avito Context Ad Clicks dataset on Kaggle is part of a competition aimed at predicting whether an ad will be clicked based on contextual information. This dataset includes user searches, ad attributes, and other related data to help build predictive models.\n\n\nData Provenance. All data is sourced from publicly available repositories with licenses permitting usage for research purposes. See Appendix D for details of data sources, licenses, and more.\n\n",
      "5 Predictive Tasks on RelBench Datasets": "\n\n5 Predictive Tasks on RelBench Datasets\n\nRelBench introduces 30 new predictive tasks defined over the databases introduced in Section\u00a04.\nA full list of tasks is given in Table\u00a02, with high-level descriptions given in Appendix A (and our website) due to space limitations.\nTasks are grouped into three task types: entity classification (Section\u00a05.1), entity regression (Section\u00a05.2), and entity link prediction (Section\u00a05.3). Tasks differ significantly in the number of train/val/test entities, number of unique entities (the same entity may appear multiple times at different timestamps), and the proportion of test entities seen during training. Note this is not data leakage, since entity predictions are timestamp dependent, and can change over time. Tasks with no overlap are pure inductive tasks, whilst other tasks are (partially) transductive.\n\n\n\n5.1 Entity Classification\n\nTable 3: Entity classification results (AUROC, higher is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nThe first task type is entity-level classification. The task is to predict binary labels of a given entity at a given seed time. We use the ROC-AUC\u00a0(Hanley and McNeil, 1983) metric for evaluation (higher is better). We compare to a LightGBM classifier baseline over the raw entity table features. Note that here only information from the single entity table is used.\n\n\n\n\nExperimental results.\n\nResults are given in Table 5.1, with RDL outperforming or matching baselines in all cases. Notably, LightGBM achieves similar performance to RDL on the study-outcome task from rel-trial. This task has extremely rich features in the target table (28 columns total), giving the LightGBM many potentially useful features even without feature engineering. It is an interesting research question how to design RDL models better able to extract these features and unify them with cross-table information in order to outperform the LightGBM model on this dataset.\n\n\n\n5.2 Entity Regression\n\nEntity-level regression tasks involve predicting numerical labels of an entity at a given seed time. We use Mean Absolute Error (MAE) as our metric (lower is better). We consider the following baselines:\n\n\n\u2022\n\nEntity mean/median calculates the mean/median label value for each entity in training data and predicts the mean/median value for the entity.\n\n\n\n\u2022\n\nGlobal mean/median calculates the global mean/median label value over the training data and predicts the same mean/median value across all entities.\n\n\n\n\u2022\n\nGlobal zero predicts zero for all entities.\n\n\n\n\u2022\n\nLightGBM learns a LightGBM\u00a0(Ke et\u00a0al., 2017) regressor over the raw entity features to predict the numerical targets. Note that only information from the single entity table is used.\n\n\n\n\n\nExperimental results. Results in Table 5.2 show our RDL implementation outperforms or matches baselines in all cases. A number of tasks, such as driver-position and study-adverse, have matching performance up to statistical significance, suggesting some room for improvement. We analyze this further in Appendix C, identifying one potential cause, suggesting an opportunity for improved performance for regression tasks.\n\n\nTable 4: Entity regression results (MAE, lower is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3 Recommendation\n\nFinally, we also introduce recommendation tasks on pairs of entities. The task is to predict a list of top K\ud835\udc3eKitalic_K target entities given a source entity at a given seed time.\nThe metric we use is Mean Average Precision (MAP) @K\ud835\udc3eKitalic_K, where K\ud835\udc3eKitalic_K is set per task (higher is better). We consider the following baselines:\n\n\n\u2022\n\nGlobal popularity computes the top K\ud835\udc3eKitalic_K most popular target entities (by count) across the entire training table and predict the K\ud835\udc3eKitalic_K globally popular target entities across all source entities.\n\n\n\n\u2022\n\nPast visit computes the top K\ud835\udc3eKitalic_K most visited target entities for each source entity within the training table and predict those past-visited target entities for each entity.\n\n\n\n\u2022\n\nLightGBM learns a LightGBM\u00a0(Ke et\u00a0al., 2017) classifier over the raw features of the source and target entities (concatenated) to predict the link. Additionally, global popularity and past visit ranks are also provided as inputs.\n\n\n\n\n\nFor recommendation, it is also important to ensure a certain density of links in the training data in order for there to be sufficient predictive signal. In Appendix A we report statistics on the average number of destination entities each source entity links to. For most tasks the density is \u22651absent1\\geq 1\u2265 1, with the exception of rel-stack which is more sparse, but is included to test in extreme sparse settings.\n\n\nExperimental results. Results are given in Table 5.3. We find that either the RDL implementation using GraphSAGE (Hamilton et\u00a0al., 2017), or ID-GNN (You et\u00a0al., 2021) as the GNN component performs best, often by a very significant margin. ID-GNN excels in cases were predictions are entity-specific (i.e., Past Visit baseline outperforms Global Popularity), whilst the plain GNN excels in the reverse case. This reflects the inductive biases of each model, with GraphSAGE being able to learn structural features, and ID-GNN able to take into account the specific node ID.\n\n\nTable 5: Recommendation results (MAP, higher is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 Expert Data Scientist User Study\n\nTo test RDL in the most challenging circumstances possible, we undertake a human trial wherein a data scientist solves each task by manually designing features and feeds them into tabular methods such at LightGBM or XGBoost\u00a0(Chen and Guestrin, 2016; Ke et\u00a0al., 2017). This represents the prior gold-standard for building predictive models on relational databases (Heaton, 2016), and the key point of comparison for RDL.\n\n\nWe structure our user study along the five main data science workflow steps:\n\n\n1.\n\nExploratory data analysis (EDA): Explore the dataset and task to understand its characteristics, including what column features there are, and if there is any missing data.\n\n\n\n2.\n\nFeature ideation: Based on EDA and intuition from prior experiences, propose a set of entity-level features that the data scientist believes may contain predictive signal for the task.\n\n\n\n3.\n\nFeature enginnering: Using query languages such as SQL to compute the proposed features, and add them as extra columns to the target table of interest.\n\n\n\n4.\n\nTabular ML: Run tabular methods such as LightGBM or XGBoost on the table with extra features to produce a predictive model, and record the test performance.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (Optional): Common tools include SHAP and LIME, which aim to explain the contribution of each input feature to the final performance.\n\n\n\n\n\nConsider for example the rel-hm dataset (schema in Appendix D) and the task of predicting customer churn. Here the customer table only contains simple biographical information such as username and joining date. To capture more predictive information, additional features, such as time since last purchase, can be computed using the other tables, and added to the customer table. We give a detailed walk-through of the data scientist\u2019s work process for solving this specific task in Appendix C. We strongly encourage the interested reader to review this, as it highlights the significant amount of task-specific effort that this workflow necessitates.\n\n\nLimitations of Manual Feature Engineering. This workflow suffers from several fundamental limitations. Most obviously, since features are hand designed they only capture part of the predictive signal in the database, useful signal is easily missed. Additionally, feature complexity is limited by human reasoning abilities, meaning that higher-order interactions between entities are often overlooked. Beyond predictive signal, the other crucial limitation of feature engineering is its extremely manual nature\u2014every time a new model is built a data scientist has to repeat this process, requiring many hours of human labor, and significant quantities of new SQL code to design features (Zheng and Casari, 2018). Our RDL models avoid these limitations (see Section 6.1).\n\n\nData Scientist. To conduct a thorough comparison to this process, we recruit a high-end data scientist with Stanford CS MSc degree, 4.0 GPA, and 5 years of experience of building machine learning models in the financial industry. This experience includes a significant amount of time building machine learning models in exactly above five steps, as well as broader data science expertise.\n\n\nUser Study Protocol.\n\n\nFigure 3: RDL vs. Data Scientist. Relational Deep Learning matches or outperforms the data scientist in 11 of 15 tasks. Left shows entity classification AUROC, right shows entity regression, reporting MAE normalized so that the RDL MAE is always 1.\n\n\n\nBecause of the open-ended nature of feature engineering and model development, we follow a specific protocol for the user study in order to standardize the amount of effort dedicated to each dataset and task. Tracking the 5 steps outlined above, we impose the following rules:\n\n\n\n\n1.\n\nEDA: The time allotted for data exploration is capped at 4 hours. This threshold was chosen to give the data scientist enough time to familiarize themselves with the schema, visualize key relationships and distributions, and take stock of any outliers in the dataset, while providing a reasonable limit to the effort applied.\n\n\n\n2.\n\nFeature ideation: Feature ideation is performed manually with pen and paper, and is limited to 1 hour. In practice, the data scientist found that 1 hour was plenty of time to enumerate all promising features at that time, especially since many ideas naturally arise during the EDA process already.\n\n\n\n3.\n\nFeature engineering: The features described during the ideation phase are then computed using SQL queries. The time taken to write SQL code to generate the features is unconstrained in order to eliminate code writing speed as a factor in the study. We do, however, record code writing time for our timing benchmarking. This stage presented the most variability in terms of time commitment, partly because it is unconstrained, but mostly because the implementation complexity of the features itself is highly variable.\n\n\n\n4.\n\nTabular ML: For tabular ML training, we provide a standardized LightGBM training script including comprehensive hyperparameter tuning. The data scientist needs only to feed the table full of engineered features into this training script, which returns test performance results. However, there is some non-trivial amount of work required to transform the output of the SQL queries from the previous section into the Python objects (arrays) required for training LightGBM. Again, the time taken for this additional pre-preocessing is recorded.\n\n\n\n5.\n\nPost-hoc analysis of feature importance: Finally, after successfully training a model, an evaluation of model predictions and feature importance is carried out. This mostly serves as a general sanity check and an interesting corollary of the data scientist\u2019s work that provides task-specific insights (see Appendix C). In practice, this took no more than a few minutes per task and this time was not counted toward the total time commitment.\n\n\n\n\n\nReproducibility. All of the data scientist\u2019s workings are released333See https://github.com/snap-stanford/relbench-user-study. to ensure reproducibility and demonstrate the significant lengths gone through to build as accurate models as possible. In Appendix C we walk through a complete example for a single dataset and task, showing the data-centric insights it yields. An important by-product is a close analysis of which features contribute to model performance, which we believe will help inspire future well-motivated RDL research directions.\n\n\nFigure 4: RDL vs. Data Scientist. Relational Deep Learning reduces the hours of human work required to solve a new task by 96% on average (from 12.3 to 0.5 hours). Left shows node-level classification, right shows node-level regression.\n\n\n\n\n6.1 Results\n\nAs well as (i) raw predictive power, we compare the data scientist to our RDL models in terms of (ii) hours of human work, and (iii) number of new lines of code required to solve each task. We measure the marginal effort, meaning that we do not include code infrastructure that is reused across tasks, including for example data loading logic and training scripts for RDL or LightGBM models.\n\n\nSummary. Figures 3, 4, and 5 show that RDL learns highly predictive models, outperforming the data scientist in 11 of 15 tasks, whilst reducing hours worked by 96%percent9696\\%96 % on average, and lines of code by 94%percent9494\\%94 % on average. On average, it took the data scientist 12.3 hours to solve each task using traditional feature engineering. By contrast it takes roughly 30 minutes to solve a task with RDL.\n\n\nThis observation is the central value proposition of relational deep learning, pointing the way to unlocking new levels of predictive power, and potentially a new economic model for solving predictive tasks on relational databases. Replacing hand-crafted solutions with end-to-end learnable models has been a key takeaway from the last 15 years of AI research. It is therefore remarkable how little impact deep learning has had on ML on relational databases, one of the most widespread applied ML use cases. To the best of our knowledge, RDL represents the first proposal for a deep learning approach for relational databases that has demonstrated efficacy compared with established data science workflows.\n\n\nWe highlight that all RelBench tasks were solved with a single set of default hyperparameters (with 2 exceptions requiring small modifications to learning rate, number of epochs, and GNN aggregation function). This demonstrates the robustness of RDL, and that the performance of RDL in Figure 3 is not due to extensive hyperparamter search. Indeed, the single set of RDL hyperparameters is compared to a carefully tuned LightGBM, which was allowed to search over 10 sets of hyperparameters.\n\n\nPredictive Power. Results shown in Figures 3. Whilst outperforming the data scientist in 11 of 15 tasks, we note that RDL best outperforms the data scientist on classification tasks, struggling more on regression. Indeed it was necessary for us to apply a \u201cboosting\u201d to the RDL model to improve performance (see Appendix C for details). Even with boosting, the data scientist model outperforms RDL in several cases. One cause we identify is that the MLP output head of the GNN is poorly suited to regression tasks (see Appendix C for our analysis). This suggests an opportunity for improved output heads for regression tasks. We stress that our RDL implementation is an initial demonstration. We believe there is significant scope for new research leading to large improvements in performance. In particular, ideas from graph ML, deep tabular ML, and time-series modeling are well suited to advance RDL.\n\n\nFigure 5: RDL vs. Data Scientist. Relational Deep Learning reduces the new lines of code needed to solve a new task by 94%. Left shows entity classification, right shows entity regression.\n\n\n\nHuman Work. Results shown in Figure 4. In our user study RDL required 96% less hours work to solve a new task, compared to the data scientist work flow. The RDL solutions always took less than an hour to write, whilst the data scientist took 12121212 hours on average, with a standard deviation of 1.61.61.61.6 hours. We emphasize that this measures marginal effort, i.e., it does not include reusable code that can be amortized over many tasks. RDL compares favorably to data scientist because a large majority of RDL code is reusable for new tasks (a GNN architecture and training loop needs only to be defined once) whereas a large portion of the data scientist\u2019s code is task specific and must be re-done afresh for every new task that needs to be solved.\n\n\nLines of Code. Results shown in Figure 5. For the RDL model, the only new addition needed to solve a new task is the code describing how to compute the training supervision for the RDL, which is stored in the training table. This requires a similar number of lines of code for each task, with 56 lines of code on average, with standard deviation 8.88.88.88.8, with the data scientist requiring with 878\u00b177plus-or-minus87877878\\pm 77878 \u00b1 77. The minimum lines of code required by RDL is 44, compared to 734 for the data scientist, and maximum is 84 compared to 1039 for the data scientist. Examples of the RDL code required to solve rel-amazon tasks can be viewed here. For the data scientist pipeline, we record the number of lines of code for EDA and SQL files, and the manipulations needed to format data to be fed into the pre-prepared LightGBM script.\n\n\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "6 Expert Data Scientist User Study": "\n\n6 Expert Data Scientist User Study\n\nTo test RDL in the most challenging circumstances possible, we undertake a human trial wherein a data scientist solves each task by manually designing features and feeds them into tabular methods such at LightGBM or XGBoost\u00a0(Chen and Guestrin, 2016; Ke et\u00a0al., 2017). This represents the prior gold-standard for building predictive models on relational databases (Heaton, 2016), and the key point of comparison for RDL.\n\n\nWe structure our user study along the five main data science workflow steps:\n\n\n1.\n\nExploratory data analysis (EDA): Explore the dataset and task to understand its characteristics, including what column features there are, and if there is any missing data.\n\n\n\n2.\n\nFeature ideation: Based on EDA and intuition from prior experiences, propose a set of entity-level features that the data scientist believes may contain predictive signal for the task.\n\n\n\n3.\n\nFeature enginnering: Using query languages such as SQL to compute the proposed features, and add them as extra columns to the target table of interest.\n\n\n\n4.\n\nTabular ML: Run tabular methods such as LightGBM or XGBoost on the table with extra features to produce a predictive model, and record the test performance.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (Optional): Common tools include SHAP and LIME, which aim to explain the contribution of each input feature to the final performance.\n\n\n\n\n\nConsider for example the rel-hm dataset (schema in Appendix D) and the task of predicting customer churn. Here the customer table only contains simple biographical information such as username and joining date. To capture more predictive information, additional features, such as time since last purchase, can be computed using the other tables, and added to the customer table. We give a detailed walk-through of the data scientist\u2019s work process for solving this specific task in Appendix C. We strongly encourage the interested reader to review this, as it highlights the significant amount of task-specific effort that this workflow necessitates.\n\n\nLimitations of Manual Feature Engineering. This workflow suffers from several fundamental limitations. Most obviously, since features are hand designed they only capture part of the predictive signal in the database, useful signal is easily missed. Additionally, feature complexity is limited by human reasoning abilities, meaning that higher-order interactions between entities are often overlooked. Beyond predictive signal, the other crucial limitation of feature engineering is its extremely manual nature\u2014every time a new model is built a data scientist has to repeat this process, requiring many hours of human labor, and significant quantities of new SQL code to design features (Zheng and Casari, 2018). Our RDL models avoid these limitations (see Section 6.1).\n\n\nData Scientist. To conduct a thorough comparison to this process, we recruit a high-end data scientist with Stanford CS MSc degree, 4.0 GPA, and 5 years of experience of building machine learning models in the financial industry. This experience includes a significant amount of time building machine learning models in exactly above five steps, as well as broader data science expertise.\n\n\nUser Study Protocol.\n\n\nFigure 3: RDL vs. Data Scientist. Relational Deep Learning matches or outperforms the data scientist in 11 of 15 tasks. Left shows entity classification AUROC, right shows entity regression, reporting MAE normalized so that the RDL MAE is always 1.\n\n\n\nBecause of the open-ended nature of feature engineering and model development, we follow a specific protocol for the user study in order to standardize the amount of effort dedicated to each dataset and task. Tracking the 5 steps outlined above, we impose the following rules:\n\n\n\n\n1.\n\nEDA: The time allotted for data exploration is capped at 4 hours. This threshold was chosen to give the data scientist enough time to familiarize themselves with the schema, visualize key relationships and distributions, and take stock of any outliers in the dataset, while providing a reasonable limit to the effort applied.\n\n\n\n2.\n\nFeature ideation: Feature ideation is performed manually with pen and paper, and is limited to 1 hour. In practice, the data scientist found that 1 hour was plenty of time to enumerate all promising features at that time, especially since many ideas naturally arise during the EDA process already.\n\n\n\n3.\n\nFeature engineering: The features described during the ideation phase are then computed using SQL queries. The time taken to write SQL code to generate the features is unconstrained in order to eliminate code writing speed as a factor in the study. We do, however, record code writing time for our timing benchmarking. This stage presented the most variability in terms of time commitment, partly because it is unconstrained, but mostly because the implementation complexity of the features itself is highly variable.\n\n\n\n4.\n\nTabular ML: For tabular ML training, we provide a standardized LightGBM training script including comprehensive hyperparameter tuning. The data scientist needs only to feed the table full of engineered features into this training script, which returns test performance results. However, there is some non-trivial amount of work required to transform the output of the SQL queries from the previous section into the Python objects (arrays) required for training LightGBM. Again, the time taken for this additional pre-preocessing is recorded.\n\n\n\n5.\n\nPost-hoc analysis of feature importance: Finally, after successfully training a model, an evaluation of model predictions and feature importance is carried out. This mostly serves as a general sanity check and an interesting corollary of the data scientist\u2019s work that provides task-specific insights (see Appendix C). In practice, this took no more than a few minutes per task and this time was not counted toward the total time commitment.\n\n\n\n\n\nReproducibility. All of the data scientist\u2019s workings are released333See https://github.com/snap-stanford/relbench-user-study. to ensure reproducibility and demonstrate the significant lengths gone through to build as accurate models as possible. In Appendix C we walk through a complete example for a single dataset and task, showing the data-centric insights it yields. An important by-product is a close analysis of which features contribute to model performance, which we believe will help inspire future well-motivated RDL research directions.\n\n\nFigure 4: RDL vs. Data Scientist. Relational Deep Learning reduces the hours of human work required to solve a new task by 96% on average (from 12.3 to 0.5 hours). Left shows node-level classification, right shows node-level regression.\n\n\n\n\n6.1 Results\n\nAs well as (i) raw predictive power, we compare the data scientist to our RDL models in terms of (ii) hours of human work, and (iii) number of new lines of code required to solve each task. We measure the marginal effort, meaning that we do not include code infrastructure that is reused across tasks, including for example data loading logic and training scripts for RDL or LightGBM models.\n\n\nSummary. Figures 3, 4, and 5 show that RDL learns highly predictive models, outperforming the data scientist in 11 of 15 tasks, whilst reducing hours worked by 96%percent9696\\%96 % on average, and lines of code by 94%percent9494\\%94 % on average. On average, it took the data scientist 12.3 hours to solve each task using traditional feature engineering. By contrast it takes roughly 30 minutes to solve a task with RDL.\n\n\nThis observation is the central value proposition of relational deep learning, pointing the way to unlocking new levels of predictive power, and potentially a new economic model for solving predictive tasks on relational databases. Replacing hand-crafted solutions with end-to-end learnable models has been a key takeaway from the last 15 years of AI research. It is therefore remarkable how little impact deep learning has had on ML on relational databases, one of the most widespread applied ML use cases. To the best of our knowledge, RDL represents the first proposal for a deep learning approach for relational databases that has demonstrated efficacy compared with established data science workflows.\n\n\nWe highlight that all RelBench tasks were solved with a single set of default hyperparameters (with 2 exceptions requiring small modifications to learning rate, number of epochs, and GNN aggregation function). This demonstrates the robustness of RDL, and that the performance of RDL in Figure 3 is not due to extensive hyperparamter search. Indeed, the single set of RDL hyperparameters is compared to a carefully tuned LightGBM, which was allowed to search over 10 sets of hyperparameters.\n\n\nPredictive Power. Results shown in Figures 3. Whilst outperforming the data scientist in 11 of 15 tasks, we note that RDL best outperforms the data scientist on classification tasks, struggling more on regression. Indeed it was necessary for us to apply a \u201cboosting\u201d to the RDL model to improve performance (see Appendix C for details). Even with boosting, the data scientist model outperforms RDL in several cases. One cause we identify is that the MLP output head of the GNN is poorly suited to regression tasks (see Appendix C for our analysis). This suggests an opportunity for improved output heads for regression tasks. We stress that our RDL implementation is an initial demonstration. We believe there is significant scope for new research leading to large improvements in performance. In particular, ideas from graph ML, deep tabular ML, and time-series modeling are well suited to advance RDL.\n\n\nFigure 5: RDL vs. Data Scientist. Relational Deep Learning reduces the new lines of code needed to solve a new task by 94%. Left shows entity classification, right shows entity regression.\n\n\n\nHuman Work. Results shown in Figure 4. In our user study RDL required 96% less hours work to solve a new task, compared to the data scientist work flow. The RDL solutions always took less than an hour to write, whilst the data scientist took 12121212 hours on average, with a standard deviation of 1.61.61.61.6 hours. We emphasize that this measures marginal effort, i.e., it does not include reusable code that can be amortized over many tasks. RDL compares favorably to data scientist because a large majority of RDL code is reusable for new tasks (a GNN architecture and training loop needs only to be defined once) whereas a large portion of the data scientist\u2019s code is task specific and must be re-done afresh for every new task that needs to be solved.\n\n\nLines of Code. Results shown in Figure 5. For the RDL model, the only new addition needed to solve a new task is the code describing how to compute the training supervision for the RDL, which is stored in the training table. This requires a similar number of lines of code for each task, with 56 lines of code on average, with standard deviation 8.88.88.88.8, with the data scientist requiring with 878\u00b177plus-or-minus87877878\\pm 77878 \u00b1 77. The minimum lines of code required by RDL is 44, compared to 734 for the data scientist, and maximum is 84 compared to 1039 for the data scientist. Examples of the RDL code required to solve rel-amazon tasks can be viewed here. For the data scientist pipeline, we record the number of lines of code for EDA and SQL files, and the manipulations needed to format data to be fed into the pre-prepared LightGBM script.\n\n\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "7 Related Work": "\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "8 Conclusion": "\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Acknowledgments and Disclosure of Funding": "\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix A Additional Task Information": "\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n",
      "Appendix B Experiment Details and Additional Results": "\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix C User Study Additional Details": "\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix D Dataset Origins and Licenes": "\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix E Additional Training Table Statistics": "\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix F Dataset Schema": "\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n",
      "Appendix G Broader Impact": "\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n"
    },
    "table_captions": null,
    "figure_captions": null,
    "bibliography": {
      "Chen and Guestrin (2016)": "\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n",
      "Cvitkovic (2019)": "\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n",
      "Deng et\u00a0al. (2009)": "\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n",
      "Devlin et\u00a0al. (2018)": "\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n",
      "Fey and Lenssen (2019)": "\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n",
      "Fey et\u00a0al. (2024)": "\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n",
      "Gilmer et\u00a0al. (2017)": "\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n",
      "Gorishniy et\u00a0al. (2021)": "\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n",
      "Hamilton et\u00a0al. (2017)": "\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Hanley and McNeil (1983)": "\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n",
      "Heaton (2016)": "\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n",
      "Hu et\u00a0al. (2020)": "\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n",
      "Hu et\u00a0al. (2024)": "\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n",
      "Huang et\u00a0al. (2024)": "\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n",
      "Johnson et\u00a0al. (2016)": "\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n",
      "Kaggle (2022)": "\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n",
      "Kapoor and Narayanan (2023)": "\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n",
      "Ke et\u00a0al. (2017)": "\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n",
      "Krizhevsky et\u00a0al. (2017)": "\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n",
      "Lundberg and Lee (2017)": "\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n",
      "Morris et\u00a0al. (2020)": "\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n",
      "Ni et\u00a0al. (2019)": "\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n",
      "Pennington et\u00a0al. (2014)": "\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n",
      "PubMed (1996)": "\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n",
      "Rendle et\u00a0al. (2012)": "\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n",
      "Schlichtkrull et\u00a0al. (2018)": "\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n",
      "\u0160\u00edr (2021)": "\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n",
      "Wang et\u00a0al. (2019)": "\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n",
      "You et\u00a0al. (2021)": "\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n",
      "Zahradn\u00edk et\u00a0al. (2023)": "\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n",
      "Zheng and Casari (2018)": "\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n"
    },
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "dc305669-693f-4722-af32-1448a5b167af": {
    "pk": "dc305669-693f-4722-af32-1448a5b167af",
    "project_name": null,
    "authors": [
      "Matthias Fey",
      "Weihua Hu",
      "Kexin Huang",
      "Jan Eric Lenssen",
      "Rishabh Ranjan",
      "Joshua Robinson",
      "Rex Ying",
      "Jiaxuan You",
      "Jure Leskovec"
    ],
    "title": "Relational Deep Learning: Graph Representation Learning on Relational Databases",
    "abstract": "Much of the world's most valued data is stored in relational databases and data warehouses, where the data is organized into many tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming. The core problem is that no machine learning method is capable of learning on multiple tables interconnected by primary-foreign key relations. Current methods can only learn from a single table, so the data must first be manually joined and aggregated into a single training table, the process known as feature engineering. Feature engineering is slow, error prone and leads to suboptimal models. Here we introduce an end-to-end deep representation learning approach to directly learn on data laid out across multiple tables. We name our approach Relational Deep Learning (RDL). The core idea is to view relational databases as a temporal, heterogeneous graph, with a node for each row in each table, and edges specified by primary-foreign key links. Message Passing Graph Neural Networks can then automatically learn across the graph to extract representations that leverage all input data, without any manual feature engineering. Relational Deep Learning leads to more accurate models that can be built much faster. To facilitate research in this area, we develop RelBench, a set of benchmark datasets and an implementation of Relational Deep Learning. The data covers a wide spectrum, from discussions on Stack Exchange to book reviews on the Amazon Product Catalog. Overall, we define a new research area that generalizes graph machine learning and broadens its applicability to a wide set of AI use cases.",
    "url": "http://arxiv.org/abs/2312.04615v1",
    "timestamp": 1701975101,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe information age is driven by data stored in ever-growing relational databases and data warehouses\nthat have come to underpin nearly all technology stacks. Data warehouses typically store information in multiple tables, with entities/rows in different tables connected using primary-foreign key relations, and managed using powerful query languages such as SQL (Codd, 1970; Chamberlin and Boyce, 1974). For this reason, data warehouses underpin many of today\u2019s large information systems, including e-commerce, social media, banking systems, healthcare, manufacturing, and open-source scientific knowledge repositories\u00a0(Johnson et\u00a0al., 2016; PubMed, 1996).\n\n\nMany predictive problems over relational data have significant implications for human decision making. A hospital wants to predict the risk of discharging a patient; an e-commerce company wishes to forecast future sales of each of their products; a telecommunications provider wants to predict which customers will churn; and a music streaming platform must decide which songs to recommend to a user. Behind each of these tasks is a rich relational schema of relational tables, and many machine learning models are built using this data\u00a0(Kaggle, 2022).\n\n\nHowever, existing learning paradigms, notably tabular learning, cannot be directly applied to an interlinked set relational tables.\nInstead, a manual feature engineering step is first taken, where a data scientist uses domain knowledge to manually join and aggregate tables to generate many features in a regular single table format.\nTo illustrate this, consider a simple e-commerce schema (Fig. 1) of three tables: Customers, Transactions and Products, where Customers and Products tables link into the Transactions table via primary-foreign keys, and the task is to predict if a customer is going to churn (i.e., make zero transactions in the next k\ud835\udc58kitalic_k days). In this case, the data scientist would aggregate information from the Transactions table to make new features for the Customers table such as: \u201cnumber of purchases of a given customer in the last 30 days\u201d, \u201csum of purchase amounts of a given customer in the last 30 days\u201d, \u201cnumber of purchases on a Sunday\u201d, \u201csum of purchase amounts on a Sunday\u201d, \u201cnumber of purchases on a Monday\u201d, and so on. The computed customer features are then stored in a single table, ready for tabular machine learning. Another challenge is the temporal nature of the churn predictive tasks. As new transactions appear, the customer\u2019s churn label and the customer\u2019s features may change from day to day, so features need to be recomputed for each day. Overall, the temporal nature of relational databases adds computational cost and further complexity, which often results in bugs and information leakage including the so-called \u201ctime travel\u201d (Kapoor and Narayanan, 2023).\n\n(a) Relational Database(b) Define Tasks(c) Relational Deep Learning\nFigure 1: Relational Deep Learning solves predictive tasks on relational data with end-to-end learnable models. There are three main steps. (a) A relational database with multiple tables connected by primary-foreign keys is given. (b) A predictive task is specified and added to the database by introducing an additional training table. (c) Relational data is transformed into its Relational Entity Graph, and a Graph Neural Network is trained over the graph with the supervision provided by the training table. The predictive task can be node level (as in this illustration), link level (pairs of nodes), or higher-order.\n\n\nThere are several issues with with the above approach: (1) it is a manual, slow and labor intensive process; (2) feature choices are essentially arbitrary and likely highly-suboptimal;\n(3) only a small fraction of the overall space of possible features can be manually explored; (4) by forcing data into a single table, information is aggregated into lower-granularity features, thus losing out on valuable fine-grain signal; (5) whenever the data distribution changes or drifts, current features become obsolete and new features have to be manually reinvented.\n\n\n\nMany domains have been in a similar position, including pre-deep-learning computer vision, where hand-chosen convolutional filters (e.g., Gabor) were used to extract features, followed by models such as SVMs or nearest neighbor search (Varma and Zisserman, 2005). Today, in contrast, deep neural networks skip the feature engineering and learn directly on the raw pixels, which results in large gains in model accuracy.\nMore broadly, the deep learning revolution has had a huge impact in many fields, including computer vision\u00a0(He et\u00a0al., 2016; Russakovsky et\u00a0al., 2015), natural language processing\u00a0(Vaswani et\u00a0al., 2017; Devlin et\u00a0al., 2018; Brown et\u00a0al., 2020), and speech\u00a0(Hannun et\u00a0al., 2014; Amodei et\u00a0al., 2016), and has led to super-human performance in many tasks. In all cases, the key was to move from manual feature engineering and handcrafted systems to fully data-driven, end-to-end representation learning systems.\nFor relational data, this transition has not yet occurred, as existing tabular deep learning approaches still heavily rely on manual feature engineering.\nConsequently, there remains a huge unexplored opportunity.\n\n\nHere we introduce Relational Deep Learning (RDL), a blueprint for fulfilling the need for an end-to-end deep learning paradigm for relational tables (Fig. 1).\nThrough end-to-end representation learning, Relational Deep Learning fully utilizes the rich predictive signals available in relational tables. The core of RDL is to represent relational tables as a temporal, heterogeneous Relational Entity Graph, where each row defines a node, columns define node features, and primary-foreign key links define edges. Graph Neural Networks (GNNs)\u00a0(Gilmer et\u00a0al., 2017; Hamilton et\u00a0al., 2017) can then be applied to build end-to-end data-driven predictive models.\n\n\nPredictive tasks are specified on relational data by introducing a training table that holds supervision label information, (Fig. 1b) but no input features.\nTraining tables have two critically important characteristics. First, labels can be automatically computed from historical relational data, without any need for outside annotation; second, they may contain any number of foreign keys, permitting many task types including entity level (1 key, as in Fig. 1b), link-level tasks such as recommendation (2 keys) and multi-entity tasks (>>>2 keys). Training tables permit many different types of prediction targets, including multi-class, multi-label, regression and more, ensuring high task generality.\n\n\nAll in all, RDL model pipeline has four main steps (Fig.\u00a02): Given a predictive machine learning task, (1) A training table containing supervision labels is constructed in a task-specific manner based on historic data in the relational database, (2) entity-level features are extracted and encoded from each row in each table to serve as node features, (3) node representations are learned through an inter-entity message-passing GNN that exchanges information between entities with primary-foreign key links, (4) a task-specific model head produces predictions for training data, and errors are backpropogated through the network.\n\n(a) Rel. Tables with Training Table(b) Entities Linked by Foreign Keys(c) Relational Entity Graph(d) Graph Neural Network\nFigure 2: Relational Deep Learning Pipeline. (a) Given relational tables and a predictive task, a training table, containing supervised label information, is constructed and attached to the entity table(s). (b) Relational tables contain individual entities that are linked by foreign-primary key relations. (c) Relational data can be viewed as a single Relational Entity graph, which has a node for each entity, and edges given by primary-foreign key links. (d) Initial node features are extracted from each row in each table using modality-specific neural networks. Then a message passing graph neural network computes relation-aware node embeddings, a model head produces predictions for training table entities, and errors are backpropogated.\n\n\n\nCrucially, RDL models natively integrate temporality by only allowing entities to receive messages from other entities with earlier timestamps. This ensures that learned representation is automatically updated during GNN forward pass when new data is collected, and prevents information leakage and time travel bugs. Furthermore, this also stabilizes the generalization across time since models are trained to make predictions at multiple time snapshots by dynamically passing messages between entities at different time snapshots, whilst remaining grounded in a single relational database.\n\n\n\nRelBench.\n\nTo facilitate research into Relational Deep Learning, we introduce RelBench, a benchmarking and an evaluation Python package. Data in RelBench cover rich relational databases from many different domains. RelBench has the following key modules (1) Data: data loading, specifying a predictive task, and (temporal) data splitting, (2) Model: transforming data to a heterogeneous graph, building graph neural network predictive models, (3) Evaluation: standardized evaluation protocol given a file of predictions. Importantly, data and evaluation modules are deep learning framework agnostic, enabling broad compatibility. To facilitate research, we provide our initial model implementation based on PyTorch Frame\u00a0(Hu et\u00a0al., 2023) for encoding table rows into input node embeddings, which is then processed by GNN models in PyTorch Geometric\u00a0(Fey and Lenssen, 2019) to update the embeddings via message passing over the relational entity graph.\n\n\nFor the initial release, RelBench contains two databases, each with two predictive tasks. The first database is from Stack Exchange, the question-answering website, and includes 7 tables such as posts, users, and votes. The predictive tasks are (1) to predict if a user is going to make a new contribution (post, answer etc.), and (2) to predict the popularity of a new question. The second database is a subset of the Amazon Product Catalog focusing on books. There are three tables: users, products, and reviews. The tasks are (1) to predict the lifetime value of a user, and (2) whether a user will stop using the site.\n\n\nOur objective is to establish deep learning on relational data as a new subfield of machine learning. We hope that this will be a fruitful research direction, with many opportunities for impactful ideas that make much better use of the rich predictive signal in relational data. This paper lays the ground for future work by making the following main sections:\n\n\n\u2022\n\nBlueprint. Relational Deep Learning, an end-to-end learnable approach that ultilizes the predictive signals available in relational data, and supports temporal predictions.\n\n\n\n\u2022\n\nBenchmarking Package RelBench, an open-source Python package for benchmarking and evaluating GNNs on relational data. RelBench beta release introduces two relational databases, and specifies two prediction tasks for each.\n\n\n\n\u2022\n\nResearch Opportunities. Outlining a new research program for Relational Deep Learning, including multi-task learning, new GNN architectures, multi-hop learning, and more.\n\n\n\n\n\n\nOrganization.\n\nSection 2 provides background on relational tables and predictive task specification.\nSection 3 introduces our central methodological contribution, a graph neural network approach to solving predictive tasks on relational data.\nSection 4 introduces RelBench, a new benchmark for relational tables, and standardized evaluation protocols.\nSection 5 outlines a landscape of new research opportunities for graph machine learning on relational data. Finally, Section 6 concludes by contextualizing our new framework within the tabular and graph machine learning literature.\n\n\n",
      "2 Predictive Tasks on Relational Databases": "\n\n2 Predictive Tasks on Relational Databases\n\nThis section outlines our problem scope: predictive tasks on relational tables. In the process, we define what we mean by relational tables, and how to specify predictive tasks on them. This section focuses exclusively on the structure of data and tasks, laying the groundwork for Section 3, which presents our GNN-based modelling approach.\n\n\n\n2.1 Relational Data\n\nA Brief History.\n\nRelational tables and relational databases emerged in the 1970s as a means to standardize data retrieval and management (Codd, 1970). As society digitized, relational databases came to fulfill a foundational purpose, and today are estimated to comprise 72% of the world\u2019s data\u00a0(DB-Engines, 2023). Whilst there is no single agreed upon definition of a relational database, three essential characteristics are shared in all cases (cf. Figure 1a):\n\n\n1.\n\nData is stored in multiple tables.\n\n\n\n2.\n\nEach row in each table contains an entity, which possesses a unique primary key ID, along with multiple attributes stored as columns of the table.\n\n\n\n3.\n\nOne entity may refer to another entity using a foreign key\u2014the primary key of another entity.\n\n\n\n\n\nAs well as a standardized storage system, relational databases typically come equipped with a powerful set of relational operations, which are used to manipulate and access data. Codd (1970) introduced 8 relational operations, including set operations such as taking the union of two tables, and other operations such as joining two tables based on their common attributes. Popular query languages such as SQL (Chamberlin and Boyce, 1974) provide commercial-grade implementations of a wide variety of relational operations.\nNext we formally define relational data, as suits our purposes.\n\n\n\nDefinition of Relational Databases.\n\nA relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) is comprised of a collection of tables \ud835\udcaf={T1,\u2026,Tn}\ud835\udcafsubscript\ud835\udc471\u2026subscript\ud835\udc47\ud835\udc5b\\mathcal{T}=\\{T_{1},\\ldots,T_{n}\\}caligraphic_T = { italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, and links between tables \u2112\u2286\ud835\udcaf\u00d7\ud835\udcaf\u2112\ud835\udcaf\ud835\udcaf\\mathcal{L}\\subseteq\\mathcal{T}\\times\\mathcal{T}caligraphic_L \u2286 caligraphic_T \u00d7 caligraphic_T (cf. Figure 2a). A link L=(TfkeyL=(T_{\\rm fkey}italic_L = ( italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT, Tpkey)T_{\\rm pkey})italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT ) between tables exists if a foreign key column in Tfkeysubscript\ud835\udc47fkeyT_{\\rm fkey}italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT points to a primary key column of Tpkeysubscript\ud835\udc47pkeyT_{\\rm pkey}italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT.\nEach table is a set T={v1,\u2026,vnT}\ud835\udc47subscript\ud835\udc631\u2026subscript\ud835\udc63subscript\ud835\udc5b\ud835\udc47T=\\{v_{1},...,v_{n_{T}}\\}italic_T = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_v start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT }, whose elements vi\u2208Tsubscript\ud835\udc63\ud835\udc56\ud835\udc47v_{i}\\in Titalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_T are called rows, or entities (cf. Figure 2b). Each entity v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T, has four constituent parts v=(pv,\ud835\udca6v,xv,tv)\ud835\udc63subscript\ud835\udc5d\ud835\udc63subscript\ud835\udca6\ud835\udc63subscript\ud835\udc65\ud835\udc63subscript\ud835\udc61\ud835\udc63v=(p_{v},\\mathcal{K}_{v},x_{v},t_{v})italic_v = ( italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ):\n\n\n1.\n\nPrimary key pvsubscript\ud835\udc5d\ud835\udc63p_{v}italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, that uniquely identifies the entity v\ud835\udc63vitalic_v.\n\n\n\n2.\n\nForeign keys \ud835\udca6v\u2286{pv\u2032:v\u2032\u2208T\u2032\u2062\u00a0and\u00a0\u2062(T,T\u2032)\u2208\u2112}subscript\ud835\udca6\ud835\udc63conditional-setsubscript\ud835\udc5dsuperscript\ud835\udc63\u2032superscript\ud835\udc63\u2032superscript\ud835\udc47\u2032\u00a0and\u00a0\ud835\udc47superscript\ud835\udc47\u2032\u2112\\mathcal{K}_{v}\\subseteq\\{p_{v^{\\prime}}:v^{\\prime}\\in T^{\\prime}\\text{ and }(%\nT,T^{\\prime})\\in\\mathcal{L}\\}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2286 { italic_p start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT : italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT and ( italic_T , italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) \u2208 caligraphic_L }, defining links between element v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T to elements v\u2032\u2208T\u2032superscript\ud835\udc63\u2032superscript\ud835\udc47\u2032v^{\\prime}\\in T^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, where pv\u2032subscript\ud835\udc5dsuperscript\ud835\udc63\u2032p_{v^{\\prime}}italic_p start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT is the primary key of an entity v\u2032superscript\ud835\udc63\u2032v^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT in table T\u2032superscript\ud835\udc47\u2032T^{\\prime}italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.\n\n\n\n3.\n\nAttributes xvsubscript\ud835\udc65\ud835\udc63x_{v}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, holding the informational content of the entity. \n\n\n\n4.\n\nTimestamp An optional timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, indicating the time an event occurred.\n\n\n\n\n\nFor example, the Transactions table in Figure 2a has the primary key (TransactionID), two foreign keys (ProductID and CustomerID), one attribute (Price), and timestamp column (Timestamp). Similarly, the Products table has the primary key (ProductID), no foreign keys, attributes (Description, Image and Size), and no timestamp. The connection between foreign keys and primary keys is illustrated by black connecting lines in Figure 2.\n\n\nIn general, the attributes in table T\ud835\udc47Titalic_T contain dTsubscript\ud835\udc51\ud835\udc47d_{T}italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT values: xv=(xv1,\u2026,xvdT)subscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ), each belonging to a particular column. Critically, all entities in the same table have the same columns (values may be absent). Formally, this is described by membership xv=(xv1,\u2026,xvdT)\u2208\ud835\udc9cT1\u00d7\u2026\u00d7\ud835\udc9cTdTsubscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47superscriptsubscript\ud835\udc9c\ud835\udc471\u2026superscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})\\in\\mathcal{A}_{T}^{1}\\times\\ldots\\times%\n\\mathcal{A}_{T}^{d_{T}}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) \u2208 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u00d7 \u2026 \u00d7 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, where \ud835\udc9cTisubscriptsuperscript\ud835\udc9c\ud835\udc56\ud835\udc47\\mathcal{A}^{i}_{T}caligraphic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT denotes the value space of i\ud835\udc56iitalic_i-th column of table T\ud835\udc47Titalic_T, and is shared between all entities v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T.\nFor example, the Products table from Fig.\u00a02a contains three different attributes: the product description (text type), the image of the product (image type), and the size of the product (numerical type). Each of these types has their own encoders as discussed in Sec.\u00a03.4.3.\n\n\n\nFact and Dimension Tables.\n\nTables are categorized into two types, fact or dimension, with complementary roles (Garcia-Molina et\u00a0al., 2008). Dimension tables provide contextual information, such as biographical information, macro statistics (such as number of beds in a hospital), or immutable properties, such as the size of a product (as in the Products table in Figure 2a). Dimension tables tend to have relatively few rows, as it is limited to one per real-world object. Fact tables record interactions between other entities, such as all patient admissions to hospital, or all customer transactions (as in the Transactions table in Figure 2a). Since entities can interact repeatedly, fact tables often contain the majority of rows in a relational database. Typically, features in dimension tables are static over their whole lifetime, while fact tables usually contain temporal information with a dedicated time column that denotes the time of appearance.\n\n\n\nTemporality as a First-Class Citizen.\n\nRelational data evolves over time as events occur and are recorded.\nThis is captured by the (optional) timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT attached to each entity v\ud835\udc63vitalic_v. For example, each transaction in Transactions has a time stamp.\nFurthermore, many tasks of interest involve forecasting future events. For example, how much will a customer spend in next k\ud835\udc58kitalic_k days. It is therefore essential that time is conferred a special status unlike other attributes. Our formulation, introduced in Section 3, achieves this through a temporal message passing scheme (similar to Rossi et\u00a0al. (2020)), that only permits nodes to receive messages from neighbors with earlier timestamps. This ensures that models do not leak information from the future during training, avoiding shortcut decision rules that achieve high training accuracy but fail at test time (Geirhos et\u00a0al., 2020). It also means that model-extracted features are automatically updated as new relational data is added.\n\n\n\n\n\n2.2 From Task to Training Table\n\nThere are many practically interesting machine learning tasks defined over relational databases, such as predicting the response of a patient to treatment, or the future sales of a product.\nThese tasks involve predicting the future state of the entities of interest.\nGiven a task we wish to solve, how can we create ground truth labels to supervise machine learning models training?\n\n\nOur key insight is that we can generate training labels using historical data.\nFor instance, at time t\ud835\udc61titalic_t, ground truth labels for predicting \u201chow much each customer will buy in the next 90909090 days?\u201d are computed by summing up each customer\u2019s spending within the interval t\ud835\udc61titalic_t and t+90\ud835\udc6190t+90italic_t + 90 days. Importantly, as long as t+90\ud835\udc6190t+90italic_t + 90 is less than the most recent timestamp in the database, then these ground truth labels can be computed purely from historical data without any need for external annotation. Further, by choosing different time points t\ud835\udc61titalic_t across the database time horizon, it is possible to naturally compute many ground truth training labels for each entity.\n\n\nTo hold the labels for a new predictive task, we introduce a new table known as a training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT (Fig.\u00a03).\nEach entity v=(\ud835\udca6v,tv,yv)\ud835\udc63subscript\ud835\udca6\ud835\udc63subscript\ud835\udc61\ud835\udc63subscript\ud835\udc66\ud835\udc63v=(\\mathcal{K}_{v},t_{v},y_{v})italic_v = ( caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) in the training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT has three components: (1) A (set of) foreign keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT indicating the entities the training example is associated to, (2) a timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, and (3) the ground truth label itself yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. In contrast to tabular learning settings, the training table does not contain input data xvsubscript\ud835\udc65\ud835\udc63x_{v}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. The training table is linked to the main relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) by updating: (1) the set of tables to \ud835\udcaf\u222a{Ttrain}\ud835\udcafsubscript\ud835\udc47train\\mathcal{T}\\cup\\{{T_{\\text{train}}}\\}caligraphic_T \u222a { italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT }, and (2) the links between tables to \u2112\u222a\u2112Ttrain\u2112subscript\u2112subscript\ud835\udc47train\\mathcal{L}\\cup\\mathcal{L}_{T_{\\text{train}}}caligraphic_L \u222a caligraphic_L start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT end_POSTSUBSCRIPT, where \u2112Ttrainsubscript\u2112subscript\ud835\udc47train\\mathcal{L}_{T_{\\text{train}}}caligraphic_L start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT end_POSTSUBSCRIPT specifies tables that training table keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT point to.\n\n\nAs discussed in Sec. 2.1, careful handling of what data the model sees during training is crucial in order to ensure temporal leakage does not happen. This is achieved using the training timestamp. When the model is trained to output target yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT for entity v\ud835\udc63vitalic_v with timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, temporal consistency is ensured by only permitting the model to receive input information from entities u\ud835\udc62uitalic_u with timestamp tu\u2264tvsubscript\ud835\udc61\ud835\udc62subscript\ud835\udc61\ud835\udc63t_{u}\\leq t_{v}italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT \u2264 italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (see Sec. 3.3 for details on training sampling).\n\n(a) Define Tasks(b) Training Table Generation(c) Link to Relational Tables\nFigure 3: Predictive Task Definition. A task over relational data is defined by attaching an additional training table to the existing linked tables. A training table entity specifies (a) ground truth label computed from historical information (b) the entity ID(s) the labels correspond to, and (c) a timestamp that controls what data the model can use to predict this label.\n\n\nThus, the purpose of the training table is twofold: to specify training inputs and outputs of the machine learning model. First, it provides supervision on the model output by specifying the the entities and their target training labels. Second, in the case of temporal tasks, the training table specifies the model input by specifying the timestamp at which each historical training label is generated.\n\n\nThis training table formulation can model a wide range of predictive tasks on relational databases:\n\n\n\u2022\n\nNode-level prediction tasks (e.g., multi-class classification, multi-label classification, regression): The training table has three columns (EntityID, Label, Time), indicating the foreign key, target label, and timestamp columns, respectively.\n\n\n\n\u2022\n\nLink prediction tasks: The training table has columns (SourceEntityID, TargetEntityID, Label, Time), indicating the foreign key columns for the source/target nodes, and the target label, and timestamp, respectively.\n\n\n\n\u2022\n\nTemporal and static prediction tasks: Temporal tasks make predictions about the future (and require a seed time), while non-temporal tasks impute missing values (Time is dropped).\n\n\n\n\n\nTraining Table Generation.\n\nIn practice, training tables can be computed using time-conditioned SQL queries from historic data in the database. Given a query that describes the prediction targets for all prediction entities, e.g. the sum of sells grouped by products, from time t\ud835\udc61titalic_t to time t+\u03b4\ud835\udc61\ud835\udefft+\\deltaitalic_t + italic_\u03b4 in the future, we can move t\ud835\udc61titalic_t back in time in fixed intervals to gather historical training, validation and test targets for all entities (cf. Fig.\u00a03b). We store t\ud835\udc61titalic_t as timestamp for the targets gathered in each step.\n\n\n\n",
      "3 Predictive Tasks as Graph Representation Learning Problems": "\n\n3 Predictive Tasks as Graph Representation Learning Problems\n\nHere, we formulate a generic machine learning architecture based on Graph Neural Networks, which solves predictive tasks on relational databases.\nThe following section will first introduce three important graph concepts, which are outlined in Fig.\u00a04: (a) The schema graph (cf.\u00a0Sec.\u00a03.1), table-level graph, where one table corresponds to one node. (b) The relational entity graph (cf. Sec.\u00a03.2), an entity-level graph, with a node for each entity in each table, and edges are defined via foreign-primary key connections between entities. (c) The time-consistent computation graph (cf.\u00a0Sec.\u00a03.3), which acts as an explicit training example for graph neural networks.\nWe describe generic procedures to map between graph types, and finally introduce our GNN blueprint for end-to-end learning on relational databases (cf.\u00a0Sec.\u00a03.4).\n\n\n\n\n\n(a) Schema Graph\n\n\n\n\n(b) Relational Entity Graph\n\n\n\n\n(c) Computation Graphs for different time t\ud835\udc61titalic_t\n\n\n\n\nFigure 4: Three different kinds of graphs. (a) The schema graph arises from the given relational tables. Each node denotes a table, and an edge between tables indicates that primary keys in one are foreign keys in the other. (b) The entity graph has one node for each entity in each table, and edges given by primary-foreign key links. The entity graph is heterogeneous with node and edge types defined by the schema graph. The nodes have a timestamp (illustrated by arrow-of-time), originating from the timestamp column of the table.\n(c) Using a temporal sampling strategy and a task description in form of training table containing different time s\ud835\udc60sitalic_s, we obtain time-consistent computation graphs as training examples that naturally respect temporal order and map well to parallel compute.\n\n\n\n\n3.1 Schema Graph\n\nThe first graph in our blueprint is the schema graph (cf. Fig. 4a), which describes the table-level structure of data.\nGiven a relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) as defined in Sec.\u00a02, we let \u2112\u22121={(Tpkey,Tfkey)\u2223(Tfkey,Tpkey)\u2208\u2112}superscript\u21121conditional-setsubscript\ud835\udc47pkeysubscript\ud835\udc47fkeysubscript\ud835\udc47fkeysubscript\ud835\udc47pkey\u2112\\mathcal{L}^{-1}=\\{(T_{\\rm pkey},T_{\\rm fkey})\\mid(T_{\\rm fkey},T_{\\rm pkey})%\n\\in\\mathcal{L}\\}caligraphic_L start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = { ( italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT ) \u2223 ( italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT ) \u2208 caligraphic_L } denote its inverse set of links.\nThen, the schema graph is the graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) that arises from the relational database, with node set \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T and edge set \u211b=\u2112\u222a\u2112\u22121\u211b\u2112superscript\u21121\\mathcal{R}=\\mathcal{L}\\cup\\mathcal{L}^{-1}caligraphic_R = caligraphic_L \u222a caligraphic_L start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT.\nInverse links ensure that all tables are reachable within the schema graph.\nThe schema graph nodes serve as type definitions for the heterogeneous relational entity graph, which we define next.\n\n\n\n\n3.2 Relational Entity Graph\n\nTo formulate a graph suitable for processing with GNNs, we introduce the relational entity graph, which has entity-level nodes and serves as the basis of the proposed relational learning framework.\n\n\nOur relational entity graph is a heterogeneous graph G=(\ud835\udcb1,\u2130,\u03d5,\u03c8)\ud835\udc3a\ud835\udcb1\u2130italic-\u03d5\ud835\udf13G=(\\mathcal{V},\\mathcal{E},\\phi,\\psi)italic_G = ( caligraphic_V , caligraphic_E , italic_\u03d5 , italic_\u03c8 ), with node set \ud835\udcb1\ud835\udcb1\\mathcal{V}caligraphic_V and edge set \u2130\u2286\ud835\udcb1\u00d7\ud835\udcb1\u2130\ud835\udcb1\ud835\udcb1\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}caligraphic_E \u2286 caligraphic_V \u00d7 caligraphic_V and type mapping functions \u03d5:\ud835\udcb1\u2192\ud835\udcaf:italic-\u03d5\u2192\ud835\udcb1\ud835\udcaf\\phi:\\mathcal{V}\\rightarrow\\mathcal{T}italic_\u03d5 : caligraphic_V \u2192 caligraphic_T and \u03c8:\u2130\u2192\u211b:\ud835\udf13\u2192\u2130\u211b\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}italic_\u03c8 : caligraphic_E \u2192 caligraphic_R, where each node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V belongs to a node type \u03d5\u2062(v)\u2208\ud835\udcafitalic-\u03d5\ud835\udc63\ud835\udcaf\\phi(v)\\in\\mathcal{T}italic_\u03d5 ( italic_v ) \u2208 caligraphic_T and each edge e\u2208\u2130\ud835\udc52\u2130e\\in\\mathcal{E}italic_e \u2208 caligraphic_E belongs to an edge type \u03c8\u2062(e)\u2208\u211b\ud835\udf13\ud835\udc52\u211b\\psi(e)\\in\\mathcal{R}italic_\u03c8 ( italic_e ) \u2208 caligraphic_R.\nSpecifically, the sets \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T and \u211b\u211b\\mathcal{R}caligraphic_R from the schema graph define the node and edge types of our relational entity graph.\n\n\nGiven a schema graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) with tables T={v1,\u2026,vnT}\u2208\ud835\udcaf\ud835\udc47subscript\ud835\udc631\u2026subscript\ud835\udc63subscript\ud835\udc5b\ud835\udc47\ud835\udcafT=\\{v_{1},...,v_{n_{T}}\\}\\in\\mathcal{T}italic_T = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_v start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT } \u2208 caligraphic_T as defined in Sec.\u00a02, we define the node set in our relational entity graph as the union of all entries in all tables \ud835\udcb1=\u22c3T\u2208\ud835\udcafT\ud835\udcb1subscript\ud835\udc47\ud835\udcaf\ud835\udc47\\mathcal{V}=\\bigcup_{T\\in\\mathcal{T}}Tcaligraphic_V = \u22c3 start_POSTSUBSCRIPT italic_T \u2208 caligraphic_T end_POSTSUBSCRIPT italic_T.\nIts edge set is then defined as\n\n\n\n\u2130={(v1,v2)\u2208\ud835\udcb1\u00d7\ud835\udcb1\u2223pv2\u2208\ud835\udca6v1\u2062\u00a0or\u00a0\u2062pv1\u2208\ud835\udca6vv}\u2062,\u2130conditional-setsubscript\ud835\udc631subscript\ud835\udc632\ud835\udcb1\ud835\udcb1subscript\ud835\udc5dsubscript\ud835\udc632subscript\ud835\udca6subscript\ud835\udc631\u00a0or\u00a0subscript\ud835\udc5dsubscript\ud835\udc631subscript\ud835\udca6subscript\ud835\udc63\ud835\udc63,\\mathcal{E}=\\{(v_{1},v_{2})\\in\\mathcal{V}\\times\\mathcal{V}\\mid p_{v_{2}}\\in%\n\\mathcal{K}_{v_{1}}\\text{ or }p_{v_{1}}\\in\\mathcal{K}_{v_{v}}\\}\\textnormal{,}caligraphic_E = { ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2208 caligraphic_V \u00d7 caligraphic_V \u2223 italic_p start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2208 caligraphic_K start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT or italic_p start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2208 caligraphic_K start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT } ,\n\n(1)\n\n\ni.e. the entity-level pairs that arise from the primary-foreign key relationships in the database.\nWe equip the relational entity graph with the following key information:\n\n\n\u2022\n\nType mapping functions \u03d5:\ud835\udcb1\u2192\ud835\udcaf:italic-\u03d5\u2192\ud835\udcb1\ud835\udcaf\\phi:\\mathcal{V}\\rightarrow\\mathcal{T}italic_\u03d5 : caligraphic_V \u2192 caligraphic_T and \u03c8:\u2130\u2192\u211b:\ud835\udf13\u2192\u2130\u211b\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}italic_\u03c8 : caligraphic_E \u2192 caligraphic_R, mapping nodes and edges to respective elements of the schema graph, making the graph heterogeneous. We set \u03d5\u2062(v)=Titalic-\u03d5\ud835\udc63\ud835\udc47\\phi(v)=Titalic_\u03d5 ( italic_v ) = italic_T for all v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T and \u03c8\u2062(v1,v2)=(\u03d5\u2062(v1),\u03d5\u2062(v2))\u2208\u211b\ud835\udf13subscript\ud835\udc631subscript\ud835\udc632italic-\u03d5subscript\ud835\udc631italic-\u03d5subscript\ud835\udc632\u211b\\psi(v_{1},v_{2})=(\\phi(v_{1}),\\phi(v_{2}))\\in\\mathcal{R}italic_\u03c8 ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( italic_\u03d5 ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_\u03d5 ( italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) \u2208 caligraphic_R if (v1,v2)\u2208\u2130subscript\ud835\udc631subscript\ud835\udc632\u2130(v_{1},v_{2})\\in\\mathcal{E}( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2208 caligraphic_E.\n\n\n\n\u2022\n\nTime mapping function \u03c4:\ud835\udcb1\u2192\ud835\udc9f:\ud835\udf0f\u2192\ud835\udcb1\ud835\udc9f\\tau:\\mathcal{V}\\rightarrow\\mathcal{D}italic_\u03c4 : caligraphic_V \u2192 caligraphic_D, mapping nodes to its timestamp: \u03c4:v\u21a6tv:\ud835\udf0fmaps-to\ud835\udc63subscript\ud835\udc61\ud835\udc63\\tau:v\\mapsto t_{v}italic_\u03c4 : italic_v \u21a6 italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (as defined in Sec.\u00a02.1), introducing time as a central component and establishes the temporality of the graph. The value \u03c4\u2062(v)\ud835\udf0f\ud835\udc63\\tau(v)italic_\u03c4 ( italic_v ) denotes the point in time in which the table row v\ud835\udc63vitalic_v became available or \u2212\u221e-\\infty- \u221e in case of non-temporal rows.\n\n\n\n\u2022\n\nEmbedding vectors \ud835\udc21v\u2208\u211dd\u03d5\u2062(v)subscript\ud835\udc21\ud835\udc63superscript\u211dsubscript\ud835\udc51italic-\u03d5\ud835\udc63\\mathbf{h}_{v}\\in\\mathbb{R}^{d_{\\phi(v)}}bold_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT end_POSTSUPERSCRIPT for each v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V, which contains an embedding vector for each node in the graph. Initial embeddings are obtained via multi-modal column encoders as described in Sec.\u00a03.4.3. Final embeddings are computed via GNNs outlined in Section 3.4.\n\n\n\n\n\nAn example of a relational entity graph for a given schema graph is given in Fig.\u00a03(b).\nThe graph contains a node for each row in the database tables. Two nodes are connected if the foreign key entry in one table row links to the primary key entry of another table row. Node and edge types are defined by the schema graph. Nodes resulting from temporal tables carry the timestamp from the respective row, allowing temporal message passing, which is described next.\n\n\n\n\n3.3 Time-Consistent Computational Graphs\n\nGiven a relational entity graph and a training table (cf.\u00a0Sec.\u00a02.2), we need to be able to query the graph at specific points in time which then serve as explicit training examples used as input to the model.\nIn particular, we create a subgraph from the relational entity graph induced by the set of foreign keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and its timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT of a training example in the training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT.\nThis subgraph then acts as a local and time-consistent computation graph to predict its ground-truth label yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT.\n\n\nAlgorithm 1  Time-Consistent Computation Graph\n\nRelational entity graph G=(\ud835\udcb1,\u2130)\ud835\udc3a\ud835\udcb1\u2130G=(\\mathcal{V},\\mathcal{E})italic_G = ( caligraphic_V , caligraphic_E ), number of hops L\ud835\udc3fLitalic_L, seed node v0\u2208\ud835\udcb1subscript\ud835\udc630\ud835\udcb1v_{0}\\in\\mathcal{V}italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 caligraphic_V, seed time t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R\n\nNeighborhood sizes (m1,\u2026,mL)\u2208\u2115Lsubscript\ud835\udc5a1\u2026subscript\ud835\udc5a\ud835\udc3fsuperscript\u2115\ud835\udc3f(m_{1},...,m_{L})\\in\\mathbb{N}^{L}( italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_m start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) \u2208 blackboard_N start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT\n\nComputation graph Gcomp=(\ud835\udcb1comp,\u2130comp)subscript\ud835\udc3acompsubscript\ud835\udcb1compsubscript\u2130compG_{\\rm comp}=(\\mathcal{V}_{\\rm comp},\\mathcal{E}_{\\rm comp})italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT = ( caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT , caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT )\n\n\n\ud835\udcb10\u2190{v0}\u2190subscript\ud835\udcb10subscript\ud835\udc630\\mathcal{V}_{0}\\leftarrow\\{v_{0}\\}caligraphic_V start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 { italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT }, \u2003\u21300\u2190\u2205\u2190subscript\u21300\\mathcal{E}_{0}\\leftarrow\\emptysetcaligraphic_E start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 \u2205\n\n\nfor\u00a0i\u2208{1,\u2026,L}\ud835\udc561\u2026\ud835\udc3fi\\in\\{1,...,L\\}italic_i \u2208 { 1 , \u2026 , italic_L }\u00a0do\n\n\u00a0\u00a0\u00a0\u00a0\u00a0for\u00a0v\u2208\ud835\udcb1i\u22121\ud835\udc63subscript\ud835\udcb1\ud835\udc561v\\in\\mathcal{V}_{i-1}italic_v \u2208 caligraphic_V start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT\u00a0do\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2130i\u2190SELECTmi\u2062({(w,v)\u2208\u2130\u2223\u03c4\u2062(v)\u2264t})\u2190subscript\u2130\ud835\udc56subscriptSELECTsubscript\ud835\udc5a\ud835\udc56conditional-set\ud835\udc64\ud835\udc63\u2130\ud835\udf0f\ud835\udc63\ud835\udc61\\mathcal{E}_{i}\\leftarrow\\textnormal{SELECT}_{m_{i}}(\\{(w,v)\\in\\mathcal{E}\\mid%\n\\tau(v)\\leq t\\})caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 SELECT start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( { ( italic_w , italic_v ) \u2208 caligraphic_E \u2223 italic_\u03c4 ( italic_v ) \u2264 italic_t } ) \u25b7\u25b7\\triangleright\u25b7 Select a maximum of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT filtered edges\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\ud835\udcb1i\u2190{w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130i}\u2190subscript\ud835\udcb1\ud835\udc56conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63subscript\u2130\ud835\udc56\\mathcal{V}_{i}\\leftarrow\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}_{i}\\}caligraphic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } \u25b7\u25b7\\triangleright\u25b7 Gather nodes for the sampled edges\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0end\u00a0for\n\n\nend\u00a0for\n\n\n\ud835\udcb1comp\u2190\u22c3i=1L\ud835\udcb1i\u2190subscript\ud835\udcb1compsuperscriptsubscript\ud835\udc561\ud835\udc3fsubscript\ud835\udcb1\ud835\udc56\\mathcal{V}_{\\rm comp}\\leftarrow\\bigcup_{i=1}^{L}\\mathcal{V}_{i}caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT \u2190 \u22c3 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT caligraphic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \u2003\u2130comp\u2190\u22c3i=1L\u2130i\u2190subscript\u2130compsuperscriptsubscript\ud835\udc561\ud835\udc3fsubscript\u2130\ud835\udc56\\mathcal{E}_{\\rm comp}\\leftarrow\\bigcup_{i=1}^{L}\\mathcal{E}_{i}caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT \u2190 \u22c3 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\n\n\n\n\nThe computational graphs obtained via neighbor sampling\u00a0(Hamilton et\u00a0al., 2017) allow the scalability of our proposed approach to modern large-scale relational data with billions of table rows, while ensuring the temporal constraints\u00a0(Wang et\u00a0al., 2021).\nSpecifically, given a number of hops L\ud835\udc3fLitalic_L to sample, a seed node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V, and a timestamp t\ud835\udc61titalic_t induced by a training example, the computation graph is defined as Gcomp=(\ud835\udcb1comp,\u2130comp)subscript\ud835\udc3acompsubscript\ud835\udcb1compsubscript\u2130compG_{\\rm comp}=(\\mathcal{V}_{\\rm comp},\\mathcal{E}_{\\rm comp})italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT = ( caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT , caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT ) as the output of Alg.\u00a01.\nThe algorithm traverses the graph starting from the seed node v\ud835\udc63vitalic_v for L\ud835\udc3fLitalic_L iterations. In iteration i\ud835\udc56iitalic_i, it gathers a maximum of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT neighbors available up to timestamp t\ud835\udc61titalic_t, using one of three selection strategies:\n\n\n\u2022\n\nUniform temporal sampling selects uniformly sampled random neighbors.\n\n\n\n\u2022\n\nOrdered temporal sampling takes the latest neighbors, ordered by time \u03c4\ud835\udf0f\\tauitalic_\u03c4.\n\n\n\n\u2022\n\nBiased temporal sampling selects random neighbors sampled from a multinomial probability distribution induced by \u03c4\ud835\udf0f\\tauitalic_\u03c4. For instance, sampling can be performed proportional to relative neighbor time or biased towards specific important historical moments.\n\n\n\n\n\nThe temporal neighbor sampling is performed purely on the graph structure of the relational entity graph, without requiring initial embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. The bounded size of computation graph Gcompsubscript\ud835\udc3acompG_{\\rm comp}italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT allows for efficient mini-batching on GPUs, independent of relational entity graph size.\nIn practice, we perform temporal neighbor sampling on-the-fly, which allows us to operate on a shared relational entity graph across all training examples, from which we can then restore local and historical snapshots very efficiently.\nExamples of computation graphs are shown in Fig.\u00a03(c).\n\n\n\n\n3.4 Task-Specific Temporal Graph Neural Networks\n\nGiven a time-consistent computational graph and its future label to predict, we define a generic multi-stage deep learning architecture as follows:\n\n\n1.\n\nTable-level column encoders that encode table row data into initial node embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, as described in Sec.\u00a03.4.3.\n\n\n\n2.\n\nA stack of L\ud835\udc3fLitalic_L relational-temporal message passing layers (cf.\u00a0Sec.\u00a03.4.1).\n\n\n\n3.\n\nA task-specific model head, mapping final node embeddings to a prediction (cf.\u00a0Sec.\u00a03.4.2).\n\n\n\n\nThe whole architecture, consisting of table-level encoders, message passing layers and task specific model heads can be trained end-to-end to obtain an optimal model for the given task.\n\n\n\n3.4.1 Relational-Temporal Message Passing\n\nThis section introduces a generic framework for heterogeneous message passing GNNs on relational entity graphs as defined in Sec.\u00a03.2.\nA message passing operator in the given relational framework needs to respect the heterogeneous nature as well as the temporal properties of the graph. This is ensured by filtering nodes based on types and time. Thus, we briefly introduce heterogeneous message passing before we turn to our temporal message passing.\n\n\nHeterogeneous Message Passing.\n\nMessage-Passing Graph Neural Networks (MP-GNNs)\u00a0(Gilmer et\u00a0al., 2017; Fey and Lenssen, 2019) are a generic computational framework to define deep learning architectures on graph-structered data. Given a heterogeneous graph G=(\ud835\udcb1,\u2130,\u03d5,\u03c8)\ud835\udc3a\ud835\udcb1\u2130italic-\u03d5\ud835\udf13G=(\\mathcal{V},\\mathcal{E},\\phi,\\psi)italic_G = ( caligraphic_V , caligraphic_E , italic_\u03d5 , italic_\u03c8 ) with initial node embeddings {\ud835\udc21v(0)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc210\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(0)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT, a single message passing iteration computes updated features {\ud835\udc21v(i+1)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(i+1)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT from features {\ud835\udc21v(i)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(i)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT given by the previous iteration.\nOne iteration takes the form: \n\n\n\n\ud835\udc21v(i+1)=f\u2062(\ud835\udc21v(i),{{g\u2062(\ud835\udc21w(i))\u2223w\u2208\ud835\udca9\u2062(v)}})\u2062,subscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63\ud835\udc53subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63conditional-set\ud835\udc54subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc64\ud835\udc64\ud835\udca9\ud835\udc63,\\mathbf{h}^{(i+1)}_{v}=f(\\mathbf{h}^{(i)}_{v},\\{\\{g(\\mathbf{h}^{(i)}_{w})\\mid w%\n\\in\\mathcal{N}(v)\\}\\})\\textnormal{,}bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_f ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , { { italic_g ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) \u2223 italic_w \u2208 caligraphic_N ( italic_v ) } } ) ,\n\n(2)\n\n\nwhere f\ud835\udc53fitalic_f and g\ud835\udc54gitalic_g are arbitrary differentiable functions with optimizable parameters and {{\u22c5}}\u22c5\\{\\{\\cdot\\}\\}{ { \u22c5 } } an permutation invariant set aggregator, such as mean, max, sum, or a combination. Heterogeneous message passing\u00a0(Schlichtkrull et\u00a0al., 2018; Hu et\u00a0al., 2020) is a nested version of Eq.\u00a02, adding an aggregation over all incoming edge types to learn distinct message types:\n\n\n\n\ud835\udc21v(i+1)=f\u03d5\u2062(v)\u2062(\ud835\udc21v(i),{{fR\u2062({{gR\u2062(\ud835\udc21w(i))\u2223w\u2208\ud835\udca9R\u2062(v)}})|\u2200R=(T,\u03d5\u2062(v))\u2208\u211b}})\u2062,subscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63subscript\ud835\udc53italic-\u03d5\ud835\udc63subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63conditional-setsubscript\ud835\udc53\ud835\udc45conditional-setsubscript\ud835\udc54\ud835\udc45subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc64\ud835\udc64subscript\ud835\udca9\ud835\udc45\ud835\udc63for-all\ud835\udc45\ud835\udc47italic-\u03d5\ud835\udc63\u211b,\\mathbf{h}^{(i+1)}_{v}=f_{\\phi(v)}\\Bigl{(}\\mathbf{h}^{(i)}_{v},\\Bigl{\\{}\\Bigl{%\n\\{}f_{R}(\\{\\{g_{R}(\\mathbf{h}^{(i)}_{w})\\mid w\\in\\mathcal{N}_{R}(v)\\}\\})\\,\\Big%\n{|}\\,\\forall R=(T,\\phi(v))\\in\\mathcal{R}\\Bigr{\\}}\\Bigr{\\}}\\Bigr{)}\\textnormal{,}bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , { { italic_f start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( { { italic_g start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) \u2223 italic_w \u2208 caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) } } ) | \u2200 italic_R = ( italic_T , italic_\u03d5 ( italic_v ) ) \u2208 caligraphic_R } } ) ,\n\n(3)\n\n\nwhere \ud835\udca9R\u2062(v)={w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130\u2062\u00a0and\u00a0\u2062\u03c8\u2062(w,v)=R}subscript\ud835\udca9\ud835\udc45\ud835\udc63conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63\u2130\u00a0and\u00a0\ud835\udf13\ud835\udc64\ud835\udc63\ud835\udc45\\mathcal{N}_{R}(v)=\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}\\textnormal{ and }%\n\\psi(w,v)=R\\}caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) = { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E and italic_\u03c8 ( italic_w , italic_v ) = italic_R } denotes the R-specific neighborhood of node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V.\nThis formulation supports a wide range of different graph neural network operators, which define the specific form of functions f\u03d5\u2062(v)subscript\ud835\udc53italic-\u03d5\ud835\udc63f_{\\phi(v)}italic_f start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT, fRsubscript\ud835\udc53\ud835\udc45f_{R}italic_f start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT, gRsubscript\ud835\udc54\ud835\udc45g_{R}italic_g start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT and {{\u22c5}}\u22c5\\{\\{\\cdot\\}\\}{ { \u22c5 } }\u00a0(Fey and Lenssen, 2019).\n\n\n\n\nTemporal Message Passing.\n\nGiven a relational entity graph G=(\ud835\udcb1,\u2130,\ud835\udcaf,\u211b)\ud835\udc3a\ud835\udcb1\u2130\ud835\udcaf\u211bG=(\\mathcal{V},\\mathcal{E},\\mathcal{T},\\mathcal{R})italic_G = ( caligraphic_V , caligraphic_E , caligraphic_T , caligraphic_R ) with attached mapping functions \u03c8,\u03d5,\u03c4\ud835\udf13italic-\u03d5\ud835\udf0f\\psi,\\phi,\\tauitalic_\u03c8 , italic_\u03d5 , italic_\u03c4 and initial node embeddings {\ud835\udc21v(0)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc210\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(0)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT and an example specific seed time t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R (cf. Sec.\u00a02.2) , we obtain a set of deep node embeddings {\ud835\udc21v(L)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(L)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT by L\ud835\udc3fLitalic_L consecutive applications of Eq.\u00a03, where we additionally filter R\ud835\udc45Ritalic_R-specific neighborhoods based on their timestamp, i.e. replace \ud835\udca9R\u2062(v)subscript\ud835\udca9\ud835\udc45\ud835\udc63\\mathcal{N}_{R}(v)caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) with\n\n\n\n\ud835\udca9R\u2264t\u2062(v)={w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130\u2062,\u00a0\u2062\u03c8\u2062(w,v)=R\u2062, and\u00a0\u2062\u03c4\u2062(w)\u2264t}\u2062,subscriptsuperscript\ud835\udca9absent\ud835\udc61\ud835\udc45\ud835\udc63conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63\u2130,\u00a0\ud835\udf13\ud835\udc64\ud835\udc63\ud835\udc45, and\u00a0\ud835\udf0f\ud835\udc64\ud835\udc61,\\mathcal{N}^{\\leq t}_{R}(v)=\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}%\n\\textnormal{, }\\psi(w,v)=R\\textnormal{, and }\\tau(w)\\leq t\\}\\text{,}caligraphic_N start_POSTSUPERSCRIPT \u2264 italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) = { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E , italic_\u03c8 ( italic_w , italic_v ) = italic_R , and italic_\u03c4 ( italic_w ) \u2264 italic_t } ,\n\n(4)\n\n\nrealized by the temporal sampling procedure presented in Sec.\u00a03.3. The formulation naturally respects time by only aggregating messages from nodes that were available before the given seed time s\ud835\udc60sitalic_s. The given formulation is agnostic to specific implementations of message passing and supports a wide range of different operators.\n\n\n\n\n\n3.4.2 Prediction with Model Heads\n\nThe model described so far is task-agnostic and simply propagates information through the relational entity graph to produce generic node embeddings. We obtain a task-specific model by combining our graph with a training table, leading to specific model heads and loss functions. We distinguish between (but are not limited to) two types of tasks: node-level prediction and link-level prediction.\n\n\nNode-level Model Head.\n\nGiven a batch of N\ud835\udc41Nitalic_N node level training table examples {(\ud835\udca6,t,y)i}i=1Nsuperscriptsubscriptsubscript\ud835\udca6\ud835\udc61\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\{(\\mathcal{K},t,y)_{i}\\}_{i=1}^{N}{ ( caligraphic_K , italic_t , italic_y ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT (cf. Sec.\u00a02.2), where \ud835\udca6={k}\ud835\udca6\ud835\udc58\\mathcal{K}=\\{k\\}caligraphic_K = { italic_k } contains the primary key of node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v~{}\\in\\mathcal{V}italic_v \u2208 caligraphic_V in the relational entity graph, t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R is the seed time, and y\u2208\u211dd\ud835\udc66superscript\u211d\ud835\udc51y\\in\\mathbb{R}^{d}italic_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is the target value. Then, the node-level model head is a function that maps node-level embeddings \ud835\udc21v(L)subscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63\\mathbf{h}^{(L)}_{v}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT to a prediction y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG, i.e.\n\n\n\nf:\u211ddv\u2192\u211dd\u2062,f:\ud835\udc21v(L)\u21a6y^\u2062.:\ud835\udc53\u2192superscript\u211dsubscript\ud835\udc51\ud835\udc63superscript\u211d\ud835\udc51,\ud835\udc53:maps-tosubscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63^\ud835\udc66.f:\\mathbb{R}^{d_{v}}\\rightarrow\\mathbb{R}^{d}\\textnormal{,}\\quad\\quad f:%\n\\mathbf{h}^{(L)}_{v}\\mapsto\\hat{y}\\textnormal{.}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_f : bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u21a6 over^ start_ARG italic_y end_ARG .\n\n(5)\n\n\n\n\n\nLink-level Model Head.\n\nSimilarly, we can define a link-level model head for training examples {(\ud835\udca6,t,y)i}i=1Nsuperscriptsubscriptsubscript\ud835\udca6\ud835\udc61\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\{(\\mathcal{K},t,y)_{i}\\}_{i=1}^{N}{ ( caligraphic_K , italic_t , italic_y ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT with \ud835\udca6={k1,k2}\ud835\udca6subscript\ud835\udc581subscript\ud835\udc582\\mathcal{K}=\\{k_{1},k_{2}\\}caligraphic_K = { italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT } containing primary keys of two different nodes v1,v2\u2208\ud835\udcb1subscript\ud835\udc631subscript\ud835\udc632\ud835\udcb1v_{1},v_{2}\\in\\mathcal{V}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 caligraphic_V in the relational entity graph. A function maps node embeddings \ud835\udc21v1(L)subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc631\\mathbf{h}^{(L)}_{v_{1}}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, \ud835\udc21v2(L)subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc632\\mathbf{h}^{(L)}_{v_{2}}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT to a prediction, i.e.\n\n\n\nf:\u211ddv1\u00d7\u211ddv2\u2192\u211dd\u2062,f:(\ud835\udc21v1(L),\ud835\udc21v2(L))\u21a6y^\u2062.:\ud835\udc53\u2192superscript\u211dsubscript\ud835\udc51subscript\ud835\udc631superscript\u211dsubscript\ud835\udc51subscript\ud835\udc632superscript\u211d\ud835\udc51,\ud835\udc53:maps-tosubscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc631subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc632^\ud835\udc66.f:\\mathbb{R}^{d_{v_{1}}}\\times\\mathbb{R}^{d_{v_{2}}}\\rightarrow\\mathbb{R}^{d}%\n\\textnormal{,}\\quad\\quad f:(\\mathbf{h}^{(L)}_{v_{1}},\\mathbf{h}^{(L)}_{v_{2}})%\n\\mapsto\\hat{y}\\textnormal{.}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_f : ( bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) \u21a6 over^ start_ARG italic_y end_ARG .\n\n(6)\n\n\nA task-specific loss L\u2062(y^,y)\ud835\udc3f^\ud835\udc66\ud835\udc66L(\\hat{y},y)italic_L ( over^ start_ARG italic_y end_ARG , italic_y ) provides gradient signals to all trainable parameters. The presented approach can be generalized to |\ud835\udca6|>2\ud835\udca62|\\mathcal{K}|>2| caligraphic_K | > 2 to specify subgraph-level tasks. In the first version, RelBench provides node-level tasks only.\n\n\n\n\n\n3.4.3 Multi-Modal Node Encoders\n\nThe final piece of the pipeline is to obtain the initial entity-level node embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT from the multi-modal input attributes xv=(xv1,\u2026,xvdT)\u2208\ud835\udc9cT1\u00d7\u2026\u00d7\ud835\udc9cTdTsubscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47superscriptsubscript\ud835\udc9c\ud835\udc471\u2026superscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})\\in\\mathcal{A}_{T}^{1}\\times\\ldots\\times%\n\\mathcal{A}_{T}^{d_{T}}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) \u2208 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u00d7 \u2026 \u00d7 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.\nDue to the nature of tabular data, each column element xvisuperscriptsubscript\ud835\udc65\ud835\udc63\ud835\udc56x_{v}^{i}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT lies in its own modality space \ud835\udc9cTdisuperscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc56\\mathcal{A}_{T}^{d_{i}}caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, e.g., image, text, categorical, and numerical values.\nTherefore, we use a modality-specific encoder to embed each attribute into embeddings.\nFor text and image modalities, we can naturally use pre-trained embedding models as the encoders\u00a0(Reimers and Gurevych, 2019).\nAfter all the attributes are embedded, we apply state-of-the-art tabular deep learning models\u00a0(Huang et\u00a0al., 2020; Arik and Pfister, 2021; Gorishniy et\u00a0al., 2021, 2022; Chen et\u00a0al., 2023) to fuse all the attribute embeddings into a single entity-level node embedding.\nIn practice, we rely on PyTorch Frame\u00a0(Hu et\u00a0al., 2023) that supports a variety of modality-specific encoders, such as pre-trained text embedding models, and as well as state-of-the-art deep learning models on tabular data.\n\n\n\n\n\n3.5 Discussion\n\nThe neural network architecture presented in this blueprint is end-to-end trainable on relational databases. This approach supports a wide range of tasks, such as classification, regression or link prediction in a unified way, with labels computed and stored in a training table.\nIt learns to solve tasks without requiring manual feature engineering, as typical in tabular learning. Instead, operations that are otherwise done manually, such as SQL JOIN+AGGREGATE\noperations, are learned by the GNN. More than simply replacing SQL operations, the GNN message and aggregation steps exactly match the functional form of SQL JOIN+AGGREGATE operations. In other words, the GNN is an exact neural version of SQL JOIN+AGGREGATE operations. We believe this is another important reason why message passing-based architectures are a natural learned replacement for hand-engineered features on relational tables.\n\n\n",
      "4 \u00a0RelBench: A Benchmark for Relational Deep Learning": "\n\n4 \u00a0RelBench: A Benchmark for Relational Deep Learning\n\nFigure 5: Overview of RelBench. RelBench enables training and evaluation of machine learning models on relational data. RelBench supports deep learning framework agnostic data loading, task specification, standardized data splitting, and transforming data into graph format. RelBench provides standardized evaluation metric computations, and a leaderboard for tracking progress. We additionally provide example training scripts built using PyTorch Geometric and PyTorch Frame.\n\n\nWe introduce RelBench, an open benchmark for Relational Deep Learning. The goal of RelBench is to facilitate scalable, robust, and reproducible machine learning research on relational tables. RelBench curates a diverse set of large-scale, challenging, and realistic benchmark databases and defines meaningful predictive tasks over these databases. In addition, RelBench develops a Python library for loading relational tables and tasks, constructing data graphs, and providing unified evaluation for predictive tasks. It also integrates seamlessly with existing Pytorch Geometric and PyTorch Frame functionalities. In its beta release111Website: https://relbench.stanford.edu222Package: https://github.com/snap-stanford/relbench, we announce the first two real-world relational databases, each with two curated predictive tasks.\n\n\nIn the subsequent sections (Sec.\u00a04.3 and\u00a04.4), we describe in detail the two relational databases and the predictive tasks. For each database, we show its entity relational diagrams and important statistics. For each task, we define the task formulation, entity filtering, significance of the task, and also unified evaluation metric. Finally, we demonstrate the usage of the RelBench\u2019s package in Sec.\u00a04.1.\n\n\n\n4.1 RelBench Package\n\nThe RelBench package is designed to allow easy and standardized access to Relational Deep Learning for researchers to push the state-of-the-art of this emerging field. It provides Python APIs to (1) download and process relational databases and their predictive tasks; (2) load standardized data splits and generate relevant train/validation/test tables; (3) evaluate on machine learning predictions. It also provides a flexible ecosystems of supporting tools such as automatic conversion to PyTorch Geometric graphs and integration with Pytorch Frame to produce embeddings for diverse column types. We additionally provide end-to-end scripts for training using RelBench package with GNNs and XGBoost\u00a0(Chen and Guestrin, 2016). We refer the readers to the code repository for a more detailed understanding of RelBench. Here we demonstrate the core functionality.\n\n\nTo load a relational database, simply do:\n\n\n\n\u2b07\n\n\n\nfrom\u00a0relbench.datasets\u00a0import\u00a0get_dataset\n\n\ndataset\u00a0=\u00a0get_dataset(name=\"rel-amazon\")\n\n\n\n\nIt will load the relational tables and process it into a standardized format. Next, to load the predictive task and the relevant training tables, do:\n\n\n\n\u2b07\n\n\n\ntask\u00a0=\u00a0dataset.get_task(\"rel-amazon-ltv\")\n\n\ntask.train_table,\u00a0task.val_table,\u00a0task.test_table\u00a0#\u00a0training/validation/testing\u00a0tables\n\n\n\n\nIt automatically constructs the training table for the relevant predictive task. Next, after the user trains the machine learning model, the user can use RelBench standardized evaluator:\n\n\u2b07\n\n\n\ntask.evaluate(pred)\n\n\n\n\n\n\n4.2 Temporal Splitting\n\nEvery dataset in RelBench has a validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT and a test timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. These are shared for all tasks in the dataset.\nThe test table for any task comprises of labels computed for the time window from ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT to ttest+\u03b4subscript\ud835\udc61test\ud835\udefft_{\\text{test}}+\\deltaitalic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT + italic_\u03b4, where the window size \u03b4\ud835\udeff\\deltaitalic_\u03b4 is specified for each task. Thus the model must make predictions using only information available up to time ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. Accordingly, to prevent accidental temporal leakage at test time RelBench only provides database rows with timestamps up to ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT for training and validation purposes. RelBench also provides default train and validation tables. The default validation table is constructed similar to the test table, but with the time window being tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT to tval+\u03b4subscript\ud835\udc61val\ud835\udefft_{\\text{val}}+\\deltaitalic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT + italic_\u03b4. To construct the default training table, we first sample time stamps tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT starting from tval\u2212\u03b4subscript\ud835\udc61val\ud835\udefft_{\\text{val}}-\\deltaitalic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT - italic_\u03b4 and moving backwards with a stride of \u03b4\ud835\udeff\\deltaitalic_\u03b4. This allows us to benefit from the latest available training information. Then for each tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we apply an entity filter to select the entities of interest (e.g., active users). Finally for each pair of timestamp and entity, we compute the training label based on the task definition. Users can explore other ways of constructing the training or validation table, for example by sampling timestamps with shorter strides to get more labels, as long as information after tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is not used for training.\n\n\n\n\n4.3 rel-amazon: Amazon product review e-commerce database\n\nFigure 6: rel-amazon contains two dimension tables (customers and products) and one fact table (reviews). Each review has a customer and a product foreign key. \n\n\n\nDatabase overview.\n\nThe rel-amazon relational database stores product and user purchasing behavior across Amazon\u2019s e-commerce platform. Notably, it contains rich information about each product and transaction. The product table includes price and category information; the review table includes overall rating, whether the user has actually bought the product, and the text of the review itself. We use the subset of book-related products. The entity relationships are described in Fig. 6.\n\n\n\n\nDataset statistics.\n\nrel-amazon covers 3 relational tables and contains 1.85M customers, 21.9M reviews, 506K products. This relational database spans from 1996-06-25 to 2018-09-28. The validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is set to 2014-01-21 and the testing timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is 2016-01-01. Thus, tasks can have a window size up to 2 years.\n\n\n\n\n4.3.1 rel-amazon-ltv: Predict the life time value (LTV) of a user\n\nTask definition: Predict the life time value of a user, defined as the sum of prices of the products that the user will buy and review in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that wrote review in the past two years before the timestamp.\n\n\nTask significance: By accurately forecasting LTV, the e-commerce platform can gain insights into user purchasing patterns and preferences, which is essential when making strategic decisions related to marketing, product recommendations, and inventory management. Understanding a user\u2019s future purchasing behavior helps in tailoring personalized shopping experiences and optimizing product assortments, ultimately enhancing customer satisfaction and loyalty.\n\n\nMachine learning task: Regression. The target ranges from $0-$33,858.4 in the given time window in the training table.\n\n\nEvaluation metric: Mean Absolute Error (MAE).\n\n\n\n\n4.3.2 rel-amazon-churn: Predict if the user churns\n\nTask definition: Predict if the user will not buy any product in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that wrote review in the past two years before the timestamp.\n\n\nTask significance: Predicting churn accurately allows companies to identify potential risks of customer attrition early on. By understanding which customers are at risk of disengagement, businesses can implement targeted interventions to improve customer retention. This may include personalized marketing, tailored offers, or enhanced customer service. Effective churn prediction enables businesses to maintain a stable customer base, ensuring sustained revenue streams and facilitating long-term planning and resource allocation.\n\n\n\nMachine learning task: Binary classification. The label is 1 when user churns and 0 vice versus.\n\n\nEvaluation metric: Average precision (AP).\n\n\n\n\n\n4.4 rel-stackex: Stack exchange question-and-answer website database\n\nFigure 7: Entity relational diagrams of Stack-Exchange.\n\n\nDatabase overview.\n\nStack Exchange is a network of question-and-answer websites on topics in diverse fields, each site covering a specific topic, where questions, answers, and users are subject to a reputation award process. The reputation system allows the sites to be self-moderating. In our benchmark, we use the stats-exchange site. We derive from the raw data dump from 2023-09-12. Figure\u00a07 shows its entity relational diagrams.\n\n\n\nDataset statistics.\n\nrel-stackex covers 7 relational tables and contains 333K users, 415K posts, 794K comments, 1.67M votes, 103K post links, 590K badges records, 1.49M post history records. This relational database spans from 2009-02-02 to 2023-09-03. The validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is set to be 2019-01-01 and the testing timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is set to be 2021-01-01. Thus, the maximum time window size for predictive task is 2 years.\n\n\n\n\n4.4.1 rel-stackex-engage: Predict if a user will be an active contributor to the site\n\nTask definition: Predict if the user will make any contribution, defined as vote, comment, or post, to the site in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that have made at least one comment/post/vote before the timestamp.\n\n\nTask significance: By accurately forecasting the levels of user contribution, website administrators can effectively gauge and oversee user activity. This insight allows for well-informed choices across various business aspects. For instance, it aids in preempting and mitigating user attrition, as well as in enhancing strategies to foster increased user interaction and involvement. This predictive task serves as a crucial tool in optimizing user experience and sustaining a dynamic and engaged user base.\n\n\nMachine learning task: Binary classification. The label is 1 when user contributes to the site and 0 otherwise.\n\n\nEvaluation metric: Average Precision (AP).\n\n\n\n\n4.4.2 rel-stackex-votes: Predict the number of upvotes a question will receive\n\nTask definition: Predict the popularity of a question post in the next six months. The popularity is defined as the number of upvotes the post will receive.\n\n\nEntity filtering: We filter on question posts that are posted recently in the past 2 years before the timestamp. This ensures that we do not predict on old questions that have been outdated.\n\n\nTask significance: Predicting the popularity of a question post is valuable as it empowers site managers to predict and prepare for the influx of traffic directed towards that particular post. This foresight is instrumental in making strategic business decisions, such as curating question recommendations and optimizing content visibility. Understanding which posts are likely to attract more attention helps in tailoring the user experience and managing resources effectively, ensuring that the most engaging and relevant content is highlighted to maintain and enhance user engagement.\n\n\nMachine learning task: Regression. The target ranges from 0-52 number of upvotes in the given time window in the training table.\n\n\nEvaluation metric: Mean Absolute Error (MAE).\n\n\n\n",
      "5 A New Program for Graph Representation Learning": "\n\n5 A New Program for Graph Representation Learning\n\nFigure 8: Relational Deep Learning brings new challenges at all levels of the machine learning stack.\n\n\nDeveloping Relational Deep Learning requires a new research program in graph representation learning on relational data. There are opportunities at all levels of the research stack, including (pre-)training methods, GNN architectures, multimodality, new graph formulations, and scaling to large distributed relational databases for many industrial settings. Here we discuss several promising aspects of this research program, aiming to stimulate the interest of the graph machine learning community.\n\n\n\n5.1 Scaling Relational Deep Learning\n\nRelational databases are often vast, with information distributed across many servers with constrained communication. However, relational data has a non-typical graph structure which may help scale Relational Deep Learning more efficiently.\n\n\nDistributed Training on Relational Data.\n\nExisting distributed machine learning techniques often assume that each server contains data of the same type. Relational data on the other hand, naturally partitions in to pieces, bringing new challenges depending on the partitioning technique used. Horizontal partitioning, known as sharding, is the most common approach. It creates database shards by splitting tables row-wise according to some criterion (e.g., all customer with zipcode in a given range). In this case, a table containing a customers personal record may lie on a distinct server from the table recording recent purchase activity, leading to communication bottlenecks when attempting to train models by sensing messages between purchases and customer records. Less common, but also possible, is vertical splitting. Different splitting options suggests an opportunity to (a) develop specialized distributed learning methods that exploit the vertical or horizontal partitioning, and (b) design further storage arrangements that may be more suited to Relational Deep Learning. The question of graph partitioning arises in all large-scale graph machine learning settings, however in this case we are fortunate to have non-typical graph structure (i.e., it follows the schema) which makes it easier to find favourable partitions.\n\n\n\nLocalized Learning.\n\nFor many predictive tasks it is neither feasible (due to database size) nor desirable (due to task narrowness) to propagate GNN messages across the entire graph during training. In such cases, sampling schemes that preserve locality by avoiding exponential growth in GNN receptive field are needed. This is easily addressed in cases with prior knowledge on the relevant entities. How to scale to deep models that remain biased models towards local computation in cases with no prior knowledge remains an interesting open question.\n\n\n\n\n\n5.2 Building Graphs from Relational Data\n\nAn essential ingredient of Relational Deep Learning is the use of an individual entity and relation-level graph on which to apply inter-entity message passing to learn entity embeddings based on relations to other entities. In Sec. 3.2 we introduced one such graph, the relational entity graph, a general procedure for viewing any relational database as a graph. Whilst a natural choice, we do not propose dogmatically viewing entities as nodes and relations as edges. Instead, the essential property of the relational entity graph is that it is full-resolution. That is, each entity and each primary-foreign key link in the relational database corresponds to its own graph-piece, so that the relational database is exactly encoded in graph form. It is this property that we expect potential alternative graph designs to share. Beyond this stipulation, many creative graph choices are possible, and we discuss some possibilities here.\n\n\nForeign-key Hypergraph.\n\nFact tables often contain entities with a fixed foreign-key pattern (e.g., in rel-amazon a row in a review table always refers to a customer and a product foreign key). The relational entity graph views a review as a node, with edges to a customer and product. However, another possibility is to view this as a single hyperedge between review, customer, and product. Alternative graph choices may alter (and improve) information propagation between entities (cf. Sec. 5.3).\n\n\n\nStale Link Pruning.\n\nEntities that have been active for a long time may have a lot of links to other entities. Many of these links may be stale, or uninformative, for certain tasks. For example, the purchasing patterns of a longtime customer during childhood are likely to be less relevant to their purchasing patterns in adulthood. Links that are stale for a certain task may hurt predictive power by obfuscating true predictive signals, and reduce model efficiency due to processing uninformative data. This situation calls for careful stale link and entity handling to focus on relevant information. Promising methods may include pruning or preaggregating stale links. More generally, how to deal with more gradual distribution drift over time is an open question.\n\n\n\n\n\n5.3 GNN Architectures for Relational Data\n\nViewing a relational database a graphs leads to graphs with structural properties that are consistent across databases. To properly exploit this structure new specialized GNN architectures are needed. Here we discuss several concrete directions for designing new architectures.\n\n\nExpressive GNNs for Relational Data.\n\nRelational entity graphs (cf. Sec. 3.2) obey certain structural constraints. For example, as nodes correspond to entities drawn from one of several tables, the relational entity graph is naturally n\ud835\udc5bnitalic_n-partite, where n\ud835\udc5bnitalic_n is the total number of tables. This suggests that GNNs for relational data should be designed to be capable of learning expressive decision rules over n\ud835\udc5bnitalic_n-partite graphs. Unfortunately, recent studies find that many GNN architectures fail to distinguish biconnected graphs (Zhang et\u00a0al., 2023). Further work is needed to design expressive n\ud835\udc5bnitalic_n-partite graph models.\n\n\nRelational entity graphs also have regularity in edge-connectivity. For instance, in rel-amazon entities in the review table always refer to one customer and one product.\nConsistent edge patterns are described by the structure of the schema graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) (cf. Sec. 3.1). How to integrate prior knowledge of the graph structure of (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) into GNNs that operate on an entity-level graph (the relational entity graph) remains an open question. These two examples serve only to illustrate the possibilities for architecture design based on the structure of relational entity graphs. Many other structural properties of relational data may lead to innovative new expressive GNN architectures.\n\n\n\nQuery Language Inspired Models.\n\nSQL operations are known to be extremely powerful operations for manipulating relational data. Their weakness is that they do not have differentiable parameters, making end-to-end learnability impossible. Despite this, there are close similarities between key SQL queries and the computation process of graph neural networks. For instance, a very common way to combine information across tables T1,T2subscript\ud835\udc471subscript\ud835\udc472T_{1},T_{2}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT in SQL is to (1) create a table T3subscript\ud835\udc473T_{3}italic_T start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT by applying a JOIN operation to table T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, by matching foreign keys in T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to primary keys in T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, then (2) produce a final table with the same number of rows as T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT by applying an AGGREGATE operation to rows in T3subscript\ud835\udc473T_{3}italic_T start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT with foreign keys pointing to the same entity in T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. There are many choices of AGGREGATE operation such as SUM, MEAN and COUNT. This process directly mirrors GNN computations of messages from neighboring nodes, followed by message aggregation. In other words, GNNs can be thought of as a neural version of SQL JOIN+AGGREGATE operations. This suggests that an opportunity for powerful new neural network architectures by designing differentiable computation blocks that algorithmically align (Xu et\u00a0al., 2020) to existing SQL operations that are known to be useful.\n\n\n\nNew Message Passing Schemes.\n\nBeyond expressivity, new architectures may also improve information propagation between entities. For instance, collaborative filtering methods enhance predictions by identify entities with similar behavior patterns, customers with similar purchase history. However, in the relational entity graph, the two related customers may not be directly linked. Instead they are indirectly be linked to one another through links to their respective purchases, which are linked to a particular shared product ID. This means that a standard message passing GNN will require four message passing steps to propagate the information that customer v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT purchased the same product as customer v2subscript\ud835\udc632v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (2-hops from v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to product, and 2-hops from product to v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT). New message passing schemes that do multiple hops or directly connect customers (more generally, entities) with similar behavior patterns may more effectively propagate key information through the model. As well as new message passing schemes, there is also opportunity for new message aggregation methods. One possibility is order dependent aggregation, that combines messages in a time-dependent way, as explored by Yang et\u00a0al. (2022). Another is schema dependent aggregation, that combines messages based on what part of the schema graph the messages are arriving from.\n\n\n\n\n\n\n5.4 Training Techniques for Relational Data\n\nBy its nature, relational data contains highly overlapping predictive signals and tasks. This interconnectedness of data and tasks is a big opportunity for new neural network training methods that maximally take advantage of this interconnectedness to identify useful predictive signals. This section discusses several such opportunities.\n\n\nMulti-Task Learning.\n\nMany predictive tasks on relational data are distinct but related. For example, predicting customer lifetime value, and forecasting individual product sales both involve anticipating future purchase patterns. In RelBench, this corresponds to defining multiple training tables, one for each task, and training a single model jointly on all tasks in order to benefit from shared predictive signals. How to group training tables to leverage their overlap is a promising area for further study.\n\n\n\nMulti-Modal Learning.\n\nEntities in relational databases often have attributes covering multiple modalities (e.g., products come with images, descriptions, as well as different categorical and numerical features). The Relational Deep Learning blueprint first extracting entity-level features, which are used as initial node-features for the GNN model.\nIn RelBench, this multimodal entity-level feature extraction is handled by using state-of-the-art pre-trained models using the PyTorch Frame library to pre-extract features. This maximizes convenience for graph-focused research, but is likely suboptimal because the entity-level feature extraction model is frozen. This is especially relevant in contexts with unusual data\u2014e.g., specialized medical documentation\u2014that generic pre-trained models will likely fail to extract important details.\nTo facilitate exploration of joint entity-level and graph-level modelling, RelBench also provides the option to load raw data, to allow researchers to experiment with different feature-extraction methods.\n\n\n\nFoundation Models and Data Type Encoders.\n\nIn practice, new predictive tasks on relational data are often specified on-the-fly, with fast responses required. Such situations preclude costly model training from scratch, instead requiring powerful and generic pre-trained models. Self-supervised labels for model pre-training can be mined from historical data, just as with training table construction. However, techniques for automatically deciding which labels to mine remains unexplored. Another desirable property of pre-trained models is that they are inductive, so they can be applied to entirely new relational databases out-of-the-box. This presents a challenges in how to deal with unseen column types and relations between tables. Such flexibility is needed in order to move towards foundation models for relational databases. More broadly, how to build column encoders is an important question. As well as distribution shifts as mentioned in the previous paragraph, there are also decisions on when to share column encoders (should two image columns use the same image encoder?), as well as special data types such as static time intervals (e.g., to describe the time period an employee worked at a company, or the time period in which a building project was conducted). Special data types may require specialized encoder choices, and possibly even deeper integration into the neural network computation graph. How best to aggregate of cross-modal information into a single fused embedding is another question for exploration.\n\n\n\n",
      "6 Related Work": "\n\n6 Related Work\n\nRelational Deep Learning touches on many areas of related work which we survey next.\n\n\nStatistical Relational Learning.\n\nSince the foundation of the field of AI, sought to design systems capable of reasoning about entities and their relations, often by explicitly building graph structures (Minsky, 1974). Each new era of AI research also brought its own form of relational learning. A prominent instance is statistical relational learning (De\u00a0Raedt, 2008), a common form of which seeks to describe objects and relations in terms of first-order logic, fused with graphical models to model uncertainty (Getoor et\u00a0al., 2001). These descriptions can then be used to generate new \u201cknowledge\u201d through inductive logic programming (Lavrac and Dzeroski, 1994). Markov logic networks, a prominent statistical relational approach, are defined by a collection of first-order logic formula with accompanying scalar weights (Richardson and Domingos, 2006). This information is then used to define a probability distribution over possible worlds (via Markov random fields) which enables probabilistic reasoning about the truth of new formulae. We see Relational Deep Learning as inheriting this lineage, since both approaches operate on data with rich relational structure, and both approaches integrate relational structure into the model design. Of course, there are important distinctions between the two methods too, such as the natural scalability of graph neural network-based methods, and that Relational Deep Learning does not rely on first-order logic to describe data, allowing broad applicability to relations that are hard to fit into this form.\n\n\n\nTabular Machine Learning.\n\nTree based methods, notably XGBoost (Chen and Guestrin, 2016), remain key workhorses of enterprise machine learning systems due to their scalability and reliability. In parallel, efforts to design deep learning architectures for tabular data have continued (Huang et\u00a0al., 2020; Arik and Pfister, 2021; Gorishniy et\u00a0al., 2021, 2022; Chen et\u00a0al., 2023), but have struggled to clearly establish dominance over tree-based methods (Shwartz-Ziv and Armon, 2022). The vast majority of tabular machine learning focuses on the single table setting, which we argue forgoes use of the rich interconnections between relational data. As such, it does not address the key problem, which is how to get the data from a multi-table to a single table representation. Recently, a nascent body of work has begun to consider multiple tables. For instance, Zhu et\u00a0al. (2023) pre-train tabular Transformers that generalize to new tables with unseen columns.\n\n\n\nKnowledge Graph Embedding.\n\nKnowledge graphs store relational data, and highly scalable knowledge graph embeddings methods have been developed over the last decade to embed data into spaces whose geometry reflects the relational struture\u00a0(Bordes et\u00a0al., 2013; Wang et\u00a0al., 2014, 2017). Whilst also dealing with relational data, this literature differs from this present work in the task being solved. The key task of knowledge graph embeddings is to predict missing entities (Q: Who was Yann LeCun\u2019s postdoc advisor? A: Geoffrey Hinton) or relations (Q: Did Geoffrey Hinton win a Turing Award? A: Yes). To assist in such completion tasks, knowledge graph methods learn an embedding space with the goal of exactly preserving the relation semantics between entities. This is different from Relational Deep Learning, which aims to make predictions about entities, or groups of entities. Because of this, Relational Deep Learning seeks to leverage relations to learn entity representations, but does not need to learn an embedding space that perfectly preserves all relation semantics. This gives more freedom and flexibility to our models, which may discard certain relational information it finds unhelpful. Nonetheless, adopting ideas from knowledge graph embedding may yet be fruitful.\n\n\n\nDeep Learning on Relational Data.\n\nProposals to use message passing neural networks on relational data have occasionally surfaced within the research community. In particular, Schlichtkrull et\u00a0al. (2018); Cvitkovic (2019); \u0160\u00edr (2021), and Zahradn\u00edk et\u00a0al. (2023) make the connection between relational data and graph neural networks and explore it with different network architectures, such as heterogeneous message passing. However, our aim is to move beyond the conceptual level, and clearly establish deep learning on relational data as a subfield of machine learning. Accordingly, we focus on the components needed to establish this new area and attract broader interest: (1) a clearly scoped design space for neural network architectures on relational data, (2) a carefully chosen suite of benchmark databases and predictive tasks around which the community can center its efforts, (3) standardized data loading and splitting, so that temporal leakage does not contaminate experimental results, (4) recognizing time as a first-class citizen, integrated into all sections of the experimental pipeline, including temporal data splitting, time-based forecasting tasks, and temporal-based message passing, and (5) standardized evaluation protocols to ensure comparability between reported results.\n\n\n",
      "7 Conclusion": "\n\n7 Conclusion\n\nA large proportion of the worlds data is natively stored in relational tables. Fully exploiting the rich signals in relational data therefore has the potential to rewrite what problems computing can solve. We believe that Relational Deep Learning will make it possible to achieve superior performance on various prediction problems spanning the breadth of human activity, leading to considerable improvements in automated decision making. There is currently a great scientific opportunity to develop the field of Relational Deep Learning, and further refine this vision.\n\n\nThis paper serves as a road map in this pursuit. We introduce a blueprint for a neural network architecture that directly processes relational data by casting predictive tasks as graph representation learning problems. In Sec. 5 we discuss the many new challenges and opportunities this presents for the graph machine learning community. To facilitate research, we introduce RelBench, a set of benchmark datasets, and a Python package for data loading, and model evaluation.\n\n\n\nAcknowledgments.\n\nWe thank Shirley Wu for useful discussions as we were selecting datasets to adopt. We gratefully acknowledge the support of DARPA under Nos. N660011924033 (MCS); NSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Initiative, Amazon, Genentech, GSK, Hitachi, Juniper Networks, and KDDI.\n\n\n"
    },
    "table_captions": null,
    "figure_captions": null,
    "bibliography": {
      "Amodei et\u00a0al. (2016)": "\nAmodei et\u00a0al. (2016)\n\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric\nBattenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang\nChen, et\u00a0al.\n\n\nDeep speech 2: End-to-end speech recognition in english and mandarin.\n\n\nIn International Conference on Machine Learning (ICML), 2016.\n\n\n",
      "Arik and Pfister (2021)": "\nArik and Pfister (2021)\n\nSercan\u00a0\u00d6 Arik and Tomas Pfister.\n\n\nTabnet: Attentive interpretable tabular learning.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume\u00a035, pages 6679\u20136687, 2021.\n\n\n",
      "Bordes et\u00a0al. (2013)": "\nBordes et\u00a0al. (2013)\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\nYakhnenko.\n\n\nTranslating embeddings for modeling multi-relational data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2013.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom\u00a0B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net\u00a0al.\n\n\nLanguage models are few-shot learners.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n",
      "Chamberlin and Boyce (1974)": "\nChamberlin and Boyce (1974)\n\nDonald\u00a0D Chamberlin and Raymond\u00a0F Boyce.\n\n\nSequel: A structured english query language.\n\n\nIn Proceedings of the 1974 ACM SIGFIDET (now SIGMOD) workshop\non Data description, access and control, pages 249\u2013264, 1974.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nKuan-Yu Chen, Ping-Han Chiang, Hsin-Rung Chou, Ting-Wei Chen, and Tien-Hao\nChang.\n\n\nTrompt: Towards a better deep neural network for tabular data.\n\n\nIn International Conference on Machine Learning (ICML), 2023.\n\n\n",
      "Chen and Guestrin (2016)": "\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n",
      "Codd (1970)": "\nCodd (1970)\n\nEdgar\u00a0F Codd.\n\n\nA relational model of data for large shared data banks.\n\n\nCommunications of the ACM, 13(6):377\u2013387,\n1970.\n\n\n",
      "Cvitkovic (2019)": "\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n",
      "DB-Engines (2023)": "\nDB-Engines (2023)\n\nDB-Engines.\n\n\nDBMS popularity broken down by database model, 2023.\n\n\nAvailable: https://db-engines.com/en/ranking_categories.\n\n\n",
      "De\u00a0Raedt (2008)": "\nDe\u00a0Raedt (2008)\n\nLuc De\u00a0Raedt.\n\n\nLogical and relational learning.\n\n\nSpringer Science & Business Media, 2008.\n\n\n",
      "Devlin et\u00a0al. (2018)": "\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n",
      "Fey and Lenssen (2019)": "\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n",
      "Garcia-Molina et\u00a0al. (2008)": "\nGarcia-Molina et\u00a0al. (2008)\n\nHector Garcia-Molina, Jeffrey\u00a0D. Ullman, and Jennifer Widom.\n\n\nDatabase Systems: The Complete Book.\n\n\nPrentice Hall Press, USA, 2 edition, 2008.\n\n\nISBN 9780131873254.\n\n\n",
      "Geirhos et\u00a0al. (2020)": "\nGeirhos et\u00a0al. (2020)\n\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,\nWieland Brendel, Matthias Bethge, and Felix\u00a0A Wichmann.\n\n\nShortcut learning in deep neural networks.\n\n\nNature Machine Intelligence, 2(11):665\u2013673, 2020.\n\n\n",
      "Getoor et\u00a0al. (2001)": "\nGetoor et\u00a0al. (2001)\n\nLise Getoor, Nir Friedman, Daphne Koller, and Avi Pfeffer.\n\n\nLearning probabilistic relational models.\n\n\nRelational data mining, pages 307\u2013335, 2001.\n\n\n",
      "Gilmer et\u00a0al. (2017)": "\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n",
      "Gorishniy et\u00a0al. (2021)": "\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n",
      "Gorishniy et\u00a0al. (2022)": "\nGorishniy et\u00a0al. (2022)\n\nYury Gorishniy, Ivan Rubachev, and Artem Babenko.\n\n\nOn embeddings for numerical features in tabular deep learning.\n\n\nAdvances in Neural Information Processing Systems,\n35:24991\u201325004, 2022.\n\n\n",
      "Hamilton et\u00a0al. (2017)": "\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Hannun et\u00a0al. (2014)": "\nHannun et\u00a0al. (2014)\n\nAwni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich\nElsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et\u00a0al.\n\n\nDeep speech: Scaling up end-to-end speech recognition.\n\n\narXiv preprint arXiv:1412.5567, 2014.\n\n\n",
      "He et\u00a0al. (2016)": "\nHe et\u00a0al. (2016)\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\n\nDeep residual learning for image recognition.\n\n\nIn IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 770\u2013778, 2016.\n\n\n",
      "Hu et\u00a0al. (2023)": "\nHu et\u00a0al. (2023)\n\nWeihua Hu, Matthias Fey, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao,\nand Vid Kocijan.\n\n\nPyTorch Frame: A Deep Learning Framework for Tabular Data, October\n2023.\n\n\nURL https://github.com/pyg-team/pytorch-frame.\n\n\n",
      "Hu et\u00a0al. (2020)": "\nHu et\u00a0al. (2020)\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun.\n\n\nHeterogeneous graph transformer.\n\n\nIn Proceedings of The Web Conference 2020, page 2704\u20132710,\n2020.\n\n\n",
      "Huang et\u00a0al. (2020)": "\nHuang et\u00a0al. (2020)\n\nXin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin.\n\n\nTabtransformer: Tabular data modeling using contextual embeddings.\n\n\narXiv preprint arXiv:2012.06678, 2020.\n\n\n",
      "Johnson et\u00a0al. (2016)": "\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n",
      "Kaggle (2022)": "\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n",
      "Kapoor and Narayanan (2023)": "\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n",
      "Lavrac and Dzeroski (1994)": "\nLavrac and Dzeroski (1994)\n\nNada Lavrac and Saso Dzeroski.\n\n\nInductive logic programming.\n\n\nIn WLP, pages 146\u2013160. Springer, 1994.\n\n\n",
      "Minsky (1974)": "\nMinsky (1974)\n\nMarvin Minsky.\n\n\nA framework for representing knowledge, 1974.\n\n\n",
      "PubMed (1996)": "\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n",
      "Reimers and Gurevych (2019)": "\nReimers and Gurevych (2019)\n\nNils Reimers and Iryna Gurevych.\n\n\nSentence-bert: Sentence embeddings using siamese bert-networks.\n\n\narXiv preprint arXiv:1908.10084, 2019.\n\n\n",
      "Richardson and Domingos (2006)": "\nRichardson and Domingos (2006)\n\nMatthew Richardson and Pedro Domingos.\n\n\nMarkov logic networks.\n\n\nMachine learning, 62:107\u2013136, 2006.\n\n\n",
      "Rossi et\u00a0al. (2020)": "\nRossi et\u00a0al. (2020)\n\nEmanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico\nMonti, and Michael Bronstein.\n\n\nTemporal graph networks for deep learning on dynamic graphs.\n\n\nICML Workshop on Graph Representation Learning 2020, 2020.\n\n\n",
      "Russakovsky et\u00a0al. (2015)": "\nRussakovsky et\u00a0al. (2015)\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et\u00a0al.\n\n\nImagenet large scale visual recognition challenge.\n\n\nInternational journal of computer vision, 115(3):211\u2013252, 2015.\n\n\n",
      "Schlichtkrull et\u00a0al. (2018)": "\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n",
      "Shwartz-Ziv and Armon (2022)": "\nShwartz-Ziv and Armon (2022)\n\nRavid Shwartz-Ziv and Amitai Armon.\n\n\nTabular data: Deep learning is not all you need.\n\n\nInformation Fusion, 81:84\u201390, 2022.\n\n\n",
      "\u0160\u00edr (2021)": "\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n",
      "Varma and Zisserman (2005)": "\nVarma and Zisserman (2005)\n\nManik Varma and Andrew Zisserman.\n\n\nA statistical approach to texture classification from single images.\n\n\nInternational journal of computer vision, 62:61\u201381,\n2005.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan\u00a0N Gomez, Lukasz Kaiser, and Illia Polosukhin.\n\n\nAttention is all you need.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Wang et\u00a0al. (2017)": "\nWang et\u00a0al. (2017)\n\nQuan Wang, Zhendong Mao, Bin Wang, and Li\u00a0Guo.\n\n\nKnowledge graph embedding: A survey of approaches and applications.\n\n\nIEEE Transactions on Knowledge and Data Engineering,\n29(12):2724\u20132743, 2017.\n\n\n",
      "Wang et\u00a0al. (2021)": "\nWang et\u00a0al. (2021)\n\nYiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, and Bryan\nHooi.\n\n\nTime-aware neighbor sampling for temporal graph networks.\n\n\nIn arXiv pre-print, 2021.\n\n\n",
      "Wang et\u00a0al. (2014)": "\nWang et\u00a0al. (2014)\n\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.\n\n\nKnowledge graph embedding by translating on hyperplanes.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume\u00a028, 2014.\n\n\n",
      "Xu et\u00a0al. (2020)": "\nXu et\u00a0al. (2020)\n\nKeyulu Xu, Jingling Li, Mozhi Zhang, Simon\u00a0S Du, Ken-ichi Kawarabayashi, and\nStefanie Jegelka.\n\n\nWhat can neural networks reason about?\n\n\nIn International Conference on Learning Representations\n(ICLR), 2020.\n\n\n",
      "Yang et\u00a0al. (2022)": "\nYang et\u00a0al. (2022)\n\nZhen Yang, Ming Ding, Bin Xu, Hongxia Yang, and Jie Tang.\n\n\nStam: A spatiotemporal aggregation method for graph neural\nnetwork-based recommendation.\n\n\nIn Proceedings of the ACM Web Conference 2022, pages\n3217\u20133228, 2022.\n\n\n",
      "Zahradn\u00edk et\u00a0al. (2023)": "\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n",
      "Zhang et\u00a0al. (2023)": "\nZhang et\u00a0al. (2023)\n\nBohang Zhang, Shengjie Luo, Liwei Wang, and Di\u00a0He.\n\n\nRethinking the expressive power of gnns via graph biconnectivity.\n\n\nIn International Conference on Learning Representations\n(ICLR), 2023.\n\n\n",
      "Zhu et\u00a0al. (2023)": "\nZhu et\u00a0al. (2023)\n\nBingzhao Zhu, Xingjian Shi, Nick Erickson, Mu\u00a0Li, George Karypis, and Mahsa\nShoaran.\n\n\nXtab: Cross-table pretraining for tabular transformers.\n\n\nIn International Conference on Machine Learning (ICML), 2023.\n\n\n"
    },
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "d6704064-a7ad-47af-9433-bf80039fc63a": {
    "pk": "d6704064-a7ad-47af-9433-bf80039fc63a",
    "project_name": null,
    "authors": [
      "Joshua Robinson",
      "Rishabh Ranjan",
      "Weihua Hu",
      "Kexin Huang",
      "Jiaqi Han",
      "Alejandro Dobles",
      "Matthias Fey",
      "Jan E. Lenssen",
      "Yiwen Yuan",
      "Zecheng Zhang",
      "Xinwei He",
      "Jure Leskovec"
    ],
    "title": "RelBench: A Benchmark for Deep Learning on Relational Databases",
    "abstract": "We present RelBench, a public benchmark for solving predictive tasks over relational databases with graph neural networks. RelBench provides databases and tasks spanning diverse domains and scales, and is intended to be a foundational infrastructure for future research. We use RelBench to conduct the first comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024), which combines graph neural network predictive models with (deep) tabular models that extract initial entity-level representations from raw tables. End-to-end learned RDL models fully exploit the predictive signal encoded in primary-foreign key links, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular models. To thoroughly evaluate RDL against this prior gold-standard, we conduct an in-depth user study where an experienced data scientist manually engineers features for each task. In this study, RDL learns better models whilst reducing human work needed by more than an order of magnitude. This demonstrates the power of deep learning for solving predictive tasks over relational databases, opening up many new research opportunities enabled by RelBench.",
    "url": "http://arxiv.org/abs/2407.20060v1",
    "timestamp": 1722264373,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nRelational databases are the most widely used database management system, underpinning much of the digital economy. Their popularity stems from their table storage structure, making maintenance relatively easy, and data simple to access using powerful query languages such as SQL. Because of their popularity, AI systems across a wide variety of domains are built using data stored in relational databases, including e-commerce, social media, banking systems, healthcare, manufacturing, and open-source scientific repositories\u00a0(Johnson et\u00a0al., 2016; PubMed, 1996).\n\n\nDespite the importance of relational databases, the rich relational information is typically foregone, as no model architecture is capable of handling varied database structures. Instead, data is \u201cflattened\u201d into a simpler format such as a single table, often by manual feature engineering, on which standard tabular models can be used\u00a0(Kaggle, 2022). This results in a significant loss in predictive signal, and creates a need for data extraction pipelines that frequently cause bugs and add to software complexity.\n\n\nFigure 1: RelBench enables training and evaluation of deep learning models on relational databases. RelBench supports framework agnostic data loading, task specification, standardized data splitting, standardized evaluation metrics, and a leaderboard for tracking progress. RelBench also includes a pilot implementation of the relational deep learning blueprint of Fey et\u00a0al. (2024).\n\n\nTo fully exploit the predictive signal encoded in the relations between entities, a new proposal is to re-cast relational data as an exact graph representation, with a node for each entity in the database, edges indicating primary-foreign key links, and node features extracted using deep tabular models, an approach termed Relational Deep Learning (RDL)\u00a0(Fey et\u00a0al., 2024). The graph representation allows Graph Neural Networks (GNNs)\u00a0(Gilmer et\u00a0al., 2017; Hamilton et\u00a0al., 2017) to be used as predictive models. RDL is the first approach for an end-to-end learnable neural network model with access to all possible predictive signal in a relational databases, and has the potential to unlock new levels of predictive power.\nHowever, the development of relational deep learning is limited by a complete lack of infrastructure to support research, including: (i) standardized benchmark databases and tasks to compare methods, (ii) initial implementation of RDL, including converting data to graph form and GNN training, and (iii) a pilot study of the effectiveness of relational deep learning.\n\n\nHere we present RelBench, the first benchmark for relational deep learning. RelBench is intended to be the foundational infrastructure for future research into relational deep learning, providing a comprehensive set of databases across a variety of domains, including e-commerce, Q&A platforms, medical, and sports databases. RelBench databases span orders of magnitude in size, from 74K entities to 41M entities, and have very different time spans, between 2 weeks and 55 years of training data. They also vary significantly in their relational structure, with the total number of tables varying between 3 and 15, and total number of columns varying from 15 to 140.\nEach database comes with multiple predictive tasks, 30 in total, including entity classification/regression and recommendation tasks, each chosen for their real-world significance.\n\n\nIn addition to databases and tasks, we release open-source software designed to make relational deep learning widely available. This includes (i) the RelBench Python package for easy database and task loading, (ii) the first open-source implementation of relational deep learning, designed to be easily modified by researchers, and (iii) a public leaderboard for tracking progress. We comprehensively benchmark our initial RDL implementation on all RelBench tasks, comparing to various baselines.\n\n\nThe most important baseline we compare to is a strong \u201cdata scientist\u201d approach, for which we recruited an experienced individual to solve each task by manually engineering features and feeding them into tabular models. This approach is the current gold-standard for building predictive models on relational databases. The study, which we open source for reproducibility, finds that RDL models match or outperform the data scientist\u2019s models in accuracy, whilst reducing human hours worked by 96%percent9696\\%96 %, and lines of code by 94%percent9494\\%94 % on average. This cons14titutes the first empirical demonstration of the central promise of RDL, and points to\na long-awaited end-to-end deep learning solution for relational data.\n\n\nOur website111https://relbench.stanford.edu. is a comprehensive entry point to RDL, describing RelBench databases and tasks, access to code on GitHub, the full relational deep learning blueprint, and tutorials for adding new databases and tasks to RelBench to allow researchers to experiment with their problems of interest.\n\n",
      "2 Overview and Design": "\n\n2 Overview and Design\n\nRelBench provides a collection of diverse real-world relational databases along with a set of realistic predictive tasks associated with each database. Concretely, we provide:\n\n\n\n\n\u2022\n\nRelational databases, consisting of a set of tables connected via primary-foreign key relationships. Each table has columns storing diverse information about each entity. Some tables also come with time columns, indicating the time at which the entity is created (e.g., transaction date).\n\n\n\n\u2022\n\nPredictive tasks over a relational database, which are defined by a training table\u00a0(Fey et\u00a0al., 2024) with columns for Entity ID, seed time, and target labels.The seed time indicates at which time the target is to be predicted, filtering future data.\n\n\n\n\n\nNext we outline key design principles of RelBench with an emphasis on data curation, data splits, research flexibility, and open-source implementation.\n\n\nData Curation. Relational databases are widespread, so there are many candidate predictive tasks.\nFor the purpose of benchmarking we carefully curate a collection of relational databases and tasks chosen for their rich relational structure and column features. We also adopt the following principles:\n\n\n\n\n\u2022\n\nDiverse domains: To ensure algorithms developed on RelBench will be useful across a wide range of application domains, we select real-world relational databases from diverse domains.\n\n\n\n\u2022\n\nDiverse task types: Tasks cover a wide range of real-world use-cases, including three representative task types: entity classification, entity regression, and recommendation.\n\n\n\n\n\nTable 1: Statistics of RelBench datasets. Datasets vary significantly in the number of tables, total number of rows, and number of columns. In this table, we only count rows available for test inference, i.e., rows upto the test time cutoff.\n\n\n\nName\nDomain\n#Tasks\nTables\nTimestamp (year-mon-day)\n\n\n#Tables\n#Rows\n#Cols\nStart\nVal\nTest\n\n\nrel-amazon\nE-commerce\n7\n3\n15,000,713\n15\n2008-01-01\n2015-10-01\n2016-01-01\n\n\nrel-avito\nE-commerce\n4\n8\n20,679,117\n42\n2015-04-25\n2015-05-08\n2015-05-14\n\n\nrel-event\nSocial\n3\n5\n41,328,337\n128\n1912-01-01\n2012-11-21\n2012-11-29\n\n\nrel-f1\nSports\n3\n9\n74,063\n67\n1950-05-13\n2005-01-01\n2010-01-01\n\n\nrel-hm\nE-commerce\n3\n3\n16,664,809\n37\n2019-09-07\n2020-09-07\n2020-09-14\n\n\nrel-stack\nSocial\n5\n7\n4,247,264\n52\n2009-02-02\n2020-10-01\n2021-01-01\n\n\nrel-trial\nMedical\n5\n15\n5,434,924\n140\n2000-01-01\n2020-01-01\n2021-01-01\n\n\nTotal\n30\n51\n103,466,370\n489\n/\n/\n/\n\n\n\n\nRelBench databases are summarized in Table\u00a01, covering E-commerce, social, medical, and sports domains. The databases vary significantly in the numbers of rows (i.e., data scale) the number of columns and tables, as well as the time ranges of the databases. Tasks are summarized in Table\u00a02, each corresponding to a predictive problem of practical interest such as predicting customer churn, predicting the number of adverse events in a clinical trial, and recommending posts to users.\n\n\nData Splits. Data is split temporally, with models trained on rows up to val_timestamp, validated on the rows between val_timestamp and test_timestamp, and tested on the rows after test_timestamp.\nOur implementation carefully hides data after test_timestamp during inference to systematically avoid test time data leakage\u00a0(Kapoor and Narayanan, 2023), and uses an elegant solution proposed by Fey et\u00a0al. (2024) to avoid time leakage during training and validation through temporal neighbor sampling. In general, it is the designers responsibility to avoid time leakage. We recommend using our carefully tested implementation where possible.\n\n\nResearch Flexibility. RelBench is designed to allow significant freedom in future research directions. For example, RelBench tasks share the same (val_timestamp and test_timestamp) splits across tasks within the same relational database. This opens up exciting opportunities for multi-task learning and pre-training to simultaneously improve different predictive tasks within the same relational database. We also expose the logic for converting databases into graphs. This allows future work to consider modified graph constructions, or creative uses of the raw data.\n\n\nOpen-source RDL Implementation.\nAs well as datasets and tasks, we provide the first open-source implementation of relational deep learning. See Figure 2 of Fey et\u00a0al. (2024) for a high-level overview. A neural network is learned over a heterogeneous temporal graph that exactly represents the database in order to make prediction over nodes (for entity classification and regression) and links (for recommendation). Our implementation is built on top of PyTorch Frame\u00a0(Hu et\u00a0al., 2024) for extracting initial node embeddings from raw table features, and PyTorch Geometric\u00a0(Fey and Lenssen, 2019) for GNN modeling. See Section 3 for further details.\n\n\nThe rest of the paper is organized as follows. Section\u00a04 describes the RelBench relational databases.\nSection\u00a05 introduces predictive tasks for each RelBench databases covering the three task types in Sections\u00a05.1, 5.2, and 5.3, respectively.\nSection\u00a05 also extensively benchmarks our RDL implementation against challenging baselines. Most importantly, we compare to a strong \u201cdata scientist\u201d baseline (Section 6), finding that end-to-end RDL models outperform manual feature engineering, the current gold-standard\nFinally, Section\u00a07 discusses related work and Section\u00a08 draws final conclusions.\n\n",
      "3 Relational Deep Learning Implementation": "\n\n3 Relational Deep Learning Implementation\n\nAs part of RelBench, we provide an initial implementation of relational deep learning, based on the blueprint of\nFey et\u00a0al. (2024).222Code available at: https://github.com/snap-stanford/relbench. Our implementation consists four major components: (1) heterogeneous temporal graph, (2) deep learning model, (3) temporal-aware training of the model, and (4) task-specific loss, which we briefly discuss now.\n\n\nHeterogeneous temporal graph.\nGiven a set of tables with primary-foreigh key relations between them we follow Fey et\u00a0al. (2024) to automatically construct a heterogeneous temporal graph, where each table represents a node type, each row in a table represents a node, and a primary-foreign-key relation between two table rows (nodes) represent an edge between the respective nodes. Some node types are associated with time attributes, representing the timestamp at which a node appears. The heterogeneous temporal graph is represented as a PyTorch Geometric graph object.\nEach node in the heterogeneous graph comes with a rich feature derived from diverse columns of the corresponding table. We use Tensor Frame provided by PyTorch Frame\u00a0(Hu et\u00a0al., 2024) to represent rich node features with diverse column types, e.g., numerical, categorical, timestamp, and text.\n\n\nDeep learning model.\nFirst, we use deep tabular models that encode raw row-level data into initial node embeddings using PyTorch Frame (Hu et\u00a0al., 2024) (specifically, we use the ResNet tabular model\u00a0(Gorishniy et\u00a0al., 2021)). These initial node embeddings are then fed into a GNN to iteratively update the node embeddings based on their neighbors.\nFor the GNN we use the heterogeneous version of the GraphSAGE model\u00a0(Hamilton et\u00a0al., 2017; Fey and Lenssen, 2019) with sum-based neighbor aggregation. Output node embeddings are fed into task-specific prediction heads and are learned end-to-end.\n\n\nTemporal-aware subgraph sampling.\nWe perform temporal neighbor sampling, which samples\na subgraph around each entity node at a given seed time.\nSeed time is the time in history at which the prediction is made. When collecting the information to make a prediction at a given seed time, it is important for the model to only use information from before the seed time and thus not learn from the future (post the seed time). Crucially, when sampling mini-batch subgraphs we make sure that all nodes within the sampled subgraph appear before the seed time\u00a0(Hamilton et\u00a0al., 2017; Fey et\u00a0al., 2024), which systematically avoids time leakage during training.\nThe sampled subgraph is fed as input to the GNN, and trained to predict the target label.\n\n\nTask-specific prediction head and loss.\nFor entity-level classification, we simply apply an MLP on an entity embedding computed by our GNN to make prediction. For the loss function, we use the binary cross entropy loss for entity classification and L1subscript\ud835\udc3f1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT loss for entity regression.\n\n\nRecommendation requires computing scores between pairs of source nodes and target nodes.\nFor this task type, we consider two representative predictive architectures: two-tower GNN\u00a0(Wang et\u00a0al., 2019) and identity-aware GNN (ID-GNN)\u00a0(You et\u00a0al., 2021). First, the two-tower GNN computes the pairwise scores via inner product between source and target node embeddings, and the standard Bayesian Personalized Ranking loss\u00a0(Rendle et\u00a0al., 2012) is used to train the two-tower model\u00a0(Wang et\u00a0al., 2019).\nSecond, the ID-GNN computes the pairwise scores by applying an MLP prediction head on target entity embeddings computed by GNN for each source entity. The ID-GNN is trained by the standard binary cross entropy loss.\n\n",
      "4 RelBench Datasets": "\n\n4 RelBench Datasets\n\nFigure 2: Example RelBench schema for rel-trial. RelBench databases have complex relational structure and rich column features. \n\n\nRelBench contains 7 datasets each with rich relational structure, providing a challenging environment for developing and comparing relational deep learning methods (see Figure 2 for an example). The datasets are carefully processed from real-world relational databases and span diverse domains and sizes.\nEach database is associated with multiple individual predictive tasks defined in Section\u00a05.\nDetailed statistics of each dataset can be found in Table\u00a01. We briefly describe each dataset.\n\n\nrel-amazon. The Amazon E-commerce database records products, users, and reviews across Amazon\u2019s E-commerce platform. It contains rich information about products and reviews. Products include the price and category of each, reviews have the overall rating, whether the user has actually bought the product, and the text of the review itself. We use the subset of book-related products.\n\n\nrel-f1. The F1 database tracks all-time Formula 1 racing data and statistics since 1950. It provides detailed information for various stakeholders including drivers, constructors, engine manufacturers, and tyre manufacturers. Highlights include data on all circuits (e.g.geographical details), and full historical data from every season. This includes overall standings, race results, and more specific data like practice sessions, qualifying positions, sprints, and pit stops.\n\n\nrel-stack. Stack Exchange is a network of question-and-answer websites on different topics, where questions, answers, and users are subject to a reputation award process. The reputation system allows the sites to be self-moderating. The database includes detailed records of activity including user biographies, posts and comments (with raw text), edit histories, voting, and related posts. In our benchmark, we use the stats-exchange site.\n\n\nTable 2: Full list of predictive tasks for each RelBench dataset (introduced in Table\u00a01). \n\n\nDataset\nTask name\nTask type\n#Rows of training table\n#Unique\n%train/test\n#Dst\n\n\nTrain\nValidation\nTest\nEntities\nEntity Overlap\nEntities\n\n\nrel-amazon\nuser-churn\nentity-cls\n4,732,555\n409,792\n351,885\n1,585,983\n88.0\n\u2014\n\n\nitem-churn\nentity-cls\n2,559,264\n177,689\n166,842\n416,352\n93.1\n\u2014\n\n\nuser-ltv\nentity-reg\n4,732,555\n409,792\n351,885\n1,585,983\n88.0\n\u2014\n\n\nitem-ltv\nentity-reg\n2,707,679\n166,978\n178,334\n427,537\n93.5\n\u2014\n\n\nuser-item-purchase\nrecommendation\n5,112,803\n351,876\n393,985\n1,632,909\n87.4\n12,562,384\n\n\nuser-item-rate\nrecommendation\n3,667,157\n257,939\n292,609\n1,481,360\n81.0\n7,665,611\n\n\n\nuser-item-review\nrecommendation\n2,324,177\n116,970\n127,021\n894,136\n74.1\n5,406,835\n\n\nrel-avito\nad-ctr\nentity-reg\n5,100\n1,766\n1,816\n4,997\n59.8\n\u2014\n\n\nuser-clicks\nentity-cls\n59,454\n21,183\n47,996\n66,449\n45.3\n\u2014\n\n\nuser-visits\nentity-cls\n86,619\n29,979\n36,129\n63,405\n64.6\n\u2014\n\n\nuser-ad-visit\nrecommendation\n86,616\n29,979\n36,129\n63,402\n64.6\n3,616,174\n\n\nrel-event\nuser-attendance\nentity-reg\n19,261\n2,014\n2,006\n9,694\n14.6\n\u2014\n\n\nuser-repeat\nentity-cls\n3,842\n268\n246\n1,514\n11.5\n\u2014\n\n\nuser-ignore\nentity-cls\n19,239\n4,185\n4,010\n9,799\n21.1\n\u2014\n\n\nrel-f1\ndriver-dnf\nentity-cls\n11,411\n566\n702\n821\n50.0\n\u2014\n\n\ndriver-top3\nentity-cls\n1,353\n588\n726\n134\n50.0\n\u2014\n\n\ndriver-position\nentity-reg\n7,453\n499\n760\n826\n44.6\n\u2014\n\n\nrel-hm\nuser-churn\nentity-cls\n3,871,410\n76,556\n74,575\n1,002,984\n89.7\n\u2014\n\n\nitem-sales\nentity-reg\n5,488,184\n105,542\n105,542\n105,542\n100.0\n\u2014\n\n\nuser-item-purchase\nrecommendation\n3,878,451\n74,575\n67,144\n1,004,046\n89.2\n13,428,473\n\n\nrel-stack\nuser-engagement\nentity-cls\n1,360,850\n85,838\n88,137\n88,137\n97.4\n\u2014\n\n\nuser-badge\nentity-cls\n3,386,276\n247,398\n255,360\n255,360\n96.9\n\u2014\n\n\npost-votes\nentity-reg\n2,453,921\n156,216\n160,903\n160,903\n97.1\n\u2014\n\n\nuser-post-comment\nrecommendation\n21,239\n825\n758\n11,453\n59.9\n44,940\n\n\npost-post-related\nrecommendation\n5,855\n226\n258\n5,924\n8.5\n7,456\n\n\nrel-trial\nstudy-outcome\nentity-cls\n11,994\n960\n825\n13,779\n0.0\n\u2014\n\n\nstudy-adverse\nentity-reg\n43,335\n3,596\n3,098\n50,029\n0.0\n\u2014\n\n\nsite-success\nentity-reg\n151,407\n19,740\n22,617\n129,542\n42.0\n\u2014\n\n\ncondition-sponsor-run\nrecommendation\n36,934\n2,081\n2,057\n3,956\n98.4\n533,624\n\n\nsite-sponsor-run\nrecommendation\n669,310\n37,003\n27,428\n445,513\n48.3\n1,565,463\n\n\n\n\nrel-trial. The clinical trial database is curated from AACT initiative, which consolidates all protocol and results data from studies registered on ClinicalTrials.gov. It offers extensive information about clinical trials, including study designs, participant demographics, intervention details, and outcomes. It is an important resource for health research, policy making, and therapeutic development.\n\n\nrel-hm. The H&M relational database hosts extensive customer and product data for online shopping experiences across its extensive network of brands and stores. This database includes detailed customer purchase histories and a rich set of metadata, encompassing everything from basic demographic information to extensive details about each product available.\n\n\nrel-event. The Event Recommendation database is obtained from user data on a mobile app called Hangtime. This app allows users to keep track of their friends\u2019 social plans. The database contains data on user actions, event metadata, and demographic information, as well as users\u2019 social relations, which captures how social relations can affect user behavior. Data is fully anonymized, with no personally identifiable information (such as names or aliases) available.\n\n\nrel-avito. Avito is a leading online advertisement platform, providing a marketplace for users to buy and sell a wide variety of products and services, including real estate, vehicles, jobs, and goods. The Avito Context Ad Clicks dataset on Kaggle is part of a competition aimed at predicting whether an ad will be clicked based on contextual information. This dataset includes user searches, ad attributes, and other related data to help build predictive models.\n\n\nData Provenance. All data is sourced from publicly available repositories with licenses permitting usage for research purposes. See Appendix D for details of data sources, licenses, and more.\n\n",
      "5 Predictive Tasks on RelBench Datasets": "\n\n5 Predictive Tasks on RelBench Datasets\n\nRelBench introduces 30 new predictive tasks defined over the databases introduced in Section\u00a04.\nA full list of tasks is given in Table\u00a02, with high-level descriptions given in Appendix A (and our website) due to space limitations.\nTasks are grouped into three task types: entity classification (Section\u00a05.1), entity regression (Section\u00a05.2), and entity link prediction (Section\u00a05.3). Tasks differ significantly in the number of train/val/test entities, number of unique entities (the same entity may appear multiple times at different timestamps), and the proportion of test entities seen during training. Note this is not data leakage, since entity predictions are timestamp dependent, and can change over time. Tasks with no overlap are pure inductive tasks, whilst other tasks are (partially) transductive.\n\n\n\n5.1 Entity Classification\n\nTable 3: Entity classification results (AUROC, higher is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nThe first task type is entity-level classification. The task is to predict binary labels of a given entity at a given seed time. We use the ROC-AUC\u00a0(Hanley and McNeil, 1983) metric for evaluation (higher is better). We compare to a LightGBM classifier baseline over the raw entity table features. Note that here only information from the single entity table is used.\n\n\n\n\nExperimental results.\n\nResults are given in Table 5.1, with RDL outperforming or matching baselines in all cases. Notably, LightGBM achieves similar performance to RDL on the study-outcome task from rel-trial. This task has extremely rich features in the target table (28 columns total), giving the LightGBM many potentially useful features even without feature engineering. It is an interesting research question how to design RDL models better able to extract these features and unify them with cross-table information in order to outperform the LightGBM model on this dataset.\n\n\n\n5.2 Entity Regression\n\nEntity-level regression tasks involve predicting numerical labels of an entity at a given seed time. We use Mean Absolute Error (MAE) as our metric (lower is better). We consider the following baselines:\n\n\n\u2022\n\nEntity mean/median calculates the mean/median label value for each entity in training data and predicts the mean/median value for the entity.\n\n\n\n\u2022\n\nGlobal mean/median calculates the global mean/median label value over the training data and predicts the same mean/median value across all entities.\n\n\n\n\u2022\n\nGlobal zero predicts zero for all entities.\n\n\n\n\u2022\n\nLightGBM learns a LightGBM\u00a0(Ke et\u00a0al., 2017) regressor over the raw entity features to predict the numerical targets. Note that only information from the single entity table is used.\n\n\n\n\n\nExperimental results. Results in Table 5.2 show our RDL implementation outperforms or matches baselines in all cases. A number of tasks, such as driver-position and study-adverse, have matching performance up to statistical significance, suggesting some room for improvement. We analyze this further in Appendix C, identifying one potential cause, suggesting an opportunity for improved performance for regression tasks.\n\n\nTable 4: Entity regression results (MAE, lower is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3 Recommendation\n\nFinally, we also introduce recommendation tasks on pairs of entities. The task is to predict a list of top K\ud835\udc3eKitalic_K target entities given a source entity at a given seed time.\nThe metric we use is Mean Average Precision (MAP) @K\ud835\udc3eKitalic_K, where K\ud835\udc3eKitalic_K is set per task (higher is better). We consider the following baselines:\n\n\n\u2022\n\nGlobal popularity computes the top K\ud835\udc3eKitalic_K most popular target entities (by count) across the entire training table and predict the K\ud835\udc3eKitalic_K globally popular target entities across all source entities.\n\n\n\n\u2022\n\nPast visit computes the top K\ud835\udc3eKitalic_K most visited target entities for each source entity within the training table and predict those past-visited target entities for each entity.\n\n\n\n\u2022\n\nLightGBM learns a LightGBM\u00a0(Ke et\u00a0al., 2017) classifier over the raw features of the source and target entities (concatenated) to predict the link. Additionally, global popularity and past visit ranks are also provided as inputs.\n\n\n\n\n\nFor recommendation, it is also important to ensure a certain density of links in the training data in order for there to be sufficient predictive signal. In Appendix A we report statistics on the average number of destination entities each source entity links to. For most tasks the density is \u22651absent1\\geq 1\u2265 1, with the exception of rel-stack which is more sparse, but is included to test in extreme sparse settings.\n\n\nExperimental results. Results are given in Table 5.3. We find that either the RDL implementation using GraphSAGE (Hamilton et\u00a0al., 2017), or ID-GNN (You et\u00a0al., 2021) as the GNN component performs best, often by a very significant margin. ID-GNN excels in cases were predictions are entity-specific (i.e., Past Visit baseline outperforms Global Popularity), whilst the plain GNN excels in the reverse case. This reflects the inductive biases of each model, with GraphSAGE being able to learn structural features, and ID-GNN able to take into account the specific node ID.\n\n\nTable 5: Recommendation results (MAP, higher is better) on RelBench. Best values are in bold. See Table\u00a0B in Appendix\u00a0B for standard deviations.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n \n\n\nRel. Gain\n\nof RDL\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 Expert Data Scientist User Study\n\nTo test RDL in the most challenging circumstances possible, we undertake a human trial wherein a data scientist solves each task by manually designing features and feeds them into tabular methods such at LightGBM or XGBoost\u00a0(Chen and Guestrin, 2016; Ke et\u00a0al., 2017). This represents the prior gold-standard for building predictive models on relational databases (Heaton, 2016), and the key point of comparison for RDL.\n\n\nWe structure our user study along the five main data science workflow steps:\n\n\n1.\n\nExploratory data analysis (EDA): Explore the dataset and task to understand its characteristics, including what column features there are, and if there is any missing data.\n\n\n\n2.\n\nFeature ideation: Based on EDA and intuition from prior experiences, propose a set of entity-level features that the data scientist believes may contain predictive signal for the task.\n\n\n\n3.\n\nFeature enginnering: Using query languages such as SQL to compute the proposed features, and add them as extra columns to the target table of interest.\n\n\n\n4.\n\nTabular ML: Run tabular methods such as LightGBM or XGBoost on the table with extra features to produce a predictive model, and record the test performance.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (Optional): Common tools include SHAP and LIME, which aim to explain the contribution of each input feature to the final performance.\n\n\n\n\n\nConsider for example the rel-hm dataset (schema in Appendix D) and the task of predicting customer churn. Here the customer table only contains simple biographical information such as username and joining date. To capture more predictive information, additional features, such as time since last purchase, can be computed using the other tables, and added to the customer table. We give a detailed walk-through of the data scientist\u2019s work process for solving this specific task in Appendix C. We strongly encourage the interested reader to review this, as it highlights the significant amount of task-specific effort that this workflow necessitates.\n\n\nLimitations of Manual Feature Engineering. This workflow suffers from several fundamental limitations. Most obviously, since features are hand designed they only capture part of the predictive signal in the database, useful signal is easily missed. Additionally, feature complexity is limited by human reasoning abilities, meaning that higher-order interactions between entities are often overlooked. Beyond predictive signal, the other crucial limitation of feature engineering is its extremely manual nature\u2014every time a new model is built a data scientist has to repeat this process, requiring many hours of human labor, and significant quantities of new SQL code to design features (Zheng and Casari, 2018). Our RDL models avoid these limitations (see Section 6.1).\n\n\nData Scientist. To conduct a thorough comparison to this process, we recruit a high-end data scientist with Stanford CS MSc degree, 4.0 GPA, and 5 years of experience of building machine learning models in the financial industry. This experience includes a significant amount of time building machine learning models in exactly above five steps, as well as broader data science expertise.\n\n\nUser Study Protocol.\n\n\nFigure 3: RDL vs. Data Scientist. Relational Deep Learning matches or outperforms the data scientist in 11 of 15 tasks. Left shows entity classification AUROC, right shows entity regression, reporting MAE normalized so that the RDL MAE is always 1.\n\n\n\nBecause of the open-ended nature of feature engineering and model development, we follow a specific protocol for the user study in order to standardize the amount of effort dedicated to each dataset and task. Tracking the 5 steps outlined above, we impose the following rules:\n\n\n\n\n1.\n\nEDA: The time allotted for data exploration is capped at 4 hours. This threshold was chosen to give the data scientist enough time to familiarize themselves with the schema, visualize key relationships and distributions, and take stock of any outliers in the dataset, while providing a reasonable limit to the effort applied.\n\n\n\n2.\n\nFeature ideation: Feature ideation is performed manually with pen and paper, and is limited to 1 hour. In practice, the data scientist found that 1 hour was plenty of time to enumerate all promising features at that time, especially since many ideas naturally arise during the EDA process already.\n\n\n\n3.\n\nFeature engineering: The features described during the ideation phase are then computed using SQL queries. The time taken to write SQL code to generate the features is unconstrained in order to eliminate code writing speed as a factor in the study. We do, however, record code writing time for our timing benchmarking. This stage presented the most variability in terms of time commitment, partly because it is unconstrained, but mostly because the implementation complexity of the features itself is highly variable.\n\n\n\n4.\n\nTabular ML: For tabular ML training, we provide a standardized LightGBM training script including comprehensive hyperparameter tuning. The data scientist needs only to feed the table full of engineered features into this training script, which returns test performance results. However, there is some non-trivial amount of work required to transform the output of the SQL queries from the previous section into the Python objects (arrays) required for training LightGBM. Again, the time taken for this additional pre-preocessing is recorded.\n\n\n\n5.\n\nPost-hoc analysis of feature importance: Finally, after successfully training a model, an evaluation of model predictions and feature importance is carried out. This mostly serves as a general sanity check and an interesting corollary of the data scientist\u2019s work that provides task-specific insights (see Appendix C). In practice, this took no more than a few minutes per task and this time was not counted toward the total time commitment.\n\n\n\n\n\nReproducibility. All of the data scientist\u2019s workings are released333See https://github.com/snap-stanford/relbench-user-study. to ensure reproducibility and demonstrate the significant lengths gone through to build as accurate models as possible. In Appendix C we walk through a complete example for a single dataset and task, showing the data-centric insights it yields. An important by-product is a close analysis of which features contribute to model performance, which we believe will help inspire future well-motivated RDL research directions.\n\n\nFigure 4: RDL vs. Data Scientist. Relational Deep Learning reduces the hours of human work required to solve a new task by 96% on average (from 12.3 to 0.5 hours). Left shows node-level classification, right shows node-level regression.\n\n\n\n\n6.1 Results\n\nAs well as (i) raw predictive power, we compare the data scientist to our RDL models in terms of (ii) hours of human work, and (iii) number of new lines of code required to solve each task. We measure the marginal effort, meaning that we do not include code infrastructure that is reused across tasks, including for example data loading logic and training scripts for RDL or LightGBM models.\n\n\nSummary. Figures 3, 4, and 5 show that RDL learns highly predictive models, outperforming the data scientist in 11 of 15 tasks, whilst reducing hours worked by 96%percent9696\\%96 % on average, and lines of code by 94%percent9494\\%94 % on average. On average, it took the data scientist 12.3 hours to solve each task using traditional feature engineering. By contrast it takes roughly 30 minutes to solve a task with RDL.\n\n\nThis observation is the central value proposition of relational deep learning, pointing the way to unlocking new levels of predictive power, and potentially a new economic model for solving predictive tasks on relational databases. Replacing hand-crafted solutions with end-to-end learnable models has been a key takeaway from the last 15 years of AI research. It is therefore remarkable how little impact deep learning has had on ML on relational databases, one of the most widespread applied ML use cases. To the best of our knowledge, RDL represents the first proposal for a deep learning approach for relational databases that has demonstrated efficacy compared with established data science workflows.\n\n\nWe highlight that all RelBench tasks were solved with a single set of default hyperparameters (with 2 exceptions requiring small modifications to learning rate, number of epochs, and GNN aggregation function). This demonstrates the robustness of RDL, and that the performance of RDL in Figure 3 is not due to extensive hyperparamter search. Indeed, the single set of RDL hyperparameters is compared to a carefully tuned LightGBM, which was allowed to search over 10 sets of hyperparameters.\n\n\nPredictive Power. Results shown in Figures 3. Whilst outperforming the data scientist in 11 of 15 tasks, we note that RDL best outperforms the data scientist on classification tasks, struggling more on regression. Indeed it was necessary for us to apply a \u201cboosting\u201d to the RDL model to improve performance (see Appendix C for details). Even with boosting, the data scientist model outperforms RDL in several cases. One cause we identify is that the MLP output head of the GNN is poorly suited to regression tasks (see Appendix C for our analysis). This suggests an opportunity for improved output heads for regression tasks. We stress that our RDL implementation is an initial demonstration. We believe there is significant scope for new research leading to large improvements in performance. In particular, ideas from graph ML, deep tabular ML, and time-series modeling are well suited to advance RDL.\n\n\nFigure 5: RDL vs. Data Scientist. Relational Deep Learning reduces the new lines of code needed to solve a new task by 94%. Left shows entity classification, right shows entity regression.\n\n\n\nHuman Work. Results shown in Figure 4. In our user study RDL required 96% less hours work to solve a new task, compared to the data scientist work flow. The RDL solutions always took less than an hour to write, whilst the data scientist took 12121212 hours on average, with a standard deviation of 1.61.61.61.6 hours. We emphasize that this measures marginal effort, i.e., it does not include reusable code that can be amortized over many tasks. RDL compares favorably to data scientist because a large majority of RDL code is reusable for new tasks (a GNN architecture and training loop needs only to be defined once) whereas a large portion of the data scientist\u2019s code is task specific and must be re-done afresh for every new task that needs to be solved.\n\n\nLines of Code. Results shown in Figure 5. For the RDL model, the only new addition needed to solve a new task is the code describing how to compute the training supervision for the RDL, which is stored in the training table. This requires a similar number of lines of code for each task, with 56 lines of code on average, with standard deviation 8.88.88.88.8, with the data scientist requiring with 878\u00b177plus-or-minus87877878\\pm 77878 \u00b1 77. The minimum lines of code required by RDL is 44, compared to 734 for the data scientist, and maximum is 84 compared to 1039 for the data scientist. Examples of the RDL code required to solve rel-amazon tasks can be viewed here. For the data scientist pipeline, we record the number of lines of code for EDA and SQL files, and the manipulations needed to format data to be fed into the pre-prepared LightGBM script.\n\n\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "6 Expert Data Scientist User Study": "\n\n6 Expert Data Scientist User Study\n\nTo test RDL in the most challenging circumstances possible, we undertake a human trial wherein a data scientist solves each task by manually designing features and feeds them into tabular methods such at LightGBM or XGBoost\u00a0(Chen and Guestrin, 2016; Ke et\u00a0al., 2017). This represents the prior gold-standard for building predictive models on relational databases (Heaton, 2016), and the key point of comparison for RDL.\n\n\nWe structure our user study along the five main data science workflow steps:\n\n\n1.\n\nExploratory data analysis (EDA): Explore the dataset and task to understand its characteristics, including what column features there are, and if there is any missing data.\n\n\n\n2.\n\nFeature ideation: Based on EDA and intuition from prior experiences, propose a set of entity-level features that the data scientist believes may contain predictive signal for the task.\n\n\n\n3.\n\nFeature enginnering: Using query languages such as SQL to compute the proposed features, and add them as extra columns to the target table of interest.\n\n\n\n4.\n\nTabular ML: Run tabular methods such as LightGBM or XGBoost on the table with extra features to produce a predictive model, and record the test performance.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (Optional): Common tools include SHAP and LIME, which aim to explain the contribution of each input feature to the final performance.\n\n\n\n\n\nConsider for example the rel-hm dataset (schema in Appendix D) and the task of predicting customer churn. Here the customer table only contains simple biographical information such as username and joining date. To capture more predictive information, additional features, such as time since last purchase, can be computed using the other tables, and added to the customer table. We give a detailed walk-through of the data scientist\u2019s work process for solving this specific task in Appendix C. We strongly encourage the interested reader to review this, as it highlights the significant amount of task-specific effort that this workflow necessitates.\n\n\nLimitations of Manual Feature Engineering. This workflow suffers from several fundamental limitations. Most obviously, since features are hand designed they only capture part of the predictive signal in the database, useful signal is easily missed. Additionally, feature complexity is limited by human reasoning abilities, meaning that higher-order interactions between entities are often overlooked. Beyond predictive signal, the other crucial limitation of feature engineering is its extremely manual nature\u2014every time a new model is built a data scientist has to repeat this process, requiring many hours of human labor, and significant quantities of new SQL code to design features (Zheng and Casari, 2018). Our RDL models avoid these limitations (see Section 6.1).\n\n\nData Scientist. To conduct a thorough comparison to this process, we recruit a high-end data scientist with Stanford CS MSc degree, 4.0 GPA, and 5 years of experience of building machine learning models in the financial industry. This experience includes a significant amount of time building machine learning models in exactly above five steps, as well as broader data science expertise.\n\n\nUser Study Protocol.\n\n\nFigure 3: RDL vs. Data Scientist. Relational Deep Learning matches or outperforms the data scientist in 11 of 15 tasks. Left shows entity classification AUROC, right shows entity regression, reporting MAE normalized so that the RDL MAE is always 1.\n\n\n\nBecause of the open-ended nature of feature engineering and model development, we follow a specific protocol for the user study in order to standardize the amount of effort dedicated to each dataset and task. Tracking the 5 steps outlined above, we impose the following rules:\n\n\n\n\n1.\n\nEDA: The time allotted for data exploration is capped at 4 hours. This threshold was chosen to give the data scientist enough time to familiarize themselves with the schema, visualize key relationships and distributions, and take stock of any outliers in the dataset, while providing a reasonable limit to the effort applied.\n\n\n\n2.\n\nFeature ideation: Feature ideation is performed manually with pen and paper, and is limited to 1 hour. In practice, the data scientist found that 1 hour was plenty of time to enumerate all promising features at that time, especially since many ideas naturally arise during the EDA process already.\n\n\n\n3.\n\nFeature engineering: The features described during the ideation phase are then computed using SQL queries. The time taken to write SQL code to generate the features is unconstrained in order to eliminate code writing speed as a factor in the study. We do, however, record code writing time for our timing benchmarking. This stage presented the most variability in terms of time commitment, partly because it is unconstrained, but mostly because the implementation complexity of the features itself is highly variable.\n\n\n\n4.\n\nTabular ML: For tabular ML training, we provide a standardized LightGBM training script including comprehensive hyperparameter tuning. The data scientist needs only to feed the table full of engineered features into this training script, which returns test performance results. However, there is some non-trivial amount of work required to transform the output of the SQL queries from the previous section into the Python objects (arrays) required for training LightGBM. Again, the time taken for this additional pre-preocessing is recorded.\n\n\n\n5.\n\nPost-hoc analysis of feature importance: Finally, after successfully training a model, an evaluation of model predictions and feature importance is carried out. This mostly serves as a general sanity check and an interesting corollary of the data scientist\u2019s work that provides task-specific insights (see Appendix C). In practice, this took no more than a few minutes per task and this time was not counted toward the total time commitment.\n\n\n\n\n\nReproducibility. All of the data scientist\u2019s workings are released333See https://github.com/snap-stanford/relbench-user-study. to ensure reproducibility and demonstrate the significant lengths gone through to build as accurate models as possible. In Appendix C we walk through a complete example for a single dataset and task, showing the data-centric insights it yields. An important by-product is a close analysis of which features contribute to model performance, which we believe will help inspire future well-motivated RDL research directions.\n\n\nFigure 4: RDL vs. Data Scientist. Relational Deep Learning reduces the hours of human work required to solve a new task by 96% on average (from 12.3 to 0.5 hours). Left shows node-level classification, right shows node-level regression.\n\n\n\n\n6.1 Results\n\nAs well as (i) raw predictive power, we compare the data scientist to our RDL models in terms of (ii) hours of human work, and (iii) number of new lines of code required to solve each task. We measure the marginal effort, meaning that we do not include code infrastructure that is reused across tasks, including for example data loading logic and training scripts for RDL or LightGBM models.\n\n\nSummary. Figures 3, 4, and 5 show that RDL learns highly predictive models, outperforming the data scientist in 11 of 15 tasks, whilst reducing hours worked by 96%percent9696\\%96 % on average, and lines of code by 94%percent9494\\%94 % on average. On average, it took the data scientist 12.3 hours to solve each task using traditional feature engineering. By contrast it takes roughly 30 minutes to solve a task with RDL.\n\n\nThis observation is the central value proposition of relational deep learning, pointing the way to unlocking new levels of predictive power, and potentially a new economic model for solving predictive tasks on relational databases. Replacing hand-crafted solutions with end-to-end learnable models has been a key takeaway from the last 15 years of AI research. It is therefore remarkable how little impact deep learning has had on ML on relational databases, one of the most widespread applied ML use cases. To the best of our knowledge, RDL represents the first proposal for a deep learning approach for relational databases that has demonstrated efficacy compared with established data science workflows.\n\n\nWe highlight that all RelBench tasks were solved with a single set of default hyperparameters (with 2 exceptions requiring small modifications to learning rate, number of epochs, and GNN aggregation function). This demonstrates the robustness of RDL, and that the performance of RDL in Figure 3 is not due to extensive hyperparamter search. Indeed, the single set of RDL hyperparameters is compared to a carefully tuned LightGBM, which was allowed to search over 10 sets of hyperparameters.\n\n\nPredictive Power. Results shown in Figures 3. Whilst outperforming the data scientist in 11 of 15 tasks, we note that RDL best outperforms the data scientist on classification tasks, struggling more on regression. Indeed it was necessary for us to apply a \u201cboosting\u201d to the RDL model to improve performance (see Appendix C for details). Even with boosting, the data scientist model outperforms RDL in several cases. One cause we identify is that the MLP output head of the GNN is poorly suited to regression tasks (see Appendix C for our analysis). This suggests an opportunity for improved output heads for regression tasks. We stress that our RDL implementation is an initial demonstration. We believe there is significant scope for new research leading to large improvements in performance. In particular, ideas from graph ML, deep tabular ML, and time-series modeling are well suited to advance RDL.\n\n\nFigure 5: RDL vs. Data Scientist. Relational Deep Learning reduces the new lines of code needed to solve a new task by 94%. Left shows entity classification, right shows entity regression.\n\n\n\nHuman Work. Results shown in Figure 4. In our user study RDL required 96% less hours work to solve a new task, compared to the data scientist work flow. The RDL solutions always took less than an hour to write, whilst the data scientist took 12121212 hours on average, with a standard deviation of 1.61.61.61.6 hours. We emphasize that this measures marginal effort, i.e., it does not include reusable code that can be amortized over many tasks. RDL compares favorably to data scientist because a large majority of RDL code is reusable for new tasks (a GNN architecture and training loop needs only to be defined once) whereas a large portion of the data scientist\u2019s code is task specific and must be re-done afresh for every new task that needs to be solved.\n\n\nLines of Code. Results shown in Figure 5. For the RDL model, the only new addition needed to solve a new task is the code describing how to compute the training supervision for the RDL, which is stored in the training table. This requires a similar number of lines of code for each task, with 56 lines of code on average, with standard deviation 8.88.88.88.8, with the data scientist requiring with 878\u00b177plus-or-minus87877878\\pm 77878 \u00b1 77. The minimum lines of code required by RDL is 44, compared to 734 for the data scientist, and maximum is 84 compared to 1039 for the data scientist. Examples of the RDL code required to solve rel-amazon tasks can be viewed here. For the data scientist pipeline, we record the number of lines of code for EDA and SQL files, and the manipulations needed to format data to be fed into the pre-prepared LightGBM script.\n\n\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "7 Related Work": "\n\n7 Related Work\n\nGraph Machine Learning Benchmarks. Challenging and realistic benchmarks drive innovation in methodology. A classic example is the ImageNet\u00a0(Deng et\u00a0al., 2009), introduced prior to the rise of deep learning, which was a key catalyst for the seminal work of Krizhevsky et\u00a0al. (2017). In graph machine learning, benchmarks such as the Open Graph Benchmark (Hu et\u00a0al., 2020), TUDataset (Morris et\u00a0al., 2020), and more recently, the Temporal Graph Benchmark (Huang et\u00a0al., 2024) have sustained the growth and maturation of graph machine learning as a field.\nRelBench differs since instead of collecting together tasks are already recognized as graph machine learning tasks, RelBench presents existing tasks typically solved using other methods, as graph ML tasks. As a consequence, RelBench significantly expands the space of problems solvable using graph ML. Whilst graph ML is a key part of this benchmark, relational deep learning is a new problem, requiring only need good GNNs, but also innovation on tabular learning to fuse multimodal input data with the GNN, temporal learning, and even graph construction. We believe that advancing the state-of-the-art on RelBench will involve progress in all of these directions.\n\n\nRelational Deep Learning.\nSeveral works have proposed to use graph neural networks for learning on relational data\u00a0(Schlichtkrull et\u00a0al., 2018; Cvitkovic, 2019; \u0160\u00edr, 2021; Zahradn\u00edk et\u00a0al., 2023). They explored different graph neural network architectures on (heterogeneous) graphs, leveraging relational structure. Recently, Fey et\u00a0al. (2024) proposed a general end-to-end learnable framework for solving predictive tasks on relational databases, treating temporality as a core concept.\nRelBench provides a comprehensive testbed to develop these ideas further.\n\n\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "8 Conclusion": "\n\n8 Conclusion\n\nThis work introduces RelBench, a benchmark to facilitate research on relational deep learning\u00a0(Fey et\u00a0al., 2024).\nRelBench provides diverse and realistic relational databases and define practical predictive tasks that cover both entity-level prediction and entity link prediction.\nIn addition, we provide the first open-source implementation of relational deep learning and validated its effectiveness over the common practice of manual feature engineering by an experienced data scientist.\nWe hope RelBench will catalyze further research on relational deep learning to achieve highly-accurate prediction over complex multi-tabular datasets without manual feature engineering.\n\n\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Acknowledgments and Disclosure of Funding": "\nAcknowledgments and Disclosure of Funding\n\nWe thank Shirley Wu, Kaidi Cao, Rok Sosic, Yu He, Qian Huang, Bruno Ribeiro and Michi Yasunaga for discussions and for providing feedback on our manuscript.\nWe also gratefully acknowledge the support of\nNSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), DMS-2327709 (IHBEM);\nStanford Data Applications Initiative,\nWu Tsai Neurosciences Institute,\nStanford Institute for Human-Centered AI,\nChan Zuckerberg Initiative,\nAmazon, Genentech, GSK, Hitachi, SAP, and UCB.\nThe content is solely the responsibility of the authors and does not necessarily represent the official views of the funding entities.\n\n\nReferences\n\n\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n\n\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n\n\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n\n\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n\n\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n\n\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n\n\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n\n\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n\n\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n\n\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n\n\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n\n\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n\n\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n\n\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n\n\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n\n\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n\n\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n\n\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n\n\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n\n\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n\n\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n\n\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n\n\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n\n\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n\n\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n\n\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n\n\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n\n\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n\n\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n\n\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n\n\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n\n\n\n\n\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n\n\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix A Additional Task Information": "\n\nAppendix A Additional Task Information\n\nFor reference, the following list documents all the predictive tasks in RelBench.\n\n\n1.\n\nrel-amazon\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: For each user, predict 1 if the customer does not review any product in the next 3 months, and 0 otherwise.\n\n\n\n(b)\n\nuser-ltv: For each user, predict the $currency-dollar\\$$ value of the total number of products they buy and review in the next 3 months.\n\n\n\n(c)\n\nitem-churn: For each product, predict 1 if the product does not receive any reviews in the next 3 months.\n\n\n\n(d)\n\nitem-ltv: For each product, predict the $currency-dollar\\$$ value of the total number purchases and reviews it recieves in the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of distinct items each customer will purchase in the\nnext 3 months.\n\n\n\n(b)\n\nuser-item-rate: Predict the list of distinct items each customer will purchase and give a 5 star review in the\nnext 3 months.\n\n\n\n(c)\n\nuser-item-review: Predict the list of distinct items each customer will purchase and give a detailed review in the\nnext 3 months.\n\n\n\n\n\n\n2.\n\nrel-avito\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-visits: Predict whether each customer will visit more than one Ad in the next 4 days.\n\n\n\n(b)\n\nuser-clicks: Predict whether each customer will click on more than one Ads in the next 4 day.\n\n\n\n(c)\n\nad-ctr: Assuming the Ad will be clicked in the next 4 days, predict the Click-Through-Rate (CTR) for each Ad.\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-ad-visit: Predict the list of ads a user will visit in the next 4 days.\n\n\n\n\n\n\n3.\n\nrel-f1\n\n\nNode-level tasks:\n\n\n(a)\n\ndriver-position: Predict the average finishing position of each driver\nall races in the next 2 months.\n\n\n\n(b)\n\ndriver-dnf: For each driver predict the if they will DNF (did not finish) a race in the next 1 month.\n\n\n\n(c)\n\ndriver-top3: For each driver predict if they will qualify in the top-3 for a race in the next 1 month.\n\n\n\n\n\n\n4.\n\nrel-hm\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-churn: Predict the churn for a customer (no transactions) in the next week.\n\n\n\n(b)\n\nitem-sales: Predict the total sales for an article (the sum of prices of the\nassociated transactions) in the next week.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-item-purchase: Predict the list of articles each customer will purchase in the next\nseven days.\n\n\n\n\n\n\n5.\n\nrel-stack\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-engagement: For each user predict if a user will make any votes, posts, or comments in the next 3 months.\n\n\n\n(b)\n\npost-votes: For each user post predict how many votes it will receive in the next 3 months\n\n\n\n(c)\n\nuser-badge: For each user predict if each user will receive in a new badge the next 3 months.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\nuser-post-comment: Predict a list of existing posts that a user will comment in the next\ntwo years.\n\n\n\n(b)\n\npost-post-related: Predict a list of existing posts that users will link a given post to in the next\ntwo years.\n\n\n\n\n\n\n6.\n\nrel-trial\n\n\nNode-level tasks:\n\n\n(a)\n\nstudy-outcome: Predict if the trials in the next 1 year will achieve its primary outcome.\n\n\n\n(b)\n\nstudy-adverse: Predict the number of affected patients with severe advsere events/death for the trial in the next 1 year.\n\n\n\n(c)\n\nsite-success: Predict the success rate of a trial site in the next 1 year.\n\n\n\n\n\nLink-level tasks:\n\n\n(a)\n\ncondition-sponsor-run: Predict whether this condition will have which sponsors.\n\n\n\n(b)\n\nsite-sponsor-run: Predict whether this sponsor will have a trial in a facility.\n\n\n\n\n\n\n7.\n\nrel-event\n\n\nNode-level tasks:\n\n\n(a)\n\nuser-attendance: Predict how many events each user will respond yes or maybe in the next seven days.\n\n\n\n(b)\n\nuser-repeat: Predict whether a user will attend an event(by responding yes or maybe) in the next 7 days if they have already attended an event in the last 14 days.\n\n\n\n(c)\n\nuser-ignore: Predict whether a user will ignore more than 2 event invitations\nin the next 7 days.\n\n\n\n\n\n\n\n",
      "Appendix B Experiment Details and Additional Results": "\n\nAppendix B Experiment Details and Additional Results\n\nTable 6: Entity classification results (AUROC mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_classification.tex\n\n\n\nDataset\nTask\nSplit\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 7: Entity regression results (MAE mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, lower is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_node_regression.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nZero\n\n \n\n\nGlobal\n\nMean\n\n \n\n\nGlobal\n\nMedian\n\n \n\n\nEntity\n\nMean\n\n \n\n\nEntity\n\nMedian\n\nLightGBM\nRDL\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Link prediction results (MAP mean\u00b1stdplus-or-minusstd{}_{\\pm\\text{std}}start_FLOATSUBSCRIPT \u00b1 std end_FLOATSUBSCRIPT over 5555 runs, higher is better) on RelBench. Best values are in bold along with those not statistically different from it.\n\n\\CatchFileDef\n\\tabledata\n\n\ntables/app_link_prediction.tex\n\n\n\nDataset\nTask\nSplit\n \n\n\nGlobal\n\nPopularity\n\n \n\n\nPast\n\nVisit\n\nLightGBM\n \n\n\nRDL\n\n(GraphSAGE)\n\n \n\n\nRDL\n\n(ID-GNN)\n\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\n\n\nB.1 Detailed Results\n\nTables B, B and B show mean and standard deviations over 5555 runs for the entity classification, entity regression and link prediction results respectively.\n\n\n\nB.2 Hyperparameter Choices\n\nAll our RDL experiments were run based on a single set of default task-specific hyperparameters, i.e. we did not perform exhaustive hyperparamter tuning, cf.\u00a0Table\u00a09.\nThis verifies the stability and robustness of RDL solutions, even against expert data scientist baselines. Specifically, all task types use a shared GNN configuration (a two-layer GNN with a hidden feature size of 128128128128 and \u201csum\u201d aggregation) and sample subgraphs identically (disjoint subgraphs of 512512512512 seed entities with a maximum of 128128128128 neighbors for each foreign key).\nAcross task types, we only vary the learning rate and the maximum number of epochs to train for.\n\n\nTable 9: Task-specific RDL default hyperparameters. \n\n\nHyperparameter\nTask type\n\n\nNode classification\nNode regression\nLink prediction\n\n\nLearning rate\n0.005\n0.005\n0.001\n\n\nMaximum epochs\n10\n10\n20\n\n\nBatch size\n512\n512\n512\n\n\nHidden feature size\n128\n128\n128\n\n\nAggregation\nsummation\nsummation\nsummation\n\n\nNumber of layers\n2\n2\n2\n\n\nNumber of neighbors\n128\n128\n128\n\n\nTemporal sampling strategy\nuniform\nuniform\nuniform\n\n\n\n\nNotably, we found that our default set of hyperparameters heavily underperformed on the node-level tasks on the rel-trial dataset. On this dataset, we used a learning rate of 0.00010.00010.00010.0001, a \u201cmean\u201d neighborhood aggregation scheme, 64646464 sampled neighbors, and trained for a maximum of 20202020 epochs.\nFor the ID-GNN link-prediction experiments on rel-trial, it was necessary to use a four-layer deep GNN in order to ensure that destination nodes are part of source node-centric subgraphs.\n\n\n\nB.3 Ablations\n\nWe also report additional results ablating parts of our relational deep learning implementation. All experiments are designed to be data-centric, aiming to validate basic properties of the chosen datasets and tasks. Examples include confirming that the graph structure, node features, and temporal-awareness all play important roles in achieving optimal performance, which also underscores the unique challenges our\u00a0RelBench dataset and tasks present.\n\n\nFigure 6: Investigation on the role of leveraging primary-foreign key (pkey-fkey) edges for the GNN. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that our proposal of using pkey-fkey edges for message passing is vital for GNN to achieve desirable performance on\u00a0RelBench. Error bars correspond to 95% confidence interval.\n\n\nGraph structure. We first investigate the role of the graph structure we adopt for GNNs on\u00a0RelBench. Specifically, we compare the following two approaches of constructing the edges: 1. Primary-foreign key (pkey-fkey), where the entities from two tables that share the same primary key and foreign key are connected through an edge; 2. Randomly permuted, where we apply a random permutation on the destination nodes in the primary-foreign key graph for each type of the edge while keeping the source nodes untouched. From Fig.\u00a06 we observe that with random permutation on the primary-foreign key edges the performance of the GNN becomes much worse, verifying the critical role of carefully constructing the graph structure through, e.g., primary-foreign key as proposed in\u00a0Fey et\u00a0al. (2024).\n\n\nFigure 7: Investigation on the role of node features. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that leveraging node features is important for GNN. Error bars correspond to 95% confidence interval.\n\n\nFigure 8: Investigation on the role of text embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We observe that adding text embedding using GloVe\u00a0(Pennington et\u00a0al., 2014) or BERT\u00a0(Devlin et\u00a0al., 2018) generally helps improve the performance. Error bars correspond to 95% confidence interval.\n\n\nNode features and text embeddings. Here we study the effect of node features used in\u00a0RelBench. In the experiments depicted in Fig.\u00a07, we compare GNN (w/ node feature) with its variant where the node features are all masked by zeros (i.e., w/o node feature). We find that utilizing rich node features incorporated in our\u00a0RelBench dataset is crucial for GNN. Moreover, we also investigate, in particular, the approach to encode texts in the data that constitutes part of the node features. In Fig.\u00a08, we compare GloVe text embedding\u00a0(Pennington et\u00a0al., 2014) and BERT text embedding\u00a0(Devlin et\u00a0al., 2018) with w/o text embedding, where the text embeddings are masked by zeros. We observe that encoding the rich texts in\u00a0RelBench with GloVe or BERT embedding consistently yields better performance compared with using no text features. We also find that BERT embedding is usually better than GloVe embedding especially for node classification tasks, which suggests that enhancing the quality of text embedding will potentially help achieve better performance.\n\n\nTemporal awareness. We also investigate the importance of injecting temporal awareness into the GNN by ablating on the time embedding. To be specific, in the implementation we add a relative time embedding when deriving the node features using the relative time span between the timestamp of the entity and the querying seed time. Results are exhibited in Fig.\u00a09. We discover that adding the time embedding significantly enhance the performance across a diverse range of tasks, demonstrating the efficacy and importance of building up the temporal awareness into the model.\n\n\nFigure 9: Investigation on the role of time embedding. At the top row are three node classification tasks with metric AUROC (higher is better) while at the bottom are three node regression tasks with metric MAE (lower is better), evaluated on the test set. We find that adding time embedding to the GNN consistently boosts the performance. Error bars correspond to 95% confidence interval.\n\n\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix C User Study Additional Details": "\n\nAppendix C User Study Additional Details\n\n\nC.1 Data Scientist Example Workflow\n\nIn this section we provide a detailed description of the data scientist workflow for the user-churn task of the rel-hm dataset. The purpose of this is to exemplify the efforts undertaken by the data scientist to solve RelBench tasks. For data scientist solutions to all tasks, see https://github.com/snap-stanford/relbench-user-study.\n\n\nRecall that the main data science workflow steps are:\n\n\n1.\n\nExploratory data analysis (EDA).\n\n\n\n2.\n\nFeature ideation.\n\n\n\n3.\n\nFeature enginnering.\n\n\n\n4.\n\nTabular ML.\n\n\n\n5.\n\nPost-hoc analysis of feature importance (optional).\n\n\n\n\n\n\nC.1.1 Exploratory Data Analysis\n\nDuring the exploratory data analysis (EDA) the data scientist familiarizes themselves with a new dataset. It is typically carried out in a Jupyter notebook, where the data scientist first loads the dataset or establishes a connection to it and then systematically explores it. The data scientist may:\n\n\n\u2022\n\nVisualize the database schema, looking at the fields of different tables and the relationships between them.\n\n\n\n\u2022\n\nClosely analyze the label sets:\n\n\n\u2013\n\nLook at the relative sizes and temporal split of the training, validation and test subsets.\n\n\n\n\u2013\n\nLook at label statistics such as the mean, the standard deviation and various quantiles.\n\n\n\n\u2013\n\nFor classification tasks, understand class (im)balance: how much bigger is the modal class than the rest? For example, in the user-churn task roughly 82% of the samples have label 1111, so there is a good amount of imbalance but not enough to strictly require up-sampling techniques.\n\n\n\n\u2013\n\nFor regression tasks, understand the label distribution: are the labels concentrated around a typical value or do they follow a power law wherein the labels span several orders of magnitude? In extreme cases, this exploration will point to a need for specialized handing of the label space for model training.\n\n\n\n\n\n\n\u2022\n\nPlot distributions and aggregations of interesting columns/fields. For example, in Figure 10 we can see three such plots. From left to right:\n\n\n\u2013\n\nThe first plot shows the distribution of age among customers. We see two distinct peaks one in the mid-twenties and another in the mid-fifties, suggesting different customer \u201carchetypes\u201d, which may have different spending patterns.\n\n\n\n\u2013\n\nThe second plot shows the number of sales per month over a two year period. We can see some seasonality with summer months being particularly good for overall sales. This suggests date related features could be useful.\n\n\n\n\u2013\n\nThe third plot shows a Lorenz curve of sales per article, showcasing the canonical Pareto Principle: 20% of the articles account for 80% of the sales.\n\n\n\n\n\n\n\u2022\n\nRun custom queries to look at interesting quantities and/or relationships between different columns. For instance, in the EDA for rel-hm, an interesting quantity to look at is the variability in item prices across the year. This reveals that most of the variability is downward, representing temporary discounts.\n\n\n\n\u2022\n\nInvestigate outliers or odd-looking patterns in the data. These usually will have some real-world explanation that may inform how the data scientist chooses to pre-process the data and construct features.\n\n\n\n\n\nIn all, this process takes in the order of a few hours (3-4 for most datasets in the user study).\n\n\n\n\n\n\n\n\nFigure 10: EDA Plots. Each plot explores different characteristics of the dataset. Understanding the data and identifying relationships between different quantities is an essential prerequisite to meaningful feature engineering.\n\n\n\n\nC.1.2 Feature Ideation\n\nHaving explored the dataset in the EDA, the data scientist will then brainstorm features that, to their judgement, will provide valuable signal to a model for a specific learning task. In the case of the user-churn task, a rather simple feature would be the customer\u2019s age, which is a field directly available in one of the tables. A slightly more complex feature would be the total amount spent by the customer so far. Finally, an example of a fairly complex feature is the average monthly sales volume of items purchased by the customer in the past week. A high value for this feature may indicate that the customer has been shopping trendy items lately, whereas a low value for this feature may indicate that the customer has been interested in more arcane or specific items.\n\n\nIn practice, the ideation phase consists of writing down all of these feature ideas in a file or a piece of paper. It is the quickest part of the whole process and in this user study took between 30 minutes and one hour.\n\n\n\n\nC.1.3 Feature Engineering\n\nWith a list of features in hand, the data scientist then proceeds to actually write code to generate all the features for each sample in the the train, validation and test subsets. In this user study, this was carried out using DuckDB SQL444See https://duckdb.org/. with some Jinja templating555See https://jinja.palletsprojects.com/en/3.1.x/intro/. for convenience.\n\n\nRevisiting the example features from the previous section, the conceptual complexity of the features closely tracks with the technical complexity of implementing them. For customer age all that is required is a simple join. The total amount spent by the customer, can be calculated using a group by clause and a couple of join\u2019s. Lastly, calculating the average monthly sales volume of items purchased by the customer in the past week requires multiple group by\u2019s, join\u2019s, and window functions distributed across multiple common table expressions (CTEs).\n\n\nA key consideration during feature engineering is the prevention of leakage. The data scientist must ensure that none of the features accidentally include information from after the sample timestamp. This is especially true for complex features like the third example above, where special care must be taken to ensure that each join has the appropriate filters to comply with the sample timestamp.\n\n\nFor some tasks, e.g., study-outcome, the initial features did leak information from the validation set into the training set. Thanks to the RelBench testing setup, leaking test data into the training data is hard to do by accident, since test data is hidden. Leaking information from validation to train (but not test to train) led to extremely high validation performance and very low test performance (test was significantly lower than LightGBM with no feature engineering). The large discrepancy between validation and test performances alerted the data scientist to the mistake, and the features were eventually fixed. This example illustrates another complexity that feature engineering introduces, with special care needed to ensure leakage does not happen.\n\n\nOther considerations that the data scientist must keep in mind during development and implementation of the features are parsing issues, runtime constrains and memory load. For example, during the user study we identified a parsing issue arising from special characters in user posts/comments in the rel-stack dataset. The backslash character, widely used LaTeXcan trip up certain text parsers if not handled with care. Furthermore, runtime and memory constraints are important to keep in mind when working with larger datasets and computing features that require nested join\u2019s and aggregations. During the user study, there were some cases where we had to refactor SQL queries to make them more efficient, increasing the overall implementation time. For some tasks we had to implement sub-sampling of the training set to reduce the burden on compute resources.\n\n\nFinally, once the features have been generated for each data subset, the data scientist will usually inspect the generated features looking for anomalies (e.g. an unusual prevalence of NULL values). In this user study we also implemented some automated sanity checks to validate the generated features beyond manual inspection.\n\n\n\n\nC.1.4 Tabular Machine Learning\n\nThe output of the Feature Engineering phase is a DuckDB table with engineered features for each data subset. There is some non-trivial amount of work required to go from those tables to the numerical arrays used for training by most Tabular ML models (LightGBM in this case). This is implemented in a Python script that loads the data, transforms it into arrays and carries out hyperparameter tuning. In this user study we ran 5 hyperparameter optimization runs, with 10 trials each, reporting the mean and standard deviation over the 5 runs. For the user-churn task this took one to two hours.\n\n\n\n\nC.1.5 Post-hoc Analysis\n\nThe last step in the process is to look at a trained model and analyze its performance and feature importance. To this end we used SHAP values (Lundberg and Lee, 2017) and the corresponding python package666See https://shap.readthedocs.io/en/latest/.. Figure 11 shows the top 30 most important features in the user-churn task. The individual violin plots show the distribution of SHAP values for a subset of the validation set, the color indicates the value of the feature. For the user-churn task, the most predictive features were primarily (1) all-time statistics of user behavior pattern, and (2) temporal information that allows the model to be aware of seasonality.\n\n\nFigure 11: Feature Importances. SHAP values of top 30 features ranked by importance. Note: week_of_year feature shows little variability because the validation set is temporally concentrated in a few weeks.\n\n\n\n\n\nC.2 Regression Output Head Analysis\n\nBy default, our RDL implementation uses a simple linear output head on top of the GNN embeddings. However we found that on regression tasks this sometimes led to lower than desirable performance. We found that performance on many regression tasks could be improved by modifying this output head. Instead of a linear layer, we took the output from the GNN, and fed these embeddings into a LightGBM model, which is trained in a second separate training phase from the GNN model.\n\n\nThe resulting model still uses an end-to-end learned GNN for cross-table feature engineering, showing that the GNN is learning useful features. Instead we attribute the weaker performance to the linear output head. We believe that further attention to the regression output head is an interesting direction for further study, with the goal of designing an output head that is performant and can be trained jointly with the GNN (unlike our LightGMB modification).\n\n\nWe run three experiments to study this phenomena, and attempt to isolate the output head as a problematic component for regression tasks.\n\n\n1.\n\nLightGBM trained on GNN-learned entity-level features on regression tasks. We find that this model performs better than the original GNN, suggesting that the linear output head of the GNN is suboptimal.\n\n\n\n2.\n\nLightGBM trained on GNN-learned entity-level features on classification tasks. We find no performance improvement, and even some degradation, compared to the original GNN model, suggesting that the observed performance boost of (1) comes not from an overall better architecture but from the correction of an innate shortcoming of the linear output head vis-a-vis regression tasks. In other words, using a LightGBM on top of the GNN is only helpful insofar as it provides a more flexible output head for regression tasks.\n\n\n\n3.\n\nEvaluate GNN performance after converting regression tasks to binary classification tasks with label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. We find that the performance gap between the data scientist models and the GNN narrow. This suggests that the GNN can learn the relevant predictive signal, but performance is affected by how the task is formulated (classification vs regression).\n\n\n\nSee Tables C.2, C.2, C.2 for the results of each of these experiments.\n\n\nIn Figure 3, for regression tasks we report the RDL results using GNN learned features with LightGBM output head. In Table 5.2 we report result for the basic GNN in order to avoid creating confusion for other researchers when comparing different GNN methods. We believe that Tables C.2, C.2, C.2 provide clear evidence that there is an opportunity for improvements and simplifications, which we leave to future work.\n\n\nTable 10: Entity regression results (MAE, lower is better) on selected RelBench datasets. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_regression.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 11: Entity classification results (AUROC, higher is better, numbers bolded if withing standard deviation of best result) on selected RelBench tasks. Training a LightGBM model on features extracted by a trained GNN does not lead to performance lift, and can even hurt performance slightly. This is evidence that output head limitations hold for regression tasks only. Note, study-outcome uses default GNN parameters for simplicity, differing form the performance reported in the main paper.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_plus_lgbm_classification.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nGNN+LightGBM\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 12: Entity classification results (AUROC, higher is better) on selected RelBench regression tasks, converted into classification tasks with binary label y=\ud835\udfcf\u2062{yregression>0}\ud835\udc661subscript\ud835\udc66regression0y=\\mathbf{1}\\{y_{\\text{regression}}>0\\}italic_y = bold_1 { italic_y start_POSTSUBSCRIPT regression end_POSTSUBSCRIPT > 0 }. Training a LightGBM model on features extracted by a trained GNN leads to performance lift. This is evidence that the linear layer output head of the base GNN is suboptimal.\n\\CatchFileDef\n\\tabledata\n\n\ntables/gnn_binarized.tex\n\n\n\nDataset\nTask\nSplit\nGNN\nData Scientist\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix D Dataset Origins and Licenes": "\n\nAppendix D Dataset Origins and Licenes\n\nThis section details the sources for all data used in RelBench. In all cases, the data providers consent for their data to be used freely for non-commercial and research purposes. The only database with potentially personally identifiable information is rel-stack, which draws from the Stack Exchange site, which sometimes has individuals\u2019 names as their username. This information shared with consent, as all users must agree to the Stack Exchange privacy policy, see: https://stackoverflow.com/legal/privacy-policy.\n\n\nrel-amazon. Data obtained from the Amazon Review Data Dump from Ni et\u00a0al. (2019). See the website: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/. Data license is not specified.\n\n\nrel-avito. Data is obtained from Kaggle https://www.kaggle.com/competitions/avito-context-ad-clicks. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes. Note that after data download, we further downsample the avito dataset by randomly selecting approximately 100,000 data point from user table and sample all other tables that have connections to the sampled users.\n\n\nrel-stack. Data was obtained from The Internet Archive, whose stated mission is to provide \u201cuniversal access to all knowledge. We downloaded our data from https://archive.org/download/stackexchange in Novermber 2023. Data license is not specified.\n\n\nrel-f1. Data was sourced from the Ergast API (https://ergast.com/mrd/) in February 2024. The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes. As far as we are able to determine the data is public and license is not specified.\n\n\nrel-trial. Data was downloaded from the ClinicalTrials.gov website in January 2024. This data is provided by the NIH, an official branch of the US Government. The terms of use state that data are available to all requesters, both within and outside the United States, at no charge. Our rel-trial database is a snapshot from January 2024, and will not be updated with newer trials results.\n\n\nrel-hm. Data is obtained from Kaggle https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations. All RelBench users must download data from Kaggle themselves, a part of which is accepting the data usage terms. These terms include use only for non-commercial and academic purposes.\n\n\nrel-event. The dataset employed in this research was initially released on Kaggle for the Event Recommendation Engine Challenge, which can be accessed at https://www.kaggle.com/c/event-recommendation-engine-challenge/data. We have obtained explicit consent from the creators of this dataset to use it within RelBench. We extend our sincere gratitude to Allan Carroll for his support and generosity in sharing the data with the academic community.\n\n\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix E Additional Training Table Statistics": "\n\nAppendix E Additional Training Table Statistics\n\nWe report additional training table statistics for all tasks, separated into entity classification (cf.\u00a0Table\u00a0E), entity regression (cf.\u00a0Table\u00a0E), and link prediction (cf.\u00a0Table\u00a0E).\n\n\nTable 13: RelBench entity classification training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_classification_stats.tex\n\n\n\nDataset\nTask\nSplit\nPositives\nNegatives\n\n\\tabledata\n\n\n\n\n\n\n\n\n\nTable 14: RelBench entity regression training table target statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/node_regression_stats.tex\n\n\n\nDataset\nTask\nSplit\nMinimum\nMedian\nMean\nMaximum\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nTable 15: RelBench link prediction training table link statistics.\n\\CatchFileDef\n\\tabledata\n\n\ntables/link_prediction_stats.tex\n\n\n\nDataset\nTask\nSplit\n#Links\n \n\n\nAvg #links per\n\nentity/timestamp\n\n%Repeated links\n\n\\tabledata\n\n\n\n\n\n\n\n\n\n\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "Appendix F Dataset Schema": "\n\nAppendix F Dataset Schema\n\nFigure 12: rel-amazon database diagram.\n\n\nFigure 13: rel-stack database diagram.\n\n\nFigure 14: rel-f1 database diagram.\n\n\nFigure 15: rel-trial database diagram.\n\n\nFigure 16: rel-hm database diagram.\n\n\nFigure 17: rel-event database diagram.\n\n\nFigure 18: rel-avito database diagram.\n\n\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n\n",
      "Appendix G Broader Impact": "\n\nAppendix G Broader Impact\n\nRelational deep learning broadens the applicability of graph machine learning to include relational databases. Whilst the blueprint is general, and can be applied to a wide variety of tasks, including potentially hazardous ones, we have taken steps to focus attention of potential positive use cases. Specifically, the beta version of RelBench considers two databases, Amazon products, and Stack Exchange, that are designed to highlight the usefulness of RDL for driving online commerce and online social networks. Future releases of RelBench will continue to expand the range of databases into domains we reasonably expect to be positive, such as biomedical data and sports fixtures. We hope these concrete steps ensure the adoption of RDL for purposes broadly beneficial to society.\n\n\nWhilst we strongly believe the RelBench has all the ingredients needed to be a long term benchamrk for relational deep learning, there are also possibilities for improvement and extension. Two such possibilities include: (1) RDL at scale: currently our implementation must load the entire database into working memory during training. For very large datasets this is not viable. Instead, a custom batch sampler is needed that acesses the database via queries to sample specific entities and their pkey-fkey neighbors; (2) Fully inductive link-prediction: our current link-prediction implementation supports predicting links for test time pairs (head,tail) where head is potentially new (unseen during training) and tail seen in the training data. Extending this formulation to be fully inductive (i.e., tail unseen during training) is possible, but out of the scope of this work for now.\n\n\n"
    },
    "table_captions": null,
    "figure_captions": null,
    "bibliography": {
      "Chen and Guestrin (2016)": "\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n",
      "Cvitkovic (2019)": "\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n",
      "Deng et\u00a0al. (2009)": "\nDeng et\u00a0al. (2009)\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\u00a0Fei-Fei.\n\n\nImagenet: A large-scale hierarchical image database.\n\n\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255. Ieee, 2009.\n\n\n",
      "Devlin et\u00a0al. (2018)": "\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n",
      "Fey and Lenssen (2019)": "\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n",
      "Fey et\u00a0al. (2024)": "\nFey et\u00a0al. (2024)\n\nMatthias Fey, Weihua Hu, Kexin Huang, Jan\u00a0Eric Lenssen, Rishabh Ranjan, Joshua\nRobinson, Rex Ying, Jiaxuan You, and Jure Leskovec.\n\n\nRelational deep learning: Graph representation learning on relational\ndatabases.\n\n\nICML Position Paper, 2024.\n\n\n",
      "Gilmer et\u00a0al. (2017)": "\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n",
      "Gorishniy et\u00a0al. (2021)": "\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n",
      "Hamilton et\u00a0al. (2017)": "\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Hanley and McNeil (1983)": "\nHanley and McNeil (1983)\n\nJames\u00a0A Hanley and Barbara\u00a0J McNeil.\n\n\nA method of comparing the areas under receiver operating\ncharacteristic curves derived from the same cases.\n\n\nRadiology, 148(3):839\u2013843, 1983.\n\n\n",
      "Heaton (2016)": "\nHeaton (2016)\n\nJeff Heaton.\n\n\nAn empirical analysis of feature engineering for predictive modeling.\n\n\nIn SoutheastCon 2016, pages 1\u20136. IEEE, 2016.\n\n\n",
      "Hu et\u00a0al. (2020)": "\nHu et\u00a0al. (2020)\n\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,\nMichele Catasta, and Jure Leskovec.\n\n\nOpen graph benchmark: Datasets for machine learning on graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n",
      "Hu et\u00a0al. (2024)": "\nHu et\u00a0al. (2024)\n\nWeihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid Kocijan,\nJure Leskovec, and Matthias Fey.\n\n\nPytorch frame: A modular framework for multi-modal tabular learning.\n\n\narXiv preprint arXiv:2404.00776, 2024.\n\n\n",
      "Huang et\u00a0al. (2024)": "\nHuang et\u00a0al. (2024)\n\nShenyang Huang, Farimah Poursafaei, Jacob Danovitch, Matthias Fey, Weihua Hu,\nEmanuele Rossi, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, and\nReihaneh Rabbany.\n\n\nTemporal graph benchmark for machine learning on temporal graphs.\n\n\nAdvances in Neural Information Processing Systems, 36, 2024.\n\n\n",
      "Johnson et\u00a0al. (2016)": "\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n",
      "Kaggle (2022)": "\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n",
      "Kapoor and Narayanan (2023)": "\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n",
      "Ke et\u00a0al. (2017)": "\nKe et\u00a0al. (2017)\n\nGuolin Ke, Qi\u00a0Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei\nYe, and Tie-Yan Liu.\n\n\nLightgbm: A highly efficient gradient boosting decision tree.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a030, 2017.\n\n\n",
      "Krizhevsky et\u00a0al. (2017)": "\nKrizhevsky et\u00a0al. (2017)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey\u00a0E Hinton.\n\n\nImagenet classification with deep convolutional neural networks.\n\n\nCommunications of the ACM, 60(6):84\u201390,\n2017.\n\n\n",
      "Lundberg and Lee (2017)": "\nLundberg and Lee (2017)\n\nScott\u00a0M Lundberg and Su-In Lee.\n\n\nA unified approach to interpreting model predictions.\n\n\nAdvances in neural information processing systems, 30, 2017.\n\n\n",
      "Morris et\u00a0al. (2020)": "\nMorris et\u00a0al. (2020)\n\nChristopher Morris, Nils\u00a0M Kriege, Franka Bause, Kristian Kersting, Petra\nMutzel, and Marion Neumann.\n\n\nTudataset: A collection of benchmark datasets for learning with\ngraphs.\n\n\narXiv preprint arXiv:2007.08663, 2020.\n\n\n",
      "Ni et\u00a0al. (2019)": "\nNi et\u00a0al. (2019)\n\nJianmo Ni, Jiacheng Li, and Julian McAuley.\n\n\nJustifying recommendations using distantly-labeled reviews and\nfine-grained aspects.\n\n\nIn Proceedings of the 2019 conference on empirical methods in\nnatural language processing and the 9th international joint conference on\nnatural language processing (EMNLP-IJCNLP), pages 188\u2013197, 2019.\n\n\n",
      "Pennington et\u00a0al. (2014)": "\nPennington et\u00a0al. (2014)\n\nJeffrey Pennington, Richard Socher, and Christopher\u00a0D Manning.\n\n\nGlove: Global vectors for word representation.\n\n\nIn Proceedings of the 2014 conference on empirical methods in\nnatural language processing (EMNLP), pages 1532\u20131543, 2014.\n\n\n",
      "PubMed (1996)": "\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n",
      "Rendle et\u00a0al. (2012)": "\nRendle et\u00a0al. (2012)\n\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n\n\nBpr: Bayesian personalized ranking from implicit feedback.\n\n\narXiv preprint arXiv:1205.2618, 2012.\n\n\n",
      "Schlichtkrull et\u00a0al. (2018)": "\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n",
      "\u0160\u00edr (2021)": "\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n",
      "Wang et\u00a0al. (2019)": "\nWang et\u00a0al. (2019)\n\nXiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua.\n\n\nNeural graph collaborative filtering.\n\n\nIn Proceedings of the 42nd international ACM SIGIR conference\non Research and development in Information Retrieval, pages 165\u2013174, 2019.\n\n\n",
      "You et\u00a0al. (2021)": "\nYou et\u00a0al. (2021)\n\nJiaxuan You, Jonathan\u00a0M Gomes-Selman, Rex Ying, and Jure Leskovec.\n\n\nIdentity-aware graph neural networks.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 10737\u201310745, 2021.\n\n\n",
      "Zahradn\u00edk et\u00a0al. (2023)": "\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n",
      "Zheng and Casari (2018)": "\nZheng and Casari (2018)\n\nAlice Zheng and Amanda Casari.\n\n\nFeature engineering for machine learning: principles and\ntechniques for data scientists.\n\n\n\" O\u2019Reilly Media, Inc.\", 2018.\n\n\n"
    },
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "cf4a446f-bf70-4ec2-9ad6-c0eebc9389af": {
    "pk": "cf4a446f-bf70-4ec2-9ad6-c0eebc9389af",
    "project_name": null,
    "authors": [
      "Matthias Fey",
      "Weihua Hu",
      "Kexin Huang",
      "Jan Eric Lenssen",
      "Rishabh Ranjan",
      "Joshua Robinson",
      "Rex Ying",
      "Jiaxuan You",
      "Jure Leskovec"
    ],
    "title": "Relational Deep Learning: Graph Representation Learning on Relational Databases",
    "abstract": "Much of the world's most valued data is stored in relational databases and data warehouses, where the data is organized into many tables connected by primary-foreign key relations. However, building machine learning models using this data is both challenging and time consuming. The core problem is that no machine learning method is capable of learning on multiple tables interconnected by primary-foreign key relations. Current methods can only learn from a single table, so the data must first be manually joined and aggregated into a single training table, the process known as feature engineering. Feature engineering is slow, error prone and leads to suboptimal models. Here we introduce an end-to-end deep representation learning approach to directly learn on data laid out across multiple tables. We name our approach Relational Deep Learning (RDL). The core idea is to view relational databases as a temporal, heterogeneous graph, with a node for each row in each table, and edges specified by primary-foreign key links. Message Passing Graph Neural Networks can then automatically learn across the graph to extract representations that leverage all input data, without any manual feature engineering. Relational Deep Learning leads to more accurate models that can be built much faster. To facilitate research in this area, we develop RelBench, a set of benchmark datasets and an implementation of Relational Deep Learning. The data covers a wide spectrum, from discussions on Stack Exchange to book reviews on the Amazon Product Catalog. Overall, we define a new research area that generalizes graph machine learning and broadens its applicability to a wide set of AI use cases.",
    "url": "http://arxiv.org/abs/2312.04615v1",
    "timestamp": 1701975101,
    "sections": {
      "1 Introduction": "\n\n1 Introduction\n\nThe information age is driven by data stored in ever-growing relational databases and data warehouses\nthat have come to underpin nearly all technology stacks. Data warehouses typically store information in multiple tables, with entities/rows in different tables connected using primary-foreign key relations, and managed using powerful query languages such as SQL (Codd, 1970; Chamberlin and Boyce, 1974). For this reason, data warehouses underpin many of today\u2019s large information systems, including e-commerce, social media, banking systems, healthcare, manufacturing, and open-source scientific knowledge repositories\u00a0(Johnson et\u00a0al., 2016; PubMed, 1996).\n\n\nMany predictive problems over relational data have significant implications for human decision making. A hospital wants to predict the risk of discharging a patient; an e-commerce company wishes to forecast future sales of each of their products; a telecommunications provider wants to predict which customers will churn; and a music streaming platform must decide which songs to recommend to a user. Behind each of these tasks is a rich relational schema of relational tables, and many machine learning models are built using this data\u00a0(Kaggle, 2022).\n\n\nHowever, existing learning paradigms, notably tabular learning, cannot be directly applied to an interlinked set relational tables.\nInstead, a manual feature engineering step is first taken, where a data scientist uses domain knowledge to manually join and aggregate tables to generate many features in a regular single table format.\nTo illustrate this, consider a simple e-commerce schema (Fig. 1) of three tables: Customers, Transactions and Products, where Customers and Products tables link into the Transactions table via primary-foreign keys, and the task is to predict if a customer is going to churn (i.e., make zero transactions in the next k\ud835\udc58kitalic_k days). In this case, the data scientist would aggregate information from the Transactions table to make new features for the Customers table such as: \u201cnumber of purchases of a given customer in the last 30 days\u201d, \u201csum of purchase amounts of a given customer in the last 30 days\u201d, \u201cnumber of purchases on a Sunday\u201d, \u201csum of purchase amounts on a Sunday\u201d, \u201cnumber of purchases on a Monday\u201d, and so on. The computed customer features are then stored in a single table, ready for tabular machine learning. Another challenge is the temporal nature of the churn predictive tasks. As new transactions appear, the customer\u2019s churn label and the customer\u2019s features may change from day to day, so features need to be recomputed for each day. Overall, the temporal nature of relational databases adds computational cost and further complexity, which often results in bugs and information leakage including the so-called \u201ctime travel\u201d (Kapoor and Narayanan, 2023).\n\n(a) Relational Database(b) Define Tasks(c) Relational Deep Learning\nFigure 1: Relational Deep Learning solves predictive tasks on relational data with end-to-end learnable models. There are three main steps. (a) A relational database with multiple tables connected by primary-foreign keys is given. (b) A predictive task is specified and added to the database by introducing an additional training table. (c) Relational data is transformed into its Relational Entity Graph, and a Graph Neural Network is trained over the graph with the supervision provided by the training table. The predictive task can be node level (as in this illustration), link level (pairs of nodes), or higher-order.\n\n\nThere are several issues with with the above approach: (1) it is a manual, slow and labor intensive process; (2) feature choices are essentially arbitrary and likely highly-suboptimal;\n(3) only a small fraction of the overall space of possible features can be manually explored; (4) by forcing data into a single table, information is aggregated into lower-granularity features, thus losing out on valuable fine-grain signal; (5) whenever the data distribution changes or drifts, current features become obsolete and new features have to be manually reinvented.\n\n\n\nMany domains have been in a similar position, including pre-deep-learning computer vision, where hand-chosen convolutional filters (e.g., Gabor) were used to extract features, followed by models such as SVMs or nearest neighbor search (Varma and Zisserman, 2005). Today, in contrast, deep neural networks skip the feature engineering and learn directly on the raw pixels, which results in large gains in model accuracy.\nMore broadly, the deep learning revolution has had a huge impact in many fields, including computer vision\u00a0(He et\u00a0al., 2016; Russakovsky et\u00a0al., 2015), natural language processing\u00a0(Vaswani et\u00a0al., 2017; Devlin et\u00a0al., 2018; Brown et\u00a0al., 2020), and speech\u00a0(Hannun et\u00a0al., 2014; Amodei et\u00a0al., 2016), and has led to super-human performance in many tasks. In all cases, the key was to move from manual feature engineering and handcrafted systems to fully data-driven, end-to-end representation learning systems.\nFor relational data, this transition has not yet occurred, as existing tabular deep learning approaches still heavily rely on manual feature engineering.\nConsequently, there remains a huge unexplored opportunity.\n\n\nHere we introduce Relational Deep Learning (RDL), a blueprint for fulfilling the need for an end-to-end deep learning paradigm for relational tables (Fig. 1).\nThrough end-to-end representation learning, Relational Deep Learning fully utilizes the rich predictive signals available in relational tables. The core of RDL is to represent relational tables as a temporal, heterogeneous Relational Entity Graph, where each row defines a node, columns define node features, and primary-foreign key links define edges. Graph Neural Networks (GNNs)\u00a0(Gilmer et\u00a0al., 2017; Hamilton et\u00a0al., 2017) can then be applied to build end-to-end data-driven predictive models.\n\n\nPredictive tasks are specified on relational data by introducing a training table that holds supervision label information, (Fig. 1b) but no input features.\nTraining tables have two critically important characteristics. First, labels can be automatically computed from historical relational data, without any need for outside annotation; second, they may contain any number of foreign keys, permitting many task types including entity level (1 key, as in Fig. 1b), link-level tasks such as recommendation (2 keys) and multi-entity tasks (>>>2 keys). Training tables permit many different types of prediction targets, including multi-class, multi-label, regression and more, ensuring high task generality.\n\n\nAll in all, RDL model pipeline has four main steps (Fig.\u00a02): Given a predictive machine learning task, (1) A training table containing supervision labels is constructed in a task-specific manner based on historic data in the relational database, (2) entity-level features are extracted and encoded from each row in each table to serve as node features, (3) node representations are learned through an inter-entity message-passing GNN that exchanges information between entities with primary-foreign key links, (4) a task-specific model head produces predictions for training data, and errors are backpropogated through the network.\n\n(a) Rel. Tables with Training Table(b) Entities Linked by Foreign Keys(c) Relational Entity Graph(d) Graph Neural Network\nFigure 2: Relational Deep Learning Pipeline. (a) Given relational tables and a predictive task, a training table, containing supervised label information, is constructed and attached to the entity table(s). (b) Relational tables contain individual entities that are linked by foreign-primary key relations. (c) Relational data can be viewed as a single Relational Entity graph, which has a node for each entity, and edges given by primary-foreign key links. (d) Initial node features are extracted from each row in each table using modality-specific neural networks. Then a message passing graph neural network computes relation-aware node embeddings, a model head produces predictions for training table entities, and errors are backpropogated.\n\n\n\nCrucially, RDL models natively integrate temporality by only allowing entities to receive messages from other entities with earlier timestamps. This ensures that learned representation is automatically updated during GNN forward pass when new data is collected, and prevents information leakage and time travel bugs. Furthermore, this also stabilizes the generalization across time since models are trained to make predictions at multiple time snapshots by dynamically passing messages between entities at different time snapshots, whilst remaining grounded in a single relational database.\n\n\n\nRelBench.\n\nTo facilitate research into Relational Deep Learning, we introduce RelBench, a benchmarking and an evaluation Python package. Data in RelBench cover rich relational databases from many different domains. RelBench has the following key modules (1) Data: data loading, specifying a predictive task, and (temporal) data splitting, (2) Model: transforming data to a heterogeneous graph, building graph neural network predictive models, (3) Evaluation: standardized evaluation protocol given a file of predictions. Importantly, data and evaluation modules are deep learning framework agnostic, enabling broad compatibility. To facilitate research, we provide our initial model implementation based on PyTorch Frame\u00a0(Hu et\u00a0al., 2023) for encoding table rows into input node embeddings, which is then processed by GNN models in PyTorch Geometric\u00a0(Fey and Lenssen, 2019) to update the embeddings via message passing over the relational entity graph.\n\n\nFor the initial release, RelBench contains two databases, each with two predictive tasks. The first database is from Stack Exchange, the question-answering website, and includes 7 tables such as posts, users, and votes. The predictive tasks are (1) to predict if a user is going to make a new contribution (post, answer etc.), and (2) to predict the popularity of a new question. The second database is a subset of the Amazon Product Catalog focusing on books. There are three tables: users, products, and reviews. The tasks are (1) to predict the lifetime value of a user, and (2) whether a user will stop using the site.\n\n\nOur objective is to establish deep learning on relational data as a new subfield of machine learning. We hope that this will be a fruitful research direction, with many opportunities for impactful ideas that make much better use of the rich predictive signal in relational data. This paper lays the ground for future work by making the following main sections:\n\n\n\u2022\n\nBlueprint. Relational Deep Learning, an end-to-end learnable approach that ultilizes the predictive signals available in relational data, and supports temporal predictions.\n\n\n\n\u2022\n\nBenchmarking Package RelBench, an open-source Python package for benchmarking and evaluating GNNs on relational data. RelBench beta release introduces two relational databases, and specifies two prediction tasks for each.\n\n\n\n\u2022\n\nResearch Opportunities. Outlining a new research program for Relational Deep Learning, including multi-task learning, new GNN architectures, multi-hop learning, and more.\n\n\n\n\n\n\nOrganization.\n\nSection 2 provides background on relational tables and predictive task specification.\nSection 3 introduces our central methodological contribution, a graph neural network approach to solving predictive tasks on relational data.\nSection 4 introduces RelBench, a new benchmark for relational tables, and standardized evaluation protocols.\nSection 5 outlines a landscape of new research opportunities for graph machine learning on relational data. Finally, Section 6 concludes by contextualizing our new framework within the tabular and graph machine learning literature.\n\n\n",
      "2 Predictive Tasks on Relational Databases": "\n\n2 Predictive Tasks on Relational Databases\n\nThis section outlines our problem scope: predictive tasks on relational tables. In the process, we define what we mean by relational tables, and how to specify predictive tasks on them. This section focuses exclusively on the structure of data and tasks, laying the groundwork for Section 3, which presents our GNN-based modelling approach.\n\n\n\n2.1 Relational Data\n\nA Brief History.\n\nRelational tables and relational databases emerged in the 1970s as a means to standardize data retrieval and management (Codd, 1970). As society digitized, relational databases came to fulfill a foundational purpose, and today are estimated to comprise 72% of the world\u2019s data\u00a0(DB-Engines, 2023). Whilst there is no single agreed upon definition of a relational database, three essential characteristics are shared in all cases (cf. Figure 1a):\n\n\n1.\n\nData is stored in multiple tables.\n\n\n\n2.\n\nEach row in each table contains an entity, which possesses a unique primary key ID, along with multiple attributes stored as columns of the table.\n\n\n\n3.\n\nOne entity may refer to another entity using a foreign key\u2014the primary key of another entity.\n\n\n\n\n\nAs well as a standardized storage system, relational databases typically come equipped with a powerful set of relational operations, which are used to manipulate and access data. Codd (1970) introduced 8 relational operations, including set operations such as taking the union of two tables, and other operations such as joining two tables based on their common attributes. Popular query languages such as SQL (Chamberlin and Boyce, 1974) provide commercial-grade implementations of a wide variety of relational operations.\nNext we formally define relational data, as suits our purposes.\n\n\n\nDefinition of Relational Databases.\n\nA relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) is comprised of a collection of tables \ud835\udcaf={T1,\u2026,Tn}\ud835\udcafsubscript\ud835\udc471\u2026subscript\ud835\udc47\ud835\udc5b\\mathcal{T}=\\{T_{1},\\ldots,T_{n}\\}caligraphic_T = { italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }, and links between tables \u2112\u2286\ud835\udcaf\u00d7\ud835\udcaf\u2112\ud835\udcaf\ud835\udcaf\\mathcal{L}\\subseteq\\mathcal{T}\\times\\mathcal{T}caligraphic_L \u2286 caligraphic_T \u00d7 caligraphic_T (cf. Figure 2a). A link L=(TfkeyL=(T_{\\rm fkey}italic_L = ( italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT, Tpkey)T_{\\rm pkey})italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT ) between tables exists if a foreign key column in Tfkeysubscript\ud835\udc47fkeyT_{\\rm fkey}italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT points to a primary key column of Tpkeysubscript\ud835\udc47pkeyT_{\\rm pkey}italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT.\nEach table is a set T={v1,\u2026,vnT}\ud835\udc47subscript\ud835\udc631\u2026subscript\ud835\udc63subscript\ud835\udc5b\ud835\udc47T=\\{v_{1},...,v_{n_{T}}\\}italic_T = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_v start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT }, whose elements vi\u2208Tsubscript\ud835\udc63\ud835\udc56\ud835\udc47v_{i}\\in Titalic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2208 italic_T are called rows, or entities (cf. Figure 2b). Each entity v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T, has four constituent parts v=(pv,\ud835\udca6v,xv,tv)\ud835\udc63subscript\ud835\udc5d\ud835\udc63subscript\ud835\udca6\ud835\udc63subscript\ud835\udc65\ud835\udc63subscript\ud835\udc61\ud835\udc63v=(p_{v},\\mathcal{K}_{v},x_{v},t_{v})italic_v = ( italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ):\n\n\n1.\n\nPrimary key pvsubscript\ud835\udc5d\ud835\udc63p_{v}italic_p start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, that uniquely identifies the entity v\ud835\udc63vitalic_v.\n\n\n\n2.\n\nForeign keys \ud835\udca6v\u2286{pv\u2032:v\u2032\u2208T\u2032\u2062\u00a0and\u00a0\u2062(T,T\u2032)\u2208\u2112}subscript\ud835\udca6\ud835\udc63conditional-setsubscript\ud835\udc5dsuperscript\ud835\udc63\u2032superscript\ud835\udc63\u2032superscript\ud835\udc47\u2032\u00a0and\u00a0\ud835\udc47superscript\ud835\udc47\u2032\u2112\\mathcal{K}_{v}\\subseteq\\{p_{v^{\\prime}}:v^{\\prime}\\in T^{\\prime}\\text{ and }(%\nT,T^{\\prime})\\in\\mathcal{L}\\}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2286 { italic_p start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT : italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT and ( italic_T , italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT ) \u2208 caligraphic_L }, defining links between element v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T to elements v\u2032\u2208T\u2032superscript\ud835\udc63\u2032superscript\ud835\udc47\u2032v^{\\prime}\\in T^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT \u2208 italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT, where pv\u2032subscript\ud835\udc5dsuperscript\ud835\udc63\u2032p_{v^{\\prime}}italic_p start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT is the primary key of an entity v\u2032superscript\ud835\udc63\u2032v^{\\prime}italic_v start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT in table T\u2032superscript\ud835\udc47\u2032T^{\\prime}italic_T start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.\n\n\n\n3.\n\nAttributes xvsubscript\ud835\udc65\ud835\udc63x_{v}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, holding the informational content of the entity. \n\n\n\n4.\n\nTimestamp An optional timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, indicating the time an event occurred.\n\n\n\n\n\nFor example, the Transactions table in Figure 2a has the primary key (TransactionID), two foreign keys (ProductID and CustomerID), one attribute (Price), and timestamp column (Timestamp). Similarly, the Products table has the primary key (ProductID), no foreign keys, attributes (Description, Image and Size), and no timestamp. The connection between foreign keys and primary keys is illustrated by black connecting lines in Figure 2.\n\n\nIn general, the attributes in table T\ud835\udc47Titalic_T contain dTsubscript\ud835\udc51\ud835\udc47d_{T}italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT values: xv=(xv1,\u2026,xvdT)subscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ), each belonging to a particular column. Critically, all entities in the same table have the same columns (values may be absent). Formally, this is described by membership xv=(xv1,\u2026,xvdT)\u2208\ud835\udc9cT1\u00d7\u2026\u00d7\ud835\udc9cTdTsubscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47superscriptsubscript\ud835\udc9c\ud835\udc471\u2026superscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})\\in\\mathcal{A}_{T}^{1}\\times\\ldots\\times%\n\\mathcal{A}_{T}^{d_{T}}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) \u2208 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u00d7 \u2026 \u00d7 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, where \ud835\udc9cTisubscriptsuperscript\ud835\udc9c\ud835\udc56\ud835\udc47\\mathcal{A}^{i}_{T}caligraphic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT denotes the value space of i\ud835\udc56iitalic_i-th column of table T\ud835\udc47Titalic_T, and is shared between all entities v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T.\nFor example, the Products table from Fig.\u00a02a contains three different attributes: the product description (text type), the image of the product (image type), and the size of the product (numerical type). Each of these types has their own encoders as discussed in Sec.\u00a03.4.3.\n\n\n\nFact and Dimension Tables.\n\nTables are categorized into two types, fact or dimension, with complementary roles (Garcia-Molina et\u00a0al., 2008). Dimension tables provide contextual information, such as biographical information, macro statistics (such as number of beds in a hospital), or immutable properties, such as the size of a product (as in the Products table in Figure 2a). Dimension tables tend to have relatively few rows, as it is limited to one per real-world object. Fact tables record interactions between other entities, such as all patient admissions to hospital, or all customer transactions (as in the Transactions table in Figure 2a). Since entities can interact repeatedly, fact tables often contain the majority of rows in a relational database. Typically, features in dimension tables are static over their whole lifetime, while fact tables usually contain temporal information with a dedicated time column that denotes the time of appearance.\n\n\n\nTemporality as a First-Class Citizen.\n\nRelational data evolves over time as events occur and are recorded.\nThis is captured by the (optional) timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT attached to each entity v\ud835\udc63vitalic_v. For example, each transaction in Transactions has a time stamp.\nFurthermore, many tasks of interest involve forecasting future events. For example, how much will a customer spend in next k\ud835\udc58kitalic_k days. It is therefore essential that time is conferred a special status unlike other attributes. Our formulation, introduced in Section 3, achieves this through a temporal message passing scheme (similar to Rossi et\u00a0al. (2020)), that only permits nodes to receive messages from neighbors with earlier timestamps. This ensures that models do not leak information from the future during training, avoiding shortcut decision rules that achieve high training accuracy but fail at test time (Geirhos et\u00a0al., 2020). It also means that model-extracted features are automatically updated as new relational data is added.\n\n\n\n\n\n2.2 From Task to Training Table\n\nThere are many practically interesting machine learning tasks defined over relational databases, such as predicting the response of a patient to treatment, or the future sales of a product.\nThese tasks involve predicting the future state of the entities of interest.\nGiven a task we wish to solve, how can we create ground truth labels to supervise machine learning models training?\n\n\nOur key insight is that we can generate training labels using historical data.\nFor instance, at time t\ud835\udc61titalic_t, ground truth labels for predicting \u201chow much each customer will buy in the next 90909090 days?\u201d are computed by summing up each customer\u2019s spending within the interval t\ud835\udc61titalic_t and t+90\ud835\udc6190t+90italic_t + 90 days. Importantly, as long as t+90\ud835\udc6190t+90italic_t + 90 is less than the most recent timestamp in the database, then these ground truth labels can be computed purely from historical data without any need for external annotation. Further, by choosing different time points t\ud835\udc61titalic_t across the database time horizon, it is possible to naturally compute many ground truth training labels for each entity.\n\n\nTo hold the labels for a new predictive task, we introduce a new table known as a training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT (Fig.\u00a03).\nEach entity v=(\ud835\udca6v,tv,yv)\ud835\udc63subscript\ud835\udca6\ud835\udc63subscript\ud835\udc61\ud835\udc63subscript\ud835\udc66\ud835\udc63v=(\\mathcal{K}_{v},t_{v},y_{v})italic_v = ( caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) in the training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT has three components: (1) A (set of) foreign keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT indicating the entities the training example is associated to, (2) a timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, and (3) the ground truth label itself yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. In contrast to tabular learning settings, the training table does not contain input data xvsubscript\ud835\udc65\ud835\udc63x_{v}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. The training table is linked to the main relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) by updating: (1) the set of tables to \ud835\udcaf\u222a{Ttrain}\ud835\udcafsubscript\ud835\udc47train\\mathcal{T}\\cup\\{{T_{\\text{train}}}\\}caligraphic_T \u222a { italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT }, and (2) the links between tables to \u2112\u222a\u2112Ttrain\u2112subscript\u2112subscript\ud835\udc47train\\mathcal{L}\\cup\\mathcal{L}_{T_{\\text{train}}}caligraphic_L \u222a caligraphic_L start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT end_POSTSUBSCRIPT, where \u2112Ttrainsubscript\u2112subscript\ud835\udc47train\\mathcal{L}_{T_{\\text{train}}}caligraphic_L start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT end_POSTSUBSCRIPT specifies tables that training table keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT point to.\n\n\nAs discussed in Sec. 2.1, careful handling of what data the model sees during training is crucial in order to ensure temporal leakage does not happen. This is achieved using the training timestamp. When the model is trained to output target yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT for entity v\ud835\udc63vitalic_v with timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, temporal consistency is ensured by only permitting the model to receive input information from entities u\ud835\udc62uitalic_u with timestamp tu\u2264tvsubscript\ud835\udc61\ud835\udc62subscript\ud835\udc61\ud835\udc63t_{u}\\leq t_{v}italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT \u2264 italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (see Sec. 3.3 for details on training sampling).\n\n(a) Define Tasks(b) Training Table Generation(c) Link to Relational Tables\nFigure 3: Predictive Task Definition. A task over relational data is defined by attaching an additional training table to the existing linked tables. A training table entity specifies (a) ground truth label computed from historical information (b) the entity ID(s) the labels correspond to, and (c) a timestamp that controls what data the model can use to predict this label.\n\n\nThus, the purpose of the training table is twofold: to specify training inputs and outputs of the machine learning model. First, it provides supervision on the model output by specifying the the entities and their target training labels. Second, in the case of temporal tasks, the training table specifies the model input by specifying the timestamp at which each historical training label is generated.\n\n\nThis training table formulation can model a wide range of predictive tasks on relational databases:\n\n\n\u2022\n\nNode-level prediction tasks (e.g., multi-class classification, multi-label classification, regression): The training table has three columns (EntityID, Label, Time), indicating the foreign key, target label, and timestamp columns, respectively.\n\n\n\n\u2022\n\nLink prediction tasks: The training table has columns (SourceEntityID, TargetEntityID, Label, Time), indicating the foreign key columns for the source/target nodes, and the target label, and timestamp, respectively.\n\n\n\n\u2022\n\nTemporal and static prediction tasks: Temporal tasks make predictions about the future (and require a seed time), while non-temporal tasks impute missing values (Time is dropped).\n\n\n\n\n\nTraining Table Generation.\n\nIn practice, training tables can be computed using time-conditioned SQL queries from historic data in the database. Given a query that describes the prediction targets for all prediction entities, e.g. the sum of sells grouped by products, from time t\ud835\udc61titalic_t to time t+\u03b4\ud835\udc61\ud835\udefft+\\deltaitalic_t + italic_\u03b4 in the future, we can move t\ud835\udc61titalic_t back in time in fixed intervals to gather historical training, validation and test targets for all entities (cf. Fig.\u00a03b). We store t\ud835\udc61titalic_t as timestamp for the targets gathered in each step.\n\n\n\n",
      "3 Predictive Tasks as Graph Representation Learning Problems": "\n\n3 Predictive Tasks as Graph Representation Learning Problems\n\nHere, we formulate a generic machine learning architecture based on Graph Neural Networks, which solves predictive tasks on relational databases.\nThe following section will first introduce three important graph concepts, which are outlined in Fig.\u00a04: (a) The schema graph (cf.\u00a0Sec.\u00a03.1), table-level graph, where one table corresponds to one node. (b) The relational entity graph (cf. Sec.\u00a03.2), an entity-level graph, with a node for each entity in each table, and edges are defined via foreign-primary key connections between entities. (c) The time-consistent computation graph (cf.\u00a0Sec.\u00a03.3), which acts as an explicit training example for graph neural networks.\nWe describe generic procedures to map between graph types, and finally introduce our GNN blueprint for end-to-end learning on relational databases (cf.\u00a0Sec.\u00a03.4).\n\n\n\n\n\n(a) Schema Graph\n\n\n\n\n(b) Relational Entity Graph\n\n\n\n\n(c) Computation Graphs for different time t\ud835\udc61titalic_t\n\n\n\n\nFigure 4: Three different kinds of graphs. (a) The schema graph arises from the given relational tables. Each node denotes a table, and an edge between tables indicates that primary keys in one are foreign keys in the other. (b) The entity graph has one node for each entity in each table, and edges given by primary-foreign key links. The entity graph is heterogeneous with node and edge types defined by the schema graph. The nodes have a timestamp (illustrated by arrow-of-time), originating from the timestamp column of the table.\n(c) Using a temporal sampling strategy and a task description in form of training table containing different time s\ud835\udc60sitalic_s, we obtain time-consistent computation graphs as training examples that naturally respect temporal order and map well to parallel compute.\n\n\n\n\n3.1 Schema Graph\n\nThe first graph in our blueprint is the schema graph (cf. Fig. 4a), which describes the table-level structure of data.\nGiven a relational database (\ud835\udcaf,\u2112)\ud835\udcaf\u2112(\\mathcal{T},\\mathcal{L})( caligraphic_T , caligraphic_L ) as defined in Sec.\u00a02, we let \u2112\u22121={(Tpkey,Tfkey)\u2223(Tfkey,Tpkey)\u2208\u2112}superscript\u21121conditional-setsubscript\ud835\udc47pkeysubscript\ud835\udc47fkeysubscript\ud835\udc47fkeysubscript\ud835\udc47pkey\u2112\\mathcal{L}^{-1}=\\{(T_{\\rm pkey},T_{\\rm fkey})\\mid(T_{\\rm fkey},T_{\\rm pkey})%\n\\in\\mathcal{L}\\}caligraphic_L start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = { ( italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT ) \u2223 ( italic_T start_POSTSUBSCRIPT roman_fkey end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT roman_pkey end_POSTSUBSCRIPT ) \u2208 caligraphic_L } denote its inverse set of links.\nThen, the schema graph is the graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) that arises from the relational database, with node set \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T and edge set \u211b=\u2112\u222a\u2112\u22121\u211b\u2112superscript\u21121\\mathcal{R}=\\mathcal{L}\\cup\\mathcal{L}^{-1}caligraphic_R = caligraphic_L \u222a caligraphic_L start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT.\nInverse links ensure that all tables are reachable within the schema graph.\nThe schema graph nodes serve as type definitions for the heterogeneous relational entity graph, which we define next.\n\n\n\n\n3.2 Relational Entity Graph\n\nTo formulate a graph suitable for processing with GNNs, we introduce the relational entity graph, which has entity-level nodes and serves as the basis of the proposed relational learning framework.\n\n\nOur relational entity graph is a heterogeneous graph G=(\ud835\udcb1,\u2130,\u03d5,\u03c8)\ud835\udc3a\ud835\udcb1\u2130italic-\u03d5\ud835\udf13G=(\\mathcal{V},\\mathcal{E},\\phi,\\psi)italic_G = ( caligraphic_V , caligraphic_E , italic_\u03d5 , italic_\u03c8 ), with node set \ud835\udcb1\ud835\udcb1\\mathcal{V}caligraphic_V and edge set \u2130\u2286\ud835\udcb1\u00d7\ud835\udcb1\u2130\ud835\udcb1\ud835\udcb1\\mathcal{E}\\subseteq\\mathcal{V}\\times\\mathcal{V}caligraphic_E \u2286 caligraphic_V \u00d7 caligraphic_V and type mapping functions \u03d5:\ud835\udcb1\u2192\ud835\udcaf:italic-\u03d5\u2192\ud835\udcb1\ud835\udcaf\\phi:\\mathcal{V}\\rightarrow\\mathcal{T}italic_\u03d5 : caligraphic_V \u2192 caligraphic_T and \u03c8:\u2130\u2192\u211b:\ud835\udf13\u2192\u2130\u211b\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}italic_\u03c8 : caligraphic_E \u2192 caligraphic_R, where each node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V belongs to a node type \u03d5\u2062(v)\u2208\ud835\udcafitalic-\u03d5\ud835\udc63\ud835\udcaf\\phi(v)\\in\\mathcal{T}italic_\u03d5 ( italic_v ) \u2208 caligraphic_T and each edge e\u2208\u2130\ud835\udc52\u2130e\\in\\mathcal{E}italic_e \u2208 caligraphic_E belongs to an edge type \u03c8\u2062(e)\u2208\u211b\ud835\udf13\ud835\udc52\u211b\\psi(e)\\in\\mathcal{R}italic_\u03c8 ( italic_e ) \u2208 caligraphic_R.\nSpecifically, the sets \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T and \u211b\u211b\\mathcal{R}caligraphic_R from the schema graph define the node and edge types of our relational entity graph.\n\n\nGiven a schema graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) with tables T={v1,\u2026,vnT}\u2208\ud835\udcaf\ud835\udc47subscript\ud835\udc631\u2026subscript\ud835\udc63subscript\ud835\udc5b\ud835\udc47\ud835\udcafT=\\{v_{1},...,v_{n_{T}}\\}\\in\\mathcal{T}italic_T = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_v start_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT } \u2208 caligraphic_T as defined in Sec.\u00a02, we define the node set in our relational entity graph as the union of all entries in all tables \ud835\udcb1=\u22c3T\u2208\ud835\udcafT\ud835\udcb1subscript\ud835\udc47\ud835\udcaf\ud835\udc47\\mathcal{V}=\\bigcup_{T\\in\\mathcal{T}}Tcaligraphic_V = \u22c3 start_POSTSUBSCRIPT italic_T \u2208 caligraphic_T end_POSTSUBSCRIPT italic_T.\nIts edge set is then defined as\n\n\n\n\u2130={(v1,v2)\u2208\ud835\udcb1\u00d7\ud835\udcb1\u2223pv2\u2208\ud835\udca6v1\u2062\u00a0or\u00a0\u2062pv1\u2208\ud835\udca6vv}\u2062,\u2130conditional-setsubscript\ud835\udc631subscript\ud835\udc632\ud835\udcb1\ud835\udcb1subscript\ud835\udc5dsubscript\ud835\udc632subscript\ud835\udca6subscript\ud835\udc631\u00a0or\u00a0subscript\ud835\udc5dsubscript\ud835\udc631subscript\ud835\udca6subscript\ud835\udc63\ud835\udc63,\\mathcal{E}=\\{(v_{1},v_{2})\\in\\mathcal{V}\\times\\mathcal{V}\\mid p_{v_{2}}\\in%\n\\mathcal{K}_{v_{1}}\\text{ or }p_{v_{1}}\\in\\mathcal{K}_{v_{v}}\\}\\textnormal{,}caligraphic_E = { ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2208 caligraphic_V \u00d7 caligraphic_V \u2223 italic_p start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2208 caligraphic_K start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT or italic_p start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT \u2208 caligraphic_K start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT } ,\n\n(1)\n\n\ni.e. the entity-level pairs that arise from the primary-foreign key relationships in the database.\nWe equip the relational entity graph with the following key information:\n\n\n\u2022\n\nType mapping functions \u03d5:\ud835\udcb1\u2192\ud835\udcaf:italic-\u03d5\u2192\ud835\udcb1\ud835\udcaf\\phi:\\mathcal{V}\\rightarrow\\mathcal{T}italic_\u03d5 : caligraphic_V \u2192 caligraphic_T and \u03c8:\u2130\u2192\u211b:\ud835\udf13\u2192\u2130\u211b\\psi:\\mathcal{E}\\rightarrow\\mathcal{R}italic_\u03c8 : caligraphic_E \u2192 caligraphic_R, mapping nodes and edges to respective elements of the schema graph, making the graph heterogeneous. We set \u03d5\u2062(v)=Titalic-\u03d5\ud835\udc63\ud835\udc47\\phi(v)=Titalic_\u03d5 ( italic_v ) = italic_T for all v\u2208T\ud835\udc63\ud835\udc47v\\in Titalic_v \u2208 italic_T and \u03c8\u2062(v1,v2)=(\u03d5\u2062(v1),\u03d5\u2062(v2))\u2208\u211b\ud835\udf13subscript\ud835\udc631subscript\ud835\udc632italic-\u03d5subscript\ud835\udc631italic-\u03d5subscript\ud835\udc632\u211b\\psi(v_{1},v_{2})=(\\phi(v_{1}),\\phi(v_{2}))\\in\\mathcal{R}italic_\u03c8 ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( italic_\u03d5 ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_\u03d5 ( italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ) \u2208 caligraphic_R if (v1,v2)\u2208\u2130subscript\ud835\udc631subscript\ud835\udc632\u2130(v_{1},v_{2})\\in\\mathcal{E}( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) \u2208 caligraphic_E.\n\n\n\n\u2022\n\nTime mapping function \u03c4:\ud835\udcb1\u2192\ud835\udc9f:\ud835\udf0f\u2192\ud835\udcb1\ud835\udc9f\\tau:\\mathcal{V}\\rightarrow\\mathcal{D}italic_\u03c4 : caligraphic_V \u2192 caligraphic_D, mapping nodes to its timestamp: \u03c4:v\u21a6tv:\ud835\udf0fmaps-to\ud835\udc63subscript\ud835\udc61\ud835\udc63\\tau:v\\mapsto t_{v}italic_\u03c4 : italic_v \u21a6 italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT (as defined in Sec.\u00a02.1), introducing time as a central component and establishes the temporality of the graph. The value \u03c4\u2062(v)\ud835\udf0f\ud835\udc63\\tau(v)italic_\u03c4 ( italic_v ) denotes the point in time in which the table row v\ud835\udc63vitalic_v became available or \u2212\u221e-\\infty- \u221e in case of non-temporal rows.\n\n\n\n\u2022\n\nEmbedding vectors \ud835\udc21v\u2208\u211dd\u03d5\u2062(v)subscript\ud835\udc21\ud835\udc63superscript\u211dsubscript\ud835\udc51italic-\u03d5\ud835\udc63\\mathbf{h}_{v}\\in\\mathbb{R}^{d_{\\phi(v)}}bold_h start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT end_POSTSUPERSCRIPT for each v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V, which contains an embedding vector for each node in the graph. Initial embeddings are obtained via multi-modal column encoders as described in Sec.\u00a03.4.3. Final embeddings are computed via GNNs outlined in Section 3.4.\n\n\n\n\n\nAn example of a relational entity graph for a given schema graph is given in Fig.\u00a03(b).\nThe graph contains a node for each row in the database tables. Two nodes are connected if the foreign key entry in one table row links to the primary key entry of another table row. Node and edge types are defined by the schema graph. Nodes resulting from temporal tables carry the timestamp from the respective row, allowing temporal message passing, which is described next.\n\n\n\n\n3.3 Time-Consistent Computational Graphs\n\nGiven a relational entity graph and a training table (cf.\u00a0Sec.\u00a02.2), we need to be able to query the graph at specific points in time which then serve as explicit training examples used as input to the model.\nIn particular, we create a subgraph from the relational entity graph induced by the set of foreign keys \ud835\udca6vsubscript\ud835\udca6\ud835\udc63\\mathcal{K}_{v}caligraphic_K start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and its timestamp tvsubscript\ud835\udc61\ud835\udc63t_{v}italic_t start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT of a training example in the training table Ttrainsubscript\ud835\udc47trainT_{\\text{train}}italic_T start_POSTSUBSCRIPT train end_POSTSUBSCRIPT.\nThis subgraph then acts as a local and time-consistent computation graph to predict its ground-truth label yvsubscript\ud835\udc66\ud835\udc63y_{v}italic_y start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT.\n\n\nAlgorithm 1  Time-Consistent Computation Graph\n\nRelational entity graph G=(\ud835\udcb1,\u2130)\ud835\udc3a\ud835\udcb1\u2130G=(\\mathcal{V},\\mathcal{E})italic_G = ( caligraphic_V , caligraphic_E ), number of hops L\ud835\udc3fLitalic_L, seed node v0\u2208\ud835\udcb1subscript\ud835\udc630\ud835\udcb1v_{0}\\in\\mathcal{V}italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 caligraphic_V, seed time t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R\n\nNeighborhood sizes (m1,\u2026,mL)\u2208\u2115Lsubscript\ud835\udc5a1\u2026subscript\ud835\udc5a\ud835\udc3fsuperscript\u2115\ud835\udc3f(m_{1},...,m_{L})\\in\\mathbb{N}^{L}( italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_m start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) \u2208 blackboard_N start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT\n\nComputation graph Gcomp=(\ud835\udcb1comp,\u2130comp)subscript\ud835\udc3acompsubscript\ud835\udcb1compsubscript\u2130compG_{\\rm comp}=(\\mathcal{V}_{\\rm comp},\\mathcal{E}_{\\rm comp})italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT = ( caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT , caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT )\n\n\n\ud835\udcb10\u2190{v0}\u2190subscript\ud835\udcb10subscript\ud835\udc630\\mathcal{V}_{0}\\leftarrow\\{v_{0}\\}caligraphic_V start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 { italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT }, \u2003\u21300\u2190\u2205\u2190subscript\u21300\\mathcal{E}_{0}\\leftarrow\\emptysetcaligraphic_E start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2190 \u2205\n\n\nfor\u00a0i\u2208{1,\u2026,L}\ud835\udc561\u2026\ud835\udc3fi\\in\\{1,...,L\\}italic_i \u2208 { 1 , \u2026 , italic_L }\u00a0do\n\n\u00a0\u00a0\u00a0\u00a0\u00a0for\u00a0v\u2208\ud835\udcb1i\u22121\ud835\udc63subscript\ud835\udcb1\ud835\udc561v\\in\\mathcal{V}_{i-1}italic_v \u2208 caligraphic_V start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT\u00a0do\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u2130i\u2190SELECTmi\u2062({(w,v)\u2208\u2130\u2223\u03c4\u2062(v)\u2264t})\u2190subscript\u2130\ud835\udc56subscriptSELECTsubscript\ud835\udc5a\ud835\udc56conditional-set\ud835\udc64\ud835\udc63\u2130\ud835\udf0f\ud835\udc63\ud835\udc61\\mathcal{E}_{i}\\leftarrow\\textnormal{SELECT}_{m_{i}}(\\{(w,v)\\in\\mathcal{E}\\mid%\n\\tau(v)\\leq t\\})caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 SELECT start_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( { ( italic_w , italic_v ) \u2208 caligraphic_E \u2223 italic_\u03c4 ( italic_v ) \u2264 italic_t } ) \u25b7\u25b7\\triangleright\u25b7 Select a maximum of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT filtered edges\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\ud835\udcb1i\u2190{w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130i}\u2190subscript\ud835\udcb1\ud835\udc56conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63subscript\u2130\ud835\udc56\\mathcal{V}_{i}\\leftarrow\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}_{i}\\}caligraphic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2190 { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } \u25b7\u25b7\\triangleright\u25b7 Gather nodes for the sampled edges\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0end\u00a0for\n\n\nend\u00a0for\n\n\n\ud835\udcb1comp\u2190\u22c3i=1L\ud835\udcb1i\u2190subscript\ud835\udcb1compsuperscriptsubscript\ud835\udc561\ud835\udc3fsubscript\ud835\udcb1\ud835\udc56\\mathcal{V}_{\\rm comp}\\leftarrow\\bigcup_{i=1}^{L}\\mathcal{V}_{i}caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT \u2190 \u22c3 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT caligraphic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, \u2003\u2130comp\u2190\u22c3i=1L\u2130i\u2190subscript\u2130compsuperscriptsubscript\ud835\udc561\ud835\udc3fsubscript\u2130\ud835\udc56\\mathcal{E}_{\\rm comp}\\leftarrow\\bigcup_{i=1}^{L}\\mathcal{E}_{i}caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT \u2190 \u22c3 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT caligraphic_E start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT\n\n\n\n\nThe computational graphs obtained via neighbor sampling\u00a0(Hamilton et\u00a0al., 2017) allow the scalability of our proposed approach to modern large-scale relational data with billions of table rows, while ensuring the temporal constraints\u00a0(Wang et\u00a0al., 2021).\nSpecifically, given a number of hops L\ud835\udc3fLitalic_L to sample, a seed node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V, and a timestamp t\ud835\udc61titalic_t induced by a training example, the computation graph is defined as Gcomp=(\ud835\udcb1comp,\u2130comp)subscript\ud835\udc3acompsubscript\ud835\udcb1compsubscript\u2130compG_{\\rm comp}=(\\mathcal{V}_{\\rm comp},\\mathcal{E}_{\\rm comp})italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT = ( caligraphic_V start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT , caligraphic_E start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT ) as the output of Alg.\u00a01.\nThe algorithm traverses the graph starting from the seed node v\ud835\udc63vitalic_v for L\ud835\udc3fLitalic_L iterations. In iteration i\ud835\udc56iitalic_i, it gathers a maximum of misubscript\ud835\udc5a\ud835\udc56m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT neighbors available up to timestamp t\ud835\udc61titalic_t, using one of three selection strategies:\n\n\n\u2022\n\nUniform temporal sampling selects uniformly sampled random neighbors.\n\n\n\n\u2022\n\nOrdered temporal sampling takes the latest neighbors, ordered by time \u03c4\ud835\udf0f\\tauitalic_\u03c4.\n\n\n\n\u2022\n\nBiased temporal sampling selects random neighbors sampled from a multinomial probability distribution induced by \u03c4\ud835\udf0f\\tauitalic_\u03c4. For instance, sampling can be performed proportional to relative neighbor time or biased towards specific important historical moments.\n\n\n\n\n\nThe temporal neighbor sampling is performed purely on the graph structure of the relational entity graph, without requiring initial embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. The bounded size of computation graph Gcompsubscript\ud835\udc3acompG_{\\rm comp}italic_G start_POSTSUBSCRIPT roman_comp end_POSTSUBSCRIPT allows for efficient mini-batching on GPUs, independent of relational entity graph size.\nIn practice, we perform temporal neighbor sampling on-the-fly, which allows us to operate on a shared relational entity graph across all training examples, from which we can then restore local and historical snapshots very efficiently.\nExamples of computation graphs are shown in Fig.\u00a03(c).\n\n\n\n\n3.4 Task-Specific Temporal Graph Neural Networks\n\nGiven a time-consistent computational graph and its future label to predict, we define a generic multi-stage deep learning architecture as follows:\n\n\n1.\n\nTable-level column encoders that encode table row data into initial node embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, as described in Sec.\u00a03.4.3.\n\n\n\n2.\n\nA stack of L\ud835\udc3fLitalic_L relational-temporal message passing layers (cf.\u00a0Sec.\u00a03.4.1).\n\n\n\n3.\n\nA task-specific model head, mapping final node embeddings to a prediction (cf.\u00a0Sec.\u00a03.4.2).\n\n\n\n\nThe whole architecture, consisting of table-level encoders, message passing layers and task specific model heads can be trained end-to-end to obtain an optimal model for the given task.\n\n\n\n3.4.1 Relational-Temporal Message Passing\n\nThis section introduces a generic framework for heterogeneous message passing GNNs on relational entity graphs as defined in Sec.\u00a03.2.\nA message passing operator in the given relational framework needs to respect the heterogeneous nature as well as the temporal properties of the graph. This is ensured by filtering nodes based on types and time. Thus, we briefly introduce heterogeneous message passing before we turn to our temporal message passing.\n\n\nHeterogeneous Message Passing.\n\nMessage-Passing Graph Neural Networks (MP-GNNs)\u00a0(Gilmer et\u00a0al., 2017; Fey and Lenssen, 2019) are a generic computational framework to define deep learning architectures on graph-structered data. Given a heterogeneous graph G=(\ud835\udcb1,\u2130,\u03d5,\u03c8)\ud835\udc3a\ud835\udcb1\u2130italic-\u03d5\ud835\udf13G=(\\mathcal{V},\\mathcal{E},\\phi,\\psi)italic_G = ( caligraphic_V , caligraphic_E , italic_\u03d5 , italic_\u03c8 ) with initial node embeddings {\ud835\udc21v(0)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc210\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(0)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT, a single message passing iteration computes updated features {\ud835\udc21v(i+1)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(i+1)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT from features {\ud835\udc21v(i)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(i)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT given by the previous iteration.\nOne iteration takes the form: \n\n\n\n\ud835\udc21v(i+1)=f\u2062(\ud835\udc21v(i),{{g\u2062(\ud835\udc21w(i))\u2223w\u2208\ud835\udca9\u2062(v)}})\u2062,subscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63\ud835\udc53subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63conditional-set\ud835\udc54subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc64\ud835\udc64\ud835\udca9\ud835\udc63,\\mathbf{h}^{(i+1)}_{v}=f(\\mathbf{h}^{(i)}_{v},\\{\\{g(\\mathbf{h}^{(i)}_{w})\\mid w%\n\\in\\mathcal{N}(v)\\}\\})\\textnormal{,}bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_f ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , { { italic_g ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) \u2223 italic_w \u2208 caligraphic_N ( italic_v ) } } ) ,\n\n(2)\n\n\nwhere f\ud835\udc53fitalic_f and g\ud835\udc54gitalic_g are arbitrary differentiable functions with optimizable parameters and {{\u22c5}}\u22c5\\{\\{\\cdot\\}\\}{ { \u22c5 } } an permutation invariant set aggregator, such as mean, max, sum, or a combination. Heterogeneous message passing\u00a0(Schlichtkrull et\u00a0al., 2018; Hu et\u00a0al., 2020) is a nested version of Eq.\u00a02, adding an aggregation over all incoming edge types to learn distinct message types:\n\n\n\n\ud835\udc21v(i+1)=f\u03d5\u2062(v)\u2062(\ud835\udc21v(i),{{fR\u2062({{gR\u2062(\ud835\udc21w(i))\u2223w\u2208\ud835\udca9R\u2062(v)}})|\u2200R=(T,\u03d5\u2062(v))\u2208\u211b}})\u2062,subscriptsuperscript\ud835\udc21\ud835\udc561\ud835\udc63subscript\ud835\udc53italic-\u03d5\ud835\udc63subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc63conditional-setsubscript\ud835\udc53\ud835\udc45conditional-setsubscript\ud835\udc54\ud835\udc45subscriptsuperscript\ud835\udc21\ud835\udc56\ud835\udc64\ud835\udc64subscript\ud835\udca9\ud835\udc45\ud835\udc63for-all\ud835\udc45\ud835\udc47italic-\u03d5\ud835\udc63\u211b,\\mathbf{h}^{(i+1)}_{v}=f_{\\phi(v)}\\Bigl{(}\\mathbf{h}^{(i)}_{v},\\Bigl{\\{}\\Bigl{%\n\\{}f_{R}(\\{\\{g_{R}(\\mathbf{h}^{(i)}_{w})\\mid w\\in\\mathcal{N}_{R}(v)\\}\\})\\,\\Big%\n{|}\\,\\forall R=(T,\\phi(v))\\in\\mathcal{R}\\Bigr{\\}}\\Bigr{\\}}\\Bigr{)}\\textnormal{,}bold_h start_POSTSUPERSCRIPT ( italic_i + 1 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT , { { italic_f start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( { { italic_g start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( bold_h start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT ) \u2223 italic_w \u2208 caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) } } ) | \u2200 italic_R = ( italic_T , italic_\u03d5 ( italic_v ) ) \u2208 caligraphic_R } } ) ,\n\n(3)\n\n\nwhere \ud835\udca9R\u2062(v)={w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130\u2062\u00a0and\u00a0\u2062\u03c8\u2062(w,v)=R}subscript\ud835\udca9\ud835\udc45\ud835\udc63conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63\u2130\u00a0and\u00a0\ud835\udf13\ud835\udc64\ud835\udc63\ud835\udc45\\mathcal{N}_{R}(v)=\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}\\textnormal{ and }%\n\\psi(w,v)=R\\}caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) = { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E and italic_\u03c8 ( italic_w , italic_v ) = italic_R } denotes the R-specific neighborhood of node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v\\in\\mathcal{V}italic_v \u2208 caligraphic_V.\nThis formulation supports a wide range of different graph neural network operators, which define the specific form of functions f\u03d5\u2062(v)subscript\ud835\udc53italic-\u03d5\ud835\udc63f_{\\phi(v)}italic_f start_POSTSUBSCRIPT italic_\u03d5 ( italic_v ) end_POSTSUBSCRIPT, fRsubscript\ud835\udc53\ud835\udc45f_{R}italic_f start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT, gRsubscript\ud835\udc54\ud835\udc45g_{R}italic_g start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT and {{\u22c5}}\u22c5\\{\\{\\cdot\\}\\}{ { \u22c5 } }\u00a0(Fey and Lenssen, 2019).\n\n\n\n\nTemporal Message Passing.\n\nGiven a relational entity graph G=(\ud835\udcb1,\u2130,\ud835\udcaf,\u211b)\ud835\udc3a\ud835\udcb1\u2130\ud835\udcaf\u211bG=(\\mathcal{V},\\mathcal{E},\\mathcal{T},\\mathcal{R})italic_G = ( caligraphic_V , caligraphic_E , caligraphic_T , caligraphic_R ) with attached mapping functions \u03c8,\u03d5,\u03c4\ud835\udf13italic-\u03d5\ud835\udf0f\\psi,\\phi,\\tauitalic_\u03c8 , italic_\u03d5 , italic_\u03c4 and initial node embeddings {\ud835\udc21v(0)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc210\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(0)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT and an example specific seed time t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R (cf. Sec.\u00a02.2) , we obtain a set of deep node embeddings {\ud835\udc21v(L)}v\u2208\ud835\udcb1subscriptsubscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63\ud835\udc63\ud835\udcb1\\{\\mathbf{h}^{(L)}_{v}\\}_{v\\in\\mathcal{V}}{ bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_v \u2208 caligraphic_V end_POSTSUBSCRIPT by L\ud835\udc3fLitalic_L consecutive applications of Eq.\u00a03, where we additionally filter R\ud835\udc45Ritalic_R-specific neighborhoods based on their timestamp, i.e. replace \ud835\udca9R\u2062(v)subscript\ud835\udca9\ud835\udc45\ud835\udc63\\mathcal{N}_{R}(v)caligraphic_N start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) with\n\n\n\n\ud835\udca9R\u2264t\u2062(v)={w\u2208\ud835\udcb1\u2223(w,v)\u2208\u2130\u2062,\u00a0\u2062\u03c8\u2062(w,v)=R\u2062, and\u00a0\u2062\u03c4\u2062(w)\u2264t}\u2062,subscriptsuperscript\ud835\udca9absent\ud835\udc61\ud835\udc45\ud835\udc63conditional-set\ud835\udc64\ud835\udcb1\ud835\udc64\ud835\udc63\u2130,\u00a0\ud835\udf13\ud835\udc64\ud835\udc63\ud835\udc45, and\u00a0\ud835\udf0f\ud835\udc64\ud835\udc61,\\mathcal{N}^{\\leq t}_{R}(v)=\\{w\\in\\mathcal{V}\\mid(w,v)\\in\\mathcal{E}%\n\\textnormal{, }\\psi(w,v)=R\\textnormal{, and }\\tau(w)\\leq t\\}\\text{,}caligraphic_N start_POSTSUPERSCRIPT \u2264 italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ( italic_v ) = { italic_w \u2208 caligraphic_V \u2223 ( italic_w , italic_v ) \u2208 caligraphic_E , italic_\u03c8 ( italic_w , italic_v ) = italic_R , and italic_\u03c4 ( italic_w ) \u2264 italic_t } ,\n\n(4)\n\n\nrealized by the temporal sampling procedure presented in Sec.\u00a03.3. The formulation naturally respects time by only aggregating messages from nodes that were available before the given seed time s\ud835\udc60sitalic_s. The given formulation is agnostic to specific implementations of message passing and supports a wide range of different operators.\n\n\n\n\n\n3.4.2 Prediction with Model Heads\n\nThe model described so far is task-agnostic and simply propagates information through the relational entity graph to produce generic node embeddings. We obtain a task-specific model by combining our graph with a training table, leading to specific model heads and loss functions. We distinguish between (but are not limited to) two types of tasks: node-level prediction and link-level prediction.\n\n\nNode-level Model Head.\n\nGiven a batch of N\ud835\udc41Nitalic_N node level training table examples {(\ud835\udca6,t,y)i}i=1Nsuperscriptsubscriptsubscript\ud835\udca6\ud835\udc61\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\{(\\mathcal{K},t,y)_{i}\\}_{i=1}^{N}{ ( caligraphic_K , italic_t , italic_y ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT (cf. Sec.\u00a02.2), where \ud835\udca6={k}\ud835\udca6\ud835\udc58\\mathcal{K}=\\{k\\}caligraphic_K = { italic_k } contains the primary key of node v\u2208\ud835\udcb1\ud835\udc63\ud835\udcb1v~{}\\in\\mathcal{V}italic_v \u2208 caligraphic_V in the relational entity graph, t\u2208\u211d\ud835\udc61\u211dt\\in\\mathbb{R}italic_t \u2208 blackboard_R is the seed time, and y\u2208\u211dd\ud835\udc66superscript\u211d\ud835\udc51y\\in\\mathbb{R}^{d}italic_y \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is the target value. Then, the node-level model head is a function that maps node-level embeddings \ud835\udc21v(L)subscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63\\mathbf{h}^{(L)}_{v}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT to a prediction y^^\ud835\udc66\\hat{y}over^ start_ARG italic_y end_ARG, i.e.\n\n\n\nf:\u211ddv\u2192\u211dd\u2062,f:\ud835\udc21v(L)\u21a6y^\u2062.:\ud835\udc53\u2192superscript\u211dsubscript\ud835\udc51\ud835\udc63superscript\u211d\ud835\udc51,\ud835\udc53:maps-tosubscriptsuperscript\ud835\udc21\ud835\udc3f\ud835\udc63^\ud835\udc66.f:\\mathbb{R}^{d_{v}}\\rightarrow\\mathbb{R}^{d}\\textnormal{,}\\quad\\quad f:%\n\\mathbf{h}^{(L)}_{v}\\mapsto\\hat{y}\\textnormal{.}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_f : bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT \u21a6 over^ start_ARG italic_y end_ARG .\n\n(5)\n\n\n\n\n\nLink-level Model Head.\n\nSimilarly, we can define a link-level model head for training examples {(\ud835\udca6,t,y)i}i=1Nsuperscriptsubscriptsubscript\ud835\udca6\ud835\udc61\ud835\udc66\ud835\udc56\ud835\udc561\ud835\udc41\\{(\\mathcal{K},t,y)_{i}\\}_{i=1}^{N}{ ( caligraphic_K , italic_t , italic_y ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT with \ud835\udca6={k1,k2}\ud835\udca6subscript\ud835\udc581subscript\ud835\udc582\\mathcal{K}=\\{k_{1},k_{2}\\}caligraphic_K = { italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT } containing primary keys of two different nodes v1,v2\u2208\ud835\udcb1subscript\ud835\udc631subscript\ud835\udc632\ud835\udcb1v_{1},v_{2}\\in\\mathcal{V}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2208 caligraphic_V in the relational entity graph. A function maps node embeddings \ud835\udc21v1(L)subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc631\\mathbf{h}^{(L)}_{v_{1}}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, \ud835\udc21v2(L)subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc632\\mathbf{h}^{(L)}_{v_{2}}bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT to a prediction, i.e.\n\n\n\nf:\u211ddv1\u00d7\u211ddv2\u2192\u211dd\u2062,f:(\ud835\udc21v1(L),\ud835\udc21v2(L))\u21a6y^\u2062.:\ud835\udc53\u2192superscript\u211dsubscript\ud835\udc51subscript\ud835\udc631superscript\u211dsubscript\ud835\udc51subscript\ud835\udc632superscript\u211d\ud835\udc51,\ud835\udc53:maps-tosubscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc631subscriptsuperscript\ud835\udc21\ud835\udc3fsubscript\ud835\udc632^\ud835\udc66.f:\\mathbb{R}^{d_{v_{1}}}\\times\\mathbb{R}^{d_{v_{2}}}\\rightarrow\\mathbb{R}^{d}%\n\\textnormal{,}\\quad\\quad f:(\\mathbf{h}^{(L)}_{v_{1}},\\mathbf{h}^{(L)}_{v_{2}})%\n\\mapsto\\hat{y}\\textnormal{.}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u00d7 blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUPERSCRIPT \u2192 blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_f : ( bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_h start_POSTSUPERSCRIPT ( italic_L ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) \u21a6 over^ start_ARG italic_y end_ARG .\n\n(6)\n\n\nA task-specific loss L\u2062(y^,y)\ud835\udc3f^\ud835\udc66\ud835\udc66L(\\hat{y},y)italic_L ( over^ start_ARG italic_y end_ARG , italic_y ) provides gradient signals to all trainable parameters. The presented approach can be generalized to |\ud835\udca6|>2\ud835\udca62|\\mathcal{K}|>2| caligraphic_K | > 2 to specify subgraph-level tasks. In the first version, RelBench provides node-level tasks only.\n\n\n\n\n\n3.4.3 Multi-Modal Node Encoders\n\nThe final piece of the pipeline is to obtain the initial entity-level node embeddings \ud835\udc21v(0)subscriptsuperscript\ud835\udc210\ud835\udc63\\mathbf{h}^{(0)}_{v}bold_h start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT from the multi-modal input attributes xv=(xv1,\u2026,xvdT)\u2208\ud835\udc9cT1\u00d7\u2026\u00d7\ud835\udc9cTdTsubscript\ud835\udc65\ud835\udc63superscriptsubscript\ud835\udc65\ud835\udc631\u2026superscriptsubscript\ud835\udc65\ud835\udc63subscript\ud835\udc51\ud835\udc47superscriptsubscript\ud835\udc9c\ud835\udc471\u2026superscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc47x_{v}=(x_{v}^{1},\\ldots,x_{v}^{d_{T}})\\in\\mathcal{A}_{T}^{1}\\times\\ldots\\times%\n\\mathcal{A}_{T}^{d_{T}}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = ( italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) \u2208 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT \u00d7 \u2026 \u00d7 caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.\nDue to the nature of tabular data, each column element xvisuperscriptsubscript\ud835\udc65\ud835\udc63\ud835\udc56x_{v}^{i}italic_x start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT lies in its own modality space \ud835\udc9cTdisuperscriptsubscript\ud835\udc9c\ud835\udc47subscript\ud835\udc51\ud835\udc56\\mathcal{A}_{T}^{d_{i}}caligraphic_A start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, e.g., image, text, categorical, and numerical values.\nTherefore, we use a modality-specific encoder to embed each attribute into embeddings.\nFor text and image modalities, we can naturally use pre-trained embedding models as the encoders\u00a0(Reimers and Gurevych, 2019).\nAfter all the attributes are embedded, we apply state-of-the-art tabular deep learning models\u00a0(Huang et\u00a0al., 2020; Arik and Pfister, 2021; Gorishniy et\u00a0al., 2021, 2022; Chen et\u00a0al., 2023) to fuse all the attribute embeddings into a single entity-level node embedding.\nIn practice, we rely on PyTorch Frame\u00a0(Hu et\u00a0al., 2023) that supports a variety of modality-specific encoders, such as pre-trained text embedding models, and as well as state-of-the-art deep learning models on tabular data.\n\n\n\n\n\n3.5 Discussion\n\nThe neural network architecture presented in this blueprint is end-to-end trainable on relational databases. This approach supports a wide range of tasks, such as classification, regression or link prediction in a unified way, with labels computed and stored in a training table.\nIt learns to solve tasks without requiring manual feature engineering, as typical in tabular learning. Instead, operations that are otherwise done manually, such as SQL JOIN+AGGREGATE\noperations, are learned by the GNN. More than simply replacing SQL operations, the GNN message and aggregation steps exactly match the functional form of SQL JOIN+AGGREGATE operations. In other words, the GNN is an exact neural version of SQL JOIN+AGGREGATE operations. We believe this is another important reason why message passing-based architectures are a natural learned replacement for hand-engineered features on relational tables.\n\n\n",
      "4 \u00a0RelBench: A Benchmark for Relational Deep Learning": "\n\n4 \u00a0RelBench: A Benchmark for Relational Deep Learning\n\nFigure 5: Overview of RelBench. RelBench enables training and evaluation of machine learning models on relational data. RelBench supports deep learning framework agnostic data loading, task specification, standardized data splitting, and transforming data into graph format. RelBench provides standardized evaluation metric computations, and a leaderboard for tracking progress. We additionally provide example training scripts built using PyTorch Geometric and PyTorch Frame.\n\n\nWe introduce RelBench, an open benchmark for Relational Deep Learning. The goal of RelBench is to facilitate scalable, robust, and reproducible machine learning research on relational tables. RelBench curates a diverse set of large-scale, challenging, and realistic benchmark databases and defines meaningful predictive tasks over these databases. In addition, RelBench develops a Python library for loading relational tables and tasks, constructing data graphs, and providing unified evaluation for predictive tasks. It also integrates seamlessly with existing Pytorch Geometric and PyTorch Frame functionalities. In its beta release111Website: https://relbench.stanford.edu222Package: https://github.com/snap-stanford/relbench, we announce the first two real-world relational databases, each with two curated predictive tasks.\n\n\nIn the subsequent sections (Sec.\u00a04.3 and\u00a04.4), we describe in detail the two relational databases and the predictive tasks. For each database, we show its entity relational diagrams and important statistics. For each task, we define the task formulation, entity filtering, significance of the task, and also unified evaluation metric. Finally, we demonstrate the usage of the RelBench\u2019s package in Sec.\u00a04.1.\n\n\n\n4.1 RelBench Package\n\nThe RelBench package is designed to allow easy and standardized access to Relational Deep Learning for researchers to push the state-of-the-art of this emerging field. It provides Python APIs to (1) download and process relational databases and their predictive tasks; (2) load standardized data splits and generate relevant train/validation/test tables; (3) evaluate on machine learning predictions. It also provides a flexible ecosystems of supporting tools such as automatic conversion to PyTorch Geometric graphs and integration with Pytorch Frame to produce embeddings for diverse column types. We additionally provide end-to-end scripts for training using RelBench package with GNNs and XGBoost\u00a0(Chen and Guestrin, 2016). We refer the readers to the code repository for a more detailed understanding of RelBench. Here we demonstrate the core functionality.\n\n\nTo load a relational database, simply do:\n\n\n\n\u2b07\n\n\n\nfrom\u00a0relbench.datasets\u00a0import\u00a0get_dataset\n\n\ndataset\u00a0=\u00a0get_dataset(name=\"rel-amazon\")\n\n\n\n\nIt will load the relational tables and process it into a standardized format. Next, to load the predictive task and the relevant training tables, do:\n\n\n\n\u2b07\n\n\n\ntask\u00a0=\u00a0dataset.get_task(\"rel-amazon-ltv\")\n\n\ntask.train_table,\u00a0task.val_table,\u00a0task.test_table\u00a0#\u00a0training/validation/testing\u00a0tables\n\n\n\n\nIt automatically constructs the training table for the relevant predictive task. Next, after the user trains the machine learning model, the user can use RelBench standardized evaluator:\n\n\u2b07\n\n\n\ntask.evaluate(pred)\n\n\n\n\n\n\n4.2 Temporal Splitting\n\nEvery dataset in RelBench has a validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT and a test timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. These are shared for all tasks in the dataset.\nThe test table for any task comprises of labels computed for the time window from ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT to ttest+\u03b4subscript\ud835\udc61test\ud835\udefft_{\\text{test}}+\\deltaitalic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT + italic_\u03b4, where the window size \u03b4\ud835\udeff\\deltaitalic_\u03b4 is specified for each task. Thus the model must make predictions using only information available up to time ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT. Accordingly, to prevent accidental temporal leakage at test time RelBench only provides database rows with timestamps up to ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT for training and validation purposes. RelBench also provides default train and validation tables. The default validation table is constructed similar to the test table, but with the time window being tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT to tval+\u03b4subscript\ud835\udc61val\ud835\udefft_{\\text{val}}+\\deltaitalic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT + italic_\u03b4. To construct the default training table, we first sample time stamps tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT starting from tval\u2212\u03b4subscript\ud835\udc61val\ud835\udefft_{\\text{val}}-\\deltaitalic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT - italic_\u03b4 and moving backwards with a stride of \u03b4\ud835\udeff\\deltaitalic_\u03b4. This allows us to benefit from the latest available training information. Then for each tisubscript\ud835\udc61\ud835\udc56t_{i}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we apply an entity filter to select the entities of interest (e.g., active users). Finally for each pair of timestamp and entity, we compute the training label based on the task definition. Users can explore other ways of constructing the training or validation table, for example by sampling timestamps with shorter strides to get more labels, as long as information after tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is not used for training.\n\n\n\n\n4.3 rel-amazon: Amazon product review e-commerce database\n\nFigure 6: rel-amazon contains two dimension tables (customers and products) and one fact table (reviews). Each review has a customer and a product foreign key. \n\n\n\nDatabase overview.\n\nThe rel-amazon relational database stores product and user purchasing behavior across Amazon\u2019s e-commerce platform. Notably, it contains rich information about each product and transaction. The product table includes price and category information; the review table includes overall rating, whether the user has actually bought the product, and the text of the review itself. We use the subset of book-related products. The entity relationships are described in Fig. 6.\n\n\n\n\nDataset statistics.\n\nrel-amazon covers 3 relational tables and contains 1.85M customers, 21.9M reviews, 506K products. This relational database spans from 1996-06-25 to 2018-09-28. The validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is set to 2014-01-21 and the testing timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is 2016-01-01. Thus, tasks can have a window size up to 2 years.\n\n\n\n\n4.3.1 rel-amazon-ltv: Predict the life time value (LTV) of a user\n\nTask definition: Predict the life time value of a user, defined as the sum of prices of the products that the user will buy and review in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that wrote review in the past two years before the timestamp.\n\n\nTask significance: By accurately forecasting LTV, the e-commerce platform can gain insights into user purchasing patterns and preferences, which is essential when making strategic decisions related to marketing, product recommendations, and inventory management. Understanding a user\u2019s future purchasing behavior helps in tailoring personalized shopping experiences and optimizing product assortments, ultimately enhancing customer satisfaction and loyalty.\n\n\nMachine learning task: Regression. The target ranges from $0-$33,858.4 in the given time window in the training table.\n\n\nEvaluation metric: Mean Absolute Error (MAE).\n\n\n\n\n4.3.2 rel-amazon-churn: Predict if the user churns\n\nTask definition: Predict if the user will not buy any product in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that wrote review in the past two years before the timestamp.\n\n\nTask significance: Predicting churn accurately allows companies to identify potential risks of customer attrition early on. By understanding which customers are at risk of disengagement, businesses can implement targeted interventions to improve customer retention. This may include personalized marketing, tailored offers, or enhanced customer service. Effective churn prediction enables businesses to maintain a stable customer base, ensuring sustained revenue streams and facilitating long-term planning and resource allocation.\n\n\n\nMachine learning task: Binary classification. The label is 1 when user churns and 0 vice versus.\n\n\nEvaluation metric: Average precision (AP).\n\n\n\n\n\n4.4 rel-stackex: Stack exchange question-and-answer website database\n\nFigure 7: Entity relational diagrams of Stack-Exchange.\n\n\nDatabase overview.\n\nStack Exchange is a network of question-and-answer websites on topics in diverse fields, each site covering a specific topic, where questions, answers, and users are subject to a reputation award process. The reputation system allows the sites to be self-moderating. In our benchmark, we use the stats-exchange site. We derive from the raw data dump from 2023-09-12. Figure\u00a07 shows its entity relational diagrams.\n\n\n\nDataset statistics.\n\nrel-stackex covers 7 relational tables and contains 333K users, 415K posts, 794K comments, 1.67M votes, 103K post links, 590K badges records, 1.49M post history records. This relational database spans from 2009-02-02 to 2023-09-03. The validation timestamp tvalsubscript\ud835\udc61valt_{\\text{val}}italic_t start_POSTSUBSCRIPT val end_POSTSUBSCRIPT is set to be 2019-01-01 and the testing timestamp ttestsubscript\ud835\udc61testt_{\\text{test}}italic_t start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is set to be 2021-01-01. Thus, the maximum time window size for predictive task is 2 years.\n\n\n\n\n4.4.1 rel-stackex-engage: Predict if a user will be an active contributor to the site\n\nTask definition: Predict if the user will make any contribution, defined as vote, comment, or post, to the site in the next 2 years.\n\n\nEntity filtering: We filter on active users defined as users that have made at least one comment/post/vote before the timestamp.\n\n\nTask significance: By accurately forecasting the levels of user contribution, website administrators can effectively gauge and oversee user activity. This insight allows for well-informed choices across various business aspects. For instance, it aids in preempting and mitigating user attrition, as well as in enhancing strategies to foster increased user interaction and involvement. This predictive task serves as a crucial tool in optimizing user experience and sustaining a dynamic and engaged user base.\n\n\nMachine learning task: Binary classification. The label is 1 when user contributes to the site and 0 otherwise.\n\n\nEvaluation metric: Average Precision (AP).\n\n\n\n\n4.4.2 rel-stackex-votes: Predict the number of upvotes a question will receive\n\nTask definition: Predict the popularity of a question post in the next six months. The popularity is defined as the number of upvotes the post will receive.\n\n\nEntity filtering: We filter on question posts that are posted recently in the past 2 years before the timestamp. This ensures that we do not predict on old questions that have been outdated.\n\n\nTask significance: Predicting the popularity of a question post is valuable as it empowers site managers to predict and prepare for the influx of traffic directed towards that particular post. This foresight is instrumental in making strategic business decisions, such as curating question recommendations and optimizing content visibility. Understanding which posts are likely to attract more attention helps in tailoring the user experience and managing resources effectively, ensuring that the most engaging and relevant content is highlighted to maintain and enhance user engagement.\n\n\nMachine learning task: Regression. The target ranges from 0-52 number of upvotes in the given time window in the training table.\n\n\nEvaluation metric: Mean Absolute Error (MAE).\n\n\n\n",
      "5 A New Program for Graph Representation Learning": "\n\n5 A New Program for Graph Representation Learning\n\nFigure 8: Relational Deep Learning brings new challenges at all levels of the machine learning stack.\n\n\nDeveloping Relational Deep Learning requires a new research program in graph representation learning on relational data. There are opportunities at all levels of the research stack, including (pre-)training methods, GNN architectures, multimodality, new graph formulations, and scaling to large distributed relational databases for many industrial settings. Here we discuss several promising aspects of this research program, aiming to stimulate the interest of the graph machine learning community.\n\n\n\n5.1 Scaling Relational Deep Learning\n\nRelational databases are often vast, with information distributed across many servers with constrained communication. However, relational data has a non-typical graph structure which may help scale Relational Deep Learning more efficiently.\n\n\nDistributed Training on Relational Data.\n\nExisting distributed machine learning techniques often assume that each server contains data of the same type. Relational data on the other hand, naturally partitions in to pieces, bringing new challenges depending on the partitioning technique used. Horizontal partitioning, known as sharding, is the most common approach. It creates database shards by splitting tables row-wise according to some criterion (e.g., all customer with zipcode in a given range). In this case, a table containing a customers personal record may lie on a distinct server from the table recording recent purchase activity, leading to communication bottlenecks when attempting to train models by sensing messages between purchases and customer records. Less common, but also possible, is vertical splitting. Different splitting options suggests an opportunity to (a) develop specialized distributed learning methods that exploit the vertical or horizontal partitioning, and (b) design further storage arrangements that may be more suited to Relational Deep Learning. The question of graph partitioning arises in all large-scale graph machine learning settings, however in this case we are fortunate to have non-typical graph structure (i.e., it follows the schema) which makes it easier to find favourable partitions.\n\n\n\nLocalized Learning.\n\nFor many predictive tasks it is neither feasible (due to database size) nor desirable (due to task narrowness) to propagate GNN messages across the entire graph during training. In such cases, sampling schemes that preserve locality by avoiding exponential growth in GNN receptive field are needed. This is easily addressed in cases with prior knowledge on the relevant entities. How to scale to deep models that remain biased models towards local computation in cases with no prior knowledge remains an interesting open question.\n\n\n\n\n\n5.2 Building Graphs from Relational Data\n\nAn essential ingredient of Relational Deep Learning is the use of an individual entity and relation-level graph on which to apply inter-entity message passing to learn entity embeddings based on relations to other entities. In Sec. 3.2 we introduced one such graph, the relational entity graph, a general procedure for viewing any relational database as a graph. Whilst a natural choice, we do not propose dogmatically viewing entities as nodes and relations as edges. Instead, the essential property of the relational entity graph is that it is full-resolution. That is, each entity and each primary-foreign key link in the relational database corresponds to its own graph-piece, so that the relational database is exactly encoded in graph form. It is this property that we expect potential alternative graph designs to share. Beyond this stipulation, many creative graph choices are possible, and we discuss some possibilities here.\n\n\nForeign-key Hypergraph.\n\nFact tables often contain entities with a fixed foreign-key pattern (e.g., in rel-amazon a row in a review table always refers to a customer and a product foreign key). The relational entity graph views a review as a node, with edges to a customer and product. However, another possibility is to view this as a single hyperedge between review, customer, and product. Alternative graph choices may alter (and improve) information propagation between entities (cf. Sec. 5.3).\n\n\n\nStale Link Pruning.\n\nEntities that have been active for a long time may have a lot of links to other entities. Many of these links may be stale, or uninformative, for certain tasks. For example, the purchasing patterns of a longtime customer during childhood are likely to be less relevant to their purchasing patterns in adulthood. Links that are stale for a certain task may hurt predictive power by obfuscating true predictive signals, and reduce model efficiency due to processing uninformative data. This situation calls for careful stale link and entity handling to focus on relevant information. Promising methods may include pruning or preaggregating stale links. More generally, how to deal with more gradual distribution drift over time is an open question.\n\n\n\n\n\n5.3 GNN Architectures for Relational Data\n\nViewing a relational database a graphs leads to graphs with structural properties that are consistent across databases. To properly exploit this structure new specialized GNN architectures are needed. Here we discuss several concrete directions for designing new architectures.\n\n\nExpressive GNNs for Relational Data.\n\nRelational entity graphs (cf. Sec. 3.2) obey certain structural constraints. For example, as nodes correspond to entities drawn from one of several tables, the relational entity graph is naturally n\ud835\udc5bnitalic_n-partite, where n\ud835\udc5bnitalic_n is the total number of tables. This suggests that GNNs for relational data should be designed to be capable of learning expressive decision rules over n\ud835\udc5bnitalic_n-partite graphs. Unfortunately, recent studies find that many GNN architectures fail to distinguish biconnected graphs (Zhang et\u00a0al., 2023). Further work is needed to design expressive n\ud835\udc5bnitalic_n-partite graph models.\n\n\nRelational entity graphs also have regularity in edge-connectivity. For instance, in rel-amazon entities in the review table always refer to one customer and one product.\nConsistent edge patterns are described by the structure of the schema graph (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) (cf. Sec. 3.1). How to integrate prior knowledge of the graph structure of (\ud835\udcaf,\u211b)\ud835\udcaf\u211b(\\mathcal{T},\\mathcal{R})( caligraphic_T , caligraphic_R ) into GNNs that operate on an entity-level graph (the relational entity graph) remains an open question. These two examples serve only to illustrate the possibilities for architecture design based on the structure of relational entity graphs. Many other structural properties of relational data may lead to innovative new expressive GNN architectures.\n\n\n\nQuery Language Inspired Models.\n\nSQL operations are known to be extremely powerful operations for manipulating relational data. Their weakness is that they do not have differentiable parameters, making end-to-end learnability impossible. Despite this, there are close similarities between key SQL queries and the computation process of graph neural networks. For instance, a very common way to combine information across tables T1,T2subscript\ud835\udc471subscript\ud835\udc472T_{1},T_{2}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT in SQL is to (1) create a table T3subscript\ud835\udc473T_{3}italic_T start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT by applying a JOIN operation to table T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, by matching foreign keys in T1subscript\ud835\udc471T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to primary keys in T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, then (2) produce a final table with the same number of rows as T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT by applying an AGGREGATE operation to rows in T3subscript\ud835\udc473T_{3}italic_T start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT with foreign keys pointing to the same entity in T2subscript\ud835\udc472T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. There are many choices of AGGREGATE operation such as SUM, MEAN and COUNT. This process directly mirrors GNN computations of messages from neighboring nodes, followed by message aggregation. In other words, GNNs can be thought of as a neural version of SQL JOIN+AGGREGATE operations. This suggests that an opportunity for powerful new neural network architectures by designing differentiable computation blocks that algorithmically align (Xu et\u00a0al., 2020) to existing SQL operations that are known to be useful.\n\n\n\nNew Message Passing Schemes.\n\nBeyond expressivity, new architectures may also improve information propagation between entities. For instance, collaborative filtering methods enhance predictions by identify entities with similar behavior patterns, customers with similar purchase history. However, in the relational entity graph, the two related customers may not be directly linked. Instead they are indirectly be linked to one another through links to their respective purchases, which are linked to a particular shared product ID. This means that a standard message passing GNN will require four message passing steps to propagate the information that customer v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT purchased the same product as customer v2subscript\ud835\udc632v_{2}italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (2-hops from v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to product, and 2-hops from product to v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT). New message passing schemes that do multiple hops or directly connect customers (more generally, entities) with similar behavior patterns may more effectively propagate key information through the model. As well as new message passing schemes, there is also opportunity for new message aggregation methods. One possibility is order dependent aggregation, that combines messages in a time-dependent way, as explored by Yang et\u00a0al. (2022). Another is schema dependent aggregation, that combines messages based on what part of the schema graph the messages are arriving from.\n\n\n\n\n\n\n5.4 Training Techniques for Relational Data\n\nBy its nature, relational data contains highly overlapping predictive signals and tasks. This interconnectedness of data and tasks is a big opportunity for new neural network training methods that maximally take advantage of this interconnectedness to identify useful predictive signals. This section discusses several such opportunities.\n\n\nMulti-Task Learning.\n\nMany predictive tasks on relational data are distinct but related. For example, predicting customer lifetime value, and forecasting individual product sales both involve anticipating future purchase patterns. In RelBench, this corresponds to defining multiple training tables, one for each task, and training a single model jointly on all tasks in order to benefit from shared predictive signals. How to group training tables to leverage their overlap is a promising area for further study.\n\n\n\nMulti-Modal Learning.\n\nEntities in relational databases often have attributes covering multiple modalities (e.g., products come with images, descriptions, as well as different categorical and numerical features). The Relational Deep Learning blueprint first extracting entity-level features, which are used as initial node-features for the GNN model.\nIn RelBench, this multimodal entity-level feature extraction is handled by using state-of-the-art pre-trained models using the PyTorch Frame library to pre-extract features. This maximizes convenience for graph-focused research, but is likely suboptimal because the entity-level feature extraction model is frozen. This is especially relevant in contexts with unusual data\u2014e.g., specialized medical documentation\u2014that generic pre-trained models will likely fail to extract important details.\nTo facilitate exploration of joint entity-level and graph-level modelling, RelBench also provides the option to load raw data, to allow researchers to experiment with different feature-extraction methods.\n\n\n\nFoundation Models and Data Type Encoders.\n\nIn practice, new predictive tasks on relational data are often specified on-the-fly, with fast responses required. Such situations preclude costly model training from scratch, instead requiring powerful and generic pre-trained models. Self-supervised labels for model pre-training can be mined from historical data, just as with training table construction. However, techniques for automatically deciding which labels to mine remains unexplored. Another desirable property of pre-trained models is that they are inductive, so they can be applied to entirely new relational databases out-of-the-box. This presents a challenges in how to deal with unseen column types and relations between tables. Such flexibility is needed in order to move towards foundation models for relational databases. More broadly, how to build column encoders is an important question. As well as distribution shifts as mentioned in the previous paragraph, there are also decisions on when to share column encoders (should two image columns use the same image encoder?), as well as special data types such as static time intervals (e.g., to describe the time period an employee worked at a company, or the time period in which a building project was conducted). Special data types may require specialized encoder choices, and possibly even deeper integration into the neural network computation graph. How best to aggregate of cross-modal information into a single fused embedding is another question for exploration.\n\n\n\n",
      "6 Related Work": "\n\n6 Related Work\n\nRelational Deep Learning touches on many areas of related work which we survey next.\n\n\nStatistical Relational Learning.\n\nSince the foundation of the field of AI, sought to design systems capable of reasoning about entities and their relations, often by explicitly building graph structures (Minsky, 1974). Each new era of AI research also brought its own form of relational learning. A prominent instance is statistical relational learning (De\u00a0Raedt, 2008), a common form of which seeks to describe objects and relations in terms of first-order logic, fused with graphical models to model uncertainty (Getoor et\u00a0al., 2001). These descriptions can then be used to generate new \u201cknowledge\u201d through inductive logic programming (Lavrac and Dzeroski, 1994). Markov logic networks, a prominent statistical relational approach, are defined by a collection of first-order logic formula with accompanying scalar weights (Richardson and Domingos, 2006). This information is then used to define a probability distribution over possible worlds (via Markov random fields) which enables probabilistic reasoning about the truth of new formulae. We see Relational Deep Learning as inheriting this lineage, since both approaches operate on data with rich relational structure, and both approaches integrate relational structure into the model design. Of course, there are important distinctions between the two methods too, such as the natural scalability of graph neural network-based methods, and that Relational Deep Learning does not rely on first-order logic to describe data, allowing broad applicability to relations that are hard to fit into this form.\n\n\n\nTabular Machine Learning.\n\nTree based methods, notably XGBoost (Chen and Guestrin, 2016), remain key workhorses of enterprise machine learning systems due to their scalability and reliability. In parallel, efforts to design deep learning architectures for tabular data have continued (Huang et\u00a0al., 2020; Arik and Pfister, 2021; Gorishniy et\u00a0al., 2021, 2022; Chen et\u00a0al., 2023), but have struggled to clearly establish dominance over tree-based methods (Shwartz-Ziv and Armon, 2022). The vast majority of tabular machine learning focuses on the single table setting, which we argue forgoes use of the rich interconnections between relational data. As such, it does not address the key problem, which is how to get the data from a multi-table to a single table representation. Recently, a nascent body of work has begun to consider multiple tables. For instance, Zhu et\u00a0al. (2023) pre-train tabular Transformers that generalize to new tables with unseen columns.\n\n\n\nKnowledge Graph Embedding.\n\nKnowledge graphs store relational data, and highly scalable knowledge graph embeddings methods have been developed over the last decade to embed data into spaces whose geometry reflects the relational struture\u00a0(Bordes et\u00a0al., 2013; Wang et\u00a0al., 2014, 2017). Whilst also dealing with relational data, this literature differs from this present work in the task being solved. The key task of knowledge graph embeddings is to predict missing entities (Q: Who was Yann LeCun\u2019s postdoc advisor? A: Geoffrey Hinton) or relations (Q: Did Geoffrey Hinton win a Turing Award? A: Yes). To assist in such completion tasks, knowledge graph methods learn an embedding space with the goal of exactly preserving the relation semantics between entities. This is different from Relational Deep Learning, which aims to make predictions about entities, or groups of entities. Because of this, Relational Deep Learning seeks to leverage relations to learn entity representations, but does not need to learn an embedding space that perfectly preserves all relation semantics. This gives more freedom and flexibility to our models, which may discard certain relational information it finds unhelpful. Nonetheless, adopting ideas from knowledge graph embedding may yet be fruitful.\n\n\n\nDeep Learning on Relational Data.\n\nProposals to use message passing neural networks on relational data have occasionally surfaced within the research community. In particular, Schlichtkrull et\u00a0al. (2018); Cvitkovic (2019); \u0160\u00edr (2021), and Zahradn\u00edk et\u00a0al. (2023) make the connection between relational data and graph neural networks and explore it with different network architectures, such as heterogeneous message passing. However, our aim is to move beyond the conceptual level, and clearly establish deep learning on relational data as a subfield of machine learning. Accordingly, we focus on the components needed to establish this new area and attract broader interest: (1) a clearly scoped design space for neural network architectures on relational data, (2) a carefully chosen suite of benchmark databases and predictive tasks around which the community can center its efforts, (3) standardized data loading and splitting, so that temporal leakage does not contaminate experimental results, (4) recognizing time as a first-class citizen, integrated into all sections of the experimental pipeline, including temporal data splitting, time-based forecasting tasks, and temporal-based message passing, and (5) standardized evaluation protocols to ensure comparability between reported results.\n\n\n",
      "7 Conclusion": "\n\n7 Conclusion\n\nA large proportion of the worlds data is natively stored in relational tables. Fully exploiting the rich signals in relational data therefore has the potential to rewrite what problems computing can solve. We believe that Relational Deep Learning will make it possible to achieve superior performance on various prediction problems spanning the breadth of human activity, leading to considerable improvements in automated decision making. There is currently a great scientific opportunity to develop the field of Relational Deep Learning, and further refine this vision.\n\n\nThis paper serves as a road map in this pursuit. We introduce a blueprint for a neural network architecture that directly processes relational data by casting predictive tasks as graph representation learning problems. In Sec. 5 we discuss the many new challenges and opportunities this presents for the graph machine learning community. To facilitate research, we introduce RelBench, a set of benchmark datasets, and a Python package for data loading, and model evaluation.\n\n\n\nAcknowledgments.\n\nWe thank Shirley Wu for useful discussions as we were selecting datasets to adopt. We gratefully acknowledge the support of DARPA under Nos. N660011924033 (MCS); NSF under Nos. OAC-1835598 (CINES), CCF-1918940 (Expeditions), Stanford Data Science Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Initiative, Amazon, Genentech, GSK, Hitachi, Juniper Networks, and KDDI.\n\n\n"
    },
    "table_captions": null,
    "figure_captions": null,
    "bibliography": {
      "Amodei et\u00a0al. (2016)": "\nAmodei et\u00a0al. (2016)\n\nDario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric\nBattenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang\nChen, et\u00a0al.\n\n\nDeep speech 2: End-to-end speech recognition in english and mandarin.\n\n\nIn International Conference on Machine Learning (ICML), 2016.\n\n\n",
      "Arik and Pfister (2021)": "\nArik and Pfister (2021)\n\nSercan\u00a0\u00d6 Arik and Tomas Pfister.\n\n\nTabnet: Attentive interpretable tabular learning.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume\u00a035, pages 6679\u20136687, 2021.\n\n\n",
      "Bordes et\u00a0al. (2013)": "\nBordes et\u00a0al. (2013)\n\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\nYakhnenko.\n\n\nTranslating embeddings for modeling multi-relational data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2013.\n\n\n",
      "Brown et\u00a0al. (2020)": "\nBrown et\u00a0al. (2020)\n\nTom\u00a0B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net\u00a0al.\n\n\nLanguage models are few-shot learners.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n\n\n",
      "Chamberlin and Boyce (1974)": "\nChamberlin and Boyce (1974)\n\nDonald\u00a0D Chamberlin and Raymond\u00a0F Boyce.\n\n\nSequel: A structured english query language.\n\n\nIn Proceedings of the 1974 ACM SIGFIDET (now SIGMOD) workshop\non Data description, access and control, pages 249\u2013264, 1974.\n\n\n",
      "Chen et\u00a0al. (2023)": "\nChen et\u00a0al. (2023)\n\nKuan-Yu Chen, Ping-Han Chiang, Hsin-Rung Chou, Ting-Wei Chen, and Tien-Hao\nChang.\n\n\nTrompt: Towards a better deep neural network for tabular data.\n\n\nIn International Conference on Machine Learning (ICML), 2023.\n\n\n",
      "Chen and Guestrin (2016)": "\nChen and Guestrin (2016)\n\nTianqi Chen and Carlos Guestrin.\n\n\nXgboost: A scalable tree boosting system.\n\n\nIn ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n(KDD), pages 785\u2013794, 2016.\n\n\n",
      "Codd (1970)": "\nCodd (1970)\n\nEdgar\u00a0F Codd.\n\n\nA relational model of data for large shared data banks.\n\n\nCommunications of the ACM, 13(6):377\u2013387,\n1970.\n\n\n",
      "Cvitkovic (2019)": "\nCvitkovic (2019)\n\nMilan Cvitkovic.\n\n\nSupervised learning on relational databases with graph neural\nnetworks.\n\n\nICLR Workshop on Representation Learning on Graphs and\nManifolds, 2019.\n\n\n",
      "DB-Engines (2023)": "\nDB-Engines (2023)\n\nDB-Engines.\n\n\nDBMS popularity broken down by database model, 2023.\n\n\nAvailable: https://db-engines.com/en/ranking_categories.\n\n\n",
      "De\u00a0Raedt (2008)": "\nDe\u00a0Raedt (2008)\n\nLuc De\u00a0Raedt.\n\n\nLogical and relational learning.\n\n\nSpringer Science & Business Media, 2008.\n\n\n",
      "Devlin et\u00a0al. (2018)": "\nDevlin et\u00a0al. (2018)\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n\n\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding.\n\n\nIn North American Chapter of the Association for Computational\nLinguistics (NAACL), 2018.\n\n\n",
      "Fey and Lenssen (2019)": "\nFey and Lenssen (2019)\n\nMatthias Fey and Jan\u00a0Eric Lenssen.\n\n\nFast graph representation learning with pytorch geometric.\n\n\nICLR 2019 (RLGM Workshop), 2019.\n\n\n",
      "Garcia-Molina et\u00a0al. (2008)": "\nGarcia-Molina et\u00a0al. (2008)\n\nHector Garcia-Molina, Jeffrey\u00a0D. Ullman, and Jennifer Widom.\n\n\nDatabase Systems: The Complete Book.\n\n\nPrentice Hall Press, USA, 2 edition, 2008.\n\n\nISBN 9780131873254.\n\n\n",
      "Geirhos et\u00a0al. (2020)": "\nGeirhos et\u00a0al. (2020)\n\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,\nWieland Brendel, Matthias Bethge, and Felix\u00a0A Wichmann.\n\n\nShortcut learning in deep neural networks.\n\n\nNature Machine Intelligence, 2(11):665\u2013673, 2020.\n\n\n",
      "Getoor et\u00a0al. (2001)": "\nGetoor et\u00a0al. (2001)\n\nLise Getoor, Nir Friedman, Daphne Koller, and Avi Pfeffer.\n\n\nLearning probabilistic relational models.\n\n\nRelational data mining, pages 307\u2013335, 2001.\n\n\n",
      "Gilmer et\u00a0al. (2017)": "\nGilmer et\u00a0al. (2017)\n\nJustin Gilmer, Samuel\u00a0S. Schoenholz, Patrick\u00a0F. Riley, Oriol Vinyals, and\nGeorge\u00a0E. Dahl.\n\n\nNeural message passing for quantum chemistry.\n\n\nIn International Conference on Machine Learning (ICML), page\n1263\u20131272, 2017.\n\n\n",
      "Gorishniy et\u00a0al. (2021)": "\nGorishniy et\u00a0al. (2021)\n\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko.\n\n\nRevisiting deep learning models for tabular data.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), volume\u00a034, pages 18932\u201318943, 2021.\n\n\n",
      "Gorishniy et\u00a0al. (2022)": "\nGorishniy et\u00a0al. (2022)\n\nYury Gorishniy, Ivan Rubachev, and Artem Babenko.\n\n\nOn embeddings for numerical features in tabular deep learning.\n\n\nAdvances in Neural Information Processing Systems,\n35:24991\u201325004, 2022.\n\n\n",
      "Hamilton et\u00a0al. (2017)": "\nHamilton et\u00a0al. (2017)\n\nWill Hamilton, Zhitao Ying, and Jure Leskovec.\n\n\nInductive representation learning on large graphs.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Hannun et\u00a0al. (2014)": "\nHannun et\u00a0al. (2014)\n\nAwni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich\nElsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et\u00a0al.\n\n\nDeep speech: Scaling up end-to-end speech recognition.\n\n\narXiv preprint arXiv:1412.5567, 2014.\n\n\n",
      "He et\u00a0al. (2016)": "\nHe et\u00a0al. (2016)\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\n\nDeep residual learning for image recognition.\n\n\nIn IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 770\u2013778, 2016.\n\n\n",
      "Hu et\u00a0al. (2023)": "\nHu et\u00a0al. (2023)\n\nWeihua Hu, Matthias Fey, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao,\nand Vid Kocijan.\n\n\nPyTorch Frame: A Deep Learning Framework for Tabular Data, October\n2023.\n\n\nURL https://github.com/pyg-team/pytorch-frame.\n\n\n",
      "Hu et\u00a0al. (2020)": "\nHu et\u00a0al. (2020)\n\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun.\n\n\nHeterogeneous graph transformer.\n\n\nIn Proceedings of The Web Conference 2020, page 2704\u20132710,\n2020.\n\n\n",
      "Huang et\u00a0al. (2020)": "\nHuang et\u00a0al. (2020)\n\nXin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin.\n\n\nTabtransformer: Tabular data modeling using contextual embeddings.\n\n\narXiv preprint arXiv:2012.06678, 2020.\n\n\n",
      "Johnson et\u00a0al. (2016)": "\nJohnson et\u00a0al. (2016)\n\nAlistair\u00a0EW Johnson, Tom\u00a0J Pollard, Lu\u00a0Shen, Li-wei\u00a0H Lehman, Mengling Feng,\nMohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony\u00a0Celi, and\nRoger\u00a0G Mark.\n\n\nMimic-iii, a freely accessible critical care database.\n\n\nScientific data, 3(1):1\u20139, 2016.\n\n\n",
      "Kaggle (2022)": "\nKaggle (2022)\n\nKaggle.\n\n\nKaggle Data Science & Machine Learning Survey, 2022.\n\n\nAvailable:\nhttps://www.kaggle.com/code/paultimothymooney/kaggle-survey-2022-all-results/notebook.\n\n\n",
      "Kapoor and Narayanan (2023)": "\nKapoor and Narayanan (2023)\n\nSayash Kapoor and Arvind Narayanan.\n\n\nLeakage and the reproducibility crisis in machine-learning-based\nscience.\n\n\nPatterns, 4(9), 2023.\n\n\n",
      "Lavrac and Dzeroski (1994)": "\nLavrac and Dzeroski (1994)\n\nNada Lavrac and Saso Dzeroski.\n\n\nInductive logic programming.\n\n\nIn WLP, pages 146\u2013160. Springer, 1994.\n\n\n",
      "Minsky (1974)": "\nMinsky (1974)\n\nMarvin Minsky.\n\n\nA framework for representing knowledge, 1974.\n\n\n",
      "PubMed (1996)": "\nPubMed (1996)\n\nPubMed.\n\n\nNational Center for Biotechnology Information, U.S. National Library\nof Medicine, 1996.\n\n\nAvailable: https://www.ncbi.nlm.nih.gov/pubmed/.\n\n\n",
      "Reimers and Gurevych (2019)": "\nReimers and Gurevych (2019)\n\nNils Reimers and Iryna Gurevych.\n\n\nSentence-bert: Sentence embeddings using siamese bert-networks.\n\n\narXiv preprint arXiv:1908.10084, 2019.\n\n\n",
      "Richardson and Domingos (2006)": "\nRichardson and Domingos (2006)\n\nMatthew Richardson and Pedro Domingos.\n\n\nMarkov logic networks.\n\n\nMachine learning, 62:107\u2013136, 2006.\n\n\n",
      "Rossi et\u00a0al. (2020)": "\nRossi et\u00a0al. (2020)\n\nEmanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico\nMonti, and Michael Bronstein.\n\n\nTemporal graph networks for deep learning on dynamic graphs.\n\n\nICML Workshop on Graph Representation Learning 2020, 2020.\n\n\n",
      "Russakovsky et\u00a0al. (2015)": "\nRussakovsky et\u00a0al. (2015)\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et\u00a0al.\n\n\nImagenet large scale visual recognition challenge.\n\n\nInternational journal of computer vision, 115(3):211\u2013252, 2015.\n\n\n",
      "Schlichtkrull et\u00a0al. (2018)": "\nSchlichtkrull et\u00a0al. (2018)\n\nMichael Schlichtkrull, Thomas\u00a0N. Kipf, Peter Bloem, Rianne van\u00a0den Berg, Ivan\nTitov, and Max Welling.\n\n\nModeling relational data with graph convolutional networks.\n\n\nIn Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler,\nRapha\u00ebl Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, editors,\nThe Semantic Web, pages 593\u2013607, Cham, 2018. Springer International\nPublishing.\n\n\n",
      "Shwartz-Ziv and Armon (2022)": "\nShwartz-Ziv and Armon (2022)\n\nRavid Shwartz-Ziv and Amitai Armon.\n\n\nTabular data: Deep learning is not all you need.\n\n\nInformation Fusion, 81:84\u201390, 2022.\n\n\n",
      "\u0160\u00edr (2021)": "\n\u0160\u00edr (2021)\n\nGustav \u0160\u00edr.\n\n\nDeep Learning with Relational Logic Representations.\n\n\nCzech Technical University, 2021.\n\n\n",
      "Varma and Zisserman (2005)": "\nVarma and Zisserman (2005)\n\nManik Varma and Andrew Zisserman.\n\n\nA statistical approach to texture classification from single images.\n\n\nInternational journal of computer vision, 62:61\u201381,\n2005.\n\n\n",
      "Vaswani et\u00a0al. (2017)": "\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan\u00a0N Gomez, Lukasz Kaiser, and Illia Polosukhin.\n\n\nAttention is all you need.\n\n\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n\n\n",
      "Wang et\u00a0al. (2017)": "\nWang et\u00a0al. (2017)\n\nQuan Wang, Zhendong Mao, Bin Wang, and Li\u00a0Guo.\n\n\nKnowledge graph embedding: A survey of approaches and applications.\n\n\nIEEE Transactions on Knowledge and Data Engineering,\n29(12):2724\u20132743, 2017.\n\n\n",
      "Wang et\u00a0al. (2021)": "\nWang et\u00a0al. (2021)\n\nYiwei Wang, Yujun Cai, Yuxuan Liang, Henghui Ding, Changhu Wang, and Bryan\nHooi.\n\n\nTime-aware neighbor sampling for temporal graph networks.\n\n\nIn arXiv pre-print, 2021.\n\n\n",
      "Wang et\u00a0al. (2014)": "\nWang et\u00a0al. (2014)\n\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen.\n\n\nKnowledge graph embedding by translating on hyperplanes.\n\n\nIn Proceedings of the AAAI conference on artificial\nintelligence, volume\u00a028, 2014.\n\n\n",
      "Xu et\u00a0al. (2020)": "\nXu et\u00a0al. (2020)\n\nKeyulu Xu, Jingling Li, Mozhi Zhang, Simon\u00a0S Du, Ken-ichi Kawarabayashi, and\nStefanie Jegelka.\n\n\nWhat can neural networks reason about?\n\n\nIn International Conference on Learning Representations\n(ICLR), 2020.\n\n\n",
      "Yang et\u00a0al. (2022)": "\nYang et\u00a0al. (2022)\n\nZhen Yang, Ming Ding, Bin Xu, Hongxia Yang, and Jie Tang.\n\n\nStam: A spatiotemporal aggregation method for graph neural\nnetwork-based recommendation.\n\n\nIn Proceedings of the ACM Web Conference 2022, pages\n3217\u20133228, 2022.\n\n\n",
      "Zahradn\u00edk et\u00a0al. (2023)": "\nZahradn\u00edk et\u00a0al. (2023)\n\nLuk\u00e1\u0161 Zahradn\u00edk, Jan Neumann, and Gustav \u0160\u00edr.\n\n\nA deep learning blueprint for relational databases.\n\n\nIn NeurIPS 2023 Second Table Representation Learning Workshop,\n2023.\n\n\n",
      "Zhang et\u00a0al. (2023)": "\nZhang et\u00a0al. (2023)\n\nBohang Zhang, Shengjie Luo, Liwei Wang, and Di\u00a0He.\n\n\nRethinking the expressive power of gnns via graph biconnectivity.\n\n\nIn International Conference on Learning Representations\n(ICLR), 2023.\n\n\n",
      "Zhu et\u00a0al. (2023)": "\nZhu et\u00a0al. (2023)\n\nBingzhao Zhu, Xingjian Shi, Nick Erickson, Mu\u00a0Li, George Karypis, and Mahsa\nShoaran.\n\n\nXtab: Cross-table pretraining for tabular transformers.\n\n\nIn International Conference on Machine Learning (ICML), 2023.\n\n\n"
    },
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "ddde54eb-7c52-419e-9840-1111033079bf": {
    "pk": "ddde54eb-7c52-419e-9840-1111033079bf",
    "project_name": null,
    "authors": [
      "Jialin Chen",
      "Rex Ying"
    ],
    "title": "TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery",
    "abstract": "Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.",
    "url": "http://arxiv.org/abs/2310.19324v1",
    "timestamp": 1698652301,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "1a98aaf3-9b6b-4e79-b2c7-3f102b3915f5": {
    "pk": "1a98aaf3-9b6b-4e79-b2c7-3f102b3915f5",
    "project_name": null,
    "authors": [
      "Jiaxuan You",
      "Jonathan Gomes-Selman",
      "Rex Ying",
      "Jure Leskovec"
    ],
    "title": "Identity-aware Graph Neural Networks",
    "abstract": "Message passing Graph Neural Networks (GNNs) provide a powerful modeling framework for relational data. However, the expressive power of existing GNNs is upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test, which means GNNs that are not able to predict node clustering coefficients and shortest path distances, and cannot differentiate between different d-regular graphs. Here we develop a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL test. ID-GNN offers a minimal but powerful solution to limitations of existing GNNs. ID-GNN extends existing GNN architectures by inductively considering nodes' identities during message passing. To embed a given node, ID-GNN first extracts the ego network centered at the node, then conducts rounds of heterogeneous message passing, where different sets of parameters are applied to the center node than to other surrounding nodes in the ego network. We further propose a simplified but faster version of ID-GNN that injects node identity information as augmented node features. Altogether, both versions of ID-GNN represent general extensions of message passing GNNs, where experiments show that transforming existing GNNs to ID-GNNs yields on average 40% accuracy improvement on challenging node, edge, and graph property prediction tasks; 3% accuracy improvement on node and graph classification benchmarks; and 15% ROC AUC improvement on real-world link prediction tasks. Additionally, ID-GNNs demonstrate improved or comparable performance over other task-specific graph networks.",
    "url": "http://arxiv.org/abs/2101.10320v2",
    "timestamp": 1611601141,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "b81cda70-51d2-49fa-ae97-996889633505": {
    "pk": "b81cda70-51d2-49fa-ae97-996889633505",
    "project_name": null,
    "authors": [
      "Jiaqing Xie",
      "Rex Ying"
    ],
    "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks",
    "abstract": "Structural features are important features in a geometrical graph. Although there are some correlation analysis of features based on covariance, there is no relevant research on structural feature correlation analysis with graph neural networks. In this paper, we introuduce graph feature to feature (Fea2Fea) prediction pipelines in a low dimensional space to explore some preliminary results on structural feature correlation, which is based on graph neural network. The results show that there exists high correlation between some of the structural features. An irredundant feature combination with initial node features, which is filtered by graph neural network has improved its classification accuracy in some graph-based tasks. We compare differences between concatenation methods on connecting embeddings between features and show that the simplest is the best. We generalize on the synthetic geometric graphs and certify the results on prediction difficulty between structural features.",
    "url": "http://arxiv.org/abs/2106.13061v4",
    "timestamp": 1624545410,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "cc874151-86bf-4fe1-867d-c48db47911cc": {
    "pk": "cc874151-86bf-4fe1-867d-c48db47911cc",
    "project_name": null,
    "authors": [
      "Jialin Chen",
      "Kenza Amara",
      "Junchi Yu",
      "Rex Ying"
    ],
    "title": "Generative Explanations for Graph Neural Network: Methods and Evaluations",
    "abstract": "Graph Neural Networks (GNNs) achieve state-of-the-art performance in various graph-related tasks. However, the black-box nature often limits their interpretability and trustworthiness. Numerous explainability methods have been proposed to uncover the decision-making logic of GNNs, by generating underlying explanatory substructures. In this paper, we conduct a comprehensive review of the existing explanation methods for GNNs from the perspective of graph generation. Specifically, we propose a unified optimization objective for generative explanation methods, comprising two sub-objectives: Attribution and Information constraints. We further demonstrate their specific manifestations in various generative model architectures and different explanation scenarios. With the unified objective of the explanation problem, we reveal the shared characteristics and distinctions among current methods, laying the foundation for future methodological advancements. Empirical results demonstrate the advantages and limitations of different explainability approaches in terms of explanation performance, efficiency, and generalizability.",
    "url": "http://arxiv.org/abs/2311.05764v1",
    "timestamp": 1699567635,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "3e5a3b10-5e64-45c2-9e93-82d6ea086009": {
    "pk": "3e5a3b10-5e64-45c2-9e93-82d6ea086009",
    "project_name": null,
    "authors": [
      "Shirley Wu",
      "Jiaxuan You",
      "Jure Leskovec",
      "Rex Ying"
    ],
    "title": "Efficient Automatic Machine Learning via Design Graphs",
    "abstract": "Despite the success of automated machine learning (AutoML), which aims to find the best design, including the architecture of deep networks and hyper-parameters, conventional AutoML methods are computationally expensive and hardly provide insights into the relations of different model design choices. To tackle the challenges, we propose FALCON, an efficient sample-based method to search for the optimal model design. Our key insight is to model the design space of possible model designs as a design graph, where the nodes represent design choices, and the edges denote design similarities. FALCON features 1) a task-agnostic module, which performs message passing on the design graph via a Graph Neural Network (GNN), and 2) a task-specific module, which conducts label propagation of the known model performance information on the design graph. Both modules are combined to predict the design performances in the design space, navigating the search direction. We conduct extensive experiments on 27 node and graph classification tasks from various application domains, and an image classification task on the CIFAR-10 dataset. We empirically show that FALCON can efficiently obtain the well-performing designs for each task using only 30 explored nodes. Specifically, FALCON has a comparable time cost with the one-shot approaches while achieving an average improvement of 3.3% compared with the best baselines.",
    "url": "http://arxiv.org/abs/2210.12257v2",
    "timestamp": 1666387559,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "99d235b1-34d2-4d03-bc10-3dcb88b49ead": {
    "pk": "99d235b1-34d2-4d03-bc10-3dcb88b49ead",
    "project_name": null,
    "authors": [
      "Guangtao Wang",
      "Rex Ying",
      "Jing Huang",
      "Jure Leskovec"
    ],
    "title": "Multi-hop Attention Graph Neural Network",
    "abstract": "Self-attention mechanism in graph neural networks (GNNs) led to state-of-the-art performance on many graph representation learning tasks. Currently, at every layer, attention is computed between connected pairs of nodes and depends solely on the representation of the two nodes. However, such attention mechanism does not account for nodes that are not directly connected but provide important network context. Here we propose Multi-hop Attention Graph Neural Network (MAGNA), a principled way to incorporate multi-hop context information into every layer of attention computation. MAGNA diffuses the attention scores across the network, which increases the receptive field for every layer of the GNN. Unlike previous approaches, MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. We demonstrate in theory and experiments that MAGNA captures large-scale structural information in every layer, and has a low-pass effect that eliminates noisy high-frequency information from graph data. Experimental results on node classification as well as the knowledge graph completion benchmarks show that MAGNA achieves state-of-the-art results: MAGNA achieves up to 5.7 percent relative error reduction over the previous state-of-the-art on Cora, Citeseer, and Pubmed. MAGNA also obtains the best performance on a large-scale Open Graph Benchmark dataset. On knowledge graph completion MAGNA advances state-of-the-art on WN18RR and FB15k-237 across four different performance metrics.",
    "url": "http://arxiv.org/abs/2009.14332v5",
    "timestamp": 1601419279,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "48a66912-0722-4426-ab98-668b1d2312e0": {
    "pk": "48a66912-0722-4426-ab98-668b1d2312e0",
    "project_name": null,
    "authors": [
      "Ines Chami",
      "Rex Ying",
      "Christopher R\u00e9",
      "Jure Leskovec"
    ],
    "title": "Hyperbolic Graph Convolutional Neural Networks",
    "abstract": "Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations, such as feature transformation and aggregation, in hyperbolic space. Furthermore, since input features are often Euclidean, it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs. We derive GCN operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure, and leads to improved performance when compared to Euclidean analogs, even with very low dimensional embeddings: compared to state-of-the-art GCNs, HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification, also improving state-of-the art on the Pubmed dataset.",
    "url": "http://arxiv.org/abs/1910.12933v1",
    "timestamp": 1572291716,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "2d647e93-1cca-48c0-8580-cac110d2d84b": {
    "pk": "2d647e93-1cca-48c0-8580-cac110d2d84b",
    "project_name": null,
    "authors": [
      "Osman \u00dclger",
      "Julian Wiederer",
      "Mohsen Ghafoorian",
      "Vasileios Belagiannis",
      "Pascal Mettes"
    ],
    "title": "Multi-Task Edge Prediction in Temporally-Dynamic Video Graphs",
    "abstract": "Graph neural networks have shown to learn effective node representations, enabling node-, link-, and graph-level inference. Conventional graph networks assume static relations between nodes, while relations between entities in a video often evolve over time, with nodes entering and exiting dynamically. In such temporally-dynamic graphs, a core problem is inferring the future state of spatio-temporal edges, which can constitute multiple types of relations. To address this problem, we propose MTD-GNN, a graph network for predicting temporally-dynamic edges for multiple types of relations. We propose a factorized spatio-temporal graph attention layer to learn dynamic node representations and present a multi-task edge prediction loss that models multiple relations simultaneously. The proposed architecture operates on top of scene graphs that we obtain from videos through object detection and spatio-temporal linking. Experimental evaluations on ActionGenome and CLEVRER show that modeling multiple relations in our temporally-dynamic graph network can be mutually beneficial, outperforming existing static and spatio-temporal graph neural networks, as well as state-of-the-art predicate classification methods.",
    "url": "http://arxiv.org/abs/2212.02875v1",
    "timestamp": 1670323260,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "99d6a517-1ee3-4e70-913f-435bfc882160": {
    "pk": "99d6a517-1ee3-4e70-913f-435bfc882160",
    "project_name": null,
    "authors": [
      "Julian Stier",
      "Michael Granitzer"
    ],
    "title": "deepstruct -- linking deep learning and graph theory",
    "abstract": "deepstruct connects deep learning models and graph theory such that different graph structures can be imposed on neural networks or graph structures can be extracted from trained neural network models. For this, deepstruct provides deep neural network models with different restrictions which can be created based on an initial graph. Further, tools to extract graph structures from trained models are available. This step of extracting graphs can be computationally expensive even for models of just a few dozen thousand parameters and poses a challenging problem. deepstruct supports research in pruning, neural architecture search, automated network design and structure analysis of neural networks.",
    "url": "http://arxiv.org/abs/2111.06679v2",
    "timestamp": 1636718293,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "95cfe90f-8a24-4ff5-a5c8-cf44c3c2be10": {
    "pk": "95cfe90f-8a24-4ff5-a5c8-cf44c3c2be10",
    "project_name": null,
    "authors": [
      "Gerrit Gro\u00dfmann",
      "Julian Zimmerlin",
      "Michael Backenk\u00f6hler",
      "Verena Wolf"
    ],
    "title": "GINA: Neural Relational Inference From Independent Snapshots",
    "abstract": "Dynamical systems in which local interactions among agents give rise to complex emerging phenomena are ubiquitous in nature and society. This work explores the problem of inferring the unknown interaction structure (represented as a graph) of such a system from measurements of its constituent agents or individual components (represented as nodes). We consider a setting where the underlying dynamical model is unknown and where different measurements (i.e., snapshots) may be independent (e.g., may stem from different experiments). We propose GINA (Graph Inference Network Architecture), a graph neural network (GNN) to simultaneously learn the latent interaction graph and, conditioned on the interaction graph, the prediction of a node's observable state based on adjacent vertices. GINA is based on the hypothesis that the ground truth interaction graph -- among all other potential graphs -- allows to predict the state of a node, given the states of its neighbors, with the highest accuracy. We test this hypothesis and demonstrate GINA's effectiveness on a wide range of interaction graphs and dynamical processes.",
    "url": "http://arxiv.org/abs/2105.14329v1",
    "timestamp": 1622302953,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "54061278-cdaa-4038-b3f0-a99df362d0f9": {
    "pk": "54061278-cdaa-4038-b3f0-a99df362d0f9",
    "project_name": null,
    "authors": [
      "Julian Stier",
      "Michael Granitzer"
    ],
    "title": "Structural Analysis of Sparse Neural Networks",
    "abstract": "Sparse Neural Networks regained attention due to their potential for mathematical and computational advantages. We give motivation to study Artificial Neural Networks (ANNs) from a network science perspective, provide a technique to embed arbitrary Directed Acyclic Graphs into ANNs and report study results on predicting the performance of image classifiers based on the structural properties of the networks' underlying graph. Results could further progress neuroevolution and add explanations for the success of distinct architectures from a structural perspective.",
    "url": "http://arxiv.org/abs/1910.07225v1",
    "timestamp": 1571216922,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.NE",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "83c29100-f490-46b5-9990-572dd5afb682": {
    "pk": "83c29100-f490-46b5-9990-572dd5afb682",
    "project_name": null,
    "authors": [
      "Thomas Monninger",
      "Julian Schmidt",
      "Jan Rupprecht",
      "David Raba",
      "Julian Jordan",
      "Daniel Frank",
      "Steffen Staab",
      "Klaus Dietmayer"
    ],
    "title": "SCENE: Reasoning about Traffic Scenes using Heterogeneous Graph Neural Networks",
    "abstract": "Understanding traffic scenes requires considering heterogeneous information about dynamic agents and the static infrastructure. In this work we propose SCENE, a methodology to encode diverse traffic scenes in heterogeneous graphs and to reason about these graphs using a heterogeneous Graph Neural Network encoder and task-specific decoders. The heterogeneous graphs, whose structures are defined by an ontology, consist of different nodes with type-specific node features and different relations with type-specific edge features. In order to exploit all the information given by these graphs, we propose to use cascaded layers of graph convolution. The result is an encoding of the scene. Task-specific decoders can be applied to predict desired attributes of the scene. Extensive evaluation on two diverse binary node classification tasks show the main strength of this methodology: despite being generic, it even manages to outperform task-specific baselines. The further application of our methodology to the task of node classification in various knowledge graphs shows its transferability to other domains.",
    "url": "http://arxiv.org/abs/2301.03512v1",
    "timestamp": 1673283928,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "6cdd9479-e47b-4759-8d3f-3d0c07216643": {
    "pk": "6cdd9479-e47b-4759-8d3f-3d0c07216643",
    "project_name": null,
    "authors": [
      "Julian Romera"
    ],
    "title": "Optimizing Communication by Compression for Multi-GPU Scalable Breadth-First Searches",
    "abstract": "The Breadth First Search (BFS) algorithm is the foundation and building block of many higher graph-based operations such as spanning trees, shortest paths and betweenness centrality. The importance of this algorithm increases each day due to it is a key requirement for many data structures which are becoming popular nowadays. When the BFS algorithm is parallelized by distributing the graph between several processors the interconnection network limits the performance. Hence, improvements on this area may benefit the overall performance of the algorithm.   This work presents an alternative compression scheme for communications in distributed BFS processing. It focuses on BFS processors using General-Purpose Graphics Processing Units.",
    "url": "http://arxiv.org/abs/1704.00513v1",
    "timestamp": 1491214952,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DC",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "aa7d8579-8801-40e8-8aa2-97bc04478d68": {
    "pk": "aa7d8579-8801-40e8-8aa2-97bc04478d68",
    "project_name": null,
    "authors": [
      "Julian McAuley",
      "Christopher Targett",
      "Qinfeng Shi",
      "Anton van den Hengel"
    ],
    "title": "Image-based Recommendations on Styles and Substitutes",
    "abstract": "Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.",
    "url": "http://arxiv.org/abs/1506.04757v1",
    "timestamp": 1434398509,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.CV",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "f8f40c3e-fe4b-4c5b-be5b-9451bba24a32": {
    "pk": "f8f40c3e-fe4b-4c5b-be5b-9451bba24a32",
    "project_name": null,
    "authors": [
      "Jiaxuan You",
      "Jure Leskovec",
      "Kaiming He",
      "Saining Xie"
    ],
    "title": "Graph Structure of Neural Networks",
    "abstract": "Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.",
    "url": "http://arxiv.org/abs/2007.06559v2",
    "timestamp": 1594663171,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "cac420c5-96d9-490a-8619-ebb4a62e2950": {
    "pk": "cac420c5-96d9-490a-8619-ebb4a62e2950",
    "project_name": null,
    "authors": [
      "Hongwei Wang",
      "Jure Leskovec"
    ],
    "title": "Unifying Graph Convolutional Neural Networks and Label Propagation",
    "abstract": "Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node is spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.",
    "url": "http://arxiv.org/abs/2002.06755v1",
    "timestamp": 1581909793,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "1dedae11-4acf-44fc-833c-5ad433427c21": {
    "pk": "1dedae11-4acf-44fc-833c-5ad433427c21",
    "project_name": null,
    "authors": [
      "Myunghwan Kim",
      "Jure Leskovec"
    ],
    "title": "Nonparametric Multi-group Membership Model for Dynamic Networks",
    "abstract": "Relational data-like graphs, networks, and matrices-is often dynamic, where the relational structure evolves over time. A fundamental problem in the analysis of time-varying network data is to extract a summary of the common structure and the dynamics of the underlying relations between the entities. Here we build on the intuition that changes in the network structure are driven by the dynamics at the level of groups of nodes. We propose a nonparametric multi-group membership model for dynamic networks. Our model contains three main components: We model the birth and death of individual groups with respect to the dynamics of the network structure via a distance dependent Indian Buffet Process. We capture the evolution of individual node group memberships via a Factorial Hidden Markov model. And, we explain the dynamics of the network structure by explicitly modeling the connectivity structure of groups. We demonstrate our model's capability of identifying the dynamics of latent groups in a number of different types of network data. Experimental results show that our model provides improved predictive performance over existing dynamic network models on future network forecasting and missing link prediction.",
    "url": "http://arxiv.org/abs/1311.2079v1",
    "timestamp": 1383944451,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.SI",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "4cbec556-d787-45e9-b929-9100ce73326d": {
    "pk": "4cbec556-d787-45e9-b929-9100ce73326d",
    "project_name": null,
    "authors": [
      "Jiaxuan You",
      "Jonathan Gomes-Selman",
      "Rex Ying",
      "Jure Leskovec"
    ],
    "title": "Identity-aware Graph Neural Networks",
    "abstract": "Message passing Graph Neural Networks (GNNs) provide a powerful modeling framework for relational data. However, the expressive power of existing GNNs is upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test, which means GNNs that are not able to predict node clustering coefficients and shortest path distances, and cannot differentiate between different d-regular graphs. Here we develop a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL test. ID-GNN offers a minimal but powerful solution to limitations of existing GNNs. ID-GNN extends existing GNN architectures by inductively considering nodes' identities during message passing. To embed a given node, ID-GNN first extracts the ego network centered at the node, then conducts rounds of heterogeneous message passing, where different sets of parameters are applied to the center node than to other surrounding nodes in the ego network. We further propose a simplified but faster version of ID-GNN that injects node identity information as augmented node features. Altogether, both versions of ID-GNN represent general extensions of message passing GNNs, where experiments show that transforming existing GNNs to ID-GNNs yields on average 40% accuracy improvement on challenging node, edge, and graph property prediction tasks; 3% accuracy improvement on node and graph classification benchmarks; and 15% ROC AUC improvement on real-world link prediction tasks. Additionally, ID-GNNs demonstrate improved or comparable performance over other task-specific graph networks.",
    "url": "http://arxiv.org/abs/2101.10320v2",
    "timestamp": 1611601141,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "b0398c45-bf91-40c0-aa19-29e4089b34d7": {
    "pk": "b0398c45-bf91-40c0-aa19-29e4089b34d7",
    "project_name": null,
    "authors": [
      "Shirley Wu",
      "Jiaxuan You",
      "Jure Leskovec",
      "Rex Ying"
    ],
    "title": "Efficient Automatic Machine Learning via Design Graphs",
    "abstract": "Despite the success of automated machine learning (AutoML), which aims to find the best design, including the architecture of deep networks and hyper-parameters, conventional AutoML methods are computationally expensive and hardly provide insights into the relations of different model design choices. To tackle the challenges, we propose FALCON, an efficient sample-based method to search for the optimal model design. Our key insight is to model the design space of possible model designs as a design graph, where the nodes represent design choices, and the edges denote design similarities. FALCON features 1) a task-agnostic module, which performs message passing on the design graph via a Graph Neural Network (GNN), and 2) a task-specific module, which conducts label propagation of the known model performance information on the design graph. Both modules are combined to predict the design performances in the design space, navigating the search direction. We conduct extensive experiments on 27 node and graph classification tasks from various application domains, and an image classification task on the CIFAR-10 dataset. We empirically show that FALCON can efficiently obtain the well-performing designs for each task using only 30 explored nodes. Specifically, FALCON has a comparable time cost with the one-shot approaches while achieving an average improvement of 3.3% compared with the best baselines.",
    "url": "http://arxiv.org/abs/2210.12257v2",
    "timestamp": 1666387559,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "f75f5a95-2aaf-4b19-baae-2ac9fc7fd51e": {
    "pk": "f75f5a95-2aaf-4b19-baae-2ac9fc7fd51e",
    "project_name": null,
    "authors": [
      "Keyulu Xu",
      "Weihua Hu",
      "Jure Leskovec",
      "Stefanie Jegelka"
    ],
    "title": "How Powerful are Graph Neural Networks?",
    "abstract": "Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.",
    "url": "http://arxiv.org/abs/1810.00826v3",
    "timestamp": 1538413891,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "23bd6366-09a5-4bc5-889a-02ce99f9b9fb": {
    "pk": "23bd6366-09a5-4bc5-889a-02ce99f9b9fb",
    "project_name": null,
    "authors": [
      "Hongwei Wang",
      "Fuzheng Zhang",
      "Mengdi Zhang",
      "Jure Leskovec",
      "Miao Zhao",
      "Wenjie Li",
      "Zhongyuan Wang"
    ],
    "title": "Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems",
    "abstract": "Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.",
    "url": "http://arxiv.org/abs/1905.04413v3",
    "timestamp": 1557535434,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.LG",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "71050341-79ac-49a0-be3e-efbf90b1e434": {
    "pk": "71050341-79ac-49a0-be3e-efbf90b1e434",
    "project_name": null,
    "authors": [
      "Yuting Feng",
      "Bogdan Cautis"
    ],
    "title": "IGNiteR: News Recommendation in Microblogging Applications (Extended Version)",
    "abstract": "News recommendation is one of the most challenging tasks in recommender systems, mainly due to the ephemeral relevance of news to users. As social media, and particularly microblogging applications like Twitter or Weibo, gains popularity as platforms for news dissemination, personalized news recommendation in this context becomes a significant challenge. We revisit news recommendation in the microblogging scenario, by taking into consideration social interactions and observations tracing how the information that is up for recommendation spreads in an underlying network. We propose a deep-learning based approach that is diffusion and influence-aware, called Influence-Graph News Recommender (IGNiteR). It is a content-based deep recommendation model that jointly exploits all the data facets that may impact adoption decisions, namely semantics, diffusion-related features pertaining to local and global influence among users, temporal attractiveness, and timeliness, as well as dynamic user preferences. To represent the news, a multi-level attention-based encoder is used to reveal the different interests of users. This news encoder relies on a CNN for the news content and on an attentive LSTM for the diffusion traces. For the latter, by exploiting previously observed news diffusions (cascades) in the microblogging medium, users are mapped to a latent space that captures potential influence on others or susceptibility of being influenced for news adoptions. Similarly, a time-sensitive user encoder enables us to capture the dynamic preferences of users with an attention-based bidirectional LSTM. We perform extensive experiments on two real-world datasets, showing that IGNiteR outperforms the state-of-the-art deep-learning based news recommendation methods.",
    "url": "http://arxiv.org/abs/2210.01942v1",
    "timestamp": 1664922838,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "035ea74d-8dbb-48d8-9107-073ca87d1af0": {
    "pk": "035ea74d-8dbb-48d8-9107-073ca87d1af0",
    "project_name": null,
    "authors": [
      "Junyi Liu"
    ],
    "title": "LFG: A Generative Network for Real-Time Recommendation",
    "abstract": "Recommender systems are essential information technologies today, and recommendation algorithms combined with deep learning have become a research hotspot in this field. The recommendation model known as LFM (Latent Factor Model), which captures latent features through matrix factorization and gradient descent to fit user preferences, has given rise to various recommendation algorithms that bring new improvements in recommendation accuracy. However, collaborative filtering recommendation models based on LFM lack flexibility and has shortcomings for real-time recommendations, as they need to redo the matrix factorization and retrain using gradient descent when new users arrive. In response to this, this paper innovatively proposes a Latent Factor Generator (LFG) network, and set the movie recommendation as research theme. The LFG dynamically generates user latent factors through deep neural networks without the need for re-factorization or retrain. Experimental results indicate that the LFG recommendation model outperforms traditional matrix factorization algorithms in recommendation accuracy, providing an effective solution to the challenges of real-time recommendations with LFM.",
    "url": "http://arxiv.org/abs/2310.20189v2",
    "timestamp": 1698729414,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "79779906-f7fd-456e-bd6a-8467ded06ca8": {
    "pk": "79779906-f7fd-456e-bd6a-8467ded06ca8",
    "project_name": null,
    "authors": [
      "Riccardo Cappuzzo",
      "Paolo Papotti",
      "Saravanan Thirumuruganathan"
    ],
    "title": "Local Embeddings for Relational Data Integration",
    "abstract": "Deep learning based techniques have been recently used with promising results for data integration problems. Some methods directly use pre-trained embeddings that were trained on a large corpus such as Wikipedia. However, they may not always be an appropriate choice for enterprise datasets with custom vocabulary. Other methods adapt techniques from natural language processing to obtain embeddings for the enterprise's relational data. However, this approach blindly treats a tuple as a sentence, thus losing a large amount of contextual information present in the tuple.   We propose algorithms for obtaining local embeddings that are effective for data integration tasks on relational databases. We make four major contributions. First, we describe a compact graph-based representation that allows the specification of a rich set of relationships inherent in the relational world. Second, we propose how to derive sentences from such a graph that effectively \"describe\" the similarity across elements (tokens, attributes, rows) in the two datasets. The embeddings are learned based on such sentences. Third, we propose effective optimization to improve the quality of the learned embeddings and the performance of integration tasks. Finally, we propose a diverse collection of criteria to evaluate relational embeddings and perform an extensive set of experiments validating them against multiple baseline methods. Our experiments show that our framework, EmbDI, produces meaningful results for data integration tasks such as schema matching and entity resolution both in supervised and unsupervised settings.",
    "url": "http://arxiv.org/abs/1909.01120v2",
    "timestamp": 1567514702,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.DB",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  },
  "56fdcc12-e45d-4cb7-b411-4f6b48a03f80": {
    "pk": "56fdcc12-e45d-4cb7-b411-4f6b48a03f80",
    "project_name": null,
    "authors": [
      "Lianghao Xia",
      "Chao Huang",
      "Yong Xu",
      "Peng Dai",
      "Mengyin Lu",
      "Liefeng Bo"
    ],
    "title": "Multi-Behavior Enhanced Recommendation with Cross-Interaction Collaborative Relation Modeling",
    "abstract": "Many previous studies aim to augment collaborative filtering with deep neural network techniques, so as to achieve better recommendation performance. However, most existing deep learning-based recommender systems are designed for modeling singular type of user-item interaction behavior, which can hardly distill the heterogeneous relations between user and item. In practical recommendation scenarios, there exist multityped user behaviors, such as browse and purchase. Due to the overlook of user's multi-behavioral patterns over different items, existing recommendation methods are insufficient to capture heterogeneous collaborative signals from user multi-behavior data. Inspired by the strength of graph neural networks for structured data modeling, this work proposes a Graph Neural Multi-Behavior Enhanced Recommendation (GNMR) framework which explicitly models the dependencies between different types of user-item interactions under a graph-based message passing architecture. GNMR devises a relation aggregation network to model interaction heterogeneity, and recursively performs embedding propagation between neighboring nodes over the user-item interaction graph. Experiments on real-world recommendation datasets show that our GNMR consistently outperforms state-of-the-art methods. The source code is available at https://github.com/akaxlh/GNMR.",
    "url": "http://arxiv.org/abs/2201.02307v1",
    "timestamp": 1641525157,
    "sections": null,
    "table_captions": null,
    "figure_captions": null,
    "bibliography": null,
    "keywords": null,
    "domain": "cs.IR",
    "references": null,
    "citation_count": 0,
    "award": null,
    "embed": null
  }
}
