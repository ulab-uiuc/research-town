{
  "868011a7-31f4-4e98-a9be-b4cec40411d0": {
    "pk": "868011a7-31f4-4e98-a9be-b4cec40411d0",
    "project_name": "research_town_demo",
    "eval_score": [],
    "content": "### Paper 1: Concept Bottleneck Models (CBMs)\n\n**Backgrounds:**\nConcept Bottleneck Models (CBMs) aim to address the 'black-box' nature of deep neural networks by mapping images to a human-understandable concept space before classification. Traditional CBMs require pre-selecting relevant concepts, which may not always be detectable. The paper introduces Discover-then-Name-CBM (DN-CBM), which inverts this paradigm by using sparse autoencoders to first discover concepts learned by the model and then naming them for classification. This approach is efficient and agnostic to the downstream task, leveraging concepts already known to the model.\n\n**Keywords:**\nConcept Bottleneck Models, Discover-then-Name-CBM, mechanistic interpretability, sparse autoencoders, human-understandable concepts, feature extraction, CLIP, interpretable models.\n\n**Insights:**\n- Investigate the application of DN-CBM in graph neural networks (GNNs) to enhance interpretability in graph-based tasks.\n- Explore the use of sparse autoencoders for discovering structural features in geometric graphs.\n- Apply the concept extraction strategy to temporal motifs in temporal graph neural networks (TGNNs) for better interpretability.\n- Extend the DN-CBM approach to hyperbolic graph convolutional neural networks (HGCNs) to discover and interpret hierarchical and scale-free graph structures.\n\n### Paper 2: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning. However, many methods use shared polynomial coefficients for all nodes, limiting flexibility. The paper introduces PolyAttn, a scalable node-wise filter leveraging the attention mechanism to learn node-wise filters efficiently. Building on PolyAttn, the model PolyFormer captures spectral information and maintains scalability, showing superior performance on both homophilic and heterophilic graphs, and handling large-scale graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, PolyAttn, PolyFormer, node-wise filters, attention mechanism, scalability, spectral information, homophilic graphs, heterophilic graphs.\n\n**Insights:**\n- Apply the PolyAttn mechanism to improve the feature-to-feature (Fea2Fea) prediction pipelines in geometric graphs.\n- Investigate the use of PolyFormer for enhancing the performance of Temporal Motifs Explainer (TempME) in temporal graph neural networks (TGNNs).\n- Explore the integration of PolyAtt"
  },
  "a04f3076-728a-4bd5-af5d-0bd6800aa243": {
    "pk": "a04f3076-728a-4bd5-af5d-0bd6800aa243",
    "project_name": "research_town_demo",
    "eval_score": [],
    "content": "### Paper 1: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning. Traditional methods often use shared polynomial coefficients for all nodes, limiting flexibility for node-level tasks. The recent DSF method attempts to address this by learning node-wise coefficients based on positional encoding, but it faces scalability issues. The proposed PolyAttn leverages the attention mechanism to learn node-wise filters efficiently, and the PolyFormer model builds on this to offer scalable and expressive graph representation learning. PolyFormer captures spectral information and maintains efficiency, excelling in both homophilic and heterophilic graphs, and can handle graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, scalability, graph representation learning, PolyAttn, PolyFormer, homophilic graphs, heterophilic graphs.\n\n**Insights:**\n- Investigate the application of PolyFormer in real-world large-scale networks, such as social media or biological networks.\n- Explore the integration of PolyFormer with other graph-based models to enhance performance in specific tasks like community detection or link prediction.\n- Study the impact of different attention mechanisms on the performance and scalability of node-wise filters in GNNs.\n- Develop methods to further optimize the initialization and updating process of positional encoding to enhance scalability.\n\n### Paper 2: SurvReLU: Bridging Deep Survival Models and Tree-based Survival Models\n\n**Backgrounds:**\nSurvival analysis models time-to-event distributions with censorship, and deep survival models using neural networks have recently dominated due to their representational power. However, their \"black-box\" nature hinders interpretability, which is crucial in real-world applications. Tree-based survival models offer better interpretability but struggle with convergence. The proposed SurvReLU bridges the gap by using deep rectified linear unit (ReLU) networks, combining the interpretability of tree-based structures with the representational power of deep survival models. Empirical studies show that SurvReLU is effective in terms of performance and interpretability.\n\n**Keywords:**\nSurvival analysis, deep survival models, tree-based survival models, interpretability, ReLU networks, SurvReLU, time-to-event distributions, censorship.\n\n**Insights:**\n- Apply SurvReLU to various real-world datasets to validate its effectiveness and interpretability in different domains, such as healthcare or finance.\n- Investigate the potential of combining SurvRe"
  },
  "bf014d10-9ded-4e06-9d92-3ebe88968e9f": {
    "pk": "bf014d10-9ded-4e06-9d92-3ebe88968e9f",
    "project_name": "research_town_demo",
    "eval_score": [],
    "content": "### Paper 1: Nonlinear Schr\u00f6dinger Network\n\n**Backgrounds:**\nThis paper introduces a novel physics-based AI model called the \"Nonlinear Schr\u00f6dinger Network\" (NLSE Network), which leverages the Nonlinear Schr\u00f6dinger Equation (NLSE) as a trainable model for learning complex patterns, including nonlinear mappings and memory effects from data. Unlike traditional physics-informed machine learning methods that use neural networks to approximate solutions of partial differential equations (PDEs), this approach directly treats the PDE as a trainable model. This method offers a more interpretable and parameter-efficient alternative to traditional black-box neural networks, achieving comparable or better accuracy in time series classification tasks while significantly reducing the number of required parameters. The parameters of the trained NLSE Network have physical meanings, providing insights into the underlying dynamics of the data transformation process.\n\n**Keywords:**\nNonlinear Schr\u00f6dinger Equation, physics-based AI, time series classification, interpretability, parameter efficiency, partial differential equations, nonlinear mappings, memory effects.\n\n**Insights:**\n- **Theoretical Exploration:** Investigate the theoretical properties of using PDEs as trainable models in machine learning, particularly focusing on their approximation and generalization capabilities.\n- **Extension to Other Equations:** Extend the approach to other master equations of physics to explore their potential in different machine learning tasks.\n- **Hybrid Models:** Develop hybrid models that combine the strengths of physics-based AI and traditional neural networks to enhance interpretability and performance.\n- **Application to GNNs:** Explore the integration of physics-based models with Graph Neural Networks (GNNs) to improve the representation and prediction tasks in graph-based data.\n\n### Paper 2: SurvReLU - Deep ReLU Networks for Survival Analysis\n\n**Backgrounds:**\nThis paper presents SurvReLU, a deep rectified linear unit (ReLU) network designed to bridge the gap between deep survival models and traditional tree-based survival models. Deep survival models, while powerful, suffer from a lack of interpretability, whereas tree-based models offer better interpretability but struggle with convergence. SurvReLU harnesses the interpretability of tree-based structures and the representational power of deep survival models. Empirical studies on both simulated and real survival benchmark datasets demonstrate the effectiveness of SurvReLU in terms of performance and interpretability.\n\n**Keywords:**\nSurvival analysis, deep ReLU networks, interpretability, tree-based models, time-to-event distributions, censorship, deep survival models.\n\n**Insights:**\n- **Interpretability in Neural Networks:** Further"
  }
}