{
  "75447599-dcd1-4833-a99e-c152638071e9": {
    "pk": "75447599-dcd1-4833-a99e-c152638071e9",
    "project_name": "research_town_demo",
    "content": "### Paper 1: Red-QAOA: Reducing Noise in Quantum Approximate Optimization Algorithm via Graph Reduction\n\n**Backgrounds:**\nThe Quantum Approximate Optimization Algorithm (QAOA) is used for combinatorial optimization by converting inputs into graphs. However, the process of finding optimal parameters in QAOA is highly sensitive to noise, especially for larger graphs that require more qubits. This paper introduces Red-QAOA, a method that uses simulated annealing-based graph reduction to create a smaller, distilled graph with nearly identical parameters to the original. This reduction results in a smaller quantum circuit, thereby reducing noise impact. Red-QAOA outperforms state-of-the-art GNN-based pooling techniques on 3200 real-world problems, significantly reducing node and edge counts while maintaining low mean square error.\n\n**Keywords:**\nQuantum Approximate Optimization Algorithm (QAOA), graph reduction, noise reduction, simulated annealing, quantum computing, combinatorial optimization, GNN-based pooling.\n\n**Insights:**\n- Investigate the application of Red-QAOA in other quantum algorithms and optimization problems.\n- Explore the integration of Red-QAOA with Position-aware Graph Neural Networks (P-GNNs) to enhance noise reduction in quantum computing tasks.\n- Study the potential of using Red-QAOA in dynamic graph scenarios, similar to the ROLAND framework, to handle real-time updates in quantum optimization problems.\n\n### Paper 2: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning, but many current methods use shared polynomial coefficients for all nodes, limiting flexibility for node-level tasks. This paper introduces PolyAttn, a scalable node-wise filter leveraging the attention mechanism to learn node-wise filters efficiently. Building on PolyAttn, the authors propose PolyFormer, a model that calculates attention scores within nodes, capturing spectral information while maintaining scalability. PolyFormer excels in learning arbitrary node-wise filters, showing superior performance on both homophilic and heterophilic graphs, and handling graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, scalability, PolyAttn, PolyFormer, graph representation learning, homophilic graphs, heterophilic graphs.\n\n**Insights:**\n- Explore the integration of PolyFormer with Identity-aware Graph Neural Networks (ID-GNNs) to enhance the expressiveness and scalability of node-wise filters.\n- Investigate the",
    "eval_score": []
  },
  "d6211246-f8ae-4049-accd-178701273540": {
    "pk": "d6211246-f8ae-4049-accd-178701273540",
    "project_name": "research_town_demo",
    "content": "### Paper 1: Red-QAOA: Reducing Noise in Quantum Approximate Optimization Algorithm via Graph Reduction\n\n**Backgrounds:**\nThe Quantum Approximate Optimization Algorithm (QAOA) is used for solving combinatorial optimization problems by converting inputs into graphs. However, the process of finding optimal parameters in QAOA is highly sensitive to noise, especially for larger graphs that require more qubits. This paper introduces Red-QAOA, a method that uses simulated annealing-based graph reduction to create a smaller, distilled graph with nearly identical parameters to the original. This reduction results in a smaller quantum circuit, thereby reducing noise impact. Red-QAOA then applies the parameters from the distilled graph back to the original graph for further optimization. The method outperforms state-of-the-art GNN-based pooling techniques, significantly reducing node and edge counts while maintaining low mean square error.\n\n**Keywords:**\nQuantum Approximate Optimization Algorithm (QAOA), graph reduction, noise reduction, simulated annealing, quantum computing, combinatorial optimization, GNN-based pooling.\n\n**Insights:**\n- Investigate the application of Red-QAOA in other quantum algorithms and optimization problems.\n- Explore the integration of Red-QAOA with Position-aware Graph Neural Networks (P-GNNs) to enhance noise reduction in quantum computing tasks.\n- Study the potential of using Red-QAOA for dynamic graphs and real-time optimization problems.\n- Develop new methods for parameter optimization in quantum algorithms that leverage graph reduction techniques.\n\n### Paper 2: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning, but many current methods use shared polynomial coefficients for all nodes, limiting flexibility for node-level tasks. This paper introduces PolyAttn, a scalable node-wise filter that leverages the attention mechanism to learn node-wise filters efficiently. Building on PolyAttn, the authors propose PolyFormer, a model that calculates attention scores within nodes, capturing spectral information while maintaining scalability. PolyFormer excels at learning arbitrary node-wise filters and shows superior performance on both homophilic and heterophilic graphs, handling graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, PolyAttn, PolyFormer, scalability, graph representation learning, homophilic graphs, heterophilic graphs.\n\n**Insights:**\n- Explore the integration of PolyFormer with Identity-aware Graph Neural Networks (ID-GNNs",
    "eval_score": []
  },
  "3ea112ee-e7c1-4dd9-a699-e06984495b30": {
    "pk": "3ea112ee-e7c1-4dd9-a699-e06984495b30",
    "project_name": "research_town_demo",
    "content": "### Paper 1: Concept Bottleneck Models (CBMs)\n\n**Backgrounds:**\nConcept Bottleneck Models (CBMs) aim to address the 'black-box' nature of deep neural networks by mapping images to a human-understandable concept space before classification. Traditional CBMs require pre-selecting relevant concepts, which may not always be detectable. The paper introduces Discover-then-Name-CBM (DN-CBM), which inverts this paradigm by using sparse autoencoders to first discover concepts and then name them. This method is efficient and agnostic to the downstream task, leveraging concepts already known to the model.\n\n**Keywords:**\nConcept Bottleneck Models, CBMs, Discover-then-Name-CBM, sparse autoencoders, mechanistic interpretability, human-understandable concepts, feature extraction, CLIP, linear probes, interpretability.\n\n**Insights:**\n- Investigate the application of DN-CBM in graph neural networks (GNNs) to enhance interpretability in graph-based tasks.\n- Explore the use of sparse autoencoders for discovering structural features in geometric graphs.\n- Apply the concept extraction strategy to temporal motifs in temporal graph neural networks (TGNNs) for better interpretability.\n- Extend the DN-CBM approach to hyperbolic graph convolutional neural networks (HGCNs) to discover and interpret hierarchical and scale-free graph structures.\n\n### Paper 2: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning. However, many methods use shared polynomial coefficients for all nodes, limiting flexibility. The paper introduces PolyAttn, a scalable node-wise filter leveraging the attention mechanism to learn node-wise filters efficiently. Building on PolyAttn, the PolyFormer model captures spectral information and offers scalability and expressiveness for node-level tasks, handling graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, PolyAttn, PolyFormer, node-wise filters, attention mechanism, scalability, spectral information, graph representation learning, homophilic graphs, heterophilic graphs.\n\n**Insights:**\n- Apply the PolyAttn mechanism to improve the feature-to-feature (Fea2Fea) prediction pipelines in geometric graphs.\n- Investigate the use of PolyFormer in temporal graph neural networks (TGNNs) to enhance the learning of temporal motifs.\n- Explore the integration of PolyAttn with hyperbolic graph convolutional neural networks (HGC",
    "eval_score": []
  },
  "d9ec967d-5c4d-4560-8580-67a9b56f46c5": {
    "pk": "d9ec967d-5c4d-4560-8580-67a9b56f46c5",
    "project_name": "research_town_demo",
    "content": "### Paper 1: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning. Traditional methods often use shared polynomial coefficients for all nodes, limiting flexibility for node-level tasks. The recent DSF method attempts to address this by learning node-wise coefficients based on positional encoding, but it faces scalability issues. The proposed PolyAttn leverages the attention mechanism to learn node-wise filters efficiently, and the PolyFormer model builds on this to offer scalable and expressive node-level task performance. PolyFormer captures spectral information and maintains efficiency, handling graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, scalability, PolyAttn, PolyFormer, graph representation learning, homophilic graphs, heterophilic graphs.\n\n**Insights:**\n- Investigate the application of PolyFormer in different types of large-scale networks, such as social networks or biological networks.\n- Explore the integration of PolyFormer with other graph neural network architectures to enhance performance on specific tasks.\n- Study the impact of different attention mechanisms on the scalability and expressiveness of node-wise filters in GNNs.\n- Develop methods to further improve the efficiency of PolyFormer for real-time applications in large-scale networks.\n\n### Paper 2: SurvReLU: Bridging Deep Survival Models and Tree-based Survival Models\n\n**Backgrounds:**\nSurvival analysis models time-to-event distributions with censorship, and deep survival models using neural networks have recently dominated due to their representational power. However, their \"black-box\" nature hinders interpretability. Tree-based survival models offer better interpretability but struggle with convergence. The proposed SurvReLU bridges the gap by using deep rectified linear unit (ReLU) networks, combining the interpretability of tree-based structures with the representational power of deep survival models. Empirical studies show that SurvReLU is effective in terms of performance and interpretability.\n\n**Keywords:**\nSurvival analysis, deep survival models, tree-based survival models, interpretability, ReLU networks, SurvReLU, time-to-event distributions, censorship.\n\n**Insights:**\n- Investigate the application of SurvReLU in different domains, such as healthcare or finance, where interpretability is crucial.\n- Explore the integration of SurvReLU with other machine learning models to enhance performance and interpretability.\n- Study the impact of different network architectures on the performance and interpretability",
    "eval_score": []
  }
}
