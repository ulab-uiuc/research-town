{
  "c0b48838-2026-4ff9-9615-296bfba66b95": {
    "pk": "c0b48838-2026-4ff9-9615-296bfba66b95",
    "project_name": "research_town_demo",
    "content": "### Paper 1: Red-QAOA: Reducing Noise in Quantum Approximate Optimization Algorithm via Graph Reduction\n\n**Backgrounds:**\nThe Quantum Approximate Optimization Algorithm (QAOA) is used for combinatorial optimization by converting inputs into graphs. However, the parameter search process in QAOA is highly sensitive to noise, especially for larger graphs requiring more qubits. This paper introduces Red-QAOA, which uses a simulated annealing-based graph reduction technique to create a smaller, distilled graph with nearly identical parameters to the original. This reduction minimizes noise impact and improves optimization outcomes. Red-QAOA outperforms state-of-the-art GNN-based pooling techniques, reducing node and edge counts significantly while maintaining low mean square error.\n\n**Keywords:**\nQuantum Approximate Optimization Algorithm (QAOA), graph reduction, noise sensitivity, simulated annealing, GNN-based pooling, combinatorial optimization.\n\n**Insights:**\n- Investigate the application of Red-QAOA techniques to other quantum algorithms and their potential to reduce noise in different quantum computing contexts.\n- Explore the integration of Red-QAOA with Position-aware Graph Neural Networks (P-GNNs) to enhance the performance of GNNs in noisy environments.\n- Study the impact of graph reduction techniques on the scalability and efficiency of Identity-aware Graph Neural Networks (ID-GNNs) and other GNN architectures.\n\n### Paper 2: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning, but many methods use shared polynomial coefficients for all nodes, limiting flexibility. This paper introduces PolyAttn, a scalable node-wise filter leveraging the attention mechanism to learn node-wise filters efficiently. Building on PolyAttn, the authors propose PolyFormer, a model that calculates attention scores within nodes, capturing spectral information while maintaining scalability. PolyFormer excels in learning arbitrary node-wise filters and performs well on both homophilic and heterophilic graphs, handling graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, PolyAttn, PolyFormer, scalability, graph representation learning.\n\n**Insights:**\n- Investigate the integration of PolyFormer with Position-aware Graph Neural Networks (P-GNNs) to enhance node embedding techniques by leveraging node-wise filters.\n- Explore the potential of using PolyFormer in dynamic graph settings, such as those addressed by ROLAND, to improve scalability and expressiveness",
    "eval_score": []
  },
  "e2edc1f0-7d32-49e8-9a80-67f19962705b": {
    "pk": "e2edc1f0-7d32-49e8-9a80-67f19962705b",
    "project_name": "research_town_demo",
    "content": "### Paper 1: Red-QAOA: Reducing Noise in Quantum Approximate Optimization Algorithm via Graph Reduction\n\n**Backgrounds:**\nThe Quantum Approximate Optimization Algorithm (QAOA) is used for combinatorial optimization by converting inputs into graphs. However, the parameter search process in QAOA is highly sensitive to noise, especially for larger graphs requiring more qubits. This paper introduces Red-QAOA, which uses a simulated annealing-based graph reduction technique to create a smaller, distilled graph with nearly identical parameters to the original. This reduction minimizes noise impact and improves optimization outcomes. Red-QAOA outperforms state-of-the-art GNN-based pooling techniques, reducing node and edge counts significantly while maintaining low mean square error.\n\n**Keywords:**\nQuantum Approximate Optimization Algorithm (QAOA), graph reduction, noise sensitivity, simulated annealing, GNN-based pooling, combinatorial optimization.\n\n**Insights:**\n- Investigate the application of Red-QAOA techniques to other quantum algorithms and their potential noise reduction benefits.\n- Explore the integration of Red-QAOA with Position-aware Graph Neural Networks (P-GNNs) to enhance performance in noisy environments.\n- Study the impact of graph reduction techniques on the scalability and efficiency of Identity-aware Graph Neural Networks (ID-GNNs) and other GNN architectures.\n- Develop new methods for parameter optimization in quantum algorithms using graph-based techniques.\n\n### Paper 2: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning, but current methods often use shared polynomial coefficients for all nodes, limiting flexibility. This paper introduces PolyAttn, a scalable node-wise filter leveraging the attention mechanism to learn node-wise filters efficiently. Building on PolyAttn, the authors propose PolyFormer, a model that calculates attention scores within nodes, capturing spectral information while maintaining scalability. PolyFormer excels in learning arbitrary node-wise filters and demonstrates superior performance on both homophilic and heterophilic graphs, handling graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, scalability, PolyAttn, PolyFormer, graph representation learning.\n\n**Insights:**\n- Investigate the integration of PolyFormer with Position-aware Graph Neural Networks (P-GNNs) to enhance node embedding accuracy.\n- Explore the application of PolyFormer in dynamic graph settings using the ROLAND framework to improve real-world dynamic graph predictions.\n-",
    "eval_score": []
  },
  "53eb285b-732a-4206-916c-b3e954ac47c8": {
    "pk": "53eb285b-732a-4206-916c-b3e954ac47c8",
    "project_name": "research_town_demo",
    "content": "### Paper 1: Discover-then-Name Concept Bottleneck Models (DN-CBM)\n\n**Backgrounds:**\nConcept Bottleneck Models (CBMs) aim to make deep neural networks more interpretable by mapping images to a human-understandable concept space before classification. Traditional CBMs require pre-selecting relevant concepts, which may not always be detectable. The Discover-then-Name CBM (DN-CBM) approach inverts this paradigm by using sparse autoencoders to first discover concepts already known to the model, then naming them and training linear probes for classification. This method is efficient and agnostic to the downstream task, yielding semantically meaningful and interpretable concepts.\n\n**Keywords:**\nConcept Bottleneck Models, interpretability, sparse autoencoders, concept extraction, CLIP, linear probes, human-understandable concepts.\n\n**Insights:**\n- Investigate the application of DN-CBM in graph neural networks (GNNs) to enhance interpretability in graph-based tasks.\n- Explore the use of sparse autoencoders for discovering structural features in geometric graphs.\n- Apply the concept extraction strategy to temporal motifs in temporal graph neural networks (TGNNs) for better interpretability.\n- Extend the DN-CBM approach to hyperbolic graph convolutional neural networks (HGCNs) to discover and interpret hierarchical and scale-free graph structures.\n\n### Paper 2: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning. However, many methods use shared polynomial coefficients for all nodes, limiting flexibility for node-level tasks. PolyAttn, a scalable node-wise filter leveraging the attention mechanism, addresses this by learning node-wise filters efficiently. PolyFormer, built on PolyAttn, captures spectral information and maintains scalability, showing superior performance on both homophilic and heterophilic graphs, and handling large-scale graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, PolyAttn, PolyFormer, scalability, spectral information, homophilic graphs, heterophilic graphs.\n\n**Insights:**\n- Apply the PolyAttn mechanism to improve the classification accuracy of structural feature correlation analysis in geometric graphs.\n- Investigate the use of PolyFormer in temporal graph neural networks (TGNNs) to enhance the scalability and expressiveness of temporal motifs.\n- Explore the integration of PolyAttn with hyperbolic graph convolutional neural networks",
    "eval_score": []
  },
  "19251335-e229-48e2-9e5b-5a113f4f96a4": {
    "pk": "19251335-e229-48e2-9e5b-5a113f4f96a4",
    "project_name": "research_town_demo",
    "content": "### Paper 1: PolyFormer: Scalable Node-wise Filters for Spectral Graph Neural Networks\n\n**Backgrounds:**\nSpectral Graph Neural Networks (GNNs) have shown superior performance in graph representation learning. Traditional methods often use shared polynomial coefficients for all nodes, limiting flexibility for node-level tasks. The recent DSF method attempts to address this by learning node-wise coefficients based on positional encoding, but it faces scalability issues. The proposed PolyAttn leverages the attention mechanism to learn node-wise filters efficiently, and the PolyFormer model builds on this to offer scalable and expressive graph representation capabilities. PolyFormer captures spectral information and maintains efficiency, excelling in both homophilic and heterophilic graphs, and can handle graphs with up to 100 million nodes.\n\n**Keywords:**\nSpectral Graph Neural Networks, node-wise filters, attention mechanism, scalability, graph representation learning, homophilic graphs, heterophilic graphs, PolyAttn, PolyFormer.\n\n**Insights:**\n- Investigate the application of PolyFormer in real-world large-scale networks, such as social media or biological networks.\n- Explore the integration of PolyFormer with other graph-based tasks, such as community detection or link prediction.\n- Study the impact of different attention mechanisms on the performance and scalability of node-wise filters in GNNs.\n- Develop methods to further enhance the efficiency and scalability of PolyFormer for even larger graphs.\n\n### Paper 2: SurvReLU: Bridging Deep Survival Models and Tree-based Survival Models\n\n**Backgrounds:**\nSurvival analysis models time-to-event distributions with censorship, and deep survival models using neural networks have recently dominated due to their representational power. However, their \"black-box\" nature hinders interpretability, which is crucial in real-world applications. Tree-based survival models offer better interpretability but struggle with convergence. The proposed SurvReLU model bridges the gap by using deep rectified linear unit (ReLU) networks, combining the interpretability of tree-based structures with the representational power of deep survival models. Empirical studies show that SurvReLU is effective in terms of performance and interpretability.\n\n**Keywords:**\nSurvival analysis, deep survival models, tree-based survival models, interpretability, ReLU networks, SurvReLU, time-to-event distributions, censorship.\n\n**Insights:**\n- Apply SurvReLU to various real-world datasets to validate its performance and interpretability in different domains, such as healthcare or finance.\n- Investigate the potential of combining SurvReLU with other",
    "eval_score": []
  }
}
