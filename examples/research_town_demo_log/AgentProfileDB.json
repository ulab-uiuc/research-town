{
  "92d38108-c048-4133-8d27-09bef7c21d3d": {
    "pk": "92d38108-c048-4133-8d27-09bef7c21d3d",
    "name": "Jiaxuan You",
    "bio": " I am a researcher focused on developing and improving graph neural networks (GNNs) for various prediction tasks on graphs. I have proposed Position-aware Graph Neural Networks (P-GNNs), a new class of GNNs that computes position-aware node embeddings by sampling sets of anchor nodes and learning a non-linear distance-weighted aggregation scheme over the anchor-sets. This allows P-GNNs to capture the positions/locations of nodes with respect to the anchor nodes, and they have been shown to consistently outperform state-of-the-art GNNs on multiple prediction tasks.\n\nI have also investigated how the graph structure of neural networks affects their predictive performance by developing a novel graph-based representation of neural networks called relational graph. I have found that a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance, and that the neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph. My findings are consistent across many different tasks and datasets, and the sweet spot can be identified efficiently.\n\nAdditionally, I have developed a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test. ID-GNN offers a minimal but powerful solution to limitations of existing GNNs by inductively considering nodes' identities during message passing. I have also proposed ROLAND, an effective graph representation learning framework for real-world dynamic graphs, which can help researchers easily repurpose any static GNN to dynamic graphs and introduces a live-update evaluation setting for dynamic graphs that mimics real-world use cases.\n\nI have also studied the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks and defined a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture. I have released GraphGym, a powerful platform for exploring different GNN designs and tasks.\n\nI have also proposed FALCON, an efficient sample-based method to search for the optimal model design, which models the design space of possible model designs as a design",
    "collaborators": [
      "Jure Leskovec",
      "Rex Ying",
      "Kaidi Cao",
      "Kaiming He",
      "Saining Xie",
      "Jonathan Gomes-Selman",
      "Tianyu Du",
      "Shirley Wu",
      "Jiaju Liu"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": false,
    "is_proj_participant_candidate": true,
    "is_reviewer_candidate": false,
    "is_chair_candidate": false
  },
  "236230f8-8f9c-41ee-ac28-308ce7398697": {
    "pk": "236230f8-8f9c-41ee-ac28-308ce7398697",
    "name": "Jure Leskovec",
    "bio": " I am a researcher focused on the analysis and modeling of large-scale networks, with a particular interest in networks with rich attribute information. I have worked on developing and analyzing various models for such networks, including the Multiplicative Attribute Graph (MAG) model and the Latent Multi-group Membership Graph (LMMG) model. The MAG model is a model of networks where nodes have categorical attributes, and the probability of an edge between two nodes is modeled as the product of individual attribute-attribute affinities. This model is scalable and can reliably capture network connectivity, as well as provide insights into how different attributes shape the network structure. The LMMG model, on the other hand, is a model of networks with rich node feature structure, where each node belongs to multiple groups and each latent group models the occurrence of links as well as the node feature structure. This model can be used for summarizing network structure, predicting links between nodes, and predicting missing features of a node.\n\nIn addition to model development and analysis, I have also worked on applying network analysis methods to various real-world datasets. For example, I have studied large-scale image retrieval benchmarks, such as those derived from online photo sharing networks, and extended them with social-network metadata, such as the groups to which each image belongs, the comment thread associated with the image, who uploaded it, their location, and their network of friends. I have also studied the problem of automatically identifying users' social circles in personal social networks, which are often big and cluttered. I posed this task as a multi-membership node clustering problem on a user's ego-network and developed a model for detecting circles that combines network structure as well as user profile information.\n\nMost recently, I have been studying the relationship between Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) in terms of feature/label smoothing and influence. Based on this theoretical analysis, I proposed an end-to-end model that unifies GCN and LPA for node classification. This model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models.\n\nOverall, my research is focused on understanding and modeling the structure and behavior of large-scale networks, with a particular focus on networks with rich attribute information. I am",
    "collaborators": [
      "Myunghwan Kim",
      "Julian McAuley",
      "Eric Horvitz",
      "Tim Althoff",
      "Hongwei Wang"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": true,
    "is_proj_participant_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "78acfc74-95a2-4e93-a8d9-ae234a696d7c": {
    "pk": "78acfc74-95a2-4e93-a8d9-ae234a696d7c",
    "name": "Stefanie Jegelka",
    "bio": " I am a researcher with a focus on machine learning and artificial intelligence. In particular, I have been exploring various topics related to neural networks, graph neural networks (GNNs), and optimization.\n\nIn the area of GNNs, I have been investigating their theoretical properties, including approximation and learning properties of message passing GNNs and higher-order GNNs. My work has highlighted mathematical connections and summarized results on representation, generalization, and extrapolation.\n\nI have also been studying the use of ResNets as universal approximators, demonstrating that they can uniformly approximate any Lebesgue integrable function in $d$ dimensions. This result has implications for the representational power of narrow deep networks, showing that they can be more powerful than fully connected networks with the same width.\n\nIn optimization, I have been examining the problem of budget allocation and bipartite influence maximization from a robust optimization perspective. I have shown that this nonconvex problem can be solved exactly by leveraging connections to continuous submodular functions and solving a constrained submodular minimization problem.\n\nIn the context of Bayesian optimization, I have proposed a new criterion called Max-value Entropy Search (MES) that uses information about the maximum function value instead of entropy. This approach has been shown to maintain or improve the good empirical performance of existing methods while significantly reducing the computational burden.\n\nI have also been studying distributionally robust optimization (DRO) with uncertainty sets measured via maximum mean discrepancy (MMD). I have shown that MMD DRO is roughly equivalent to regularization by the Hilbert norm, revealing deep connections to classic results in statistical learning.\n\nIn addition, I have been exploring the implicit bias induced by the training of neural networks. I have shown that when training a deep linear network with logistic or exponential loss on linearly separable data, the weights converge to rank-1 matrices. I have extended this result to the last few linear layers of nonlinear ReLU-activated feedforward networks containing fully-connected layers and skip connections.\n\nFinally, I have been studying the theoretical foundations of learning with invariances. I have provided minimax optimal rates for kernel ridge regression on compact manifolds, with a target function that is invariant to a group action on the manifold. My work has also highlighted the benefits of taking a differential geometric viewpoint when studying",
    "collaborators": [
      "Matthew Staib",
      "Thien Le",
      "Behrooz Tahmasebi",
      "Chengtao Li",
      "Suvrit Sra",
      "Hongzhou Lin",
      "Zi Wang"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": false,
    "is_proj_participant_candidate": false,
    "is_reviewer_candidate": false,
    "is_chair_candidate": true
  },
  "b34c5ea9-d19d-4016-a6cb-eda28e4f9df8": {
    "pk": "b34c5ea9-d19d-4016-a6cb-eda28e4f9df8",
    "name": "Silvio Lattanzi",
    "bio": " I am a researcher in the field of computer science, specializing in algorithms and data analysis. In my work, I focus on developing efficient and practical solutions to challenging problems related to social networks, clustering, and graph analysis.\n\nOne of my main areas of expertise is reconciling online social networks, which involves identifying all the accounts belonging to the same individual across different online networks. This problem has both theoretical and practical significance, as it can provide a richer understanding of social dynamics and have applications in areas such as targeted advertising and fraud detection. I have made significant contributions to this field by formally defining the problem and designing a simple, local, and efficient parallel algorithm to solve it. I have also proven strong theoretical guarantees on the algorithm's performance and confirmed its effectiveness through experiments on synthetic and real social network data sets.\n\nIn addition to my work on social networks, I am also interested in clustering algorithms and their applications. I have developed a generalization and extension of the $k$-means++ algorithm, which is a popular choice for optimizing the $k$-means clustering objective. My algorithm achieves a $9 + \\varepsilon$ approximation ratio, which is the best possible for local search, and has been shown to yield substantial practical improvements over previous approaches.\n\nI have also studied the problem of estimating the common mean of independent symmetric random variables with different and unknown standard deviations. I have proposed a fully adaptive estimator that is invariant to permutations of the elements of the sample and satisfies strong theoretical guarantees.\n\nFurthermore, I have considered the classic facility location problem in fully dynamic data streams, where elements can be both inserted and deleted. I have developed an algorithm that maintains a constant approximation and incurs polylogarithmic amortized recourse per update, and have complemented my theoretical results with an experimental analysis showing the practical efficiency of my method.\n\nOverall, my research is focused on developing efficient and practical algorithms for solving challenging problems in social networks, clustering, and graph analysis. I am committed to rigorous theoretical analysis and empirical evaluation, and strive to make contributions that have both theoretical and practical significance.",
    "collaborators": [
      "Nikos Parotsidis",
      "Michael Kapralov",
      "Nitish Korula",
      "Lorenzo Beretta",
      "Vincent Cohen-Addad",
      "Hossein Esfandiari",
      "Vahab Mirrokni",
      "Luc Devroye",
      "Gabor Lugosi",
      "Nikita Zhivotovskiy"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": true,
    "is_proj_participant_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "8cca8c25-28c9-4904-b079-a1f8cc2abd92": {
    "pk": "8cca8c25-28c9-4904-b079-a1f8cc2abd92",
    "name": "Rex Ying",
    "bio": " I am a researcher with a focus on graph neural networks and their applications in various fields. I have worked on several projects that involve analyzing and extracting meaningful information from graphs.\n\nIn one project, I explored the structural features of geometric graphs and their correlation using graph neural networks. I introduced graph feature to feature (Fea2Fea) prediction pipelines in a low dimensional space and showed that there exists high correlation between some of the structural features. I also demonstrated that an irredundant feature combination with initial node features, filtered by graph neural network, improved its classification accuracy in some graph-based tasks.\n\nIn another project, I addressed the challenge of explainability and trustworthiness in temporal graph neural networks (TGNNs). I proposed a novel approach called Temporal Motifs Explainer (TempME) which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation.\n\nI have also worked on extending GCNs to hyperbolic geometry, which presents several unique challenges. I proposed Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs.\n\nAdditionally, I have worked on the problem of learning node embeddings that capture a node's position within the broader graph structure. I proposed Position-aware Graph Neural Networks (P-GNNs), a new class of GNNs for computing position-aware node embeddings. P-GNNs can capture positions/locations of nodes with respect to the anchor nodes, and have several advantages such as being inductive, scalable, and able to incorporate node feature information.\n\nI am also interested in the problem of encoding long sequences in Natural Language Processing (NLP) and proposed a graph-based method that alleviates long dependency issues. I evaluated the model on six datasets with up to 53k samples and 4034 average tokens' length and showed that it surpasses competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence dataset.",
    "collaborators": [
      "Jure Leskovec",
      "Jialin Chen",
      "William L. Hamilton",
      "Jiaqing Xie",
      "Ines Chami",
      "Christopher R\u00e9",
      "Jiaxuan You",
      "Irene Li",
      "Aosong Feng",
      "Dragomir Radev"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": false,
    "is_proj_participant_candidate": false,
    "is_reviewer_candidate": true,
    "is_chair_candidate": false
  }
}
