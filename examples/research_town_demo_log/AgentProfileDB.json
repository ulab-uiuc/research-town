{
  "a6cc9685-bab5-4341-a82c-bcb06abe9994": {
    "pk": "a6cc9685-bab5-4341-a82c-bcb06abe9994",
    "name": "Jiaxuan You",
    "bio": " I am a researcher focused on developing and improving graph neural networks (GNNs) for various prediction tasks on graphs. I have proposed Position-aware Graph Neural Networks (P-GNNs), a new class of GNNs that computes position-aware node embeddings by sampling sets of anchor nodes and learning a non-linear distance-weighted aggregation scheme over the anchor-sets. This allows P-GNNs to capture the positions/locations of nodes with respect to the anchor nodes, and they have been shown to consistently outperform state-of-the-art GNNs on multiple prediction tasks.\n\nI have also investigated how the graph structure of neural networks affects their predictive performance by developing a novel graph-based representation of neural networks called relational graph. I have found that a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance, and that the neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph.\n\nAdditionally, I have developed a class of message passing GNNs, named Identity-aware Graph Neural Networks (ID-GNNs), with greater expressive power than the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test. ID-GNNs extend existing GNN architectures by inductively considering nodes' identities during message passing, and experiments show that transforming existing GNNs to ID-GNNs yields on average 40% accuracy improvement on challenging node, edge, and graph property prediction tasks.\n\nI have also proposed ROLAND, an effective graph representation learning framework for real-world dynamic graphs, which can help researchers easily repurpose any static GNN to dynamic graphs, and introduced a live-update evaluation setting for dynamic graphs that mimics real-world use cases.\n\nFurthermore, I have defined and systematically studied the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks, and released GraphGym, a powerful platform for exploring different GNN designs and tasks.\n\nRecently, I have proposed AutoTransfer, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest, which significantly improves search efficiency with the transferred design priors,",
    "collaborators": [
      "Jure Leskovec",
      "Rex Ying",
      "Kaidi Cao",
      "Kaiming He",
      "Saining Xie",
      "Jonathan Gomes-Selman",
      "Tianyu Du",
      "Shirley Wu",
      "Jiaju Liu"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": false,
    "is_proj_participant_candidate": true,
    "is_reviewer_candidate": false,
    "is_chair_candidate": false
  },
  "fd8def3c-58ad-4028-8f43-69668d44aa4c": {
    "pk": "fd8def3c-58ad-4028-8f43-69668d44aa4c",
    "name": "Jure Leskovec",
    "bio": " I am a researcher focused on the analysis and modeling of large-scale networks, with a particular interest in networks with rich attribute information. I have worked on developing and analyzing various models for such networks, including the Multiplicative Attribute Graph (MAG) model and the Latent Multi-group Membership Graph (LMMG) model. The MAG model is a model of networks where nodes have categorical attributes, and the probability of an edge between two nodes is modeled as the product of individual attribute-attribute affinities. This model is scalable and can reliably capture network connectivity, as well as provide insights into how different attributes shape the network structure. The LMMG model, on the other hand, is a model of networks with rich node feature structure, where each node belongs to multiple groups and each latent group models the occurrence of links as well as the node feature structure. This model can be used for summarizing network structure, predicting links between nodes, and predicting missing features of a node.\n\nIn addition to model development and analysis, I have also worked on applying network analysis methods to various real-world datasets. For example, I have studied large-scale image retrieval benchmarks, such as those derived from online photo sharing networks, and extended them with social-network metadata, such as the groups to which each image belongs, the comment thread associated with the image, who uploaded it, their location, and their network of friends. I have also studied the problem of automatically identifying users' social circles in personal social networks, which are often big and cluttered. I posed this task as a multi-membership node clustering problem on a user's ego-network and developed a model for detecting circles that combines network structure as well as user profile information.\n\nMost recently, I have been studying the relationship between Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) in terms of feature/label smoothing and influence. Based on this theoretical analysis, I proposed an end-to-end model that unifies GCN and LPA for node classification. This model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models.\n\nOverall, my research is focused on the development and application of network analysis methods, with a particular focus on networks with rich attribute information. I am interested in understanding the",
    "collaborators": [
      "Myunghwan Kim",
      "Julian McAuley",
      "Eric Horvitz",
      "Tim Althoff",
      "Hongwei Wang"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": true,
    "is_proj_participant_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "a4465f9e-7599-4a56-b5c1-269d61710c41": {
    "pk": "a4465f9e-7599-4a56-b5c1-269d61710c41",
    "name": "Stefanie Jegelka",
    "bio": " I am a researcher with a focus on machine learning and artificial intelligence. In particular, I have been exploring various topics related to neural networks, graph neural networks (GNNs), and optimization.\n\nIn the area of GNNs, I have been investigating their theoretical properties, including approximation and learning properties of message passing GNNs and higher-order GNNs. My work has uncovered mathematical connections and summarized results on representation, generalization, and extrapolation.\n\nI have also been studying the use of ResNets as universal approximators, demonstrating that a very deep ResNet with stacked modules can uniformly approximate any Lebesgue integrable function in a given number of dimensions. This result highlights the increased representational power of narrow deep networks by the ResNet architecture.\n\nIn optimization, I have been examining the problem of optimal resource allocation for maximizing influence, spread of information, or coverage. I have approached this problem from a robust optimization perspective, considering an adversary who may choose the least favorable parameters within a confidence set. This has led to the study of nonconvex-concave saddle point problems and their connections to continuous submodular functions.\n\nAdditionally, I have proposed a new criterion for Bayesian optimization, Max-value Entropy Search (MES), which uses information about the maximum function value instead of entropies. This criterion has been shown to maintain or improve the good empirical performance of existing methods while significantly reducing the computational burden.\n\nIn the study of distributionally robust optimization (DRO), I have explored the use of uncertainty sets measured via maximum mean discrepancy (MMD). I have shown that MMD DRO is roughly equivalent to regularization by the Hilbert norm, revealing deep connections to classic results in statistical learning.\n\nFinally, I have been investigating the implicit bias induced by the training of neural networks, specifically in the context of deep linear networks and nonlinear ReLU-activated feedforward networks. I have shown that the weights of these networks converge to rank-1 matrices in the limit of gradient flow and gradient descent with appropriate step size.\n\nOverall, my research combines theoretical analysis with practical applications, contributing to the development of more efficient and robust machine learning algorithms and models.",
    "collaborators": [
      "Matthew Staib",
      "Thien Le",
      "Behrooz Tahmasebi",
      "Chengtao Li",
      "Suvrit Sra",
      "Hongzhou Lin",
      "Zi Wang"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": false,
    "is_proj_participant_candidate": false,
    "is_reviewer_candidate": false,
    "is_chair_candidate": true
  },
  "555e5c39-f22d-40dc-a3d9-314784423407": {
    "pk": "555e5c39-f22d-40dc-a3d9-314784423407",
    "name": "Silvio Lattanzi",
    "bio": " I am a researcher in the field of computer science, specializing in algorithms and data analysis. In my work, I focus on developing efficient and practical solutions to challenging problems related to social networks, clustering, and graph analysis.\n\nOne of my main areas of expertise is reconciling online social networks, which involves identifying all the accounts belonging to the same individual across different online networks. This problem has both theoretical and practical significance, as it can provide a richer understanding of social dynamics and have applications in areas such as targeted advertising and fraud detection. I have made significant contributions to this field by formally defining the problem and designing a simple, local, and efficient parallel algorithm to solve it. I have also proven strong theoretical guarantees on the algorithm's performance and confirmed its effectiveness through experiments on synthetic and real social network data sets.\n\nIn addition to my work on social networks, I am also interested in clustering algorithms and their applications. I have developed a generalization and extension of the $k$-means++ algorithm, which is a popular choice for optimizing the $k$-means clustering objective. My algorithm achieves a $9 + \\varepsilon$ approximation ratio, which is the best possible for local search, and has been shown to yield substantial practical improvements over previous approaches.\n\nI have also studied the problem of estimating the common mean of independent symmetric random variables with different and unknown standard deviations. I have proposed a fully adaptive estimator that is invariant to permutations of the elements of the sample and satisfies strong theoretical guarantees.\n\nFurthermore, I have considered the classic facility location problem in fully dynamic data streams, where elements can be both inserted and deleted. I have developed an algorithm that maintains a constant approximation and incurs polylogarithmic amortized recourse per update, and have complemented my theoretical results with an experimental analysis showing the practical efficiency of my method.\n\nOverall, my research is focused on developing efficient and practical algorithms for solving challenging problems in social networks, clustering, and graph analysis. I am committed to rigorous theoretical analysis and empirical evaluation, and strive to make contributions that have both theoretical and practical significance.",
    "collaborators": [
      "Nikos Parotsidis",
      "Michael Kapralov",
      "Nitish Korula",
      "Lorenzo Beretta",
      "Vincent Cohen-Addad",
      "Hossein Esfandiari",
      "Vahab Mirrokni",
      "Luc Devroye",
      "Gabor Lugosi",
      "Nikita Zhivotovskiy"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": true,
    "is_proj_participant_candidate": true,
    "is_reviewer_candidate": true,
    "is_chair_candidate": true
  },
  "297964ca-d935-452a-9324-2ab09407d95f": {
    "pk": "297964ca-d935-452a-9324-2ab09407d95f",
    "name": "Rex Ying",
    "bio": " I am a researcher with a focus on graph neural networks and their applications in various fields. I have worked on several projects that involve analyzing and extracting meaningful information from graphs.\n\nIn one project, I explored the structural features of geometric graphs and their correlation using graph neural networks. I introduced graph feature to feature (Fea2Fea) prediction pipelines in a low dimensional space and showed that there exists high correlation between some of the structural features. I also demonstrated that an irredundant feature combination with initial node features, filtered by graph neural network, improved its classification accuracy in some graph-based tasks.\n\nIn another project, I addressed the challenge of explainability and trustworthiness in temporal graph neural networks (TGNNs). I proposed a novel approach called Temporal Motifs Explainer (TempME) which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation.\n\nI have also worked on extending GCNs to hyperbolic geometry, which presents several unique challenges. I proposed Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs.\n\nAdditionally, I have worked on the problem of learning node embeddings that capture a node's position within the broader graph structure. I proposed Position-aware Graph Neural Networks (P-GNNs), a new class of GNNs for computing position-aware node embeddings. P-GNNs can capture positions/locations of nodes with respect to the anchor nodes, and have several advantages such as being inductive, scalable, and able to incorporate node feature information.\n\nI am also interested in the problem of encoding long sequences in Natural Language Processing (NLP) and proposed a graph-based method that alleviates long dependency issues. I evaluated the model on six datasets with up to 53k samples and 4034 average tokens' length and showed that it surpasses competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence dataset.",
    "collaborators": [
      "Jure Leskovec",
      "Jialin Chen",
      "William L. Hamilton",
      "Jiaqing Xie",
      "Ines Chami",
      "Christopher R\u00e9",
      "Jiaxuan You",
      "Irene Li",
      "Aosong Feng",
      "Dragomir Radev"
    ],
    "institute": null,
    "embed": null,
    "is_proj_leader_candidate": false,
    "is_proj_participant_candidate": false,
    "is_reviewer_candidate": true,
    "is_chair_candidate": false
  }
}
